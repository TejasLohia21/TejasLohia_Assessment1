Hash,Message,Filename,Source Code (prev),Source Code (current),Diff,LLM Inference,rectified message
b51eb87b4bce896550a647562a9526b34603eab9,"Added possibility to write MOT compliant results to .txt, small fix",track.py,"import argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\n\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom numpy import random\n\n# https://github.com/pytorch/pytorch/issues/3678\nimport sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.datasets import LoadStreams, LoadImages\nfrom yolov5.utils.general import (\n    check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, plot_one_box, strip_optimizer)\nfrom yolov5.utils.torch_utils import select_device, load_classifier, time_synchronized\n\nfrom deep_sort.utils.parser import get_config\nfrom deep_sort.deep_sort import DeepSort\n\n\npalette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n\ndef bbox_rel(image_width, image_height,  *xyxy):\n    """""""""""""""" Calculates the relative bounding box from absolute pixel values. """"""""""""\n    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n    bbox_top = min","import argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\n\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom numpy import random\n\n# https://github.com/pytorch/pytorch/issues/3678\nimport sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.datasets import LoadStreams, LoadImages\nfrom yolov5.utils.general import (\n    check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, plot_one_box, strip_optimizer)\nfrom yolov5.utils.torch_utils import select_device, load_classifier, time_synchronized\n\nfrom deep_sort.utils.parser import get_config\nfrom deep_sort.deep_sort import DeepSort\n\n\npalette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n\ndef bbox_rel(image_width, image_height,  *xyxy):\n    """""""""""""""" Calculates the relative bounding box from absolute pixel values. """"""""""""\n    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n    bbox_top = min","@@ -114,6 +114,10 @@ def detect(opt, save_img=False):\n     t0 = time.time()\n     img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n     _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n+\n+    save_path = str(Path(out))\n+    txt_path = str(Path(out)) + '/results.txt'\n+\n     for frame_idx, (path, img, im0s, vid_cap) in enumerate(dataset):\n         img = torch.from_numpy(img).to(device)\n         img = img.half() if half else img.float()  # uint8 to fp16/32\n@@ -136,8 +140,7 @@ def detect(opt, save_img=False):\n             else:\n                 p, s, im0 = path, '', im0s\n \n-            save_path = str(Path(out) / Path(p).name)\n-            txt_path = str(Path(out) / Path(p).stem) + ('_%g' % dataset.frame if dataset.mode == 'video' else '')\n+            \n             s += '%gx%g ' % img.shape[2:]  # print string\n             gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # Â normalization gain whwh\n             if det is not None and len(det):\n@@ -181,8 +184,7 @@ def detect(opt, save_img=False):\n                         bbox_w = output[2]\n                         bbox_h = output[3]\n                         identities = output[-1]\n-                        print(txt_path + '.txt')\n-                        with open(txt_path + '.txt', 'a') as f:\n+                        with open(txt_path, 'a') as f:\n                             f.write(('%g ' * 10 + '\n') % (frame_idx, identities, bbox_left, \\n                                 bbox_top, bbox_w, bbox_h, -1, -1, -1, -1))  # label format\n \n@@ -224,7 +226,7 @@ def detect(opt, save_img=False):\n \n if __name__ == '__main__':\n     parser = argparse.ArgumentParser()\n-    parser.add_argument('--weights', type=str, default='yolov5/weights/yolov5x.pt', help='model.pt path')\n+    parser.add_argument('--weights', type=str, default='yolov5/weights/yolov5m.pt', help='model.pt path')\n     parser.add_argument('--source', type=str, default='inference/images'",add,Added hypest who apparently actually wrote the C # port
21bc8b50d5727207b0fc2ca44f0cb2c98db9c542,Fixed video save bug,track.py,"from yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import (\n    check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, plot_one_box, strip_optimizer)\nfrom yolov5.utils.torch_utils import select_device, load_classifier, time_synchronized\nfrom deep_sort.utils.parser import get_config\nfrom deep_sort.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n# https://github.com/pytorch/pytorch/issues/3678\nimport sys\nsys.path.insert(0, './yolov5')\n\n\npalette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n\ndef bbox_rel(image_width, image_height,  *xyxy):\n    """""""""""""""" Calculates the relative bounding box from absolute pixel values. """"""""""""\n    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n    bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n    bbox_w = abs(xyxy[0].item() - xyxy[2].item())\n","from yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import (\n    check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, plot_one_box, strip_optimizer)\nfrom yolov5.utils.torch_utils import select_device, load_classifier, time_synchronized\nfrom deep_sort.utils.parser import get_config\nfrom deep_sort.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n# https://github.com/pytorch/pytorch/issues/3678\nimport sys\nsys.path.insert(0, './yolov5')\n\n\npalette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n\ndef bbox_rel(image_width, image_height,  *xyxy):\n    """""""""""""""" Calculates the relative bounding box from absolute pixel values. """"""""""""\n    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n    bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n    bbox_w = abs(xyxy[0].item() - xyxy[2].item())\n","@@ -50,11 +50,11 @@ def draw_boxes(img, bbox, identities=None, offset=(0,0)):\n         y1 += offset[1]\n         y2 += offset[1]\n         # box text and bar\n-        id = int(identities[i]) if identities is not None else 0    \n+        id = int(identities[i]) if identities is not None else 0   \n         color = compute_color_for_labels(id)\n         label = '{}{:d}'.format("""""""", id)\n-        t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 2 , 2)[0]\n-        cv2.rectangle(img, (x1, y1),(x2,y2), color, 3)\n+        t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 2, 2)[0]\n+        cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)\n         cv2.rectangle(img, (x1, y1), (x1 + t_size[0] + 3, y1 + t_size[1] + 4), color, -1)\n         cv2.putText(img, label, (x1, y1 + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 2, [255, 255, 255], 2)\n     return img\n@@ -68,10 +68,11 @@ def detect(opt, save_img=False):\n     # initialize deepsort\n     cfg = get_config()\n     cfg.merge_from_file(opt.config_deepsort)\n-    deepsort = DeepSort(cfg.DEEPSORT.REID_CKPT, \n-                        max_dist=cfg.DEEPSORT.MAX_DIST, min_confidence=cfg.DEEPSORT.MIN_CONFIDENCE, \n-                        nms_max_overlap=cfg.DEEPSORT.NMS_MAX_OVERLAP, max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE, \n-                        max_age=cfg.DEEPSORT.MAX_AGE, n_init=cfg.DEEPSORT.N_INIT, nn_budget=cfg.DEEPSORT.NN_BUDGET, use_cuda=True)\n+    deepsort = DeepSort(cfg.DEEPSORT.REID_CKPT,\n+                        max_dist=cfg.DEEPSORT.MAX_DIST, min_confidence=cfg.DEEPSORT.MIN_CONFIDENCE,\n+                        nms_max_overlap=cfg.DEEPSORT.NMS_MAX_OVERLAP, max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE,\n+                        max_age=cfg.DEEPSORT.MAX_AGE, n_init=cfg.DEEPSORT.N_INIT, nn_budget=cfg.DEEPSORT.NN_BUDGET,\n+                        use_cuda=True)\n \n     # Initialize\n     device = select_device(opt.device)\n@@ -81,10 +82,7 @@ def detect(opt, save_img=False):\n     half = devi",add,Added STORM - 236 to Changelog
5fe232162fbc7d96b892e594f192c21793183b45,increment ages when no detections fix,track.py,"import sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords\nfrom yolov5.utils.torch_utils import select_device, time_synchronized\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\n\npalette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n\ndef bbox_rel(*xyxy):\n    """""""""""""""" Calculates the relative bounding box from absolute pixel values. """"""""""""\n    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n    bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n    bbox_w = abs(xyxy[0].item() - xyxy[2].item())\n    bbox_h = abs(xyxy[1].item() - xyxy[3].item())\n    x_c = (bbox_left + bbox_w / 2)\n    y_c = (bbox_top + bbox_h / 2)\n    w = bbox_w\n    h","import sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords\nfrom yolov5.utils.torch_utils import select_device, time_synchronized\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\n\npalette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n\ndef bbox_rel(*xyxy):\n    """""""""""""""" Calculates the relative bounding box from absolute pixel values. """"""""""""\n    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n    bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n    bbox_w = abs(xyxy[0].item() - xyxy[2].item())\n    bbox_h = abs(xyxy[1].item() - xyxy[3].item())\n    x_c = (bbox_left + bbox_w / 2)\n    y_c = (bbox_top + bbox_h / 2)\n    w = bbox_w\n    h","@@ -140,10 +140,6 @@ def detect(opt, save_img=False):\n             s += '%gx%g ' % img.shape[2:]  # print string\n             save_path = str(Path(out) / Path(p).name)\n \n-            if det is None:\n-                print('increment ages!')\n-                deepsort.increment_ages()\n-\n             if det is not None and len(det):\n                 # Rescale boxes from img_size to im0 size\n                 det[:, :4] = scale_coords(\n@@ -188,6 +184,9 @@ def detect(opt, save_img=False):\n                             f.write(('%g ' * 10 + '\n') % (frame_idx, identity, bbox_left,\n                                                            bbox_top, bbox_w, bbox_h, -1, -1, -1, -1))  # label format\n \n+            else:\n+                deepsort.increment_ages()\n+\n             # Print time (inference + NMS)\n             print('%sDone. (%.3fs)' % (s, t2 - t1))\n \n",add,Fix typo
6599919ba5723a2a3e5586952fda8612be483ae6,bugfix: writing of MOT16 compliant results,track.py,"import sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, \\n    check_imshow\nfrom yolov5.utils.torch_utils import select_device, time_synchronized\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\n\npalette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n\ndef bbox_rel(*xyxy):\n    """""""""""""""" Calculates the relative bounding box from absolute pixel values. """"""""""""\n    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n    bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n    bbox_w = abs(xyxy[0].item() - xyxy[2].item())\n    bbox_h = abs(xyxy[1].item() - xyxy[3].item())\n    x_c = (bbox_le","import sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, \\n    check_imshow\nfrom yolov5.utils.torch_utils import select_device, time_synchronized\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\n\npalette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n\ndef xyxy_to_xywh(*xyxy):\n    """""""""""""""" Calculates the relative bounding box from absolute pixel values. """"""""""""\n    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n    bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n    bbox_w = abs(xyxy[0].item() - xyxy[2].item())\n    bbox_h = abs(xyxy[1].item() - xyxy[3].item())\n    x_c = (bbo","@@ -23,7 +23,7 @@ import torch.backends.cudnn as cudnn\n palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n \n \n-def bbox_rel(*xyxy):\n+def xyxy_to_xywh(*xyxy):\n     """""""""""""""" Calculates the relative bounding box from absolute pixel values. """"""""""""\n     bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n     bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n@@ -35,6 +35,18 @@ def bbox_rel(*xyxy):\n     h = bbox_h\n     return x_c, y_c, w, h\n \n+def xyxy_to_tlwh(bbox_xyxy):\n+    tlwh_bboxs = []\n+    for i, box in enumerate(bbox_xyxy):\n+        x1, y1, x2, y2 = [int(i) for i in box]\n+        top = x1\n+        left = y1\n+        w = int(x2 - x1)\n+        h = int(y2 - y1)\n+        tlwh_obj = [top, left, w, h]\n+        tlwh_bboxs.append(tlwh_obj)\n+    return tlwh_bboxs\n+\n \n def compute_color_for_labels(label):\n     """"""""""""\n@@ -153,20 +165,21 @@ def detect(opt):\n                     n = (det[:, -1] == c).sum()  # detections per class\n                     s += '%g %ss, ' % (n, names[int(c)])  # add to string\n \n-                bbox_xywh = []\n+                xywh_bboxs = []\n                 confs = []\n \n                 # Adapt detections to deep sort input format\n                 for *xyxy, conf, cls in det:\n-                    x_c, y_c, bbox_w, bbox_h = bbox_rel(*xyxy)\n-                    obj = [x_c, y_c, bbox_w, bbox_h]\n-                    bbox_xywh.append(obj)\n+                    # to deep sort format\n+                    x_c, y_c, bbox_w, bbox_h = xyxy_to_xywh(*xyxy)\n+                    xywh_obj = [x_c, y_c, bbox_w, bbox_h]\n+                    xywh_bboxs.append(xywh_obj)\n                     confs.append([conf.item()])\n \n-                xywhs = torch.Tensor(bbox_xywh)\n+                xywhs = torch.Tensor(xywh_bboxs)\n                 confss = torch.Tensor(confs)\n \n-                # Pass detections to deepsort\n+                # pass detections to deepsort\n                 outputs = deepsort.update(xywhs, confss, im0)\n",add,Added format51l
3b100c552af371b273b6679a72c63800f3afbf69,"fixed MOT complian result bug, switched bbox_left & bbox_top",track.py,"import sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, \\n    check_imshow\nfrom yolov5.utils.torch_utils import select_device, time_synchronized\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\n\npalette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n\ndef xyxy_to_xywh(*xyxy):\n    """""""""""""""" Calculates the relative bounding box from absolute pixel values. """"""""""""\n    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n    bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n    bbox_w = abs(xyxy[0].item() - xyxy[2].item())\n    bbox_h = abs(xyxy[1].item() - xyxy[3].item())\n    x_c = (bbo","import sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, \\n    check_imshow\nfrom yolov5.utils.torch_utils import select_device, time_synchronized\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\n\npalette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n\ndef xyxy_to_xywh(*xyxy):\n    """""""""""""""" Calculates the relative bounding box from absolute pixel values. """"""""""""\n    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n    bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n    bbox_w = abs(xyxy[0].item() - xyxy[2].item())\n    bbox_h = abs(xyxy[1].item() - xyxy[3].item())\n    x_c = (bbo","@@ -199,8 +199,8 @@ def detect(opt):\n                             bbox_h = tlwh_bbox[3]\n                             identity = output[-1]\n                             with open(txt_path, 'a') as f:\n-                                f.write(('%g ' * 10 + '\n') % (frame_idx, identity, bbox_left,\n-                                                            bbox_top, bbox_w, bbox_h, -1, -1, -1, -1))  # label format\n+                                f.write(('%g ' * 10 + '\n') % (frame_idx, identity, bbox_top,\n+                                                            bbox_left, bbox_w, bbox_h, -1, -1, -1, -1))  # label format\n \n             else:\n                 deepsort.increment_ages()\n@@ -242,7 +242,7 @@ def detect(opt):\n if __name__ == '__main__':\n     parser = argparse.ArgumentParser()\n     parser.add_argument('--weights', type=str,\n-                        default='yolov5/weights/yolov5s.pt', help='model.pt path')\n+                        default='yolov5/weights/crowdhuman_yolov5m.pt', help='model.pt path')\n     # file/folder, 0 for webcam\n     parser.add_argument('--source', type=str,\n                         default='inference/images', help='source')\n@@ -265,8 +265,7 @@ if __name__ == '__main__':\n     parser.add_argument('--save-txt', action='store_true',\n                         help='save results to *.txt')\n     # class 0 is person\n-    parser.add_argument('--classes', nargs='+', type=int,\n-                        default=[0], help='filter by class')\n+    parser.add_argument('--classes', nargs='+', default=[0], type=int, help='filter by class')\n     parser.add_argument('--agnostic-nms', action='store_true',\n                         help='class-agnostic NMS')\n     parser.add_argument('--augment', action='store_true',\n",add,Added STORM - 1273 to Changelog
6ffe4d26bb7191e0df4c24b0ae34c6af6dbce83f,delete default class filter value,track.py,"import sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.utils.google_utils import attempt_download\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, \\n    check_imshow\nfrom yolov5.utils.torch_utils import select_device, time_synchronized\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\n\npalette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n\ndef xyxy_to_xywh(*xyxy):\n    """""""""""""""" Calculates the relative bounding box from absolute pixel values. """"""""""""\n    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n    bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n    bbox_w = abs(xyxy[0].item() - xyxy[2].item())\n    bbox_","import sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.utils.google_utils import attempt_download\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, \\n    check_imshow\nfrom yolov5.utils.torch_utils import select_device, time_synchronized\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\n\npalette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n\ndef xyxy_to_xywh(*xyxy):\n    """""""""""""""" Calculates the relative bounding box from absolute pixel values. """"""""""""\n    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n    bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n    bbox_w = abs(xyxy[0].item() - xyxy[2].item())\n    bbox_","@@ -265,7 +265,7 @@ if __name__ == '__main__':\n     parser.add_argument('--save-vid', action='store_true', help='save video tracking results')\n     parser.add_argument('--save-txt', action='store_true', help='save MOT compliant results to *.txt')\n     # class 0 is person, 1 is bycicle, 2 is car... 79 is oven\n-    parser.add_argument('--classes', nargs='+', default=[0], type=int, help='filter by class')\n+    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 16 17')\n     parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n     parser.add_argument('--augment', action='store_true', help='augmented inference')\n     parser.add_argument('--evaluate', action='store_true', help='augmented inference')\n",add,Added STORM - 187 to Changelog
276a9d0d6eb8ab0f1efe18f581f20566cfd41d59,"Class and confidence display (#135)

* classes passed to tracker and class_id for each track returned

* class_id stored in track

* tracks initialized with their respective tracks

* class write to txt

* neater track logic

* var name fix

* thinner bboxes

Co-authored-by: mikel-brostrom <yolov5.deep.sort@gmail.com>",deep_sort_pytorch/deep_sort/deep_sort.py,"import numpy as np\nimport torch\n\nfrom .deep.feature_extractor import Extractor\nfrom .sort.nn_matching import NearestNeighborDistanceMetric\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\n\n\n__all__ = ['DeepSort']\n\n\nclass DeepSort(object):\n    def __init__(self, model_path, max_dist=0.2, min_confidence=0.3, nms_max_overlap=1.0, max_iou_distance=0.7, max_age=70, n_init=3, nn_budget=100, use_cuda=True):\n        self.min_confidence = min_confidence\n        self.nms_max_overlap = nms_max_overlap\n\n        self.extractor = Extractor(model_path, use_cuda=use_cuda)\n\n        max_cosine_distance = max_dist\n        metric = NearestNeighborDistanceMetric(\n            """"cosine"""", max_cosine_distance, nn_budget)\n        self.tracker = Tracker(\n            metric, max_iou_distance=max_iou_distance, max_age=max_age, n_init=n_init)\n\n    def update(self, bbox_xywh, confidences, ori_img):\n        self.height, self.width = ori_img.shape[:2]\n        # genera","import numpy as np\nimport torch\n\nfrom .deep.feature_extractor import Extractor\nfrom .sort.nn_matching import NearestNeighborDistanceMetric\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\n\n\n__all__ = ['DeepSort']\n\n\nclass DeepSort(object):\n    def __init__(self, model_path, max_dist=0.2, min_confidence=0.3, nms_max_overlap=1.0, max_iou_distance=0.7, max_age=70, n_init=3, nn_budget=100, use_cuda=True):\n        self.min_confidence = min_confidence\n        self.nms_max_overlap = nms_max_overlap\n\n        self.extractor = Extractor(model_path, use_cuda=use_cuda)\n\n        max_cosine_distance = max_dist\n        metric = NearestNeighborDistanceMetric(\n            """"cosine"""", max_cosine_distance, nn_budget)\n        self.tracker = Tracker(\n            metric, max_iou_distance=max_iou_distance, max_age=max_age, n_init=n_init)\n\n    def update(self, bbox_xywh, confidences, classes, ori_img):\n        self.height, self.width = ori_img.shape[:2]\n       ","@@ -23,7 +23,7 @@ class DeepSort(object):\n         self.tracker = Tracker(\n             metric, max_iou_distance=max_iou_distance, max_age=max_age, n_init=n_init)\n \n-    def update(self, bbox_xywh, confidences, ori_img):\n+    def update(self, bbox_xywh, confidences, classes, ori_img):\n         self.height, self.width = ori_img.shape[:2]\n         # generate detections\n         features = self._get_features(bbox_xywh, ori_img)\n@@ -37,7 +37,7 @@ class DeepSort(object):\n \n         # update tracker\n         self.tracker.predict()\n-        self.tracker.update(detections)\n+        self.tracker.update(detections, classes)\n \n         # output bbox identities\n         outputs = []\n@@ -47,7 +47,8 @@ class DeepSort(object):\n             box = track.to_tlwh()\n             x1, y1, x2, y2 = self._tlwh_to_xyxy(box)\n             track_id = track.track_id\n-            outputs.append(np.array([x1, y1, x2, y2, track_id], dtype=np.int))\n+            class_id = track.class_id\n+            outputs.append(np.array([x1, y1, x2, y2, track_id, class_id], dtype=np.int))\n         if len(outputs) > 0:\n             outputs = np.stack(outputs, axis=0)\n         return outputs\n",add,Added STORM - 370 to Changelog
276a9d0d6eb8ab0f1efe18f581f20566cfd41d59,"Class and confidence display (#135)

* classes passed to tracker and class_id for each track returned

* class_id stored in track

* tracks initialized with their respective tracks

* class write to txt

* neater track logic

* var name fix

* thinner bboxes

Co-authored-by: mikel-brostrom <yolov5.deep.sort@gmail.com>",deep_sort_pytorch/deep_sort/sort/track.py,"# vim: expandtab:ts=4:sw=4\n\n\nclass TrackState:\n    """"""""""""\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n\n    """"""""""""\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    """"""""""""\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initial state distribution.\n    track_id : int\n        A unique track identifier.\n    n_init : int\n        Number of consecutive detect","# vim: expandtab:ts=4:sw=4\n\n\nclass TrackState:\n    """"""""""""\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n\n    """"""""""""\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    """"""""""""\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initial state distribution.\n    track_id : int\n        A unique track identifier.\n    n_init : int\n        Number of consecutive detect","@@ -63,11 +63,12 @@ class Track:\n \n     """"""""""""\n \n-    def __init__(self, mean, covariance, track_id, n_init, max_age,\n+    def __init__(self, mean, covariance, track_id, class_id, n_init, max_age,\n                  feature=None):\n         self.mean = mean\n         self.covariance = covariance\n         self.track_id = track_id\n+        self.class_id = class_id\n         self.hits = 1\n         self.age = 1\n         self.time_since_update = 0\n",add,Added STORM - 132 to Changelog
276a9d0d6eb8ab0f1efe18f581f20566cfd41d59,"Class and confidence display (#135)

* classes passed to tracker and class_id for each track returned

* class_id stored in track

* tracks initialized with their respective tracks

* class write to txt

* neater track logic

* var name fix

* thinner bboxes

Co-authored-by: mikel-brostrom <yolov5.deep.sort@gmail.com>",deep_sort_pytorch/deep_sort/sort/tracker.py,"# vim: expandtab:ts=4:sw=4\nfrom __future__ import absolute_import\nimport numpy as np\nfrom . import kalman_filter\nfrom . import linear_assignment\nfrom . import iou_matching\nfrom .track import Track\n\n\nclass Tracker:\n    """"""""""""\n    This is the multi-target tracker.\n\n    Parameters\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        A distance metric for measurement-to-track association.\n    max_age : int\n        Maximum number of missed misses before a track is deleted.\n    n_init : int\n        Number of consecutive detections before the track is confirmed. The\n        track state is set to `Deleted` if a miss occurs within the first\n        `n_init` frames.\n\n    Attributes\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        The distance metric used for measurement to track association.\n    max_age : int\n        Maximum number of missed misses before a track is deleted.\n    n_init : int\n        Number of fr","# vim: expandtab:ts=4:sw=4\nfrom __future__ import absolute_import\nimport numpy as np\nfrom . import kalman_filter\nfrom . import linear_assignment\nfrom . import iou_matching\nfrom .track import Track\n\n\nclass Tracker:\n    """"""""""""\n    This is the multi-target tracker.\n\n    Parameters\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        A distance metric for measurement-to-track association.\n    max_age : int\n        Maximum number of missed misses before a track is deleted.\n    n_init : int\n        Number of consecutive detections before the track is confirmed. The\n        track state is set to `Deleted` if a miss occurs within the first\n        `n_init` frames.\n\n    Attributes\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        The distance metric used for measurement to track association.\n    max_age : int\n        Maximum number of missed misses before a track is deleted.\n    n_init : int\n        Number of fr","@@ -60,7 +60,7 @@ class Tracker:\n             track.increment_age()\n             track.mark_missed()\n \n-    def update(self, detections):\n+    def update(self, detections, classes):\n         """"""""""""Perform measurement update and track management.\n \n         Parameters\n@@ -80,7 +80,7 @@ class Tracker:\n         for track_idx in unmatched_tracks:\n             self.tracks[track_idx].mark_missed()\n         for detection_idx in unmatched_detections:\n-            self._initiate_track(detections[detection_idx])\n+            self._initiate_track(detections[detection_idx], classes[detection_idx].item())\n         self.tracks = [t for t in self.tracks if not t.is_deleted()]\n \n         # Update distance metric.\n@@ -135,9 +135,9 @@ class Tracker:\n         unmatched_tracks = list(set(unmatched_tracks_a + unmatched_tracks_b))\n         return matches, unmatched_tracks, unmatched_detections\n \n-    def _initiate_track(self, detection):\n+    def _initiate_track(self, detection, class_id):\n         mean, covariance = self.kf.initiate(detection.to_xyah())\n         self.tracks.append(Track(\n-            mean, covariance, self._next_id, self.n_init, self.max_age,\n+            mean, covariance, self._next_id, class_id, self.n_init, self.max_age,\n             detection.feature))\n         self._next_id += 1\n",add,Added the UNSTARTED state to the YouTube PlayerState enum
276a9d0d6eb8ab0f1efe18f581f20566cfd41d59,"Class and confidence display (#135)

* classes passed to tracker and class_id for each track returned

* class_id stored in track

* tracks initialized with their respective tracks

* class write to txt

* neater track logic

* var name fix

* thinner bboxes

Co-authored-by: mikel-brostrom <yolov5.deep.sort@gmail.com>",track.py,"import sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.utils.google_utils import attempt_download\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\nfrom yolov5.utils.torch_utils import select_device, time_synchronized\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\n\npalette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n\ndef xyxy_to_tlwh(bbox_xyxy):\n    tlwh_bboxs = []\n    for i, box in enumerate(bbox_xyxy):\n        x1, y1, x2, y2 = [int(i) for i in box]\n        top = x1\n        left = y1\n        w = int(x2 - x1)\n        h = int(y2 - y1)\n        tlwh_obj = [top, left, w, h]\n        tl","import sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.utils.google_utils import attempt_download\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\nfrom yolov5.utils.torch_utils import select_device, time_synchronized\nfrom yolov5.utils.plots import plot_one_box\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\ndef compute_color_for_id(label):\n    """"""""""""\n    Simple function that adds fixed color depending on the class\n    """"""""""""\n    palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n    color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]\n    return tuple(color)\n\n\ndef d","@@ -6,6 +6,7 @@ from yolov5.models.experimental import attempt_load\n from yolov5.utils.datasets import LoadImages, LoadStreams\n from yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\n from yolov5.utils.torch_utils import select_device, time_synchronized\n+from yolov5.utils.plots import plot_one_box\n from deep_sort_pytorch.utils.parser import get_config\n from deep_sort_pytorch.deep_sort import DeepSort\n import argparse\n@@ -19,51 +20,16 @@ import torch\n import torch.backends.cudnn as cudnn\n \n \n-\n-palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n-\n-\n-def xyxy_to_tlwh(bbox_xyxy):\n-    tlwh_bboxs = []\n-    for i, box in enumerate(bbox_xyxy):\n-        x1, y1, x2, y2 = [int(i) for i in box]\n-        top = x1\n-        left = y1\n-        w = int(x2 - x1)\n-        h = int(y2 - y1)\n-        tlwh_obj = [top, left, w, h]\n-        tlwh_bboxs.append(tlwh_obj)\n-    return tlwh_bboxs\n-\n-\n-def compute_color_for_labels(label):\n+def compute_color_for_id(label):\n     """"""""""""\n     Simple function that adds fixed color depending on the class\n     """"""""""""\n+    palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n+\n     color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]\n     return tuple(color)\n \n \n-def draw_boxes(img, bbox, identities=None, offset=(0, 0)):\n-    for i, box in enumerate(bbox):\n-        x1, y1, x2, y2 = [int(i) for i in box]\n-        x1 += offset[0]\n-        x2 += offset[0]\n-        y1 += offset[1]\n-        y2 += offset[1]\n-        # box text and bar\n-        id = int(identities[i]) if identities is not None else 0\n-        color = compute_color_for_labels(id)\n-        label = '{}{:d}'.format("""""""", id)\n-        t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 2, 2)[0]\n-        cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)\n-        cv2.rectangle(\n-            img, (x1, y1), (x1 + t_size[0] + 3, y1 + t_size[1] + 4), color, -1)\n-        cv2.putText(i",add,Added STORM - 146 to Changelog
9a0851e5acc8ea66896c3175ce042d90028c81de,fix bad initial kf predictions for new objects in the field of view,deep_sort_pytorch/deep_sort/deep_sort.py,"import numpy as np\nimport torch\n\nfrom .deep.feature_extractor import Extractor\nfrom .sort.nn_matching import NearestNeighborDistanceMetric\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\n\n\n__all__ = ['DeepSort']\n\n\nclass DeepSort(object):\n    def __init__(self, model_path, max_dist=0.2, min_confidence=0.3, max_iou_distance=0.7, max_age=70, n_init=3, nn_budget=100, use_cuda=True):\n        self.min_confidence = min_confidence\n\n        self.extractor = Extractor(model_path, use_cuda=use_cuda)\n\n        max_cosine_distance = max_dist\n        metric = NearestNeighborDistanceMetric(\n            """"cosine"""", max_cosine_distance, nn_budget)\n        self.tracker = Tracker(\n            metric, max_iou_distance=max_iou_distance, max_age=max_age, n_init=n_init)\n\n    def update(self, bbox_xywh, confidences, classes, ori_img, choose_bbox='yolo'):\n        self.height, self.width = ori_img.shape[:2]\n        # generate detections\n        features = self._","import numpy as np\nimport torch\n\nfrom .deep.feature_extractor import Extractor\nfrom .sort.nn_matching import NearestNeighborDistanceMetric\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\n\n\n__all__ = ['DeepSort']\n\n\nclass DeepSort(object):\n    def __init__(self, model_path, max_dist=0.2, min_confidence=0.3, max_iou_distance=0.7, max_age=70, n_init=3, nn_budget=100, use_cuda=True):\n        self.min_confidence = min_confidence\n\n        self.extractor = Extractor(model_path, use_cuda=use_cuda)\n\n        max_cosine_distance = max_dist\n        metric = NearestNeighborDistanceMetric(\n            """"cosine"""", max_cosine_distance, nn_budget)\n        self.tracker = Tracker(\n            metric, max_iou_distance=max_iou_distance, max_age=max_age, n_init=n_init)\n\n    def update(self, bbox_xywh, confidences, classes, ori_img):\n        self.height, self.width = ori_img.shape[:2]\n        # generate detections\n        features = self._get_features(bbox_xy","@@ -22,7 +22,7 @@ class DeepSort(object):\n         self.tracker = Tracker(\n             metric, max_iou_distance=max_iou_distance, max_age=max_age, n_init=n_init)\n \n-    def update(self, bbox_xywh, confidences, classes, ori_img, choose_bbox='yolo'):\n+    def update(self, bbox_xywh, confidences, classes, ori_img):\n         self.height, self.width = ori_img.shape[:2]\n         # generate detections\n         features = self._get_features(bbox_xywh, ori_img)\n@@ -43,10 +43,7 @@ class DeepSort(object):\n         for track in self.tracker.tracks:\n             if not track.is_confirmed() or track.time_since_update > 1:\n                 continue\n-            if choose_bbox == 'yolo':\n-                box = track.get_yolo_pred()\n-            elif choose_bbox == 'kf':\n-                box = track.to_tlwh()\n+            box = track.to_tlwh()\n             x1, y1, x2, y2 = self._tlwh_to_xyxy(box)\n             track_id = track.track_id\n             class_id = track.class_id\n",add,Add note about data volume to enable_metrics_collection
9a0851e5acc8ea66896c3175ce042d90028c81de,fix bad initial kf predictions for new objects in the field of view,deep_sort_pytorch/deep_sort/sort/kalman_filter.py,"# vim: expandtab:ts=4:sw=4\nimport numpy as np\nimport scipy.linalg\n\n\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919}\n\n\nclass KalmanFilter(object):\n    """"""""""""\n    A simple Kalman filter for tracking bounding boxes in image space.\n\n    The 8-dimensional state space\n\n        x, y, a, h, vx, vy, va, vh\n\n    contains the bounding box center position (x, y), aspect ratio a, height h,\n    and their respective velocities.\n\n    Object motion follows a constant velocity model. The bounding box location\n    (x, y, a, h) is taken as direct observation of the state space (linear\n    observation model).\n\n    """"""""""""\n\n    def __init__(self):\n      ","# vim: expandtab:ts=4:sw=4\nimport numpy as np\nimport scipy.linalg\n\n\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919}\n\n\nclass KalmanFilter(object):\n    """"""""""""\n    A simple Kalman filter for tracking bounding boxes in image space.\n\n    The 8-dimensional state space\n\n        x, y, a, h, vx, vy, va, vh\n\n    contains the bounding box center position (x, y), aspect ratio a, height h,\n    and their respective velocities.\n\n    Object motion follows a constant velocity model. The bounding box location\n    (x, y, a, h) is taken as direct observation of the state space (linear\n    observation model).\n\n    """"""""""""\n\n    def __init__(self):\n      ","@@ -74,13 +74,13 @@ class KalmanFilter(object):\n         mean = np.r_[mean_pos, mean_vel]\n \n         std = [\n-            2 * self._std_weight_position * measurement[3],\n-            2 * self._std_weight_position * measurement[3],\n-            1e-2,\n-            2 * self._std_weight_position * measurement[3],\n-            10 * self._std_weight_velocity * measurement[3],\n-            10 * self._std_weight_velocity * measurement[3],\n-            1e-5,\n+            2 * self._std_weight_position * measurement[0],   # the center point x\n+            2 * self._std_weight_position * measurement[1],   # the center point y\n+            1 * measurement[2],                               # the ratio of width/height\n+            2 * self._std_weight_position * measurement[3],   # the height\n+            10 * self._std_weight_velocity * measurement[0],\n+            10 * self._std_weight_velocity * measurement[1],\n+            0.1 * measurement[2],\n             10 * self._std_weight_velocity * measurement[3]]\n         covariance = np.diag(np.square(std))\n         return mean, covariance\n@@ -105,21 +105,20 @@ class KalmanFilter(object):\n \n         """"""""""""\n         std_pos = [\n-            self._std_weight_position * mean[3],\n-            self._std_weight_position * mean[3],\n-            1e-2,\n+            self._std_weight_position * mean[0],\n+            self._std_weight_position * mean[1],\n+            1 * mean[2],\n             self._std_weight_position * mean[3]]\n         std_vel = [\n-            self._std_weight_velocity * mean[3],\n-            self._std_weight_velocity * mean[3],\n-            1e-5,\n+            self._std_weight_velocity * mean[0],\n+            self._std_weight_velocity * mean[1],\n+            0.1 * mean[2],\n             self._std_weight_velocity * mean[3]]\n-        motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\n \n+        motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\n         mean = np.dot(self._mot",add,Add note about data volume to enable EGL
9a0851e5acc8ea66896c3175ce042d90028c81de,fix bad initial kf predictions for new objects in the field of view,track.py,"import sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator, colors\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\ndef detect(opt):\n    out, source, yolo_weights, deep_sort_weights, show_vid, save_vid, save_txt, imgsz, evaluate = \\n        opt.output, opt.source, opt.yolo_weights, opt.deep_sort_weights, opt.show_vid, opt.save_vid, \\n            opt.save_txt, opt.img_size, opt.evaluate\n    webcam = sourc","import sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator, colors\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\ndef detect(opt):\n    out, source, yolo_weights, deep_sort_weights, show_vid, save_vid, save_txt, imgsz, evaluate = \\n        opt.output, opt.source, opt.yolo_weights, opt.deep_sort_weights, opt.show_vid, opt.save_vid, \\n            opt.save_txt, opt.img_size, opt.evaluate\n    webcam = sourc","@@ -125,7 +125,7 @@ def detect(opt):\n                 clss = det[:, 5]\n \n                 # pass detections to deepsort\n-                outputs = deepsort.update(xywhs.cpu(), confs.cpu(), clss.cpu(), im0, 'kf')\n+                outputs = deepsort.update(xywhs.cpu(), confs.cpu(), clss.cpu(), im0)\n                 \n                 # draw boxes for visualization\n                 if len(outputs) > 0:\n",add,Added STORM - 370 to Changelog
4d338ae4bdd62e5fec0c3479c765a76e385972d8,Fix frame idx MOT txt export bug,track.py,"# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator, colors\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\n","# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator, colors\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\n","@@ -155,7 +155,7 @@ def detect(opt):\n                             bbox_h = output[3] - output[1]\n                             # Write MOT compliant results to file\n                             with open(txt_path, 'a') as f:\n-                               f.write(('%g ' * 10 + '\n') % (frame_idx, id, bbox_left,\n+                               f.write(('%g ' * 10 + '\n') % (frame_idx + 1, id, bbox_left,\n                                                            bbox_top, bbox_w, bbox_h, -1, -1, -1, -1))  # label format\n \n             else:\n",add,Add note about data volume to enable_metrics_collection
2e68fb2dc702d999a9c936d33a40e5e44ef6237f,fix dt bug,track.py,"# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator, colors\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\ni","# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import LOGGER, check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator, colors\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimpor","@@ -13,7 +13,7 @@ from yolov5.models.experimental import attempt_load\n from yolov5.utils.downloads import attempt_download\n from yolov5.models.common import DetectMultiBackend\n from yolov5.utils.datasets import LoadImages, LoadStreams\n-from yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\n+from yolov5.utils.general import LOGGER, check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\n from yolov5.utils.torch_utils import select_device, time_sync\n from yolov5.utils.plots import Annotator, colors\n from deep_sort_pytorch.utils.parser import get_config\n@@ -32,7 +32,7 @@ import torch.backends.cudnn as cudnn\n def detect(opt):\n     out, source, yolo_weights, deep_sort_weights, show_vid, save_vid, save_txt, imgsz, evaluate, half = \\n         opt.output, opt.source, opt.yolo_weights, opt.deep_sort_weights, opt.show_vid, opt.save_vid, \\n-            opt.save_txt, opt.img_size, opt.evaluate, opt.half\n+            opt.save_txt, opt.imgsz, opt.evaluate, opt.half\n     webcam = source == '0' or source.startswith(\n         'rtsp') or source.startswith('http') or source.endswith('.txt')\n \n@@ -60,7 +60,7 @@ def detect(opt):\n \n     # Load model\n     device = select_device(device)\n-    model = DetectMultiBackend(args.yolo_weights, device=device, dnn=args.dnn)\n+    model = DetectMultiBackend(opt.yolo_weights, device=device, dnn=opt.dnn)\n     stride, names, pt, jit, onnx = model.stride, model.names, model.pt, model.jit, model.onnx\n     imgsz = check_img_size(imgsz, s=stride)  # check image size\n \n@@ -94,6 +94,8 @@ def detect(opt):\n     txt_file_name = source.split('/')[-1].split('.')[0]\n     txt_path = str(Path(out)) + '/' + txt_file_name + '.txt'\n \n+    if pt and device.type != 'cpu':\n+        model(torch.zeros(1, 3, *imgsz).to(device).type_as(next(model.model.parameters())))  # warmup\n     dt, seen = [0.0, 0.0, 0.0], 0\n     for frame_idx, (path, img, im0s, vid_cap, s) in enumerat",add,Add note about data volume to enable
60f856a60097dd65bd50381572077462186800b2,fix https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/issues/224,track.py,"# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import LOGGER, check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator, colors\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimpor","# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import LOGGER, check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator, colors\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimpor","@@ -205,8 +205,7 @@ def detect(opt):\n     t = tuple(x / seen * 1E3 for x in dt)  # speeds per image\n     LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)\n     if save_txt or save_vid:\n-        s = f""""\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}"""" if save_txt else ''\n-        LOGGER.info(f""""Results saved to {colorstr('bold', save_dir)}{s}"""")\n+        print('Results saved to %s' % os.getcwd() + os.sep + out)\n         if platform == 'darwin':  # MacOS\n             os.system('open ' + save_path)\n \n",add,Add forced default for text type as oCC
d110f6db4d63f0d3549d7012303d0c5b45ff8494,s as default yolo model,track.py,"# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import LOGGER, check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator, colors\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimpor","# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import LOGGER, check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator, colors\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimpor","@@ -212,7 +212,7 @@ def detect(opt):\n \n if __name__ == '__main__':\n     parser = argparse.ArgumentParser()\n-    parser.add_argument('--yolo_weights', nargs='+', type=str, default='yolov5l.pt', help='model.pt path(s)')\n+    parser.add_argument('--yolo_weights', nargs='+', type=str, default='yolov5s.pt', help='model.pt path(s)')\n     parser.add_argument('--deep_sort_weights', type=str, default='deep_sort_pytorch/deep_sort/deep/checkpoint/ckpt.t7', help='ckpt.t7 path')\n     # file/folder, 0 for webcam\n     parser.add_argument('--source', type=str, default='0', help='source')\n",add,Added permission to def
0ab9f16b19d3fa1a973fa58298b60f276b650afe,fix logging for no detections,track.py,"# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import (LOGGER, check_img_size, non_max_suppression, scale_coords, \n                                  check_imshow, xyxy2xywh, increment_path)\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator","# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import (LOGGER, check_img_size, non_max_suppression, scale_coords, \n                                  check_imshow, xyxy2xywh, increment_path)\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator","@@ -185,11 +185,11 @@ def detect(opt):\n                                 f.write(('%g ' * 10 + '\n') % (frame_idx + 1, id, bbox_left,  # MOT format\n                                                                bbox_top, bbox_w, bbox_h, -1, -1, -1, -1))\n \n+                LOGGER.info(f'{s}Done. YOLO:({dt[1]:.3f}s), DeepSort:({t5 - t4:.3f}s)')\n+\n             else:\n                 deepsort.increment_ages()\n-\n-            # Print time (inference-only)\n-            LOGGER.info(f'{s}Done. YOLO:({t3 - t2:.3f}s), DeepSort:({t5 - t4:.3f}s)')\n+                LOGGER.info('No detections')\n \n             # Stream results\n             im0 = annotator.result()\n@@ -226,7 +226,7 @@ def detect(opt):\n \n if __name__ == '__main__':\n     parser = argparse.ArgumentParser()\n-    parser.add_argument('--yolo_model', nargs='+', type=str, default='yolov5x.pt', help='model.pt path(s)')\n+    parser.add_argument('--yolo_model', nargs='+', type=str, default='yolov5m.pt', help='model.pt path(s)')\n     parser.add_argument('--deep_sort_model', type=str, default='osnet_x0_25')\n     parser.add_argument('--source', type=str, default='0', help='source')  # file/folder, 0 for webcam\n     parser.add_argument('--output', type=str, default='inference/output', help='output folder')  # output folder\n",add,Added STORM - 1270 to Changelog
d3a3651f7855bb32b4d3520d6bc40b9af03f54aa,fix yolov5 timing,track.py,"# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import (LOGGER, check_img_size, non_max_suppression, scale_coords, \n                                  check_imshow, xyxy2xywh, increment_path)\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator","# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import (LOGGER, check_img_size, non_max_suppression, scale_coords, \n                                  check_imshow, xyxy2xywh, increment_path)\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator","@@ -185,7 +185,7 @@ def detect(opt):\n                                 f.write(('%g ' * 10 + '\n') % (frame_idx + 1, id, bbox_left,  # MOT format\n                                                                bbox_top, bbox_w, bbox_h, -1, -1, -1, -1))\n \n-                LOGGER.info(f'{s}Done. YOLO:({dt[1]:.3f}s), DeepSort:({t5 - t4:.3f}s)')\n+                LOGGER.info(f'{s}Done. YOLO:({t3 - t2:.3f}s), DeepSort:({t5 - t4:.3f}s)')\n \n             else:\n                 deepsort.increment_ages()\n",add,Don ' t define the dependency on real_commad twice
d2d5c59c1116b31fcbd907229da1c6c3746b51c6,fix feature map visualizations,track.py,"# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import (LOGGER, check_img_size, non_max_suppression, scale_coords, \n                                  check_imshow, xyxy2xywh, increment_path)\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator","# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import (LOGGER, check_img_size, non_max_suppression, scale_coords, \n                                  check_imshow, xyxy2xywh, increment_path)\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator","@@ -67,8 +67,13 @@ def detect(opt):\n         os.makedirs(out)  # make new output folder\n \n     # Directories\n-    exp_name = yolo_model.split(""""."""")[0] if type(yolo_model) is str else """"ensemble""""\n-    exp_name = exp_name + """"_"""" + deep_sort_model\n+    if type(yolo_model) is str:\n+        exp_name = yolo_model.split(""""."""")[0]\n+    elif type(yolo_model) is list and len(yolo_model) == 1:\n+        exp_name = yolo_model[0].split(""""."""")[0]\n+    else:\n+        exp_name = """"ensemble""""\n+    exp_name = exp_name + """"_"""" + deep_sort_model.split('/')[-1].split('.')[0]\n     save_dir = increment_path(Path(project) / exp_name, exist_ok=exist_ok)  # increment run if project name exists\n     save_dir.mkdir(parents=True, exist_ok=True)  # make dir\n \n@@ -121,7 +126,7 @@ def detect(opt):\n         dt[0] += t2 - t1\n \n         # Inference\n-        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if opt.visualize else False\n+        visualize = increment_path(save_dir / Path(path[0]).stem, mkdir=True) if opt.visualize else False\n         pred = model(img, augment=opt.augment, visualize=visualize)\n         t3 = time_sync()\n         dt[1] += t3 - t2\n",add,Add warning about data volume to enable_metrics_collection
701591a42963f45e42103588a6ecbc17ae684068,fixed multi-video tracking txt saving,track.py,"# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import (LOGGER, check_img_size, non_max_suppression, scale_coords, \n                                  check_imshow, xyxy2xywh, increment_path)\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator","# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import (LOGGER, check_img_size, non_max_suppression, scale_coords, \n                                  check_imshow, xyxy2xywh, increment_path)\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator","@@ -64,8 +64,9 @@ def detect(opt):\n     else:\n         exp_name = """"ensemble""""\n     exp_name = exp_name + """"_"""" + deep_sort_model.split('/')[-1].split('.')[0]\n+    print(exp_name)\n     save_dir = increment_path(Path(project) / exp_name, exist_ok=exist_ok)  # increment run if project name exists\n-    save_dir.mkdir(parents=True, exist_ok=True)  # make dir\n+    (save_dir / 'tracks' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n \n     # Load model\n     model = DetectMultiBackend(yolo_model, device=device, dnn=opt.dnn)\n@@ -92,25 +93,27 @@ def detect(opt):\n     else:\n         dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt and not jit)\n         nr_sources = 1\n-    vid_path, vid_writer = [None] * nr_sources, [None] * nr_sources\n-    \n+    vid_path, vid_writer, txt_path = [None] * nr_sources, [None] * nr_sources, [None] * nr_sources\n+\n     # initialize deepsort\n     cfg = get_config()\n     cfg.merge_from_file(opt.config_deepsort)\n-    \n+\n     # Create as many trackers as there are video sources\n     deepsort_list = []\n     for i in range(nr_sources):\n-        deepsort_list.append(DeepSort(deep_sort_model,\n-                                      device,\n-                                      max_dist=cfg.DEEPSORT.MAX_DIST,\n-                                      max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE,\n-                                      max_age=cfg.DEEPSORT.MAX_AGE, n_init=cfg.DEEPSORT.N_INIT, nn_budget=cfg.DEEPSORT.NN_BUDGET,\n-                                    )\n+        deepsort_list.append(\n+            DeepSort(\n+                deep_sort_model,\n+                device,\n+                max_dist=cfg.DEEPSORT.MAX_DIST,\n+                max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE,\n+                max_age=cfg.DEEPSORT.MAX_AGE, n_init=cfg.DEEPSORT.N_INIT, nn_budget=cfg.DEEPSORT.NN_BUDGET,\n+            )\n         )\n \n     outputs = [None] * nr_sources\n-                 ",add,Add note about data volume to enable_metrics_collection
c865655cfc8b50d3cc97b7b99c233abd264e333b,saving video and txt for multiple streams fixed,track.py,"# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import (LOGGER, check_img_size, non_max_suppression, scale_coords, \n                                  check_imshow, xyxy2xywh, increment_path)\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator","# limit the number of cpus used by high performance libraries\nimport os\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nsys.path.insert(0, './yolov5')\n\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import (LOGGER, check_img_size, non_max_suppression, scale_coords,\n                                  check_imshow, xyxy2xywh, increment_path, strip_optimizer, colorstr)\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.ut","@@ -23,8 +23,8 @@ from yolov5.models.experimental import attempt_load\n from yolov5.utils.downloads import attempt_download\n from yolov5.models.common import DetectMultiBackend\n from yolov5.utils.datasets import LoadImages, LoadStreams\n-from yolov5.utils.general import (LOGGER, check_img_size, non_max_suppression, scale_coords, \n-                                  check_imshow, xyxy2xywh, increment_path)\n+from yolov5.utils.general import (LOGGER, check_img_size, non_max_suppression, scale_coords,\n+                                  check_imshow, xyxy2xywh, increment_path, strip_optimizer, colorstr)\n from yolov5.utils.torch_utils import select_device, time_sync\n from yolov5.utils.plots import Annotator, colors\n from deep_sort.utils.parser import get_config\n@@ -38,9 +38,10 @@ ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n \n \n def detect(opt):\n-    out, source, yolo_model, deep_sort_model, show_vid, save_vid, save_txt, imgsz, evaluate, half, project, name, exist_ok= \\n+    out, source, yolo_model, deep_sort_model, show_vid, save_vid, save_txt, imgsz, evaluate, half, \\n+        project, exist_ok, update = \\n         opt.output, opt.source, opt.yolo_model, opt.deep_sort_model, opt.show_vid, opt.save_vid, \\n-        opt.save_txt, opt.imgsz, opt.evaluate, opt.half, opt.project, opt.name, opt.exist_ok\n+        opt.save_txt, opt.imgsz, opt.evaluate, opt.half, opt.project, opt.exist_ok, opt.update\n     webcam = source == '0' or source.startswith(\n         'rtsp') or source.startswith('http') or source.endswith('.txt')\n \n@@ -209,38 +210,40 @@ def detect(opt):\n                 deepsort_list[i].increment_ages()\n                 LOGGER.info('No detections')\n \n-            # Stream results\n+            # Stream resultsq\n             im0 = annotator.result()\n             if show_vid:\n-                cv2.namedWindow(str(p), cv2.WINDOW_NORMAL) \n                 cv2.imshow(str(p), im0)\n-                if cv2.waitKey(1) == ord('q'):  # q to",add,Added note about dates .
1b4b80eecfd4606b939e537c7f0e3f20091e6b6e,csine as distance metric as suggested here: https://github.com/KaiyangZhou/deep-person-reid/issues/502,deep_sort/deep_sort.py,"import numpy as np\nimport torch\nimport sys\nimport gdown\nfrom os.path import exists as file_exists, join\n\nfrom .sort.nn_matching import NearestNeighborDistanceMetric\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\nfrom .deep.reid_model_factory import show_downloadeable_models, get_model_link, is_model_in_factory, \\n    is_model_type_in_model_path, get_model_type, show_supported_models\n\nsys.path.append('deep_sort/deep/reid')\nfrom torchreid.utils import FeatureExtractor\nfrom torchreid.utils.tools import download_url\n\nshow_downloadeable_models()\n\n__all__ = ['DeepSort']\n\n\nclass DeepSort(object):\n    def __init__(self, model, device, max_dist=0.2, max_iou_distance=0.7, max_age=70, n_init=3, nn_budget=100):\n        # models trained on: market1501, dukemtmcreid and msmt17\n        if is_model_in_factory(model):\n            # download the model\n            model_path = join('deep_sort/deep/checkpoint', model + '.pth')\n            if not file_exi","import numpy as np\nimport torch\nimport sys\nimport gdown\nfrom os.path import exists as file_exists, join\n\nfrom .sort.nn_matching import NearestNeighborDistanceMetric\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\nfrom .deep.reid_model_factory import show_downloadeable_models, get_model_link, is_model_in_factory, \\n    is_model_type_in_model_path, get_model_type, show_supported_models\n\nsys.path.append('deep_sort/deep/reid')\nfrom torchreid.utils import FeatureExtractor\nfrom torchreid.utils.tools import download_url\n\nshow_downloadeable_models()\n\n__all__ = ['DeepSort']\n\n\nclass DeepSort(object):\n    def __init__(self, model, device, max_dist=0.2, max_iou_distance=0.7, max_age=70, n_init=3, nn_budget=100):\n        # models trained on: market1501, dukemtmcreid and msmt17\n        if is_model_in_factory(model):\n            # download the model\n            model_path = join('deep_sort/deep/checkpoint', model + '.pth')\n            if not file_exi","@@ -49,7 +49,7 @@ class DeepSort(object):\n \n         self.max_dist = max_dist\n         metric = NearestNeighborDistanceMetric(\n-            """"euclidean"""", self.max_dist, nn_budget)\n+            """"cosine"""", self.max_dist, nn_budget)\n         self.tracker = Tracker(\n             metric, max_iou_distance=max_iou_distance, max_age=max_age, n_init=n_init)\n \n",add,Add note about data volume to enable_metrics_collection
d3131074d8094a4b1ef5b7e2f495bb4964b2043b,check None with any,strong_sort/sort/kalman_filter.py,"# vim: expandtab:ts=4:sw=4\nimport numpy as np\nimport scipy.linalg\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919}\n\n\nclass KalmanFilter(object):\n    """"""""""""\n    A simple Kalman filter for tracking bounding boxes in image space.\n    The 8-dimensional state space\n        x, y, a, h, vx, vy, va, vh\n    contains the bounding box center position (x, y), aspect ratio a, height h,\n    and their respective velocities.\n    Object motion follows a constant velocity model. The bounding box location\n    (x, y, a, h) is taken as direct observation of the state space (linear\n    observation model).\n    """"""""""""\n\n    def __init__(self):\n        ndim, dt = 4","# vim: expandtab:ts=4:sw=4\nimport numpy as np\nimport scipy.linalg\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919}\n\n\nclass KalmanFilter(object):\n    """"""""""""\n    A simple Kalman filter for tracking bounding boxes in image space.\n    The 8-dimensional state space\n        x, y, a, h, vx, vy, va, vh\n    contains the bounding box center position (x, y), aspect ratio a, height h,\n    and their respective velocities.\n    Object motion follows a constant velocity model. The bounding box location\n    (x, y, a, h) is taken as direct observation of the state space (linear\n    observation model).\n    """"""""""""\n\n    def __init__(self):\n        ndim, dt = 4","@@ -126,9 +126,9 @@ class KalmanFilter(object):\n             estimate.\n         """"""""""""\n         std = [\n-            self._std_weight_position * mean[0],\n-            self._std_weight_position * mean[1],\n-            0.1 * mean[2],\n+            self._std_weight_position * mean[3],\n+            self._std_weight_position * mean[3],\n+            1e-1,\n             self._std_weight_position * mean[3]]\n \n \n",add,Add more keywords to the approve_regex
d3131074d8094a4b1ef5b7e2f495bb4964b2043b,check None with any,strong_sort/sort/track.py,"# vim: expandtab:ts=4:sw=4\nimport cv2\nimport numpy as np\nfrom strong_sort.sort.kalman_filter import KalmanFilter\n\n\nclass TrackState:\n    """"""""""""\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n\n    """"""""""""\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    """"""""""""\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initial state distribution.\n    track_id : int\n ","# vim: expandtab:ts=4:sw=4\nimport cv2\nimport numpy as np\nfrom strong_sort.sort.kalman_filter import KalmanFilter\n\n\nclass TrackState:\n    """"""""""""\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n\n    """"""""""""\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    """"""""""""\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initial state distribution.\n    track_id : int\n ","@@ -151,7 +151,7 @@ class Track:\n         """"""""""""\n         assert src.shape == dst.shape, """"the source image must be the same format to the target image!""""\n         \n-        if src or dst is None: \n+        if src.any() or dst.any() is None: \n             return None, None\n \n         # BGR2GRAY\n",add,Added example for MAP type in documentation
8cc9dbda7c1709b5890b6b2f392aeca51f84b8f6,fix exp folder path creation,track.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom yolov5.models.common import DetectMultiBacke","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom yolov5.models.common import DetectMultiBacke","@@ -84,7 +84,7 @@ def run(\n     if not isinstance(yolo_weights, list):  # single yolo model\n         exp_name = yolo_weights.stem\n     elif type(yolo_weights) is list and len(yolo_weights) == 1:  # single models after --yolo_weights\n-        exp_name = yolo_weights[0].stem\n+        exp_name = Path(yolo_weights[0]).stem\n     else:  # multiple models after --yolo_weights\n         exp_name = 'ensemble'\n     exp_name = name if name else exp_name + """"_"""" + strong_sort_weights.stem\n",add,Add more keywords to the approve_regex
b69afab421c91e89a2497b97220e549c1af0ee19,asert now part of condition for ECC after checking src and dst are not none,strong_sort/sort/track.py,"# vim: expandtab:ts=4:sw=4\nimport cv2\nimport numpy as np\nfrom strong_sort.sort.kalman_filter import KalmanFilter\n\n\nclass TrackState:\n    """"""""""""\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n\n    """"""""""""\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    """"""""""""\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initial state distribution.\n    track_id : int\n ","# vim: expandtab:ts=4:sw=4\nimport cv2\nimport numpy as np\nfrom strong_sort.sort.kalman_filter import KalmanFilter\n\n\nclass TrackState:\n    """"""""""""\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n\n    """"""""""""\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    """"""""""""\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initial state distribution.\n    track_id : int\n ","@@ -149,9 +149,12 @@ class Track:\n         src_aligned: ndarray\n             aligned source image of gray\n         """"""""""""\n-        assert src.shape == dst.shape, """"the source image must be the same format to the target image!""""\n-        \n-        if src.any() or dst.any() is None: \n+\n+        # skip if current and previous frame are not initialized (1st inference)\n+        if (src.any() or dst.any() is None):\n+            return None, None\n+        # skip if current and previous fames are not the same size\n+        elif (src.shape != dst.shape):\n             return None, None\n \n         # BGR2GRAY\n",add,Add note about data volume to enable_metrics_collection
d6fed1d7c3a907fcb9e0599a15dc5ef0a3d22265,fix multi backend normalization,strong_sort/reid_multibackend.py,"import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport torchvision.transforms as transforms\nimport cv2\nfrom os.path import exists as file_exists\nfrom deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n\nfrom torchreid.utils import FeatureExtractor\nfrom torchreid.utils.tools import download_url\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f""""{msg}{f} acceptable suffix is {suffix}""""\n\n\nclass ReIDDetectMultiBackend(nn.Module):\n    # ReID models MultiBackend class for python inference on various backends\n    def __init__(self, weights='osnet_x0_25_msmt17.pt', device=torch","import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport torchvision.transforms as transforms\nimport cv2\nfrom os.path import exists as file_exists\nfrom deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n\nfrom torchreid.utils import FeatureExtractor\nfrom torchreid.utils.tools import download_url\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f""""{msg}{f} acceptable suffix is {suffix}""""\n\n\nclass ReIDDetectMultiBackend(nn.Module):\n    # ReID models MultiBackend class for python inference on various backends\n    def __init__(self, weights='osnet_x0_25_msmt17.pt', device=torch","@@ -74,9 +74,11 @@ class ReIDDetectMultiBackend(nn.Module):\n             output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n             print(output_data.shape)\n             \n+        pixel_mean=[0.485, 0.456, 0.406]\n+        pixel_std=[0.229, 0.224, 0.225]\n         self.norm = transforms.Compose([\n             transforms.ToTensor(),\n-            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n+            transforms.Normalize(pixel_mean, pixel_std),\n         ])\n         self.size = (256, 128)\n         self.fp16 = fp16\n@@ -102,9 +104,10 @@ class ReIDDetectMultiBackend(nn.Module):\n         #     im = torch.zeros(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # input\n         #     for _ in range(2 if self.jit else 1):  #\n         #         self.forward(im)  # warmup\n+\n     def preprocessing(self, im_crops):\n         def _resize(im, size):\n-                return cv2.resize(im.astype(np.float32)/255., size)\n+            return cv2.resize(im.astype(np.float32), size)\n \n         im = torch.cat([self.norm(_resize(im, self.size)).unsqueeze(0) for im in im_crops], dim=0).float()\n         return im\n",add,Add forced default for newline
fcae8822dc3dc258c6ac3379a9b07b70c591cdd2,fix package imports,strong_sort/reid_multibackend.py,"import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport torchvision.transforms as transforms\nimport cv2\nfrom os.path import exists as file_exists\nfrom deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n\nfrom torchreid.utils import FeatureExtractor\nfrom torchreid.utils.tools import download_url\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f""""{msg}{f} acceptable suffix is {suffix}""""\n\n\nclass ReIDDetectMultiBackend(nn.Module):\n    # ReID models MultiBackend class for python inference on various backends\n    def __init__(self, weights='osnet_x0_25_msmt17.pt', device=torch","import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport torchvision.transforms as transforms\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists\nfrom .deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n\nfrom torchreid.utils import FeatureExtractor\nfrom torchreid.utils.tools import download_url\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f""""{msg}{f} acceptable suffix is {suffix}""""\n\n\nclass ReIDDetectMultiBackend(nn.Module):\n    # ReID models MultiBackend class for python inference on various backends\n    def __init__(self, weights='osnet_x0_25_msmt17.pt","@@ -4,8 +4,9 @@ from pathlib import Path\n import numpy as np\n import torchvision.transforms as transforms\n import cv2\n+import gdown\n from os.path import exists as file_exists\n-from deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n+from .deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n \n from torchreid.utils import FeatureExtractor\n from torchreid.utils.tools import download_url\n@@ -130,7 +131,6 @@ class ReIDDetectMultiBackend(nn.Module):\n                 im = im.cpu().numpy()  # FP32\n                 y = self.executable_network([im])[self.output_layer]\n             else:  # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)\n-                \n                 im = im.permute(0, 3, 2, 1).cpu().numpy()  # torch BCHW to numpy BHWC shape(1,320,192,3)\n                 input, output = self.input_details[0], self.output_details[0]\n                 int8 = input['dtype'] == np.uint8  # is TFLite quantized uint8 model\n",add,Added format51l
fcae8822dc3dc258c6ac3379a9b07b70c591cdd2,fix package imports,strong_sort/strong_sort.py,"import numpy as np\nimport torch\nimport sys\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists, join\nimport torchvision.transforms as transforms\n\nfrom .sort.nn_matching import NearestNeighborDistanceMetric\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\nfrom .deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n\nfrom torchreid.utils import FeatureExtractor\nfrom torchreid.utils.tools import download_url\nfrom reid_multibackend import ReIDDetectMultiBackend\n\nimport numpy as np\nimport tensorflow as tf\n\n__all__ = ['StrongSORT']\n\n\nclass StrongSORT(object):\n    def __init__(self, \n                 model_weights,\n                 device, max_dist=0.2,\n                 max_iou_distance=0.7,\n                 max_age=70, n_init=3,\n                 nn_budget=100,\n                 mc_lambda=0.995,\n                 ema_alpha=0.9\n                ):\n        \n        self.model = ReIDDetectMultiBa","import numpy as np\nimport torch\nimport sys\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists, join\nimport torchvision.transforms as transforms\n\nfrom .sort.nn_matching import NearestNeighborDistanceMetric\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\nfrom .deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n\nfrom torchreid.utils import FeatureExtractor\nfrom torchreid.utils.tools import download_url\nfrom .reid_multibackend import ReIDDetectMultiBackend\n\nimport numpy as np\nimport tensorflow as tf\n\n__all__ = ['StrongSORT']\n\n\nclass StrongSORT(object):\n    def __init__(self, \n                 model_weights,\n                 device, max_dist=0.2,\n                 max_iou_distance=0.7,\n                 max_age=70, n_init=3,\n                 nn_budget=100,\n                 mc_lambda=0.995,\n                 ema_alpha=0.9\n                ):\n        \n        self.model = ReIDDetectMultiB","@@ -13,7 +13,7 @@ from .deep.reid_model_factory import show_downloadeable_models, get_model_url, g\n \n from torchreid.utils import FeatureExtractor\n from torchreid.utils.tools import download_url\n-from reid_multibackend import ReIDDetectMultiBackend\n+from .reid_multibackend import ReIDDetectMultiBackend\n \n import numpy as np\n import tensorflow as tf\n",add,Added note about dates .
8e8cf6673f5b45369f0182ec62b9ba7785779597,reid warmup fix,strong_sort/reid_multibackend.py,"import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport torchvision.transforms as transforms\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists\nfrom .deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n\nfrom torchreid.utils import FeatureExtractor\nfrom torchreid.utils.tools import download_url\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f""""{msg}{f} acceptable suffix is {suffix}""""\n\n\nclass ReIDDetectMultiBackend(nn.Module):\n    # ReID models MultiBackend class for python inference on various backends\n    def __init__(self, weights='osnet_x0_25_msmt17.pt","import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport torchvision.transforms as transforms\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists\nfrom .deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n\nfrom torchreid.utils import FeatureExtractor\nfrom torchreid.utils.tools import download_url\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f""""{msg}{f} acceptable suffix is {suffix}""""\n\n\nclass ReIDDetectMultiBackend(nn.Module):\n    # ReID models MultiBackend class for python inference on various backends\n    def __init__(self, weights='osnet_x0_25_msmt17.pt","@@ -107,11 +107,13 @@ class ReIDDetectMultiBackend(nn.Module):\n         tflite &= not edgetpu  # *.tflite\n         return pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs\n     \n-    def warmup(self, imgsz=(1, 3, 256, 128)):\n+    def warmup(self, imgsz=(1, 256, 128, 3)):\n         # Warmup model by running inference once\n         warmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb\n         if any(warmup_types) and self.device.type != 'cpu':\n             im = torch.zeros(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # input\n+            im = im.cpu().numpy()\n+            print(im.shape)\n             for _ in range(2 if self.jit else 1):  #\n                 self.forward(im)  # warmup\n \n@@ -136,8 +138,6 @@ class ReIDDetectMultiBackend(nn.Module):\n             elif self.jit:  # TorchScript\n                 y = self.model(im)[0]\n             elif self.onnx:  # ONNX Runtime\n-                print(type(im))\n-                print(im.shape)\n                 im = im.permute(0, 1, 3, 2).cpu().numpy()  # torch to numpy  # torch to numpy\n                 y = self.session.run([self.session.get_outputs()[0].name], {self.session.get_inputs()[0].name: im})[0]\n             elif self.xml:  # OpenVINO\n",add,Add note about data volume to enable EGL
24d185ab735a9c4f64fd77d88bed5995d5c51a39,fix fp16 for reid models,strong_sort/reid_multibackend.py,"import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport torchvision.transforms as transforms\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists\nfrom .deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n\nfrom torchreid.utils import FeatureExtractor\nfrom torchreid.utils.tools import download_url\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f""""{msg}{f} acceptable suffix is {suffix}""""\n\n\nclass ReIDDetectMultiBackend(nn.Module):\n    # ReID models MultiBackend class for python inference on various backends\n    def __init__(self, weights='osnet_x0_25_msmt17.pt","import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport torchvision.transforms as transforms\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists\nfrom .deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n\nfrom torchreid.utils import FeatureExtractor\nfrom torchreid.utils.tools import download_url\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f""""{msg}{f} acceptable suffix is {suffix}""""\n\n\nclass ReIDDetectMultiBackend(nn.Module):\n    # ReID models MultiBackend class for python inference on various backends\n    def __init__(self, weights='osnet_x0_25_msmt17.pt","@@ -30,7 +30,8 @@ class ReIDDetectMultiBackend(nn.Module):\n         w = str(weights[0] if isinstance(weights, list) else weights)\n         self.pt, self.jit, self.onnx, self.xml, self.engine, self.coreml, \\n             self.saved_model, self.pb, self.tflite, self.edgetpu, self.tfjs = self.model_type(w)  # get backend\n-        \n+        fp16 &= (self.pt or self.jit or self.onnx or self.engine) and device.type != 'cpu'  # FP16\n+        self.fp16 = fp16\n         if self.pt:  # PyTorch\n             model_name = get_model_name(weights)\n             model_url = get_model_url(weights)\n@@ -50,6 +51,7 @@ class ReIDDetectMultiBackend(nn.Module):\n                 model_path=weights,\n                 device=str(device)\n             )\n+            \n             self.extractor.model.half() if fp16 else  self.extractor.model.float()\n         elif self.onnx:  # ONNX Runtime\n             # LOGGER.info(f'Loading {w} for ONNX Runtime inference...')\n@@ -91,7 +93,6 @@ class ReIDDetectMultiBackend(nn.Module):\n             transforms.Normalize(pixel_mean, pixel_std),\n         ])\n         self.size = (256, 128)\n-        self.fp16 = fp16\n         self.device = device\n         \n         \n",add,Add note about data volume to enable_metrics_collection
9dcc39a0cbb8b93ccbfd1f63fd9543df4403ae19,fix in tflite export,reid_export.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport subprocess\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom yolov5.models.common impo","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom yolo","@@ -12,6 +12,7 @@ import sys\n import numpy as np\n from pathlib import Path\n import torch\n+import pandas as pd\n import subprocess\n import torch.backends.cudnn as cudnn\n \n@@ -48,6 +49,17 @@ def file_size(path):\n         return 0.0\n \n \n+def export_formats():\n+    # YOLOv5 export formats\n+    x = [\n+        ['PyTorch', '-', '.pt', True, True],\n+        ['ONNX', 'onnx', '.onnx', True, True],\n+        ['OpenVINO', 'openvino', '_openvino_model', True, False],\n+        ['TensorFlow Lite', 'tflite', '.tflite', True, False],\n+    ]\n+    return pd.DataFrame(x, columns=['Format', 'Argument', 'Suffix', 'CPU', 'GPU'])\n+\n+\n def export_onnx(model, im, file, opset, train=False, dynamic=True, simplify=False):\n     # ONNX export\n     try:\n@@ -138,9 +150,9 @@ def export_tflite(file, half, prefix=colorstr('TFLite:')):\n         import openvino.inference_engine as ie\n         LOGGER.info(f'\n{prefix} starting export with openvino {ie.__version__}...')\n         output = Path(str(file).replace(f'_openvino_model{os.sep}', f'_tflite_model{os.sep}'))\n-        f = (Path(str(file).replace(f'_openvino_model{os.sep}', f'_tflite_model{os.sep}')).parent).joinpath(list(Path(file).glob('*.xml'))[0])\n+        modelxml = list(Path(file).glob('*.xml'))[0]\n         cmd = f""""openvino2tensorflow \\n-            --model_path {f} \\n+            --model_path {modelxml} \\n             --model_output_path {output} \\n             --output_pb \\n             --output_saved_model \\n@@ -167,7 +179,7 @@ if __name__ == """"__main__"""":\n         """"-p"""",\n         """"--weights"""",\n         type=Path,\n-        default=""""./weight/osnet_x0_25_msmt17.pt"""",\n+        default=""""./weight/mobilenetv2_x1_0_msmt17.pt"""",\n         help=""""Path to weights"""",\n     )\n     parser.add_argument(\n@@ -183,6 +195,10 @@ if __name__ == """"__main__"""":\n         default=[256, 128],\n         help='image (h, w)'\n     )\n+    parser.add_argument('--include',\n+                        nargs='+',\n+             ",add,Added note about dates .
5b2f1aa4b19d7978f0bb0ef009e9d970eca899d8,only onnx export as default,reid_export.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = P","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = P","@@ -327,7 +327,7 @@ if __name__ == """"__main__"""":\n     parser.add_argument('--half', action='store_true', help='FP16 half-precision export')\n     parser.add_argument('--include',\n                         nargs='+',\n-                        default=['onnx', 'engine'],\n+                        default=['onnx'],\n                         help='torchscript, onnx, openvino, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs')\n     args = parser.parse_args()\n \n",add,Add note about data volume to enable_metrics_collection
752c3a5f4072d458f32bb6ab4ebc1dd42c8498cd,fix opt bug,track.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom yolov5.models.common import DetectMultiBacke","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom yolov5.models.common import DetectMultiBacke","@@ -110,7 +110,7 @@ def run(\n \n     # initialize StrongSORT\n     cfg = get_config()\n-    cfg.merge_from_file(opt.config_strongsort)\n+    cfg.merge_from_file(config_strongsort)\n \n     # Create as many strong sort instances as there are video sources\n     strongsort_list = []\n",add,Add note about data volume to enable_metrics_collection
39317437bae64848591f978ebab62d483c483451,fix strong-sort weigths bug,strong_sort/reid_multibackend.py,"import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport torchvision.transforms as transforms\nimport cv2\nfrom collections import OrderedDict, namedtuple\nimport gdown\nfrom os.path import exists as file_exists\nfrom .deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n\nfrom yolov5.utils.general import LOGGER, check_version\nfrom torchreid.utils import FeatureExtractor\nfrom torchreid.utils.tools import download_url\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f""""{msg}{f} acceptable suffix is {suffix}""""\n\n\nclass ReIDDetectMultiBackend(nn.Module):\n    # ReID models MultiBack","import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport torchvision.transforms as transforms\nimport cv2\nfrom collections import OrderedDict, namedtuple\nimport gdown\nfrom os.path import exists as file_exists\nfrom .deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n\nfrom yolov5.utils.general import LOGGER, check_version\nfrom torchreid.utils import FeatureExtractor\nfrom torchreid.utils.tools import download_url\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f""""{msg}{f} acceptable suffix is {suffix}""""\n\n\nclass ReIDDetectMultiBackend(nn.Module):\n    # ReID models MultiBack","@@ -29,9 +29,9 @@ class ReIDDetectMultiBackend(nn.Module):\n     # ReID models MultiBackend class for python inference on various backends\n     def __init__(self, weights='osnet_x0_25_msmt17.pt', device=torch.device('cpu'), fp16=False):\n         super().__init__()\n-        w = str(weights[0] if isinstance(weights, list) else weights)\n+        weights = weights[0] if isinstance(weights, list) else weights\n         self.pt, self.jit, self.onnx, self.xml, self.engine, self.coreml, \\n-            self.saved_model, self.pb, self.tflite, self.edgetpu, self.tfjs = self.model_type(w)  # get backend\n+            self.saved_model, self.pb, self.tflite, self.edgetpu, self.tfjs = self.model_type(weights)  # get backend\n         fp16 &= (self.pt or self.jit or self.onnx or self.engine) and device.type != 'cpu'  # FP16\n         self.fp16 = fp16\n         if self.pt:  # PyTorch\n@@ -43,7 +43,7 @@ class ReIDDetectMultiBackend(nn.Module):\n             elif file_exists(weights):\n                 pass\n             elif model_url is None:\n-                print('No URL associated to the chosen DeepSort weights. Choose between:')\n+                print(f'No URL associated to the chosen DeepSort weights ({weights}). Choose between:')\n                 show_downloadeable_models()\n                 exit()\n \n",add,Added Type name for DFI ( # 3 )
ad93a50e4f44a9477d62649235bbc3c04586935e,fix MOT17 bug,val.py,"import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom track import run\n\n\ndef setup_evaluation(dst_val_tools_folder, benchmark):\n    \n    # source: https://github.com/JonathonLuiten/TrackEval#official-evaluation-code\","import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom track import run\n\n\ndef setup_evaluation(dst_val_tools_folder, benchmark):\n    \n    # source: https://github.com/JonathonLuiten/TrackEval#official-evaluation-code\","@@ -94,7 +94,7 @@ def main(opt):\n         # overwrite MOT17 evaluation sequences to evaluate so that they are not duplicated\n         with open(dst_val_tools_folder / """"data/gt/mot_challenge/seqmaps/MOT17-train.txt"""", """"w"""") as f:\n             for line in seq_paths:\n-                f.write(str(line))\n+                f.write(str(line.parent.stem) + '\n')\n     else:\n         # this is not the case for MOT16 and MOT20\n         seq_paths = [p / 'img1' for p in Path(mot_seqs_path).iterdir() if Path(p).is_dir()]\n",update,Fix warning
5773f2a219eb6bc964b22732e2156f1e25f70252,fix switching classes in ocsort,trackers/ocsort/ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom __future__ import print_function\n\nimport numpy as np\nfrom .association import *\nfrom yolov5.utils.general import xywh2xyxy\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age-dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w/2.\n    y = bbox[1] + h/2.\n    s = w * h  # scale is just area\n    r = w / float(h+1e-6)\n    return np.array([x, y, s, r]).re","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom __future__ import print_function\n\nimport numpy as np\nfrom .association import *\nfrom yolov5.utils.general import xywh2xyxy\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age-dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w/2.\n    y = bbox[1] + h/2.\n    s = w * h  # scale is just area\n    r = w / float(h+1e-6)\n    return np.array([x, y, s, r]).re","@@ -61,7 +61,7 @@ class KalmanBoxTracker(object):\n     """"""""""""\n     count = 0\n \n-    def __init__(self, bbox, delta_t=3, orig=False):\n+    def __init__(self, bbox, cls, delta_t=3, orig=False):\n         """"""""""""\n         Initialises a tracker using initial bounding box.\n \n@@ -93,7 +93,7 @@ class KalmanBoxTracker(object):\n         self.hit_streak = 0\n         self.age = 0\n         #self.conf = conf\n-        self.cls = 0\n+        self.cls = cls\n         """"""""""""\n         NOTE: [-1,-1,-1,-1,-1] is a compromising placeholder for non-observation status, the same for the return of \n         function k_previous_obs. It is ugly and I do not like it. But to support generate observation array in a \n@@ -105,11 +105,13 @@ class KalmanBoxTracker(object):\n         self.velocity = None\n         self.delta_t = delta_t\n \n-    def update(self, bbox):\n+    def update(self, bbox, cls):\n         """"""""""""\n         Updates the state vector with observed bbox.\n         """"""""""""\n+        \n         if bbox is not None:\n+            self.cls = cls\n             if self.last_observation.sum() >= 0:  # no previous observation\n                 previous_box = None\n                 for i in range(self.delta_t):\n@@ -209,7 +211,7 @@ class OCSort(object):\n         bboxes = xywh2xyxy(xywhs)\n         confs = confs.numpy()\n \n-        output_results = np.column_stack((bboxes, confs))\n+        output_results = np.column_stack((bboxes, confs, classes))\n         \n         inds_low = confs > 0.1\n         inds_high = confs < self.det_thresh\n@@ -243,7 +245,7 @@ class OCSort(object):\n         matched, unmatched_dets, unmatched_trks = associate(\n             dets, trks, self.iou_threshold, velocities, k_observations, self.inertia)\n         for m in matched:\n-            self.trackers[m[1]].update(dets[m[0], :])\n+            self.trackers[m[1]].update(dets[m[0], :5], dets[m[0], 5])\n \n         """"""""""""\n             Second round of associaton by OCR\n@@ -265,7 +267,7 @@ class OC",add,Added STORM - 1270 to Changelog
b03b25c70a2d12311ca38c7b79ea6f3fd822369f,fix import issues,track.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom yolov5.models.common import DetectMultiBacke","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strong_sort'))  # add strong_sort ROOT to PATH\nif str(ROOT / 'trackers' / 'ocsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'o","@@ -22,8 +22,12 @@ if str(ROOT) not in sys.path:\n     sys.path.append(str(ROOT))  # add ROOT to PATH\n if str(ROOT / 'yolov5') not in sys.path:\n     sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\n-if str(ROOT / 'strong_sort') not in sys.path:\n-    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\n+if str(ROOT / 'trackers' / 'strong_sort') not in sys.path:\n+    sys.path.append(str(ROOT / 'trackers' / 'strong_sort'))  # add strong_sort ROOT to PATH\n+if str(ROOT / 'trackers' / 'ocsort') not in sys.path:\n+    sys.path.append(str(ROOT / 'trackers' / 'ocsort'))  # add strong_sort ROOT to PATH\n+if str(ROOT / 'trackers' / 'strong_sort' / 'deep' / 'reid' / 'torchreid') not in sys.path:\n+    sys.path.append(str(ROOT / 'trackers' / 'strong_sort' / 'deep' / 'reid' / 'torchreid'))  # add strong_sort ROOT to PATH\n ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n \n import logging\n",add,Added the user group to the contributors
b03b25c70a2d12311ca38c7b79ea6f3fd822369f,fix import issues,trackers/strong_sort/reid_multibackend.py,"import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nfrom itertools import islice\nimport torchvision.transforms as transforms\nimport cv2\nimport torchvision.transforms as T\nfrom collections import OrderedDict, namedtuple\nimport gdown\nfrom os.path import exists as file_exists\nfrom .deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name, download_url, load_pretrained_weights\n\nfrom yolov5.utils.general import LOGGER, check_version, check_requirements\n\nfrom trackers.strong_sort.deep.reid.torchreid.models import build_model\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f""""{msg}{f} ","import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nfrom itertools import islice\nimport torchvision.transforms as transforms\nimport cv2\nimport sys\nimport torchvision.transforms as T\nfrom collections import OrderedDict, namedtuple\nimport gdown\nfrom os.path import exists as file_exists\n\nfrom yolov5.utils.general import LOGGER, check_version, check_requirements\nfrom deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name, download_url, load_pretrained_weights\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f""""{msg}{f} acceptable suffix is {suffix}""""\n\n\nclass ReIDDetectMultiBacken","@@ -5,15 +5,14 @@ import numpy as np\n from itertools import islice\n import torchvision.transforms as transforms\n import cv2\n+import sys\n import torchvision.transforms as T\n from collections import OrderedDict, namedtuple\n import gdown\n from os.path import exists as file_exists\n-from .deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name, download_url, load_pretrained_weights\n \n from yolov5.utils.general import LOGGER, check_version, check_requirements\n-\n-from trackers.strong_sort.deep.reid.torchreid.models import build_model\n+from deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name, download_url, load_pretrained_weights\n \n \n def check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n@@ -31,6 +30,7 @@ class ReIDDetectMultiBackend(nn.Module):\n     # ReID models MultiBackend class for python inference on various backends\n     def __init__(self, weights='osnet_x0_25_msmt17.pt', device=torch.device('cpu'), fp16=False):\n         super().__init__()\n+        from models import build_model\n         w = weights[0] if isinstance(weights, list) else weights\n         self.pt, self.jit, self.onnx, self.xml, self.engine, self.coreml, \\n             self.saved_model, self.pb, self.tflite, self.edgetpu, self.tfjs = self.model_type(w)  # get backend\n",add,Added note about dates .
b03b25c70a2d12311ca38c7b79ea6f3fd822369f,fix import issues,trackers/strong_sort/sort/nn_matching.py,"# vim: expandtab:ts=4:sw=4\nimport numpy as np\nimport sys\nimport torch\n#sys.path.append('trackers/strong_sort/deep/reid/torchreid')\nfrom ..deep.reid.torchreid.metrics.distance import compute_distance_matrix\n\n\ndef _pdist(a, b):\n    """"""""""""Compute pair-wise squared distance between points in `a` and `b`.\n    Parameters\n    ----------\n    a : array_like\n        An NxM matrix of N samples of dimensionality M.\n    b : array_like\n        An LxM matrix of L samples of dimensionality M.\n    Returns\n    -------\n    ndarray\n        Returns a matrix of size len(a), len(b) such that eleement (i, j)\n        contains the squared distance between `a[i]` and `b[j]`.\n    """"""""""""\n    a, b = np.asarray(a), np.asarray(b)\n    if len(a) == 0 or len(b) == 0:\n        return np.zeros((len(a), len(b)))\n    a2, b2 = np.square(a).sum(axis=1), np.square(b).sum(axis=1)\n    r2 = -2. * np.dot(a, b.T) + a2[:, None] + b2[None, :]\n    r2 = np.clip(r2, 0., float(np.inf))\n    return r2\n\n\ndef _c","# vim: expandtab:ts=4:sw=4\nimport numpy as np\nimport sys\nimport torch\n\n\ndef _pdist(a, b):\n    """"""""""""Compute pair-wise squared distance between points in `a` and `b`.\n    Parameters\n    ----------\n    a : array_like\n        An NxM matrix of N samples of dimensionality M.\n    b : array_like\n        An LxM matrix of L samples of dimensionality M.\n    Returns\n    -------\n    ndarray\n        Returns a matrix of size len(a), len(b) such that eleement (i, j)\n        contains the squared distance between `a[i]` and `b[j]`.\n    """"""""""""\n    a, b = np.asarray(a), np.asarray(b)\n    if len(a) == 0 or len(b) == 0:\n        return np.zeros((len(a), len(b)))\n    a2, b2 = np.square(a).sum(axis=1), np.square(b).sum(axis=1)\n    r2 = -2. * np.dot(a, b.T) + a2[:, None] + b2[None, :]\n    r2 = np.clip(r2, 0., float(np.inf))\n    return r2\n\n\ndef _cosine_distance(a, b, data_is_normalized=False):\n    """"""""""""Compute pair-wise cosine distance between points in `a` and `b`.\n    Parameter","@@ -2,8 +2,6 @@\n import numpy as np\n import sys\n import torch\n-#sys.path.append('trackers/strong_sort/deep/reid/torchreid')\n-from ..deep.reid.torchreid.metrics.distance import compute_distance_matrix\n \n \n def _pdist(a, b):\n@@ -66,9 +64,9 @@ def _nn_euclidean_distance(x, y):\n         A vector of length M that contains for each entry in `y` the\n         smallest Euclidean distance to a sample in `x`.\n     """"""""""""\n-    x_ = torch.from_numpy(np.asarray(x) / np.linalg.norm(x, axis=1, keepdims=True))\n-    y_ = torch.from_numpy(np.asarray(y) / np.linalg.norm(y, axis=1, keepdims=True))\n-    distances = compute_distance_matrix(x_, y_, metric='euclidean')\n+    # x_ = torch.from_numpy(np.asarray(x) / np.linalg.norm(x, axis=1, keepdims=True))\n+    # y_ = torch.from_numpy(np.asarray(y) / np.linalg.norm(y, axis=1, keepdims=True))\n+    distances = distances = _pdist(x, y)\n     return np.maximum(0.0, torch.min(distances, axis=0)[0].numpy())\n \n \n",add,Add wizSymbolicLookup annotation
b03b25c70a2d12311ca38c7b79ea6f3fd822369f,fix import issues,trackers/strong_sort/strong_sort.py,"import numpy as np\nimport torch\nimport sys\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists, join\nimport torchvision.transforms as transforms\n\nfrom .sort.nn_matching import NearestNeighborDistanceMetric\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\n\nfrom .deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\nfrom .reid_multibackend import ReIDDetectMultiBackend\n\n__all__ = ['StrongSORT']\n\n\nclass StrongSORT(object):\n    def __init__(self, \n                 model_weights,\n                 device,\n                 fp16,\n                 max_dist=0.2,\n                 max_iou_distance=0.7,\n                 max_age=70, n_init=3,\n                 nn_budget=100,\n                 mc_lambda=0.995,\n                 ema_alpha=0.9\n                ):\n\n        self.model = ReIDDetectMultiBackend(weights=model_weights, device=device, fp16=fp16)\n        \n        self.max_dist = max_dist\n      ","import numpy as np\nimport torch\nimport sys\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists, join\nimport torchvision.transforms as transforms\n\nfrom sort.nn_matching import NearestNeighborDistanceMetric\nfrom sort.detection import Detection\nfrom sort.tracker import Tracker\n\nfrom reid_multibackend import ReIDDetectMultiBackend\n\n\nclass StrongSORT(object):\n    def __init__(self, \n                 model_weights,\n                 device,\n                 fp16,\n                 max_dist=0.2,\n                 max_iou_distance=0.7,\n                 max_age=70, n_init=3,\n                 nn_budget=100,\n                 mc_lambda=0.995,\n                 ema_alpha=0.9\n                ):\n\n        self.model = ReIDDetectMultiBackend(weights=model_weights, device=device, fp16=fp16)\n        \n        self.max_dist = max_dist\n        metric = NearestNeighborDistanceMetric(\n            """"cosine"""", self.max_dist, nn_budget)\n        self.tracker = Tracker(\n","@@ -6,14 +6,11 @@ import gdown\n from os.path import exists as file_exists, join\n import torchvision.transforms as transforms\n \n-from .sort.nn_matching import NearestNeighborDistanceMetric\n-from .sort.detection import Detection\n-from .sort.tracker import Tracker\n+from sort.nn_matching import NearestNeighborDistanceMetric\n+from sort.detection import Detection\n+from sort.tracker import Tracker\n \n-from .deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name\n-from .reid_multibackend import ReIDDetectMultiBackend\n-\n-__all__ = ['StrongSORT']\n+from reid_multibackend import ReIDDetectMultiBackend\n \n \n class StrongSORT(object):\n",add,Added example for MAP type in documentation
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/multi_tracker_zoo.py,"from .strong_sort.utils.parser import get_config\nfrom .strong_sort.strong_sort import StrongSORT\nfrom .ocsort.ocsort import OCSort\n\n\ndef create_tracker(tracker_type, appearance_descriptor_weights, device, half):\n    if tracker_type == 'strongsort':\n        # initialize StrongSORT\n        cfg = get_config()\n        cfg.merge_from_file('trackers/strong_sort/configs/strong_sort.yaml')\n\n        strongsort = StrongSORT(\n            appearance_descriptor_weights,\n            device,\n            half,\n            max_dist=cfg.STRONGSORT.MAX_DIST,\n            max_iou_distance=cfg.STRONGSORT.MAX_IOU_DISTANCE,\n            max_age=cfg.STRONGSORT.MAX_AGE,\n            n_init=cfg.STRONGSORT.N_INIT,\n            nn_budget=cfg.STRONGSORT.NN_BUDGET,\n            mc_lambda=cfg.STRONGSORT.MC_LAMBDA,\n            ema_alpha=cfg.STRONGSORT.EMA_ALPHA,\n\n        )\n        return strongsort\n    elif tracker_type == 'ocsort':\n        ocsort = OCSort(\n            det_thresh=0.45,\n        ","from trackers.strong_sort.utils.parser import get_config\nfrom trackers.strong_sort.strong_sort import StrongSORT\nfrom trackers.ocsort.ocsort import OCSort\n\n\ndef create_tracker(tracker_type, appearance_descriptor_weights, device, half):\n    if tracker_type == 'strongsort':\n        # initialize StrongSORT\n        cfg = get_config()\n        cfg.merge_from_file('trackers/strong_sort/configs/strong_sort.yaml')\n\n        strongsort = StrongSORT(\n            appearance_descriptor_weights,\n            device,\n            half,\n            max_dist=cfg.STRONGSORT.MAX_DIST,\n            max_iou_distance=cfg.STRONGSORT.MAX_IOU_DISTANCE,\n            max_age=cfg.STRONGSORT.MAX_AGE,\n            n_init=cfg.STRONGSORT.N_INIT,\n            nn_budget=cfg.STRONGSORT.NN_BUDGET,\n            mc_lambda=cfg.STRONGSORT.MC_LAMBDA,\n            ema_alpha=cfg.STRONGSORT.EMA_ALPHA,\n\n        )\n        return strongsort\n    elif tracker_type == 'ocsort':\n        ocsort = OCSort(\n            de","@@ -1,6 +1,6 @@\n-from .strong_sort.utils.parser import get_config\n-from .strong_sort.strong_sort import StrongSORT\n-from .ocsort.ocsort import OCSort\n+from trackers.strong_sort.utils.parser import get_config\n+from trackers.strong_sort.strong_sort import StrongSORT\n+from trackers.ocsort.ocsort import OCSort\n \n \n def create_tracker(tracker_type, appearance_descriptor_weights, device, half):\n",add,Added Type name for DFI ( # 323 )
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/__init__.py,,"from __future__ import absolute_import\nimport torch\n\nfrom .pcb import *\nfrom .mlfn import *\nfrom .hacnn import *\nfrom .osnet import *\nfrom .senet import *\nfrom .mudeep import *\nfrom .nasnet import *\nfrom .resnet import *\nfrom .densenet import *\nfrom .xception import *\nfrom .osnet_ain import *\nfrom .resnetmid import *\nfrom .shufflenet import *\nfrom .squeezenet import *\nfrom .inceptionv4 import *\nfrom .mobilenetv2 import *\nfrom .resnet_ibn_a import *\nfrom .resnet_ibn_b import *\nfrom .shufflenetv2 import *\nfrom .inceptionresnetv2 import *\n\n__model_factory = {\n    # image classification models\n    'resnet18': resnet18,\n    'resnet34': resnet34,\n    'resnet50': resnet50,\n    'resnet101': resnet101,\n    'resnet152': resnet152,\n    'resnext50_32x4d': resnext50_32x4d,\n    'resnext101_32x8d': resnext101_32x8d,\n    'resnet50_fc512': resnet50_fc512,\n    'se_resnet50': se_resnet50,\n    'se_resnet50_fc512': se_resnet50_fc512,\n    'se_resnet101': se_resnet101,\n  ","@@ -0,0 +1,122 @@\n+from __future__ import absolute_import\n+import torch\n+\n+from .pcb import *\n+from .mlfn import *\n+from .hacnn import *\n+from .osnet import *\n+from .senet import *\n+from .mudeep import *\n+from .nasnet import *\n+from .resnet import *\n+from .densenet import *\n+from .xception import *\n+from .osnet_ain import *\n+from .resnetmid import *\n+from .shufflenet import *\n+from .squeezenet import *\n+from .inceptionv4 import *\n+from .mobilenetv2 import *\n+from .resnet_ibn_a import *\n+from .resnet_ibn_b import *\n+from .shufflenetv2 import *\n+from .inceptionresnetv2 import *\n+\n+__model_factory = {\n+    # image classification models\n+    'resnet18': resnet18,\n+    'resnet34': resnet34,\n+    'resnet50': resnet50,\n+    'resnet101': resnet101,\n+    'resnet152': resnet152,\n+    'resnext50_32x4d': resnext50_32x4d,\n+    'resnext101_32x8d': resnext101_32x8d,\n+    'resnet50_fc512': resnet50_fc512,\n+    'se_resnet50': se_resnet50,\n+    'se_resnet50_fc512': se_resnet50_fc512,\n+    'se_resnet101': se_resnet101,\n+    'se_resnext50_32x4d': se_resnext50_32x4d,\n+    'se_resnext101_32x4d': se_resnext101_32x4d,\n+    'densenet121': densenet121,\n+    'densenet169': densenet169,\n+    'densenet201': densenet201,\n+    'densenet161': densenet161,\n+    'densenet121_fc512': densenet121_fc512,\n+    'inceptionresnetv2': inceptionresnetv2,\n+    'inceptionv4': inceptionv4,\n+    'xception': xception,\n+    'resnet50_ibn_a': resnet50_ibn_a,\n+    'resnet50_ibn_b': resnet50_ibn_b,\n+    # lightweight models\n+    'nasnsetmobile': nasnetamobile,\n+    'mobilenetv2_x1_0': mobilenetv2_x1_0,\n+    'mobilenetv2_x1_4': mobilenetv2_x1_4,\n+    'shufflenet': shufflenet,\n+    'squeezenet1_0': squeezenet1_0,\n+    'squeezenet1_0_fc512': squeezenet1_0_fc512,\n+    'squeezenet1_1': squeezenet1_1,\n+    'shufflenet_v2_x0_5': shufflenet_v2_x0_5,\n+    'shufflenet_v2_x1_0': shufflenet_v2_x1_0,\n+    'shufflenet_v2_x1_5': shufflenet_v2_x1_5,\n+    'shufflenet_v2_x2_",add,Added TSV test file for
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/densenet.py,,"""""""""""""\nCode source: https://github.com/pytorch/vision\n""""""""""""\nfrom __future__ import division, absolute_import\nimport re\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils import model_zoo\n\n__all__ = [\n    'densenet121', 'densenet169', 'densenet201', 'densenet161',\n    'densenet121_fc512'\n]\n\nmodel_urls = {\n    'densenet121':\n    'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n    'densenet169':\n    'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n    'densenet201':\n    'https://download.pytorch.org/models/densenet201-c1103571.pth',\n    'densenet161':\n    'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n}\n\n\nclass _DenseLayer(nn.Sequential):\n\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n        ","@@ -0,0 +1,380 @@\n+""""""""""""\n+Code source: https://github.com/pytorch/vision\n+""""""""""""\n+from __future__ import division, absolute_import\n+import re\n+from collections import OrderedDict\n+import torch\n+import torch.nn as nn\n+from torch.nn import functional as F\n+from torch.utils import model_zoo\n+\n+__all__ = [\n+    'densenet121', 'densenet169', 'densenet201', 'densenet161',\n+    'densenet121_fc512'\n+]\n+\n+model_urls = {\n+    'densenet121':\n+    'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n+    'densenet169':\n+    'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n+    'densenet201':\n+    'https://download.pytorch.org/models/densenet201-c1103571.pth',\n+    'densenet161':\n+    'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n+}\n+\n+\n+class _DenseLayer(nn.Sequential):\n+\n+    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n+        super(_DenseLayer, self).__init__()\n+        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n+        self.add_module('relu1', nn.ReLU(inplace=True)),\n+        self.add_module(\n+            'conv1',\n+            nn.Conv2d(\n+                num_input_features,\n+                bn_size * growth_rate,\n+                kernel_size=1,\n+                stride=1,\n+                bias=False\n+            )\n+        ),\n+        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n+        self.add_module('relu2', nn.ReLU(inplace=True)),\n+        self.add_module(\n+            'conv2',\n+            nn.Conv2d(\n+                bn_size * growth_rate,\n+                growth_rate,\n+                kernel_size=3,\n+                stride=1,\n+                padding=1,\n+                bias=False\n+            )\n+        ),\n+        self.drop_rate = drop_rate\n+\n+    def forward(self, x):\n+        new_features = super(_DenseLayer, self).forward(x)\n+        if self.drop_rate > 0:\n+            new_features = F.dro",add,Added the user group to the contributors
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/hacnn.py,,"from __future__ import division, absolute_import\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = ['HACNN']\n\n\nclass ConvBlock(nn.Module):\n    """"""""""""Basic convolutional block.\n    \n    convolution + batch normalization + relu.\n\n    Args:\n        in_c (int): number of input channels.\n        out_c (int): number of output channels.\n        k (int or tuple): kernel size.\n        s (int or tuple): stride.\n        p (int or tuple): padding.\n    """"""""""""\n\n    def __init__(self, in_c, out_c, k, s=1, p=0):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Conv2d(in_c, out_c, k, stride=s, padding=p)\n        self.bn = nn.BatchNorm2d(out_c)\n\n    def forward(self, x):\n        return F.relu(self.bn(self.conv(x)))\n\n\nclass InceptionA(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super(InceptionA, self).__init__()\n        mid_channels = out_channels // 4\n\n        self.stream1 = nn.Sequential(\n  ","@@ -0,0 +1,414 @@\n+from __future__ import division, absolute_import\n+import torch\n+from torch import nn\n+from torch.nn import functional as F\n+\n+__all__ = ['HACNN']\n+\n+\n+class ConvBlock(nn.Module):\n+    """"""""""""Basic convolutional block.\n+    \n+    convolution + batch normalization + relu.\n+\n+    Args:\n+        in_c (int): number of input channels.\n+        out_c (int): number of output channels.\n+        k (int or tuple): kernel size.\n+        s (int or tuple): stride.\n+        p (int or tuple): padding.\n+    """"""""""""\n+\n+    def __init__(self, in_c, out_c, k, s=1, p=0):\n+        super(ConvBlock, self).__init__()\n+        self.conv = nn.Conv2d(in_c, out_c, k, stride=s, padding=p)\n+        self.bn = nn.BatchNorm2d(out_c)\n+\n+    def forward(self, x):\n+        return F.relu(self.bn(self.conv(x)))\n+\n+\n+class InceptionA(nn.Module):\n+\n+    def __init__(self, in_channels, out_channels):\n+        super(InceptionA, self).__init__()\n+        mid_channels = out_channels // 4\n+\n+        self.stream1 = nn.Sequential(\n+            ConvBlock(in_channels, mid_channels, 1),\n+            ConvBlock(mid_channels, mid_channels, 3, p=1),\n+        )\n+        self.stream2 = nn.Sequential(\n+            ConvBlock(in_channels, mid_channels, 1),\n+            ConvBlock(mid_channels, mid_channels, 3, p=1),\n+        )\n+        self.stream3 = nn.Sequential(\n+            ConvBlock(in_channels, mid_channels, 1),\n+            ConvBlock(mid_channels, mid_channels, 3, p=1),\n+        )\n+        self.stream4 = nn.Sequential(\n+            nn.AvgPool2d(3, stride=1, padding=1),\n+            ConvBlock(in_channels, mid_channels, 1),\n+        )\n+\n+    def forward(self, x):\n+        s1 = self.stream1(x)\n+        s2 = self.stream2(x)\n+        s3 = self.stream3(x)\n+        s4 = self.stream4(x)\n+        y = torch.cat([s1, s2, s3, s4], dim=1)\n+        return y\n+\n+\n+class InceptionB(nn.Module):\n+\n+    def __init__(self, in_channels, out_channels):\n+      ",add,Add forced default for text type as oCC
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/inceptionresnetv2.py,,"""""""""""""\nCode imported from https://github.com/Cadene/pretrained-models.pytorch\n""""""""""""\nfrom __future__ import division, absolute_import\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = ['inceptionresnetv2']\n\npretrained_settings = {\n    'inceptionresnetv2': {\n        'imagenet': {\n            'url':\n            'http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 299, 299],\n            'input_range': [0, 1],\n            'mean': [0.5, 0.5, 0.5],\n            'std': [0.5, 0.5, 0.5],\n            'num_classes': 1000\n        },\n        'imagenet+background': {\n            'url':\n            'http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 299, 299],\n            'input_range': [0, 1],\n            'mean': [0.5, 0.5, 0.5],\n            'std': [0.5, 0.5,","@@ -0,0 +1,361 @@\n+""""""""""""\n+Code imported from https://github.com/Cadene/pretrained-models.pytorch\n+""""""""""""\n+from __future__ import division, absolute_import\n+import torch\n+import torch.nn as nn\n+import torch.utils.model_zoo as model_zoo\n+\n+__all__ = ['inceptionresnetv2']\n+\n+pretrained_settings = {\n+    'inceptionresnetv2': {\n+        'imagenet': {\n+            'url':\n+            'http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth',\n+            'input_space': 'RGB',\n+            'input_size': [3, 299, 299],\n+            'input_range': [0, 1],\n+            'mean': [0.5, 0.5, 0.5],\n+            'std': [0.5, 0.5, 0.5],\n+            'num_classes': 1000\n+        },\n+        'imagenet+background': {\n+            'url':\n+            'http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth',\n+            'input_space': 'RGB',\n+            'input_size': [3, 299, 299],\n+            'input_range': [0, 1],\n+            'mean': [0.5, 0.5, 0.5],\n+            'std': [0.5, 0.5, 0.5],\n+            'num_classes': 1001\n+        }\n+    }\n+}\n+\n+\n+class BasicConv2d(nn.Module):\n+\n+    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n+        super(BasicConv2d, self).__init__()\n+        self.conv = nn.Conv2d(\n+            in_planes,\n+            out_planes,\n+            kernel_size=kernel_size,\n+            stride=stride,\n+            padding=padding,\n+            bias=False\n+        ) # verify bias false\n+        self.bn = nn.BatchNorm2d(\n+            out_planes,\n+            eps=0.001, # value found in tensorflow\n+            momentum=0.1, # default pytorch value\n+            affine=True\n+        )\n+        self.relu = nn.ReLU(inplace=False)\n+\n+    def forward(self, x):\n+        x = self.conv(x)\n+        x = self.bn(x)\n+        x = self.relu(x)\n+        return x\n+\n+\n+class Mixed_5b(nn.Module):\n+\n+    def __init__(self):\n+        super(Mixed_5b, self).__i",add,Add note about data volume to enable EGL
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/inceptionv4.py,,"from __future__ import division, absolute_import\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = ['inceptionv4']\n""""""""""""\nCode imported from https://github.com/Cadene/pretrained-models.pytorch\n""""""""""""\n\npretrained_settings = {\n    'inceptionv4': {\n        'imagenet': {\n            'url':\n            'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 299, 299],\n            'input_range': [0, 1],\n            'mean': [0.5, 0.5, 0.5],\n            'std': [0.5, 0.5, 0.5],\n            'num_classes': 1000\n        },\n        'imagenet+background': {\n            'url':\n            'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 299, 299],\n            'input_range': [0, 1],\n            'mean': [0.5, 0.5, 0.5],\n            'std': [0.5, 0.5, 0.5],\n            'num","@@ -0,0 +1,381 @@\n+from __future__ import division, absolute_import\n+import torch\n+import torch.nn as nn\n+import torch.utils.model_zoo as model_zoo\n+\n+__all__ = ['inceptionv4']\n+""""""""""""\n+Code imported from https://github.com/Cadene/pretrained-models.pytorch\n+""""""""""""\n+\n+pretrained_settings = {\n+    'inceptionv4': {\n+        'imagenet': {\n+            'url':\n+            'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth',\n+            'input_space': 'RGB',\n+            'input_size': [3, 299, 299],\n+            'input_range': [0, 1],\n+            'mean': [0.5, 0.5, 0.5],\n+            'std': [0.5, 0.5, 0.5],\n+            'num_classes': 1000\n+        },\n+        'imagenet+background': {\n+            'url':\n+            'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth',\n+            'input_space': 'RGB',\n+            'input_size': [3, 299, 299],\n+            'input_range': [0, 1],\n+            'mean': [0.5, 0.5, 0.5],\n+            'std': [0.5, 0.5, 0.5],\n+            'num_classes': 1001\n+        }\n+    }\n+}\n+\n+\n+class BasicConv2d(nn.Module):\n+\n+    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n+        super(BasicConv2d, self).__init__()\n+        self.conv = nn.Conv2d(\n+            in_planes,\n+            out_planes,\n+            kernel_size=kernel_size,\n+            stride=stride,\n+            padding=padding,\n+            bias=False\n+        ) # verify bias false\n+        self.bn = nn.BatchNorm2d(\n+            out_planes,\n+            eps=0.001, # value found in tensorflow\n+            momentum=0.1, # default pytorch value\n+            affine=True\n+        )\n+        self.relu = nn.ReLU(inplace=True)\n+\n+    def forward(self, x):\n+        x = self.conv(x)\n+        x = self.bn(x)\n+        x = self.relu(x)\n+        return x\n+\n+\n+class Mixed_3a(nn.Module):\n+\n+    def __init__(self):\n+        super(Mixed_3a, self).__init__()\n+        self.ma",add,Add note about data volume to enable EGL
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/mlfn.py,,"from __future__ import division, absolute_import\nimport torch\nimport torch.utils.model_zoo as model_zoo\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = ['mlfn']\n\nmodel_urls = {\n    # training epoch = 5, top1 = 51.6\n    'imagenet':\n    'https://mega.nz/#!YHxAhaxC!yu9E6zWl0x5zscSouTdbZu8gdFFytDdl-RAdD2DEfpk',\n}\n\n\nclass MLFNBlock(nn.Module):\n\n    def __init__(\n        self, in_channels, out_channels, stride, fsm_channels, groups=32\n    ):\n        super(MLFNBlock, self).__init__()\n        self.groups = groups\n        mid_channels = out_channels // 2\n\n        # Factor Modules\n        self.fm_conv1 = nn.Conv2d(in_channels, mid_channels, 1, bias=False)\n        self.fm_bn1 = nn.BatchNorm2d(mid_channels)\n        self.fm_conv2 = nn.Conv2d(\n            mid_channels,\n            mid_channels,\n            3,\n            stride=stride,\n            padding=1,\n            bias=False,\n            groups=self.groups\n        )\n        self.fm_bn2 =","@@ -0,0 +1,269 @@\n+from __future__ import division, absolute_import\n+import torch\n+import torch.utils.model_zoo as model_zoo\n+from torch import nn\n+from torch.nn import functional as F\n+\n+__all__ = ['mlfn']\n+\n+model_urls = {\n+    # training epoch = 5, top1 = 51.6\n+    'imagenet':\n+    'https://mega.nz/#!YHxAhaxC!yu9E6zWl0x5zscSouTdbZu8gdFFytDdl-RAdD2DEfpk',\n+}\n+\n+\n+class MLFNBlock(nn.Module):\n+\n+    def __init__(\n+        self, in_channels, out_channels, stride, fsm_channels, groups=32\n+    ):\n+        super(MLFNBlock, self).__init__()\n+        self.groups = groups\n+        mid_channels = out_channels // 2\n+\n+        # Factor Modules\n+        self.fm_conv1 = nn.Conv2d(in_channels, mid_channels, 1, bias=False)\n+        self.fm_bn1 = nn.BatchNorm2d(mid_channels)\n+        self.fm_conv2 = nn.Conv2d(\n+            mid_channels,\n+            mid_channels,\n+            3,\n+            stride=stride,\n+            padding=1,\n+            bias=False,\n+            groups=self.groups\n+        )\n+        self.fm_bn2 = nn.BatchNorm2d(mid_channels)\n+        self.fm_conv3 = nn.Conv2d(mid_channels, out_channels, 1, bias=False)\n+        self.fm_bn3 = nn.BatchNorm2d(out_channels)\n+\n+        # Factor Selection Module\n+        self.fsm = nn.Sequential(\n+            nn.AdaptiveAvgPool2d(1),\n+            nn.Conv2d(in_channels, fsm_channels[0], 1),\n+            nn.BatchNorm2d(fsm_channels[0]),\n+            nn.ReLU(inplace=True),\n+            nn.Conv2d(fsm_channels[0], fsm_channels[1], 1),\n+            nn.BatchNorm2d(fsm_channels[1]),\n+            nn.ReLU(inplace=True),\n+            nn.Conv2d(fsm_channels[1], self.groups, 1),\n+            nn.BatchNorm2d(self.groups),\n+            nn.Sigmoid(),\n+        )\n+\n+        self.downsample = None\n+        if in_channels != out_channels or stride > 1:\n+            self.downsample = nn.Sequential(\n+                nn.Conv2d(\n+                    in_channels, out_channels, 1, stride=stride, bias",add,Added note about dates .
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/mobilenetv2.py,,"from __future__ import division, absolute_import\nimport torch.utils.model_zoo as model_zoo\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = ['mobilenetv2_x1_0', 'mobilenetv2_x1_4']\n\nmodel_urls = {\n    # 1.0: top-1 71.3\n    'mobilenetv2_x1_0':\n    'https://mega.nz/#!NKp2wAIA!1NH1pbNzY_M2hVk_hdsxNM1NUOWvvGPHhaNr-fASF6c',\n    # 1.4: top-1 73.9\n    'mobilenetv2_x1_4':\n    'https://mega.nz/#!RGhgEIwS!xN2s2ZdyqI6vQ3EwgmRXLEW3khr9tpXg96G9SUJugGk',\n}\n\n\nclass ConvBlock(nn.Module):\n    """"""""""""Basic convolutional block.\n    \n    convolution (bias discarded) + batch normalization + relu6.\n\n    Args:\n        in_c (int): number of input channels.\n        out_c (int): number of output channels.\n        k (int or tuple): kernel size.\n        s (int or tuple): stride.\n        p (int or tuple): padding.\n        g (int): number of blocked connections from input channels\n            to output channels (default: 1).\n    """"""""""""\n\n    def __init__(self, in_c,","@@ -0,0 +1,274 @@\n+from __future__ import division, absolute_import\n+import torch.utils.model_zoo as model_zoo\n+from torch import nn\n+from torch.nn import functional as F\n+\n+__all__ = ['mobilenetv2_x1_0', 'mobilenetv2_x1_4']\n+\n+model_urls = {\n+    # 1.0: top-1 71.3\n+    'mobilenetv2_x1_0':\n+    'https://mega.nz/#!NKp2wAIA!1NH1pbNzY_M2hVk_hdsxNM1NUOWvvGPHhaNr-fASF6c',\n+    # 1.4: top-1 73.9\n+    'mobilenetv2_x1_4':\n+    'https://mega.nz/#!RGhgEIwS!xN2s2ZdyqI6vQ3EwgmRXLEW3khr9tpXg96G9SUJugGk',\n+}\n+\n+\n+class ConvBlock(nn.Module):\n+    """"""""""""Basic convolutional block.\n+    \n+    convolution (bias discarded) + batch normalization + relu6.\n+\n+    Args:\n+        in_c (int): number of input channels.\n+        out_c (int): number of output channels.\n+        k (int or tuple): kernel size.\n+        s (int or tuple): stride.\n+        p (int or tuple): padding.\n+        g (int): number of blocked connections from input channels\n+            to output channels (default: 1).\n+    """"""""""""\n+\n+    def __init__(self, in_c, out_c, k, s=1, p=0, g=1):\n+        super(ConvBlock, self).__init__()\n+        self.conv = nn.Conv2d(\n+            in_c, out_c, k, stride=s, padding=p, bias=False, groups=g\n+        )\n+        self.bn = nn.BatchNorm2d(out_c)\n+\n+    def forward(self, x):\n+        return F.relu6(self.bn(self.conv(x)))\n+\n+\n+class Bottleneck(nn.Module):\n+\n+    def __init__(self, in_channels, out_channels, expansion_factor, stride=1):\n+        super(Bottleneck, self).__init__()\n+        mid_channels = in_channels * expansion_factor\n+        self.use_residual = stride == 1 and in_channels == out_channels\n+        self.conv1 = ConvBlock(in_channels, mid_channels, 1)\n+        self.dwconv2 = ConvBlock(\n+            mid_channels, mid_channels, 3, stride, 1, g=mid_channels\n+        )\n+        self.conv3 = nn.Sequential(\n+            nn.Conv2d(mid_channels, out_channels, 1, bias=False),\n+            nn.BatchNorm2d(out_channels),\n+        )",add,Add warning about data volume to enable_metrics_collection
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/mudeep.py,,"from __future__ import division, absolute_import\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = ['MuDeep']\n\n\nclass ConvBlock(nn.Module):\n    """"""""""""Basic convolutional block.\n    \n    convolution + batch normalization + relu.\n\n    Args:\n        in_c (int): number of input channels.\n        out_c (int): number of output channels.\n        k (int or tuple): kernel size.\n        s (int or tuple): stride.\n        p (int or tuple): padding.\n    """"""""""""\n\n    def __init__(self, in_c, out_c, k, s, p):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Conv2d(in_c, out_c, k, stride=s, padding=p)\n        self.bn = nn.BatchNorm2d(out_c)\n\n    def forward(self, x):\n        return F.relu(self.bn(self.conv(x)))\n\n\nclass ConvLayers(nn.Module):\n    """"""""""""Preprocessing layers.""""""""""""\n\n    def __init__(self):\n        super(ConvLayers, self).__init__()\n        self.conv1 = ConvBlock(3, 48, k=3, s=1, p=1)\n        self.conv2 = C","@@ -0,0 +1,206 @@\n+from __future__ import division, absolute_import\n+import torch\n+from torch import nn\n+from torch.nn import functional as F\n+\n+__all__ = ['MuDeep']\n+\n+\n+class ConvBlock(nn.Module):\n+    """"""""""""Basic convolutional block.\n+    \n+    convolution + batch normalization + relu.\n+\n+    Args:\n+        in_c (int): number of input channels.\n+        out_c (int): number of output channels.\n+        k (int or tuple): kernel size.\n+        s (int or tuple): stride.\n+        p (int or tuple): padding.\n+    """"""""""""\n+\n+    def __init__(self, in_c, out_c, k, s, p):\n+        super(ConvBlock, self).__init__()\n+        self.conv = nn.Conv2d(in_c, out_c, k, stride=s, padding=p)\n+        self.bn = nn.BatchNorm2d(out_c)\n+\n+    def forward(self, x):\n+        return F.relu(self.bn(self.conv(x)))\n+\n+\n+class ConvLayers(nn.Module):\n+    """"""""""""Preprocessing layers.""""""""""""\n+\n+    def __init__(self):\n+        super(ConvLayers, self).__init__()\n+        self.conv1 = ConvBlock(3, 48, k=3, s=1, p=1)\n+        self.conv2 = ConvBlock(48, 96, k=3, s=1, p=1)\n+        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n+\n+    def forward(self, x):\n+        x = self.conv1(x)\n+        x = self.conv2(x)\n+        x = self.maxpool(x)\n+        return x\n+\n+\n+class MultiScaleA(nn.Module):\n+    """"""""""""Multi-scale stream layer A (Sec.3.1)""""""""""""\n+\n+    def __init__(self):\n+        super(MultiScaleA, self).__init__()\n+        self.stream1 = nn.Sequential(\n+            ConvBlock(96, 96, k=1, s=1, p=0),\n+            ConvBlock(96, 24, k=3, s=1, p=1),\n+        )\n+        self.stream2 = nn.Sequential(\n+            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n+            ConvBlock(96, 24, k=1, s=1, p=0),\n+        )\n+        self.stream3 = ConvBlock(96, 24, k=1, s=1, p=0)\n+        self.stream4 = nn.Sequential(\n+            ConvBlock(96, 16, k=1, s=1, p=0),\n+            ConvBlock(16, 24, k=3, s=1, p=1),\n+            ConvBlock(24, ",add,Add note about data volume to enable_metrics_collection
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/nasnet.py,,"from __future__ import division, absolute_import\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = ['nasnetamobile']\n""""""""""""\nNASNet Mobile\nThanks to Anastasiia (https://github.com/DagnyT) for the great help, support and motivation!\n\n\n------------------------------------------------------------------------------------\n      Architecture       | Top-1 Acc | Top-5 Acc |  Multiply-Adds |  Params (M)\n------------------------------------------------------------------------------------\n|   NASNet-A (4 @ 1056)  |   74.08%  |   91.74%  |       564 M    |     5.3        |\n------------------------------------------------------------------------------------\n# References:\n - [Learning Transferable Architectures for Scalable Image Recognition]\n    (https://arxiv.org/abs/1707.07012)\n""""""""""""\n""""""""""""\nCode imported from https://github.com/Cadene/pretrained-models.pytorch\n""""""""""""\n\npretrained_settings = {\n    'nasn","@@ -0,0 +1,1131 @@\n+from __future__ import division, absolute_import\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+import torch.utils.model_zoo as model_zoo\n+\n+__all__ = ['nasnetamobile']\n+""""""""""""\n+NASNet Mobile\n+Thanks to Anastasiia (https://github.com/DagnyT) for the great help, support and motivation!\n+\n+\n+------------------------------------------------------------------------------------\n+      Architecture       | Top-1 Acc | Top-5 Acc |  Multiply-Adds |  Params (M)\n+------------------------------------------------------------------------------------\n+|   NASNet-A (4 @ 1056)  |   74.08%  |   91.74%  |       564 M    |     5.3        |\n+------------------------------------------------------------------------------------\n+# References:\n+ - [Learning Transferable Architectures for Scalable Image Recognition]\n+    (https://arxiv.org/abs/1707.07012)\n+""""""""""""\n+""""""""""""\n+Code imported from https://github.com/Cadene/pretrained-models.pytorch\n+""""""""""""\n+\n+pretrained_settings = {\n+    'nasnetamobile': {\n+        'imagenet': {\n+            # 'url': 'https://github.com/veronikayurchuk/pretrained-models.pytorch/releases/download/v1.0/nasnetmobile-7e03cead.pth.tar',\n+            'url':\n+            'http://data.lip6.fr/cadene/pretrainedmodels/nasnetamobile-7e03cead.pth',\n+            'input_space': 'RGB',\n+            'input_size': [3, 224, 224], # resize 256\n+            'input_range': [0, 1],\n+            'mean': [0.5, 0.5, 0.5],\n+            'std': [0.5, 0.5, 0.5],\n+            'num_classes': 1000\n+        },\n+        # 'imagenet+background': {\n+        #     # 'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth',\n+        #     'input_space': 'RGB',\n+        #     'input_size': [3, 224, 224], # resize 256\n+        #     'input_range': [0, 1],\n+        #     'mean': [0.5, 0.5, 0.5],\n+        #     'std': [0.5, 0.5, 0.5],\n+        #     'num_classes': 1001\n+        # }\n+    }\n+}\",add,Add more keywords to the approve_regex
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/osnet.py,,"from __future__ import division, absolute_import\nimport warnings\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = [\n    'osnet_x1_0', 'osnet_x0_75', 'osnet_x0_5', 'osnet_x0_25', 'osnet_ibn_x1_0'\n]\n\npretrained_urls = {\n    'osnet_x1_0':\n    'https://drive.google.com/uc?id=1LaG1EJpHrxdAxKnSCJ_i0u-nbxSAeiFY',\n    'osnet_x0_75':\n    'https://drive.google.com/uc?id=1uwA9fElHOk3ZogwbeY5GkLI6QPTX70Hq',\n    'osnet_x0_5':\n    'https://drive.google.com/uc?id=16DGLbZukvVYgINws8u8deSaOqjybZ83i',\n    'osnet_x0_25':\n    'https://drive.google.com/uc?id=1rb8UN5ZzPKRc_xvtHlyDh-cSz88YX9hs',\n    'osnet_ibn_x1_0':\n    'https://drive.google.com/uc?id=1sr90V6irlYYDd4_4ISU2iruoRG8J__6l'\n}\n\n\n##########\n# Basic layers\n##########\nclass ConvLayer(nn.Module):\n    """"""""""""Convolution layer (conv + bn + relu).""""""""""""\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n","@@ -0,0 +1,598 @@\n+from __future__ import division, absolute_import\n+import warnings\n+import torch\n+from torch import nn\n+from torch.nn import functional as F\n+\n+__all__ = [\n+    'osnet_x1_0', 'osnet_x0_75', 'osnet_x0_5', 'osnet_x0_25', 'osnet_ibn_x1_0'\n+]\n+\n+pretrained_urls = {\n+    'osnet_x1_0':\n+    'https://drive.google.com/uc?id=1LaG1EJpHrxdAxKnSCJ_i0u-nbxSAeiFY',\n+    'osnet_x0_75':\n+    'https://drive.google.com/uc?id=1uwA9fElHOk3ZogwbeY5GkLI6QPTX70Hq',\n+    'osnet_x0_5':\n+    'https://drive.google.com/uc?id=16DGLbZukvVYgINws8u8deSaOqjybZ83i',\n+    'osnet_x0_25':\n+    'https://drive.google.com/uc?id=1rb8UN5ZzPKRc_xvtHlyDh-cSz88YX9hs',\n+    'osnet_ibn_x1_0':\n+    'https://drive.google.com/uc?id=1sr90V6irlYYDd4_4ISU2iruoRG8J__6l'\n+}\n+\n+\n+##########\n+# Basic layers\n+##########\n+class ConvLayer(nn.Module):\n+    """"""""""""Convolution layer (conv + bn + relu).""""""""""""\n+\n+    def __init__(\n+        self,\n+        in_channels,\n+        out_channels,\n+        kernel_size,\n+        stride=1,\n+        padding=0,\n+        groups=1,\n+        IN=False\n+    ):\n+        super(ConvLayer, self).__init__()\n+        self.conv = nn.Conv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size,\n+            stride=stride,\n+            padding=padding,\n+            bias=False,\n+            groups=groups\n+        )\n+        if IN:\n+            self.bn = nn.InstanceNorm2d(out_channels, affine=True)\n+        else:\n+            self.bn = nn.BatchNorm2d(out_channels)\n+        self.relu = nn.ReLU(inplace=True)\n+\n+    def forward(self, x):\n+        x = self.conv(x)\n+        x = self.bn(x)\n+        x = self.relu(x)\n+        return x\n+\n+\n+class Conv1x1(nn.Module):\n+    """"""""""""1x1 convolution + bn + relu.""""""""""""\n+\n+    def __init__(self, in_channels, out_channels, stride=1, groups=1):\n+        super(Conv1x1, self).__init__()\n+        self.conv = nn.Conv2d(\n+            in_channels,\n+            out_channel",add,Added the user group to the contributors
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/osnet_ain.py,,"from __future__ import division, absolute_import\nimport warnings\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = [\n    'osnet_ain_x1_0', 'osnet_ain_x0_75', 'osnet_ain_x0_5', 'osnet_ain_x0_25'\n]\n\npretrained_urls = {\n    'osnet_ain_x1_0':\n    'https://drive.google.com/uc?id=1-CaioD9NaqbHK_kzSMW8VE4_3KcsRjEo',\n    'osnet_ain_x0_75':\n    'https://drive.google.com/uc?id=1apy0hpsMypqstfencdH-jKIUEFOW4xoM',\n    'osnet_ain_x0_5':\n    'https://drive.google.com/uc?id=1KusKvEYyKGDTUBVRxRiz55G31wkihB6l',\n    'osnet_ain_x0_25':\n    'https://drive.google.com/uc?id=1SxQt2AvmEcgWNhaRb2xC4rP6ZwVDP0Wt'\n}\n\n\n##########\n# Basic layers\n##########\nclass ConvLayer(nn.Module):\n    """"""""""""Convolution layer (conv + bn + relu).""""""""""""\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        groups=1,\n        IN=False\n    ):\n        super(ConvLayer, self).__init","@@ -0,0 +1,609 @@\n+from __future__ import division, absolute_import\n+import warnings\n+import torch\n+from torch import nn\n+from torch.nn import functional as F\n+\n+__all__ = [\n+    'osnet_ain_x1_0', 'osnet_ain_x0_75', 'osnet_ain_x0_5', 'osnet_ain_x0_25'\n+]\n+\n+pretrained_urls = {\n+    'osnet_ain_x1_0':\n+    'https://drive.google.com/uc?id=1-CaioD9NaqbHK_kzSMW8VE4_3KcsRjEo',\n+    'osnet_ain_x0_75':\n+    'https://drive.google.com/uc?id=1apy0hpsMypqstfencdH-jKIUEFOW4xoM',\n+    'osnet_ain_x0_5':\n+    'https://drive.google.com/uc?id=1KusKvEYyKGDTUBVRxRiz55G31wkihB6l',\n+    'osnet_ain_x0_25':\n+    'https://drive.google.com/uc?id=1SxQt2AvmEcgWNhaRb2xC4rP6ZwVDP0Wt'\n+}\n+\n+\n+##########\n+# Basic layers\n+##########\n+class ConvLayer(nn.Module):\n+    """"""""""""Convolution layer (conv + bn + relu).""""""""""""\n+\n+    def __init__(\n+        self,\n+        in_channels,\n+        out_channels,\n+        kernel_size,\n+        stride=1,\n+        padding=0,\n+        groups=1,\n+        IN=False\n+    ):\n+        super(ConvLayer, self).__init__()\n+        self.conv = nn.Conv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size,\n+            stride=stride,\n+            padding=padding,\n+            bias=False,\n+            groups=groups\n+        )\n+        if IN:\n+            self.bn = nn.InstanceNorm2d(out_channels, affine=True)\n+        else:\n+            self.bn = nn.BatchNorm2d(out_channels)\n+        self.relu = nn.ReLU()\n+\n+    def forward(self, x):\n+        x = self.conv(x)\n+        x = self.bn(x)\n+        return self.relu(x)\n+\n+\n+class Conv1x1(nn.Module):\n+    """"""""""""1x1 convolution + bn + relu.""""""""""""\n+\n+    def __init__(self, in_channels, out_channels, stride=1, groups=1):\n+        super(Conv1x1, self).__init__()\n+        self.conv = nn.Conv2d(\n+            in_channels,\n+            out_channels,\n+            1,\n+            stride=stride,\n+            padding=0,\n+            bias=False,\n+          ",add,Added the user group to the contributors
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/pcb.py,,"from __future__ import division, absolute_import\nimport torch.utils.model_zoo as model_zoo\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = ['pcb_p6', 'pcb_p4']\n\nmodel_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""""""""3x3 convolution with padding""""""""""""\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False\n    )\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bas","@@ -0,0 +1,314 @@\n+from __future__ import division, absolute_import\n+import torch.utils.model_zoo as model_zoo\n+from torch import nn\n+from torch.nn import functional as F\n+\n+__all__ = ['pcb_p6', 'pcb_p4']\n+\n+model_urls = {\n+    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n+    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n+    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n+    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n+    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n+}\n+\n+\n+def conv3x3(in_planes, out_planes, stride=1):\n+    """"""""""""3x3 convolution with padding""""""""""""\n+    return nn.Conv2d(\n+        in_planes,\n+        out_planes,\n+        kernel_size=3,\n+        stride=stride,\n+        padding=1,\n+        bias=False\n+    )\n+\n+\n+class BasicBlock(nn.Module):\n+    expansion = 1\n+\n+    def __init__(self, inplanes, planes, stride=1, downsample=None):\n+        super(BasicBlock, self).__init__()\n+        self.conv1 = conv3x3(inplanes, planes, stride)\n+        self.bn1 = nn.BatchNorm2d(planes)\n+        self.relu = nn.ReLU(inplace=True)\n+        self.conv2 = conv3x3(planes, planes)\n+        self.bn2 = nn.BatchNorm2d(planes)\n+        self.downsample = downsample\n+        self.stride = stride\n+\n+    def forward(self, x):\n+        residual = x\n+\n+        out = self.conv1(x)\n+        out = self.bn1(out)\n+        out = self.relu(out)\n+\n+        out = self.conv2(out)\n+        out = self.bn2(out)\n+\n+        if self.downsample is not None:\n+            residual = self.downsample(x)\n+\n+        out += residual\n+        out = self.relu(out)\n+\n+        return out\n+\n+\n+class Bottleneck(nn.Module):\n+    expansion = 4\n+\n+    def __init__(self, inplanes, planes, stride=1, downsample=None):\n+        super(Bottleneck, self).__init__()\n+        self.conv1 = nn.Conv2d(inplanes, planes, ker",add,Added the user for to extensions
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/resnet.py,,"""""""""""""\nCode source: https://github.com/pytorch/vision\n""""""""""""\nfrom __future__ import division, absolute_import\nimport torch.utils.model_zoo as model_zoo\nfrom torch import nn\n\n__all__ = [\n    'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152',\n    'resnext50_32x4d', 'resnext101_32x8d', 'resnet50_fc512'\n]\n\nmodel_urls = {\n    'resnet18':\n    'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n    'resnet34':\n    'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n    'resnet50':\n    'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'resnet101':\n    'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n    'resnet152':\n    'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n    'resnext50_32x4d':\n    'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n    'resnext101_32x8d':\n    'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n}\n\n\ndef conv3x3(in_planes, out_planes, ","@@ -0,0 +1,530 @@\n+""""""""""""\n+Code source: https://github.com/pytorch/vision\n+""""""""""""\n+from __future__ import division, absolute_import\n+import torch.utils.model_zoo as model_zoo\n+from torch import nn\n+\n+__all__ = [\n+    'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152',\n+    'resnext50_32x4d', 'resnext101_32x8d', 'resnet50_fc512'\n+]\n+\n+model_urls = {\n+    'resnet18':\n+    'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n+    'resnet34':\n+    'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n+    'resnet50':\n+    'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n+    'resnet101':\n+    'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n+    'resnet152':\n+    'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n+    'resnext50_32x4d':\n+    'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n+    'resnext101_32x8d':\n+    'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n+}\n+\n+\n+def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n+    """"""""""""3x3 convolution with padding""""""""""""\n+    return nn.Conv2d(\n+        in_planes,\n+        out_planes,\n+        kernel_size=3,\n+        stride=stride,\n+        padding=dilation,\n+        groups=groups,\n+        bias=False,\n+        dilation=dilation\n+    )\n+\n+\n+def conv1x1(in_planes, out_planes, stride=1):\n+    """"""""""""1x1 convolution""""""""""""\n+    return nn.Conv2d(\n+        in_planes, out_planes, kernel_size=1, stride=stride, bias=False\n+    )\n+\n+\n+class BasicBlock(nn.Module):\n+    expansion = 1\n+\n+    def __init__(\n+        self,\n+        inplanes,\n+        planes,\n+        stride=1,\n+        downsample=None,\n+        groups=1,\n+        base_width=64,\n+        dilation=1,\n+        norm_layer=None\n+    ):\n+        super(BasicBlock, self).__init__()\n+        if norm_layer is None:\n+            norm_layer = nn.BatchNorm2d\n+        if groups != 1 or base_width != 64:",add,Added TypeSpec . h for global .
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/resnet_ibn_a.py,,"""""""""""""\nCredit to https://github.com/XingangPan/IBN-Net.\n""""""""""""\nfrom __future__ import division, absolute_import\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = ['resnet50_ibn_a']\n\nmodel_urls = {\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"3x3 convolution with padding""""\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False\n    )\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.Batch","@@ -0,0 +1,289 @@\n+""""""""""""\n+Credit to https://github.com/XingangPan/IBN-Net.\n+""""""""""""\n+from __future__ import division, absolute_import\n+import math\n+import torch\n+import torch.nn as nn\n+import torch.utils.model_zoo as model_zoo\n+\n+__all__ = ['resnet50_ibn_a']\n+\n+model_urls = {\n+    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n+    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n+    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n+}\n+\n+\n+def conv3x3(in_planes, out_planes, stride=1):\n+    """"3x3 convolution with padding""""\n+    return nn.Conv2d(\n+        in_planes,\n+        out_planes,\n+        kernel_size=3,\n+        stride=stride,\n+        padding=1,\n+        bias=False\n+    )\n+\n+\n+class BasicBlock(nn.Module):\n+    expansion = 1\n+\n+    def __init__(self, inplanes, planes, stride=1, downsample=None):\n+        super(BasicBlock, self).__init__()\n+        self.conv1 = conv3x3(inplanes, planes, stride)\n+        self.bn1 = nn.BatchNorm2d(planes)\n+        self.relu = nn.ReLU(inplace=True)\n+        self.conv2 = conv3x3(planes, planes)\n+        self.bn2 = nn.BatchNorm2d(planes)\n+        self.downsample = downsample\n+        self.stride = stride\n+\n+    def forward(self, x):\n+        residual = x\n+\n+        out = self.conv1(x)\n+        out = self.bn1(out)\n+        out = self.relu(out)\n+\n+        out = self.conv2(out)\n+        out = self.bn2(out)\n+\n+        if self.downsample is not None:\n+            residual = self.downsample(x)\n+\n+        out += residual\n+        out = self.relu(out)\n+\n+        return out\n+\n+\n+class IBN(nn.Module):\n+\n+    def __init__(self, planes):\n+        super(IBN, self).__init__()\n+        half1 = int(planes / 2)\n+        self.half = half1\n+        half2 = planes - half1\n+        self.IN = nn.InstanceNorm2d(half1, affine=True)\n+        self.BN = nn.BatchNorm2d(half2)\n+\n+    def forward(self, x):\n+     ",add,Added STORM - 1404 to the CHANGELOG .
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/resnet_ibn_b.py,,"""""""""""""\nCredit to https://github.com/XingangPan/IBN-Net.\n""""""""""""\nfrom __future__ import division, absolute_import\nimport math\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = ['resnet50_ibn_b']\n\nmodel_urls = {\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"3x3 convolution with padding""""\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False\n    )\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)","@@ -0,0 +1,274 @@\n+""""""""""""\n+Credit to https://github.com/XingangPan/IBN-Net.\n+""""""""""""\n+from __future__ import division, absolute_import\n+import math\n+import torch.nn as nn\n+import torch.utils.model_zoo as model_zoo\n+\n+__all__ = ['resnet50_ibn_b']\n+\n+model_urls = {\n+    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n+    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n+    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n+}\n+\n+\n+def conv3x3(in_planes, out_planes, stride=1):\n+    """"3x3 convolution with padding""""\n+    return nn.Conv2d(\n+        in_planes,\n+        out_planes,\n+        kernel_size=3,\n+        stride=stride,\n+        padding=1,\n+        bias=False\n+    )\n+\n+\n+class BasicBlock(nn.Module):\n+    expansion = 1\n+\n+    def __init__(self, inplanes, planes, stride=1, downsample=None):\n+        super(BasicBlock, self).__init__()\n+        self.conv1 = conv3x3(inplanes, planes, stride)\n+        self.bn1 = nn.BatchNorm2d(planes)\n+        self.relu = nn.ReLU(inplace=True)\n+        self.conv2 = conv3x3(planes, planes)\n+        self.bn2 = nn.BatchNorm2d(planes)\n+        self.downsample = downsample\n+        self.stride = stride\n+\n+    def forward(self, x):\n+        residual = x\n+\n+        out = self.conv1(x)\n+        out = self.bn1(out)\n+        out = self.relu(out)\n+\n+        out = self.conv2(out)\n+        out = self.bn2(out)\n+\n+        if self.downsample is not None:\n+            residual = self.downsample(x)\n+\n+        out += residual\n+        out = self.relu(out)\n+\n+        return out\n+\n+\n+class Bottleneck(nn.Module):\n+    expansion = 4\n+\n+    def __init__(self, inplanes, planes, stride=1, downsample=None, IN=False):\n+        super(Bottleneck, self).__init__()\n+        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n+        self.bn1 = nn.BatchNorm2d(planes)\n+        self.conv2 = nn.Conv2d(\n+         ",add,Added formatchart to_metrics_collection
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/resnetmid.py,,"from __future__ import division, absolute_import\nimport torch\nimport torch.utils.model_zoo as model_zoo\nfrom torch import nn\n\n__all__ = ['resnet50mid']\n\nmodel_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""""""""3x3 convolution with padding""""""""""""\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False\n    )\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n  ","@@ -0,0 +1,307 @@\n+from __future__ import division, absolute_import\n+import torch\n+import torch.utils.model_zoo as model_zoo\n+from torch import nn\n+\n+__all__ = ['resnet50mid']\n+\n+model_urls = {\n+    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n+    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n+    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n+    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n+    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n+}\n+\n+\n+def conv3x3(in_planes, out_planes, stride=1):\n+    """"""""""""3x3 convolution with padding""""""""""""\n+    return nn.Conv2d(\n+        in_planes,\n+        out_planes,\n+        kernel_size=3,\n+        stride=stride,\n+        padding=1,\n+        bias=False\n+    )\n+\n+\n+class BasicBlock(nn.Module):\n+    expansion = 1\n+\n+    def __init__(self, inplanes, planes, stride=1, downsample=None):\n+        super(BasicBlock, self).__init__()\n+        self.conv1 = conv3x3(inplanes, planes, stride)\n+        self.bn1 = nn.BatchNorm2d(planes)\n+        self.relu = nn.ReLU(inplace=True)\n+        self.conv2 = conv3x3(planes, planes)\n+        self.bn2 = nn.BatchNorm2d(planes)\n+        self.downsample = downsample\n+        self.stride = stride\n+\n+    def forward(self, x):\n+        residual = x\n+\n+        out = self.conv1(x)\n+        out = self.bn1(out)\n+        out = self.relu(out)\n+\n+        out = self.conv2(out)\n+        out = self.bn2(out)\n+\n+        if self.downsample is not None:\n+            residual = self.downsample(x)\n+\n+        out += residual\n+        out = self.relu(out)\n+\n+        return out\n+\n+\n+class Bottleneck(nn.Module):\n+    expansion = 4\n+\n+    def __init__(self, inplanes, planes, stride=1, downsample=None):\n+        super(Bottleneck, self).__init__()\n+        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n+   ",add,Added the user for to extensions
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/senet.py,,"from __future__ import division, absolute_import\nimport math\nfrom collections import OrderedDict\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = [\n    'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n    'se_resnext50_32x4d', 'se_resnext101_32x4d', 'se_resnet50_fc512'\n]\n""""""""""""\nCode imported from https://github.com/Cadene/pretrained-models.pytorch\n""""""""""""\n\npretrained_settings = {\n    'senet154': {\n        'imagenet': {\n            'url':\n            'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet50': {\n        'imagenet': {\n            'url':\n            'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth',\n            'input_space': 'R","@@ -0,0 +1,688 @@\n+from __future__ import division, absolute_import\n+import math\n+from collections import OrderedDict\n+import torch.nn as nn\n+from torch.utils import model_zoo\n+\n+__all__ = [\n+    'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n+    'se_resnext50_32x4d', 'se_resnext101_32x4d', 'se_resnet50_fc512'\n+]\n+""""""""""""\n+Code imported from https://github.com/Cadene/pretrained-models.pytorch\n+""""""""""""\n+\n+pretrained_settings = {\n+    'senet154': {\n+        'imagenet': {\n+            'url':\n+            'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth',\n+            'input_space': 'RGB',\n+            'input_size': [3, 224, 224],\n+            'input_range': [0, 1],\n+            'mean': [0.485, 0.456, 0.406],\n+            'std': [0.229, 0.224, 0.225],\n+            'num_classes': 1000\n+        }\n+    },\n+    'se_resnet50': {\n+        'imagenet': {\n+            'url':\n+            'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth',\n+            'input_space': 'RGB',\n+            'input_size': [3, 224, 224],\n+            'input_range': [0, 1],\n+            'mean': [0.485, 0.456, 0.406],\n+            'std': [0.229, 0.224, 0.225],\n+            'num_classes': 1000\n+        }\n+    },\n+    'se_resnet101': {\n+        'imagenet': {\n+            'url':\n+            'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth',\n+            'input_space': 'RGB',\n+            'input_size': [3, 224, 224],\n+            'input_range': [0, 1],\n+            'mean': [0.485, 0.456, 0.406],\n+            'std': [0.229, 0.224, 0.225],\n+            'num_classes': 1000\n+        }\n+    },\n+    'se_resnet152': {\n+        'imagenet': {\n+            'url':\n+            'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth',\n+            'input_space': 'RGB',\n+            'input_size': [3, 224, 224],\n+            'input_range': [0, 1],\n+            'mean': [0.485, 0.",add,Added the user group to the deploy profile
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/shufflenet.py,,"from __future__ import division, absolute_import\nimport torch\nimport torch.utils.model_zoo as model_zoo\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = ['shufflenet']\n\nmodel_urls = {\n    # training epoch = 90, top1 = 61.8\n    'imagenet':\n    'https://mega.nz/#!RDpUlQCY!tr_5xBEkelzDjveIYBBcGcovNCOrgfiJO9kiidz9fZM',\n}\n\n\nclass ChannelShuffle(nn.Module):\n\n    def __init__(self, num_groups):\n        super(ChannelShuffle, self).__init__()\n        self.g = num_groups\n\n    def forward(self, x):\n        b, c, h, w = x.size()\n        n = c // self.g\n        # reshape\n        x = x.view(b, self.g, n, h, w)\n        # transpose\n        x = x.permute(0, 2, 1, 3, 4).contiguous()\n        # flatten\n        x = x.view(b, c, h, w)\n        return x\n\n\nclass Bottleneck(nn.Module):\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        num_groups,\n        group_conv1x1=True\n    ):\n        super(Bottle","@@ -0,0 +1,198 @@\n+from __future__ import division, absolute_import\n+import torch\n+import torch.utils.model_zoo as model_zoo\n+from torch import nn\n+from torch.nn import functional as F\n+\n+__all__ = ['shufflenet']\n+\n+model_urls = {\n+    # training epoch = 90, top1 = 61.8\n+    'imagenet':\n+    'https://mega.nz/#!RDpUlQCY!tr_5xBEkelzDjveIYBBcGcovNCOrgfiJO9kiidz9fZM',\n+}\n+\n+\n+class ChannelShuffle(nn.Module):\n+\n+    def __init__(self, num_groups):\n+        super(ChannelShuffle, self).__init__()\n+        self.g = num_groups\n+\n+    def forward(self, x):\n+        b, c, h, w = x.size()\n+        n = c // self.g\n+        # reshape\n+        x = x.view(b, self.g, n, h, w)\n+        # transpose\n+        x = x.permute(0, 2, 1, 3, 4).contiguous()\n+        # flatten\n+        x = x.view(b, c, h, w)\n+        return x\n+\n+\n+class Bottleneck(nn.Module):\n+\n+    def __init__(\n+        self,\n+        in_channels,\n+        out_channels,\n+        stride,\n+        num_groups,\n+        group_conv1x1=True\n+    ):\n+        super(Bottleneck, self).__init__()\n+        assert stride in [1, 2], 'Warning: stride must be either 1 or 2'\n+        self.stride = stride\n+        mid_channels = out_channels // 4\n+        if stride == 2:\n+            out_channels -= in_channels\n+        # group conv is not applied to first conv1x1 at stage 2\n+        num_groups_conv1x1 = num_groups if group_conv1x1 else 1\n+        self.conv1 = nn.Conv2d(\n+            in_channels,\n+            mid_channels,\n+            1,\n+            groups=num_groups_conv1x1,\n+            bias=False\n+        )\n+        self.bn1 = nn.BatchNorm2d(mid_channels)\n+        self.shuffle1 = ChannelShuffle(num_groups)\n+        self.conv2 = nn.Conv2d(\n+            mid_channels,\n+            mid_channels,\n+            3,\n+            stride=stride,\n+            padding=1,\n+            groups=mid_channels,\n+            bias=False\n+        )\n+        self.bn2 = nn.BatchNorm2d(mid_chann",add,Add forced default for text type
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/shufflenetv2.py,,"""""""""""""\nCode source: https://github.com/pytorch/vision\n""""""""""""\nfrom __future__ import division, absolute_import\nimport torch\nimport torch.utils.model_zoo as model_zoo\nfrom torch import nn\n\n__all__ = [\n    'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5',\n    'shufflenet_v2_x2_0'\n]\n\nmodel_urls = {\n    'shufflenetv2_x0.5':\n    'https://download.pytorch.org/models/shufflenetv2_x0.5-f707e7126e.pth',\n    'shufflenetv2_x1.0':\n    'https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth',\n    'shufflenetv2_x1.5': None,\n    'shufflenetv2_x2.0': None,\n}\n\n\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n    channels_per_group = num_channels // groups\n\n    # reshape\n    x = x.view(batchsize, groups, channels_per_group, height, width)\n\n    x = torch.transpose(x, 1, 2).contiguous()\n\n    # flatten\n    x = x.view(batchsize, -1, height, width)\n\n    return x\n\n\nclass InvertedResidual(nn.Module):\n\n","@@ -0,0 +1,262 @@\n+""""""""""""\n+Code source: https://github.com/pytorch/vision\n+""""""""""""\n+from __future__ import division, absolute_import\n+import torch\n+import torch.utils.model_zoo as model_zoo\n+from torch import nn\n+\n+__all__ = [\n+    'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5',\n+    'shufflenet_v2_x2_0'\n+]\n+\n+model_urls = {\n+    'shufflenetv2_x0.5':\n+    'https://download.pytorch.org/models/shufflenetv2_x0.5-f707e7126e.pth',\n+    'shufflenetv2_x1.0':\n+    'https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth',\n+    'shufflenetv2_x1.5': None,\n+    'shufflenetv2_x2.0': None,\n+}\n+\n+\n+def channel_shuffle(x, groups):\n+    batchsize, num_channels, height, width = x.data.size()\n+    channels_per_group = num_channels // groups\n+\n+    # reshape\n+    x = x.view(batchsize, groups, channels_per_group, height, width)\n+\n+    x = torch.transpose(x, 1, 2).contiguous()\n+\n+    # flatten\n+    x = x.view(batchsize, -1, height, width)\n+\n+    return x\n+\n+\n+class InvertedResidual(nn.Module):\n+\n+    def __init__(self, inp, oup, stride):\n+        super(InvertedResidual, self).__init__()\n+\n+        if not (1 <= stride <= 3):\n+            raise ValueError('illegal stride value')\n+        self.stride = stride\n+\n+        branch_features = oup // 2\n+        assert (self.stride != 1) or (inp == branch_features << 1)\n+\n+        if self.stride > 1:\n+            self.branch1 = nn.Sequential(\n+                self.depthwise_conv(\n+                    inp, inp, kernel_size=3, stride=self.stride, padding=1\n+                ),\n+                nn.BatchNorm2d(inp),\n+                nn.Conv2d(\n+                    inp,\n+                    branch_features,\n+                    kernel_size=1,\n+                    stride=1,\n+                    padding=0,\n+                    bias=False\n+                ),\n+                nn.BatchNorm2d(branch_features),\n+                nn.ReLU(inplace=True),\n+            )\",add,Add note about data volume to enable EGL
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/squeezenet.py,,"""""""""""""\nCode source: https://github.com/pytorch/vision\n""""""""""""\nfrom __future__ import division, absolute_import\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = ['squeezenet1_0', 'squeezenet1_1', 'squeezenet1_0_fc512']\n\nmodel_urls = {\n    'squeezenet1_0':\n    'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n    'squeezenet1_1':\n    'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n}\n\n\nclass Fire(nn.Module):\n\n    def __init__(\n        self, inplanes, squeeze_planes, expand1x1_planes, expand3x3_planes\n    ):\n        super(Fire, self).__init__()\n        self.inplanes = inplanes\n        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        self.expand1x1 = nn.Conv2d(\n            squeeze_planes, expand1x1_planes, kernel_size=1\n        )\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        self.expand3x3 ","@@ -0,0 +1,236 @@\n+""""""""""""\n+Code source: https://github.com/pytorch/vision\n+""""""""""""\n+from __future__ import division, absolute_import\n+import torch\n+import torch.nn as nn\n+import torch.utils.model_zoo as model_zoo\n+\n+__all__ = ['squeezenet1_0', 'squeezenet1_1', 'squeezenet1_0_fc512']\n+\n+model_urls = {\n+    'squeezenet1_0':\n+    'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n+    'squeezenet1_1':\n+    'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n+}\n+\n+\n+class Fire(nn.Module):\n+\n+    def __init__(\n+        self, inplanes, squeeze_planes, expand1x1_planes, expand3x3_planes\n+    ):\n+        super(Fire, self).__init__()\n+        self.inplanes = inplanes\n+        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n+        self.squeeze_activation = nn.ReLU(inplace=True)\n+        self.expand1x1 = nn.Conv2d(\n+            squeeze_planes, expand1x1_planes, kernel_size=1\n+        )\n+        self.expand1x1_activation = nn.ReLU(inplace=True)\n+        self.expand3x3 = nn.Conv2d(\n+            squeeze_planes, expand3x3_planes, kernel_size=3, padding=1\n+        )\n+        self.expand3x3_activation = nn.ReLU(inplace=True)\n+\n+    def forward(self, x):\n+        x = self.squeeze_activation(self.squeeze(x))\n+        return torch.cat(\n+            [\n+                self.expand1x1_activation(self.expand1x1(x)),\n+                self.expand3x3_activation(self.expand3x3(x))\n+            ], 1\n+        )\n+\n+\n+class SqueezeNet(nn.Module):\n+    """"""""""""SqueezeNet.\n+\n+    Reference:\n+        Iandola et al. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters\n+        and< 0.5 MB model size. arXiv:1602.07360.\n+\n+    Public keys:\n+        - ``squeezenet1_0``: SqueezeNet (version=1.0).\n+        - ``squeezenet1_1``: SqueezeNet (version=1.1).\n+        - ``squeezenet1_0_fc512``: SqueezeNet (version=1.0) + FC.\n+    """"""""""""\n+\n+    def __init__(\n+        self,\n+        num_classes,\n+   ",add,Add note about data volume to enable EGL
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/deep/models/xception.py,,"from __future__ import division, absolute_import\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = ['xception']\n\npretrained_settings = {\n    'xception': {\n        'imagenet': {\n            'url':\n            'http://data.lip6.fr/cadene/pretrainedmodels/xception-43020ad28.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 299, 299],\n            'input_range': [0, 1],\n            'mean': [0.5, 0.5, 0.5],\n            'std': [0.5, 0.5, 0.5],\n            'num_classes': 1000,\n            'scale':\n            0.8975 # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n        }\n    }\n}\n\n\nclass SeparableConv2d(nn.Module):\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size=1,\n        stride=1,\n        padding=0,\n        dilation=1,\n        bias=False\n    ):\n        super(SeparableConv2d,","@@ -0,0 +1,344 @@\n+from __future__ import division, absolute_import\n+import torch.nn as nn\n+import torch.nn.functional as F\n+import torch.utils.model_zoo as model_zoo\n+\n+__all__ = ['xception']\n+\n+pretrained_settings = {\n+    'xception': {\n+        'imagenet': {\n+            'url':\n+            'http://data.lip6.fr/cadene/pretrainedmodels/xception-43020ad28.pth',\n+            'input_space': 'RGB',\n+            'input_size': [3, 299, 299],\n+            'input_range': [0, 1],\n+            'mean': [0.5, 0.5, 0.5],\n+            'std': [0.5, 0.5, 0.5],\n+            'num_classes': 1000,\n+            'scale':\n+            0.8975 # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n+        }\n+    }\n+}\n+\n+\n+class SeparableConv2d(nn.Module):\n+\n+    def __init__(\n+        self,\n+        in_channels,\n+        out_channels,\n+        kernel_size=1,\n+        stride=1,\n+        padding=0,\n+        dilation=1,\n+        bias=False\n+    ):\n+        super(SeparableConv2d, self).__init__()\n+\n+        self.conv1 = nn.Conv2d(\n+            in_channels,\n+            in_channels,\n+            kernel_size,\n+            stride,\n+            padding,\n+            dilation,\n+            groups=in_channels,\n+            bias=bias\n+        )\n+        self.pointwise = nn.Conv2d(\n+            in_channels, out_channels, 1, 1, 0, 1, 1, bias=bias\n+        )\n+\n+    def forward(self, x):\n+        x = self.conv1(x)\n+        x = self.pointwise(x)\n+        return x\n+\n+\n+class Block(nn.Module):\n+\n+    def __init__(\n+        self,\n+        in_filters,\n+        out_filters,\n+        reps,\n+        strides=1,\n+        start_with_relu=True,\n+        grow_first=True\n+    ):\n+        super(Block, self).__init__()\n+\n+        if out_filters != in_filters or strides != 1:\n+            self.skip = nn.Conv2d(\n+                in_filters, out_filters, 1, stride=strides, bias=False\n+           ",add,Added the user group to the contributors
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/reid_multibackend.py,"import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nfrom itertools import islice\nimport torchvision.transforms as transforms\nimport cv2\nimport sys\nimport torchvision.transforms as T\nfrom collections import OrderedDict, namedtuple\nimport gdown\nfrom os.path import exists as file_exists\n\nfrom yolov5.utils.general import LOGGER, check_version, check_requirements\nfrom deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name, download_url, load_pretrained_weights\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f""""{msg}{f} acceptable suffix is {suffix}""""\n\n\nclass ReIDDetectMultiBacken","import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nfrom itertools import islice\nimport torchvision.transforms as transforms\nimport cv2\nimport sys\nimport torchvision.transforms as T\nfrom collections import OrderedDict, namedtuple\nimport gdown\nfrom os.path import exists as file_exists\n\nfrom yolov5.utils.general import LOGGER, check_version, check_requirements\nfrom trackers.strong_sort.deep.reid_model_factory import (show_downloadeable_models, get_model_url, get_model_name,\n                                                          download_url, load_pretrained_weights)\nfrom trackers.strong_sort.deep.models import build_model\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suff","@@ -12,7 +12,9 @@ import gdown\n from os.path import exists as file_exists\n \n from yolov5.utils.general import LOGGER, check_version, check_requirements\n-from deep.reid_model_factory import show_downloadeable_models, get_model_url, get_model_name, download_url, load_pretrained_weights\n+from trackers.strong_sort.deep.reid_model_factory import (show_downloadeable_models, get_model_url, get_model_name,\n+                                                          download_url, load_pretrained_weights)\n+from trackers.strong_sort.deep.models import build_model\n \n \n def check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n@@ -30,7 +32,7 @@ class ReIDDetectMultiBackend(nn.Module):\n     # ReID models MultiBackend class for python inference on various backends\n     def __init__(self, weights='osnet_x0_25_msmt17.pt', device=torch.device('cpu'), fp16=False):\n         super().__init__()\n-        from deep.reid.torchreid.models import build_model\n+\n         w = weights[0] if isinstance(weights, list) else weights\n         self.pt, self.jit, self.onnx, self.xml, self.engine, self.coreml, \\n             self.saved_model, self.pb, self.tflite, self.edgetpu, self.tfjs = self.model_type(w)  # get backend\n",add,Add note about data volume to enable
2353349ce0e1eb0359ff010f848b266d0de51a26,fix path mess,trackers/strong_sort/sort/nn_matching.py,"# vim: expandtab:ts=4:sw=4\nimport numpy as np\nimport sys\nimport torch\n\n\ndef _pdist(a, b):\n    """"""""""""Compute pair-wise squared distance between points in `a` and `b`.\n    Parameters\n    ----------\n    a : array_like\n        An NxM matrix of N samples of dimensionality M.\n    b : array_like\n        An LxM matrix of L samples of dimensionality M.\n    Returns\n    -------\n    ndarray\n        Returns a matrix of size len(a), len(b) such that eleement (i, j)\n        contains the squared distance between `a[i]` and `b[j]`.\n    """"""""""""\n    a, b = np.asarray(a), np.asarray(b)\n    if len(a) == 0 or len(b) == 0:\n        return np.zeros((len(a), len(b)))\n    a2, b2 = np.square(a).sum(axis=1), np.square(b).sum(axis=1)\n    r2 = -2. * np.dot(a, b.T) + a2[:, None] + b2[None, :]\n    r2 = np.clip(r2, 0., float(np.inf))\n    return r2\n\n\ndef _cosine_distance(a, b, data_is_normalized=False):\n    """"""""""""Compute pair-wise cosine distance between points in `a` and `b`.\n    Parameter","# vim: expandtab:ts=4:sw=4\nimport numpy as np\nimport sys\nimport torch\n\n\ndef _pdist(a, b):\n    """"""""""""Compute pair-wise squared distance between points in `a` and `b`.\n    Parameters\n    ----------\n    a : array_like\n        An NxM matrix of N samples of dimensionality M.\n    b : array_like\n        An LxM matrix of L samples of dimensionality M.\n    Returns\n    -------\n    ndarray\n        Returns a matrix of size len(a), len(b) such that eleement (i, j)\n        contains the squared distance between `a[i]` and `b[j]`.\n    """"""""""""\n    a, b = np.asarray(a), np.asarray(b)\n    if len(a) == 0 or len(b) == 0:\n        return np.zeros((len(a), len(b)))\n    a2, b2 = np.square(a).sum(axis=1), np.square(b).sum(axis=1)\n    r2 = -2. * np.dot(a, b.T) + a2[:, None] + b2[None, :]\n    r2 = np.clip(r2, 0., float(np.inf))\n    return r2\n\n\ndef _cosine_distance(a, b, data_is_normalized=False):\n    """"""""""""Compute pair-wise cosine distance between points in `a` and `b`.\n    Parameter","@@ -86,8 +86,8 @@ def _nn_cosine_distance(x, y):\n     """"""""""""\n     x_ = torch.from_numpy(np.asarray(x))\n     y_ = torch.from_numpy(np.asarray(y))\n-    distances = compute_distance_matrix(x_, y_, metric='cosine')\n-    distances = distances.cpu().detach().numpy()\n+    distances = _cosine_distance(x_, y_)\n+    distances = distances\n     return distances.min(axis=0)\n \n \n",fix,Add note about data volume to enable_metrics_collection
dafccdded02314031c9b6dd0423ff9e51a2ad390,detection fix output,trackers/bytetrack/byte_tracker.py,"import numpy as np\nfrom collections import deque\nimport os\nimport os.path as osp\nimport copy\nimport torch\nimport torch.nn.functional as F\n\nfrom yolov5.utils.general import xywh2xyxy\n\n\nfrom trackers.bytetrack.kalman_filter import KalmanFilter\nfrom trackers.bytetrack import matching\nfrom trackers.bytetrack.basetrack import BaseTrack, TrackState\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n    def __init__(self, tlwh, score):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\n\n    @staticmethod\n    def multi_predic","import numpy as np\nfrom collections import deque\nimport os\nimport os.path as osp\nimport copy\nimport torch\nimport torch.nn.functional as F\n\nfrom yolov5.utils.general import xywh2xyxy\n\n\nfrom trackers.bytetrack.kalman_filter import KalmanFilter\nfrom trackers.bytetrack import matching\nfrom trackers.bytetrack.basetrack import BaseTrack, TrackState\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n    def __init__(self, tlwh, score):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\n\n    @staticmethod\n    def multi_predic","@@ -288,13 +288,17 @@ class BYTETracker(object):\n         self.tracked_stracks, self.lost_stracks = remove_duplicate_stracks(self.tracked_stracks, self.lost_stracks)\n         # get scores of lost tracks\n         output_stracks = [track for track in self.tracked_stracks if track.is_activated]\n-\n+        print('wooo')\n+        print(len(output_stracks))\n         outputs = []\n         for t in output_stracks:\n             output= []\n             tlwh = t.tlwh\n             tid = t.track_id\n-            output.extend(tlwh)\n+            tlwh = np.expand_dims(tlwh, axis=0)\n+            xyxy = xywh2xyxy(tlwh)\n+            xyxy = np.squeeze(xyxy, axis=0)\n+            output.extend(xyxy)\n             output.append(tid)\n             output.append(t.score)\n             outputs.append(output)\n",add,Added note about dates .
07a87b600c65e4a441460a2698fce03181f68b4d,delete debug prints,trackers/bytetrack/byte_tracker.py,"import numpy as np\nfrom collections import deque\nimport os\nimport os.path as osp\nimport copy\nimport torch\nimport torch.nn.functional as F\n\nfrom yolov5.utils.general import xywh2xyxy\n\n\nfrom trackers.bytetrack.kalman_filter import KalmanFilter\nfrom trackers.bytetrack import matching\nfrom trackers.bytetrack.basetrack import BaseTrack, TrackState\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n    def __init__(self, tlwh, score):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\n\n    @staticmethod\n    def multi_predic","import numpy as np\nfrom collections import deque\nimport os\nimport os.path as osp\nimport copy\nimport torch\nimport torch.nn.functional as F\n\nfrom yolov5.utils.general import xywh2xyxy\n\n\nfrom trackers.bytetrack.kalman_filter import KalmanFilter\nfrom trackers.bytetrack import matching\nfrom trackers.bytetrack.basetrack import BaseTrack, TrackState\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n    def __init__(self, tlwh, score):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\n\n    @staticmethod\n    def multi_predic","@@ -288,8 +288,6 @@ class BYTETracker(object):\n         self.tracked_stracks, self.lost_stracks = remove_duplicate_stracks(self.tracked_stracks, self.lost_stracks)\n         # get scores of lost tracks\n         output_stracks = [track for track in self.tracked_stracks if track.is_activated]\n-        print('wooo')\n-        print(len(output_stracks))\n         outputs = []\n         for t in output_stracks:\n             output= []\n",fix,Added note about dates .
6db29f064ecd83649935aa84f4bba8f46c63f791,solved bytetrack bug,trackers/bytetrack/byte_tracker.py,"import numpy as np\nfrom collections import deque\nimport os\nimport os.path as osp\nimport copy\nimport torch\nimport torch.nn.functional as F\n\nfrom yolov5.utils.general import xywh2xyxy\n\n\nfrom trackers.bytetrack.kalman_filter import KalmanFilter\nfrom trackers.bytetrack import matching\nfrom trackers.bytetrack.basetrack import BaseTrack, TrackState\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n    def __init__(self, tlwh, score):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\n\n    @staticmethod\n    def multi_predic","import numpy as np\nfrom collections import deque\nimport os\nimport os.path as osp\nimport copy\nimport torch\nimport torch.nn.functional as F\n\nfrom yolov5.utils.general import xywh2xyxy, xyxy2xywh\n\n\nfrom trackers.bytetrack.kalman_filter import KalmanFilter\nfrom trackers.bytetrack import matching\nfrom trackers.bytetrack.basetrack import BaseTrack, TrackState\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n    def __init__(self, tlwh, score):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\n\n    @staticmethod\n    def m","@@ -6,7 +6,7 @@ import copy\n import torch\n import torch.nn.functional as F\n \n-from yolov5.utils.general import xywh2xyxy\n+from yolov5.utils.general import xywh2xyxy, xyxy2xywh\n \n \n from trackers.bytetrack.kalman_filter import KalmanFilter\n@@ -170,6 +170,7 @@ class BYTETracker(object):\n         removed_stracks = []\n \n         xyxys = dets[:, 0:4]\n+        xywh = xyxy2xywh(xyxys)\n         confs = dets[:, 4]\n         clss = dets[:, 5]\n         \n@@ -182,15 +183,14 @@ class BYTETracker(object):\n         inds_high = confs < self.track_thresh\n \n         inds_second = np.logical_and(inds_low, inds_high)\n-        dets_second = xyxys[inds_second]\n-        dets = xyxys[remain_inds]\n+        dets_second = xywh[inds_second]\n+        dets = xywh[remain_inds]\n         scores_keep = confs[remain_inds]\n         scores_second = confs[inds_second]\n \n         if len(dets) > 0:\n             '''Detections'''\n-            detections = [STrack(xywh, s) for\n-                          (xywh, s) in zip(dets, scores_keep)]\n+            detections = [STrack(xyxy, s) for (xyxy, s) in zip(dets, scores_keep)]\n         else:\n             detections = []\n \n@@ -226,8 +226,7 @@ class BYTETracker(object):\n         # association the untrack to the low score detections\n         if len(dets_second) > 0:\n             '''Detections'''\n-            detections_second = [STrack(STrack.tlbr_to_tlwh(tlbr), s) for\n-                          (tlbr, s) in zip(dets_second, scores_second)]\n+            detections_second = [STrack(xywh, s) for (xywh, s) in zip(dets_second, scores_second)]\n         else:\n             detections_second = []\n         r_tracked_stracks = [strack_pool[i] for i in u_track if strack_pool[i].state == TrackState.Tracked]\n",add,Added note about dates .
4b8e4528794973e24a0eb620bd88847fda16f237,fix bug for trt version 7,reid_export.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nprint(ROOT)\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom yolov5.utils.torch_uti","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nprint(ROOT)\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom yolov5.utils.torch_uti","@@ -182,7 +182,7 @@ def export_engine(model, im, file, half, dynamic, simplify, workspace=4, verbose\n         if trt.__version__[0] == '7':  # TensorRT 7 handling https://github.com/ultralytics/yolov5/issues/6012\n             grid = model.model[-1].anchor_grid\n             model.model[-1].anchor_grid = [a[..., :1, :1, :] for a in grid]\n-            export_onnx(model, im, file, 12, train, dynamic, simplify)  # opset 12\n+            export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12\n             model.model[-1].anchor_grid = grid\n         else:  # TensorRT >= 8\n             check_version(trt.__version__, '8.0.0', hard=True)  # require tensorrt>=8.0.0\n@@ -251,7 +251,7 @@ if __name__ == """"__main__"""":\n     parser.add_argument('--include',\n                         nargs='+',\n                         default=['torchscript'],\n-                        help='torchscript, onnx, openvino, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs')\n+                        help='torchscript, onnx, openvino, engine')\n     args = parser.parse_args()\n \n     t = time.time()\n@@ -295,7 +295,7 @@ if __name__ == """"__main__"""":\n     if jit:\n         f[0] = export_torchscript(model, im, args.weights, args.optimize)  # opset 12\n     if engine:  # TensorRT required before ONNX\n-        f[1] = export_engine(model, im, f, args.half, args.dynamic, args.simplify, args.workspace, args.verbose)\n+        f[1] = export_engine(model, im, args.weights, args.half, args.dynamic, args.simplify, args.workspace, args.verbose)\n     if onnx:  # OpenVINO requires ONNX\n         f[2] = export_onnx(model, im, args.weights, args.opset, args.dynamic, args.simplify)  # opset 12\n     if openvino:\n",add,Add note about data volume to enable_metrics_collection
ec90d9e3776312ceb345d5054390f4d4be75a6c3,fix dynamic onnx export arguments,reid_export.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nprint(ROOT)\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom yolov5.utils.torch_uti","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nprint(ROOT)\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom yolov5.utils.torch_uti","@@ -92,6 +92,10 @@ def export_onnx(model, im, file, opset, dynamic, simplify, prefix=colorstr('ONNX\n \n         f = file.with_suffix('.onnx')\n         LOGGER.info(f'\n{prefix} starting export with onnx {onnx.__version__}...')\n+        \n+        if dynamic:\n+            dynamic = {'images': {0: 'batch'}}  # shape(1,3,640,640)\n+            dynamic['output'] = {0: 'batch'}  # shape(1,25200,85)\n \n         torch.onnx.export(\n             model.cpu() if dynamic else model,  # --dynamic only compatible with cpu\n@@ -122,10 +126,11 @@ def export_onnx(model, im, file, opset, dynamic, simplify, prefix=colorstr('ONNX\n                 onnx.save(model_onnx, f)\n             except Exception as e:\n                 LOGGER.info(f'simplifier failure: {e}')\n+        LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n+        return f\n     except Exception as e:\n         LOGGER.info(f'export failure: {e}')\n-    LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n-    return f\n+    \n         \n         \n def export_openvino(file, half, prefix=colorstr('OpenVINO:')):\n@@ -167,9 +172,8 @@ def export_tflite(file, half, prefix=colorstr('TFLite:')):\n         LOGGER.info(f'\n{prefix} export failure: {e}')\n         \n         \n-def export_engine(model, im, file, half, dynamic, simplify, workspace=4, verbose=False):\n+def export_engine(model, im, file, half, dynamic, simplify, workspace=4, verbose=False, prefix=colorstr('TensorRT:')):\n     # YOLOv5 TensorRT export https://developer.nvidia.com/tensorrt\n-    prefix = colorstr('TensorRT:')\n     try:\n         assert im.device.type != 'cpu', 'export running on CPU but must be on GPU, i.e. `python export.py --device 0`'\n         try:\n",add,Added TypeSpec . h for dev
db790624801ff4879f2948e56b693edaf221fef1,fix caemra update check,track.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strong_sort'))  # add strong_sort ROOT to PATH\nif str(ROOT / 'trackers' / 'ocsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'o","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strong_sort'))  # add strong_sort ROOT to PATH\nif str(ROOT / 'trackers' / 'ocsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'o","@@ -178,7 +178,7 @@ def run(\n             annotator = Annotator(im0, line_width=line_thickness, pil=not ascii)\n             \n             if prev_frames[i] is not None and curr_frames[i] is not None:  # camera motion compensation\n-                if hasattr(tracker_list[i].tracker, camera_update)\n+                if hasattr(tracker_list[i].tracker, 'camera_update'):\n                     tracker_list[i].tracker.camera_update(prev_frames[i], curr_frames[i])\n \n             if det is not None and len(det):\n",add,Add note about data volume to enable_metrics_collection
c4e27cdc9cab6f3763e4546aa41f1d5df89a015d,fix caemra update check,track.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strong_sort'))  # add strong_sort ROOT to PATH\nif str(ROOT / 'trackers' / 'ocsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'o","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strong_sort'))  # add strong_sort ROOT to PATH\nif str(ROOT / 'trackers' / 'ocsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'o","@@ -178,7 +178,7 @@ def run(\n             annotator = Annotator(im0, line_width=line_thickness, pil=not ascii)\n             \n             if prev_frames[i] is not None and curr_frames[i] is not None:  # camera motion compensation\n-                if hasattr(tracker_list[i].tracker, 'camera_update'):\n+                if hasattr(tracker_list[i], 'tracker') and hasattr(tracker_list[i].tracker, 'camera_update'):\n                     tracker_list[i].tracker.camera_update(prev_frames[i], curr_frames[i])\n \n             if det is not None and len(det):\n",update,Add note about data volume to enable_metrics_collection
4ce526504fd4726cf59c19250ca8d017e8c91acc,fix passing of args to tracking subprocess,val.py,"import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom track import run\n\n\ndef setup_evaluation(dst_val_tools_folder, benchmark):\n    \n    # source: https://github.com/JonathonLuiten/TrackEval#official-evaluation-code\","import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom track import run\n\n\ndef setup_evaluation(dst_val_tools_folder, benchmark):\n    \n    # source: https://github.com/JonathonLuiten/TrackEval#official-evaluation-code\","@@ -60,7 +60,7 @@ def setup_evaluation(dst_val_tools_folder, benchmark):\n def parse_opt():\n     parser = argparse.ArgumentParser()\n     parser.add_argument('--yolo-weights', nargs='+', type=str, default=WEIGHTS / 'crowdhuman_yolov5m.pt', help='model.pt path(s)')\n-    parser.add_argument('--appearance-descriptor-weights', type=str, default=WEIGHTS / 'mobilenetv2_x1_0_msmt17.pt')\n+    parser.add_argument('--reid-weights', type=str, default=WEIGHTS / 'mobilenetv2_x1_0_msmt17.pt')\n     parser.add_argument('--tracking-method', type=str, default='strongsort', help='strongsort, ocsort')\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n     parser.add_argument('--project', default=ROOT / 'runs/track', help='save results to project/name')\n@@ -125,8 +125,8 @@ def main(opt):\n                 """"--yolo-weights"""", opt.yolo_weights,\\n                 """"--reid-weights"""",  opt.reid_weights,\\n                 """"--tracking-method"""", opt.tracking_method,\\n-                """"--conf-thres"""", opt.conf_thres,\\n-                """"--imgsz"""", opt.imgsz,\\n+                """"--conf-thres"""", str(opt.conf_thres),\\n+                """"--imgsz"""", str(opt.imgsz[0]),\\n                 """"--classes"""", str(0),\\n                 """"--name"""", save_dir.name,\\n                 """"--project"""", opt.project,\\n",add,Add note about data volume to enable_metrics_collection
f58a621ab7e1f5920f5b5d44a3ef49dd05306afa,MOT17 is now default,val.py,"import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom track import run\n\n\ndef setup_evaluation(dst_val_tools_folder, benchmark):\n    \n    # source: https://github.com/JonathonLuiten/TrackEval#official-evaluation-code\","import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom track import run\n\n\ndef setup_evaluation(dst_val_tools_folder, benchmark):\n    \n    # source: https://github.com/JonathonLuiten/TrackEval#official-evaluation-code\","@@ -65,7 +65,7 @@ def parse_opt():\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n     parser.add_argument('--project', default=ROOT / 'runs/track', help='save results to project/name')\n     parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n-    parser.add_argument('--benchmark', type=str,  default='MOT16', help='MOT16, MOT17, MOT20')\n+    parser.add_argument('--benchmark', type=str,  default='MOT17', help='MOT16, MOT17, MOT20')\n     parser.add_argument('--split', type=str,  default='train', help='existing project/name ok, do not increment')\n     parser.add_argument('--eval-existing', type=str, default='', help='evaluate existing tracker results under mot_callenge/MOTXX-YY/...')\n \n",add,Added curContext . blendFunc to fake - dom .
924f5ecf2878b536752aa2b7ca11039d8738dc73,fixed list bug when passing to subprocess,val.py,"import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom yolov5.utils.torch_utils import select_device\nfrom track import run\n\n\ndef download_official_mot_eval_tool(dst_val_tools_folder):\n    # source: https://github.com/","import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom yolov5.utils.torch_utils import select_device\nfrom track import run\n\n\ndef download_official_mot_eval_tool(dst_val_tools_folder):\n    # source: https://github.com/","@@ -64,7 +64,7 @@ def download_mot_dataset(dst_val_tools_folder, benchmark):\n     \n def parse_opt():\n     parser = argparse.ArgumentParser()\n-    parser.add_argument('--yolo-weights', nargs='+', type=str, default=WEIGHTS / 'crowdhuman_yolov5m.pt', help='model.pt path(s)')\n+    parser.add_argument('--yolo-weights', type=str, default=WEIGHTS / 'crowdhuman_yolov5m.pt', help='model.pt path(s)')\n     parser.add_argument('--reid-weights', type=str, default=WEIGHTS / 'osnet_x1_0_dukemtmcreid.pt')\n     parser.add_argument('--tracking-method', type=str, default='strongsort', help='strongsort, ocsort')\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n@@ -142,22 +142,22 @@ def main(opt):\n             dst_seq_path = seq_path.parent / seq_path.parent.name\n             if not dst_seq_path.is_dir():\n                 src_seq_path = seq_path\n-                shutil.move(str(src_seq_path), str(dst_seq_path))\n-\n+                shutil.move(str(src_seq_path), str(dst_seq_path))   \n+            \n             p = subprocess.Popen([\n-                """"python"""", """"track.py"""",\\n-                """"--yolo-weights"""", opt.yolo_weights,\\n-                """"--reid-weights"""",  opt.reid_weights,\\n-                """"--tracking-method"""", opt.tracking_method,\\n-                """"--conf-thres"""", str(opt.conf_thres),\\n-                """"--imgsz"""", str(opt.imgsz[0]),\\n-                """"--classes"""", str(0),\\n-                """"--name"""", save_dir.name,\\n-                """"--project"""", opt.project,\\n-                """"--device"""", str(tracking_subprocess_device),\\n-                """"--source"""", dst_seq_path,\\n-                """"--exist-ok"""",\\n-                """"--save-txt"""",\\n+                """"python"""", """"track.py"""", \\n+                """"--yolo-weights"""", opt.yolo_weights, \\n+                """"--reid-weights"""",  opt.reid_weights, \\n+                """"--tracking-method"""", opt.tracking_method, \\n+                """"--conf-thres"""", str(op",add,Add warning about data volume to enable_metrics_collection
92cbfbc9e15c6d1968538e3e32253ef2fd4e6874,Fixed validation script so it can run on Windows. Added some things to gitignore as well.,.gitignore,.vscode/\n\n# interpreter bytecode\n__pycache__/\n\n# exports\n*_openvino_model\n*.torchscript\n*.pt\n*.onnx\n*.engine,.*/\n.*\n\n# interpreter bytecode\n__pycache__/\n\n# exports\n*_openvino_model\n*.torchscript\n*.pt\n*.onnx\n*.engine\n\n*.pyc\n\nruns/\nval_utils/\nvenv/,"@@ -1,4 +1,5 @@\n-.vscode/\n+.*/\n+.*\n \n # interpreter bytecode\n __pycache__/\n@@ -8,4 +9,10 @@ __pycache__/\n *.torchscript\n *.pt\n *.onnx\n-*.engine\n\ No newline at end of file\n+*.engine\n+\n+*.pyc\n+\n+runs/\n+val_utils/\n+venv/\n\ No newline at end of file\n",add,Add note about data volume to enable EGL
92cbfbc9e15c6d1968538e3e32253ef2fd4e6874,Fixed validation script so it can run on Windows. Added some things to gitignore as well.,val.py,"import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom yolov5.utils.torch_utils import select_device\nfrom track import run\n\n\ndef download_official_mot_eval_tool(dst_val_tools_folder):\n    # source: https://github.com/","import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom yolov5.utils.torch_utils import select_device\nfrom track import run\n\n\ndef download_official_mot_eval_tool(dst_val_tools_folder):\n    # source: https://github.com/","@@ -40,23 +40,37 @@ def download_official_mot_eval_tool(dst_val_tools_folder):\n         LOGGER.info('Eval repo already downloaded')\n         \n def download_mot_dataset(dst_val_tools_folder, benchmark):\n+    import wget\n+    import ssl\n+\n+    try:\n+        _create_unverified_https_context = ssl._create_unverified_context\n+    except AttributeError:\n+        # Legacy Python that doesn't verify HTTPS certificates by default\n+        pass\n+    else:\n+        # Handle target environment that doesn't support HTTPS verification\n+        ssl._create_default_https_context = _create_unverified_https_context\n+\n     gt_data_url = 'https://omnomnom.vision.rwth-aachen.de/data/TrackEval/data.zip'\n-    subprocess.run([""""wget"""", """"-nc"""", gt_data_url, """"-O"""", dst_val_tools_folder / 'data.zip']) # python module has no -nc nor -N flag\n     if not (dst_val_tools_folder / 'data').is_dir():\n+        wget.download(gt_data_url, out=str(dst_val_tools_folder / 'data.zip'))\n         with zipfile.ZipFile(dst_val_tools_folder / 'data.zip', 'r') as zip_ref:\n             zip_ref.extractall(dst_val_tools_folder)\n+        os.remove(dst_val_tools_folder / 'data.zip') \n         LOGGER.info('MOTs ground truth downloaded')\n     else:\n         LOGGER.info('gt already downloaded')\n \n     mot_gt_data_url = 'https://motchallenge.net/data/' + benchmark + '.zip'\n-    subprocess.run([""""wget"""", """"-nc"""", mot_gt_data_url, """"-O"""", dst_val_tools_folder / (benchmark + '.zip')]) # python module has no -nc nor -N flag\n     if not (dst_val_tools_folder / 'data' / benchmark).is_dir():\n+        wget.download(mot_gt_data_url, out=str(dst_val_tools_folder / (benchmark + '.zip')))\n         with zipfile.ZipFile(dst_val_tools_folder / (benchmark + '.zip'), 'r') as zip_ref:\n             if opt.benchmark == 'MOT16':\n                 zip_ref.extractall(dst_val_tools_folder / 'data' / 'MOT16')\n             else:\n                 zip_ref.extractall(dst_val_tools_folder / 'data')\n+        os.remo",add,Add note about data volume to enable_metrics_collection
c00b8c6c0fbe71888b61d2a565eb073506db1eeb,"Fix for windows, also added download flag file so I don't have to have large zip files clogging up my storage space",val.py,"import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom yolov5.utils.torch_utils import select_device\nfrom track import run\n\n\ndef download_official_mot_eval_tool(dst_val_tools_folder):\n    # source: https://github.com/","import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom yolov5.utils.torch_utils import select_device\nfrom track import run\n\n\ndef download_official_mot_eval_tool(val_tools_target_location):\n    # s","@@ -11,7 +11,7 @@ import zipfile\n from pathlib import Path\n import shutil\n import threading\n-\n+from tqdm import tqdm\n \n FILE = Path(__file__).resolve()\n ROOT = FILE.parents[0]  # yolov5 strongsort root directory\n@@ -30,52 +30,73 @@ from yolov5.utils.torch_utils import select_device\n from track import run\n \n \n-def download_official_mot_eval_tool(dst_val_tools_folder):\n+def download_official_mot_eval_tool(val_tools_target_location):\n     # source: https://github.com/JonathonLuiten/TrackEval#official-evaluation-code\n     val_tools_url = """"https://github.com/JonathonLuiten/TrackEval""""\n     try:\n-        Repo.clone_from(val_tools_url, dst_val_tools_folder)\n+        Repo.clone_from(val_tools_url, val_tools_target_location)\n         LOGGER.info('Official MOT evaluation repo downloaded')\n     except git.exc.GitError as err:\n         LOGGER.info('Eval repo already downloaded')\n         \n-def download_mot_dataset(dst_val_tools_folder, benchmark):\n-    import wget\n-    import ssl\n-\n-    try:\n-        _create_unverified_https_context = ssl._create_unverified_context\n-    except AttributeError:\n-        # Legacy Python that doesn't verify HTTPS certificates by default\n-        pass\n-    else:\n-        # Handle target environment that doesn't support HTTPS verification\n-        ssl._create_default_https_context = _create_unverified_https_context\n+def download_mot_dataset(val_tools_target_location, benchmark):\n \n-    gt_data_url = 'https://omnomnom.vision.rwth-aachen.de/data/TrackEval/data.zip'\n-    if not (dst_val_tools_folder / 'data').is_dir():\n-        wget.download(gt_data_url, out=str(dst_val_tools_folder / 'data.zip'))\n-        with zipfile.ZipFile(dst_val_tools_folder / 'data.zip', 'r') as zip_ref:\n-            zip_ref.extractall(dst_val_tools_folder)\n-        os.remove(dst_val_tools_folder / 'data.zip') \n-        LOGGER.info('MOTs ground truth downloaded')\n-    else:\n-        LOGGER.info('gt already downloaded')\n+    download",add,Add note about data volume to enable_metrics_collection
6be5e2d7acc00c2d6a3d57130c761244ee94a237,smaller ReID model as default,track_seg.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strong_sort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom yolov5.models.co","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strong_sort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom yolov5.models.co","@@ -282,7 +282,7 @@ def run(\n \n def parse_opt():\n     parser = argparse.ArgumentParser()\n-    parser.add_argument('--yolo-weights', nargs='+', type=Path, default=WEIGHTS / 'yolov5x-seg.pt', help='model.pt path(s)')\n+    parser.add_argument('--yolo-weights', nargs='+', type=Path, default=WEIGHTS / 'yolov5s-seg.pt', help='model.pt path(s)')\n     parser.add_argument('--reid-weights', type=Path, default=WEIGHTS / 'osnet_x0_25_msmt17.pt')\n     parser.add_argument('--tracking-method', type=str, default='strongsort', help='strongsort, ocsort, bytetrack')\n     parser.add_argument('--source', type=str, default='0', help='file/dir/URL/glob, 0 for webcam')  \n",add,Add Table about data volume to enable_metrics_collection
bd3c96845fc74c515dd7c074ccace63abcfc4363,fix timer when no detections,track.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strong_sort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strong_sort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom","@@ -279,7 +279,7 @@ def run(\n             prev_frames[i] = curr_frames[i]\n             \n         # Print total time (preprocessing + inference + NMS + tracking)\n-        LOGGER.info(f""""{s}{'' if len(det) else '(no detections), '}{sum([dt.dt for dt in dt]) * 1E3:.1f}ms"""")\n+        LOGGER.info(f""""{s}{'' if len(det) else '(no detections), '}{sum([dt.dt for dt in dt if hasattr(dt, 'dt')]) * 1E3:.1f}ms"""")\n \n     # Print results\n     t = tuple(x.t / seen * 1E3 for x in dt)  # speeds per image\n",add,Add
b86dc4c4030949d6b8fdc972e404918aabcaec50,fix save crop bug,track.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strong_sort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strong_sort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom","@@ -214,7 +214,7 @@ def run(\n                 if len(outputs[i]) > 0:\n                     for j, (output) in enumerate(outputs[i]):\n     \n-                        bboxes = output[0:4]\n+                        bbox = output[0:4]\n                         id = output[4]\n                         cls = output[5]\n                         conf = output[6]\n@@ -236,14 +236,14 @@ def run(\n                             label = None if hide_labels else (f'{id} {names[c]}' if hide_conf else \\n                                 (f'{id} {conf:.2f}' if hide_class else f'{id} {names[c]} {conf:.2f}'))\n                             color = colors(c, True)\n-                            annotator.box_label(bboxes, label, color=color)\n+                            annotator.box_label(bbox, label, color=color)\n \n                             if save_trajectories and tracking_method == 'strongsort':\n                                 q = output[7]\n                                 tracker_list[i].trajectory(im0, q, color=color)\n                             if save_crop:\n                                 txt_file_name = txt_file_name if (isinstance(path, list) and len(path) > 1) else ''\n-                                save_one_box(bboxes, imc, file=save_dir / 'crops' / txt_file_name / names[c] / f'{id}' / f'{p.stem}.jpg', BGR=True)\n+                                save_one_box(bbox.astype(np.int16), imc, file=save_dir / 'crops' / txt_file_name / names[c] / f'{id}' / f'{p.stem}.jpg', BGR=True)\n                 \n             else:\n                 pass\n",add,Added the UNSTARTED state to the YouTube PlayerState enum
1de57db175c0674ecb31d24e8fb3416c12f2881e,ValueError: setting an array element with a sequence.,trackers/strong_sort/strong_sort.py,"import numpy as np\nimport torch\nimport sys\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists, join\nimport torchvision.transforms as transforms\n\nfrom sort.nn_matching import NearestNeighborDistanceMetric\nfrom sort.detection import Detection\nfrom sort.tracker import Tracker\n\nfrom reid_multibackend import ReIDDetectMultiBackend\n\nfrom yolov5.utils.general import xyxy2xywh\n\n\nclass StrongSORT(object):\n    def __init__(self, \n                 model_weights,\n                 device,\n                 fp16,\n                 max_dist=0.2,\n                 max_iou_distance=0.7,\n                 max_age=70,\n                 max_unmatched_preds=7,\n                 n_init=3,\n                 nn_budget=100,\n                 mc_lambda=0.995,\n                 ema_alpha=0.9\n                ):\n\n        self.model = ReIDDetectMultiBackend(weights=model_weights, device=device, fp16=fp16)\n        \n        self.max_dist = max_dist\n        metric = NearestNeig","import numpy as np\nimport torch\nimport sys\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists, join\nimport torchvision.transforms as transforms\n\nfrom sort.nn_matching import NearestNeighborDistanceMetric\nfrom sort.detection import Detection\nfrom sort.tracker import Tracker\n\nfrom reid_multibackend import ReIDDetectMultiBackend\n\nfrom yolov5.utils.general import xyxy2xywh\n\n\nclass StrongSORT(object):\n    def __init__(self, \n                 model_weights,\n                 device,\n                 fp16,\n                 max_dist=0.2,\n                 max_iou_distance=0.7,\n                 max_age=70,\n                 max_unmatched_preds=7,\n                 n_init=3,\n                 nn_budget=100,\n                 mc_lambda=0.995,\n                 ema_alpha=0.9\n                ):\n\n        self.model = ReIDDetectMultiBackend(weights=model_weights, device=device, fp16=fp16)\n        \n        self.max_dist = max_dist\n        metric = NearestNeig","@@ -76,7 +76,7 @@ class StrongSORT(object):\n             class_id = track.class_id\n             conf = track.conf\n             queue = track.q\n-            outputs.append(np.array([x1, y1, x2, y2, track_id, class_id, conf, queue]))\n+            outputs.append(np.array([x1, y1, x2, y2, track_id, class_id, conf, queue], dtype=object))\n         if len(outputs) > 0:\n             outputs = np.stack(outputs, axis=0)\n         return outputs\n",add,Added back playInBackground property in js
36a537399304b723b20f2e69f5fbcb3cfb13ab91,MOT17 default,val.py,"import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nfrom io import StringIO\nimport git\nimport yaml\nimport optuna\nimport re\nimport pandas as pd\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom yolov5.utils.torch_utils import select_device\nfrom track imp","import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nfrom io import StringIO\nimport git\nimport yaml\nimport optuna\nimport re\nimport pandas as pd\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom yolov5.utils.torch_utils import select_device\nfrom track imp","@@ -45,7 +45,7 @@ def parse_opt():\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n     parser.add_argument('--project', default=ROOT / 'runs/track', help='save results to project/name')\n     parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n-    parser.add_argument('--benchmark', type=str,  default='MOT17-copy', help='MOT16, MOT17, MOT20')\n+    parser.add_argument('--benchmark', type=str,  default='MOT17', help='MOT16, MOT17, MOT20')\n     parser.add_argument('--split', type=str,  default='train', help='existing project/name ok, do not increment')\n     parser.add_argument('--eval-existing', type=str, default='', help='evaluate existing tracker results under mot_callenge/MOTXX-YY/...')\n     parser.add_argument('--conf-thres', type=float, default=0.45, help='confidence threshold')\n@@ -306,7 +306,7 @@ class Objective(Evaluator):\n                 \n         elif self.opt.tracking_method == 'ocsort':\n             \n-            self.opt.conf_thres = trial.suggest_float(""""conf_thres"""", 0.4, 0.5)\n+            self.opt.conf_thres = trial.suggest_float(""""conf_thres"""", 0.35, 0.55)\n             self.iou_thresh = trial.suggest_float(""""iou_thresh"""", 0.1, 0.4)\n             self.use_byte = trial.suggest_categorical(""""use_byte"""", [True, False])\n             \n",add,Don ' t define the right button twice .
f667d60013aa844d3b0e5d798039a22ad4776b2e,fix ocsrot search sapce,val.py,"import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nfrom io import StringIO\nimport git\nimport yaml\nimport optuna\nimport re\nimport pandas as pd\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom yolov5.utils.torch_utils import select_device\nfrom track imp","import os\nimport sys\nimport torch\nimport logging\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nfrom io import StringIO\nimport git\nimport yaml\nimport optuna\nimport re\nimport pandas as pd\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nimport threading\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'strong_sort') not in sys.path:\n    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom yolov5.utils.general import LOGGER, check_requirements, print_args, increment_path\nfrom yolov5.utils.torch_utils import select_device\nfrom track imp","@@ -308,11 +308,11 @@ class Objective(Evaluator):\n             \n             self.opt.conf_thres = trial.suggest_float(""""conf_thres"""", 0.35, 0.55)\n             max_age = trial.suggest_int(""""max_age"""", 10, 60, step=10)\n-            min_hits = trial.suggest_int(""""min_hits"""", 1, 65, step=1)\n+            min_hits = trial.suggest_int(""""min_hits"""", 1, 5, step=1)\n             iou_thresh = trial.suggest_float(""""iou_thresh"""", 0.1, 0.4)\n             delta_t = trial.suggest_int(""""delta_t"""", 1, 5, step=1)\n             asso_func = trial.suggest_categorical(""""asso_func"""", ['iou', 'giou'])\n-            inertia = trial.suggest_float(""""inertia"""", 0.1, 0.4, step=0.1)\n+            inertia = trial.suggest_float(""""inertia"""", 0.1, 0.4)\n             use_byte = trial.suggest_categorical(""""use_byte"""", [True, False])\n             \n             d['OCSORT'] = \\n",add,Added curContext . blendFunc to fake - dom .
aace5e705f32d36878ab3c105545e4085c2117cf,fix trackeval path issues,val.py,"""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport optuna\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PA","""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport optuna\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PA","@@ -143,7 +143,7 @@ class Evaluator:\n             seq_paths = [p / 'img1' for p in Path(mot_seqs_path).iterdir() if Path(p).is_dir()]\n         \n         save_dir = increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok)  # increment run\n-        MOT_results_folder = val_tools_path / 'data' / 'trackers' / 'mot_challenge' / Path(str(opt.benchmark) + '-' + str(opt.split)) / save_dir.name / 'data'\n+        MOT_results_folder = val_tools_path / 'data' / 'trackers' / 'mot_challenge' / opt.benchmark / save_dir.name / 'data'\n         (MOT_results_folder).mkdir(parents=True, exist_ok=True)  # make \n         return seq_paths, save_dir, MOT_results_folder\n \n@@ -238,18 +238,20 @@ class Evaluator:\n                 dst = MOT_results_folder / Path(src.stem + '.txt')\n             dst.parent.mkdir(parents=True, exist_ok=True)  # make\n             shutil.copyfile(src, dst)\n-\n         # run the evaluation on the generated txts\n         d = [seq_path.parent.name for seq_path in seq_paths]\n         p = subprocess.run(\n             args=[\n                 sys.executable,  val_tools_path / """"scripts/run_mot_challenge.py"""",\n+                """"--GT_FOLDER"""", val_tools_path / 'data' / self.opt.benchmark / self.opt.split,\n                 """"--BENCHMARK"""", self.opt.benchmark,\n-                """"--TRACKERS_TO_EVAL"""",  self.opt.eval_existing if self.opt.eval_existing else MOT_results_folder.parent.name,\n+                """"--TRACKERS_TO_EVAL"""",  self.opt.eval_existing if self.opt.eval_existing else self.opt.benchmark,\n                 """"--SPLIT_TO_EVAL"""", """"train"""",\n                 """"--METRICS"""", """"HOTA"""", """"CLEAR"""", """"Identity"""",\n                 """"--USE_PARALLEL"""", """"True"""",\n+                """"--TRACKER_SUB_FOLDER"""", str(Path(*Path(MOT_results_folder).parts[-2:])),\n                 """"--NUM_PARALLEL_CORES"""", """"4"""",\n+                """"--SKIP_SPLIT_FOL"""", """"True"""",\n                 """"--SEQ_INFO""""] + d,\n             universal_newlines=True,\n             s",add,Add KHR_gl_texture_2D_image extension string .
647b67c5163a0ab8f7bdb990ed4701da25818c53,fix mc_lambda search space,evolve.py,"""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport joblib\nimport yaml\nimport optuna\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif ","""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport joblib\nimport yaml\nimport optuna\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif ","@@ -79,9 +79,9 @@ class Objective(Evaluator):\n             ema_alpha = trial.suggest_float(""""ema_alpha"""", 0.7, 0.95)\n             max_dist = trial.suggest_float(""""max_dist"""", 0.1, 0.4)\n             max_iou_dist = trial.suggest_float(""""max_iou_dist"""", 0.5, 0.9)\n-            max_age = trial.suggest_int(""""max_age"""", 10, 200, step=10)\n+            max_age = trial.suggest_int(""""max_age"""", 10, 150, step=10)\n             n_init = trial.suggest_int(""""n_init"""", 1, 3, step=1)\n-            mc_lambda = trial.suggest_categorical(""""mc_lambda"""", [0.995])\n+            mc_lambda = trial.suggest_float(""""mc_lambda"""", 0.90, 0.999)\n             nn_budget = trial.suggest_categorical(""""nn_budget"""", [100])\n             max_unmatched_preds = trial.suggest_categorical(""""max_unmatched_preds"""", [0])\n \n",add,Add note about data volume to enable_metrics_collection
5a5f40a19bc6a94452d960701ea6678a68a6821f,fixpath agnostic bug fix,evolve.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport joblib\nimport yaml\nimport optuna\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport joblib\nimport yaml\nimport optuna\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT","@@ -231,7 +231,7 @@ def parse_opt():\n     parser.add_argument('--tracking-method', type=str, default='strongsort', help='strongsort, ocsort')\n     parser.add_argument('--tracking-config', type=Path, default=None)\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n-    parser.add_argument('--project', default=ROOT / 'runs/evolve', help='save results to project/name')\n+    parser.add_argument('--project', default=ROOT / 'runs' / 'evolve', help='save results to project/name')\n     parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n     parser.add_argument('--benchmark', type=str,  default='MOT17', help='MOT16, MOT17, MOT20')\n     parser.add_argument('--split', type=str,  default='train', help='existing project/name ok, do not increment')\n",add,Added information on how to set port
5a5f40a19bc6a94452d960701ea6678a68a6821f,fixpath agnostic bug fix,track.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom y","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom y","@@ -63,7 +63,7 @@ def run(\n         augment=False,  # augmented inference\n         visualize=False,  # visualize features\n         update=False,  # update all models\n-        project=ROOT / 'runs/track',  # save results to project/name\n+        project=ROOT / 'runs' / 'track',  # save results to project/name\n         name='exp',  # save results to project/name\n         exist_ok=False,  # existing project/name ok, do not increment\n         line_thickness=2,  # bounding box thickness (pixels)\n@@ -285,7 +285,7 @@ def run(\n     t = tuple(x.t / seen * 1E3 for x in dt)  # speeds per image\n     LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS, %.1fms {tracking_method} update per image at shape {(1, 3, *imgsz)}' % t)\n     if save_txt or save_vid:\n-        s = f""""\n{len(list(save_dir.glob('tracks/*.txt')))} tracks saved to {save_dir / 'tracks'}"""" if save_txt else ''\n+        s = f""""\n{len(list((save_dir / 'tracks').glob('*.txt')))} tracks saved to {save_dir / 'tracks'}"""" if save_txt else ''\n         LOGGER.info(f""""Results saved to {colorstr('bold', save_dir)}{s}"""")\n     if update:\n         strip_optimizer(yolo_weights)  # update model (to fix SourceChangeWarning)\n@@ -316,7 +316,7 @@ def parse_opt():\n     parser.add_argument('--augment', action='store_true', help='augmented inference')\n     parser.add_argument('--visualize', action='store_true', help='visualize features')\n     parser.add_argument('--update', action='store_true', help='update all models')\n-    parser.add_argument('--project', default=ROOT / 'runs/track', help='save results to project/name')\n+    parser.add_argument('--project', default=ROOT / 'runs' / 'track', help='save results to project/name')\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n     parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n     parser.add_argument('--line-thickness', default=2, type=int, help='bo",add,Added Type name for global pattern
5a5f40a19bc6a94452d960701ea6678a68a6821f,fixpath agnostic bug fix,val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport optuna\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(s","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport optuna\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(s","@@ -238,7 +238,7 @@ class Evaluator:\n         d = [seq_path.parent.name for seq_path in seq_paths]\n         p = subprocess.run(\n             args=[\n-                sys.executable,  val_tools_path / """"scripts/run_mot_challenge.py"""",\n+                sys.executable,  val_tools_path / 'scripts' / 'run_mot_challenge.py',\n                 """"--GT_FOLDER"""", val_tools_path / 'data' / self.opt.benchmark / self.opt.split,\n                 """"--BENCHMARK"""", self.opt.benchmark,\n                 """"--TRACKERS_TO_EVAL"""",  self.opt.eval_existing if self.opt.eval_existing else self.opt.benchmark,\n@@ -293,7 +293,7 @@ def parse_opt():\n     parser.add_argument('--tracking-method', type=str, default='strongsort', help='strongsort, ocsort')\n     parser.add_argument('--tracking-config', type=Path, default=None)\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n-    parser.add_argument('--project', default=ROOT / 'runs/val', help='save results to project/name')\n+    parser.add_argument('--project', default=ROOT / 'runs' / 'val', help='save results to project/name')\n     parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n     parser.add_argument('--benchmark', type=str,  default='MOT17', help='MOT16, MOT17, MOT20')\n     parser.add_argument('--split', type=str,  default='train', help='existing project/name ok, do not increment')\n",add,Add note about data volume to enable_metrics_collection
e52d0d1015d1ec3f991a681fc1ae5030dd474abe,fix broken import,track.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom y","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom y","@@ -35,7 +35,7 @@ from yolov5.utils.general import (LOGGER, Profile, check_img_size, non_max_suppr\n                                   check_imshow, xyxy2xywh, increment_path, strip_optimizer, colorstr, print_args, check_file)\n from yolov5.utils.torch_utils import select_device, time_sync\n from yolov5.utils.plots import Annotator, colors, save_one_box\n-from utils.segment.general import masks2segments, process_mask, process_mask_native\n+from yolov5.utils.segment.general import masks2segments, process_mask, process_mask_native\n from trackers.multi_tracker_zoo import create_tracker\n \n \n",add,Added KHR_gl_texture_2D_image extension string
18c000ffe64ba17c666ccb26943c500934ee1d6a,"Bug Fix to show both masks and boxes at same time

Bug Fix to show both masks and boxes at same time in the result video. It seems like that annotator.masks() will override the whole image, so maybe it should be placed before all of operations to draw boxes. After this change, video can show both masks and boxes at same time",track.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom y","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom y","@@ -205,8 +205,16 @@ def run(\n                 \n                 # draw boxes for visualization\n                 if len(outputs[i]) > 0:\n+                    if save_vid and is_seg:\n+                        # Mask plotting\n+                        annotator.masks(\n+                            masks,\n+                            colors=[colors(x, True) for x in det[:, 5]],\n+                            im_gpu=torch.as_tensor(im0, dtype=torch.float16).to(device).permute(2, 0, 1).flip(0).contiguous() /\n+                            255 if retina_masks else im[i]\n+                        )\n                     for j, (output) in enumerate(outputs[i]):\n-    \n+                        \n                         bbox = output[0:4]\n                         id = output[4]\n                         cls = output[5]\n@@ -230,14 +238,6 @@ def run(\n                                 (f'{id} {conf:.2f}' if hide_class else f'{id} {names[c]} {conf:.2f}'))\n                             color = colors(c, True)\n                             annotator.box_label(bbox, label, color=color)\n-                            if is_seg:\n-                                # Mask plotting\n-                                annotator.masks(\n-                                    masks,\n-                                    colors=[colors(x, True) for x in det[:, 5]],\n-                                    im_gpu=torch.as_tensor(im0, dtype=torch.float16).to(device).permute(2, 0, 1).flip(0).contiguous() /\n-                                    255 if retina_masks else im[i]\n-                                )\n                             if save_trajectories and tracking_method == 'strongsort':\n                                 q = output[7]\n                                 tracker_list[i].trajectory(im0, q, color=color)\n",add,Added the user for to extensions
c4d9291f80209f771fda0b0cb0f2a35c3c603318,AttributeError: 'list' object has no attribute 'astype',track.py,"import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom y","import argparse\n\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom y","@@ -243,7 +243,7 @@ def run(\n                                 tracker_list[i].trajectory(im0, q, color=color)\n                             if save_crop:\n                                 txt_file_name = txt_file_name if (isinstance(path, list) and len(path) > 1) else ''\n-                                save_one_box(bbox.astype(np.int16), imc, file=save_dir / 'crops' / txt_file_name / names[c] / f'{id}' / f'{p.stem}.jpg', BGR=True)\n+                                save_one_box(np.array(bbox, dtype=np.int16), imc, file=save_dir / 'crops' / txt_file_name / names[c] / f'{id}' / f'{p.stem}.jpg', BGR=True)\n                             \n             else:\n                 pass\n",fix,Add note about data volume to enable_metrics_collection
da44cf33780ce0faf7b4bde95d5fe2d30264e25d,fix,trackers/botsort/bot_sort.py,"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import deque\n\nfrom trackers.botsort import  matching\nfrom trackers.botsort.gmc import GMC\nfrom trackers.botsort.basetrack import BaseTrack, TrackState\nfrom trackers.botsort.kalman_filter import KalmanFilter\n\n# from fast_reid.fast_reid_interfece import FastReIDInterface\n\nfrom trackers.strong_sort.reid_multibackend import ReIDDetectMultiBackend\nfrom yolov5.utils.general import xyxy2xywh, xywh2xyxy\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        self.update_cls(cls, score)\n\n        self.score = score\n        self.tracklet_len = 0\n\","import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import deque\n\nfrom trackers.botsort import  matching\nfrom trackers.botsort.gmc import GMC\nfrom trackers.botsort.basetrack import BaseTrack, TrackState\nfrom trackers.botsort.kalman_filter import KalmanFilter\n\n# from fast_reid.fast_reid_interfece import FastReIDInterface\n\nfrom reid_multibackend import ReIDDetectMultiBackend\nfrom yolov5.utils.general import xyxy2xywh, xywh2xyxy\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        self.update_cls(cls, score)\n\n        self.score = score\n        self.tracklet_len = 0\n\n        self.smoot","@@ -10,7 +10,7 @@ from trackers.botsort.kalman_filter import KalmanFilter\n \n # from fast_reid.fast_reid_interfece import FastReIDInterface\n \n-from trackers.strong_sort.reid_multibackend import ReIDDetectMultiBackend\n+from reid_multibackend import ReIDDetectMultiBackend\n from yolov5.utils.general import xyxy2xywh, xywh2xyxy\n \n class STrack(BaseTrack):\n@@ -19,7 +19,7 @@ class STrack(BaseTrack):\n     def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n \n         # wait activate\n-        self._tlwh = np.asarray(tlwh, dtype=np.float)\n+        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n         self.kalman_filter = None\n         self.mean, self.covariance = None, None\n         self.is_activated = False\n",add,Add note about data volume to enable
ab728b7de9ad169866b1a5b5a394f7aa8ac101db,fix model type bug,trackers/strongsort/reid_multibackend.py,"import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nfrom itertools import islice\nimport torchvision.transforms as transforms\nimport cv2\nimport sys\nimport torchvision.transforms as T\nfrom collections import OrderedDict, namedtuple\nimport gdown\nfrom os.path import exists as file_exists\n\n\nfrom yolov8.ultralytics.yolo.utils.checks import check_requirements, check_version\nfrom yolov8.ultralytics.yolo.utils import LOGGER\nfrom trackers.strongsort.deep.reid_model_factory import (show_downloadeable_models, get_model_url, get_model_name,\n                                                          download_url, load_pretrained_weights)\nfrom trackers.strongsort.deep.models import build_model\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [f","import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nfrom itertools import islice\nimport torchvision.transforms as transforms\nimport cv2\nimport sys\nimport torchvision.transforms as T\nfrom collections import OrderedDict, namedtuple\nimport gdown\nfrom os.path import exists as file_exists\n\n\nfrom yolov8.ultralytics.yolo.utils.checks import check_requirements, check_version\nfrom yolov8.ultralytics.yolo.utils import LOGGER\nfrom trackers.strongsort.deep.reid_model_factory import (show_downloadeable_models, get_model_url, get_model_name,\n                                                          download_url, load_pretrained_weights)\nfrom trackers.strongsort.deep.models import build_model\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [f","@@ -36,8 +36,7 @@ class ReIDDetectMultiBackend(nn.Module):\n         super().__init__()\n \n         w = weights[0] if isinstance(weights, list) else weights\n-        self.pt, self.jit, self.onnx, self.xml, self.engine, self.coreml, self.saved_model, \\n-            self.pb, self.tflite, self.edgetpu, self.tfjs, self.paddle = self.model_type(w)  # get backend\n+        self.pt, self.jit, self.onnx, self.xml, self.engine, self.tflite = self.model_type(w)  # get backend\n         self.fp16 = fp16\n         self.fp16 &= self.pt or self.jit or self.engine  # FP16\n \n@@ -165,11 +164,10 @@ class ReIDDetectMultiBackend(nn.Module):\n     @staticmethod\n     def model_type(p='path/to/model.pt'):\n         # Return model type from model path, i.e. path='path/to/model.onnx' -> type=onnx\n-        from reid_export import export_formats\n+        from trackers.reid_export import export_formats\n         sf = list(export_formats().Suffix)  # export suffixes\n         check_suffix(p, sf)  # checks\n         types = [s in Path(p).name for s in sf]\n-        types[8] &= not types[9]  # tflite &= not edgetpu\n         return types\n \n     def _preprocess(self, im_batch):\n@@ -232,7 +230,7 @@ class ReIDDetectMultiBackend(nn.Module):\n \n     def warmup(self, imgsz=[(256, 128, 3)]):\n         # Warmup model by running inference once\n-        warmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb\n+        warmup_types = self.pt, self.jit, self.onnx, self.engine, self.tflite\n         if any(warmup_types) and self.device.type != 'cpu':\n             im = [np.empty(*imgsz).astype(np.uint8)]  # input\n             for _ in range(2 if self.jit else 1):  #\n",add,Add note about data volume to enable pre - dom API
e3e9190a8082e057e163aeab5748a5ab53f2b2de,delete debug print,evolve.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport joblib\nimport yaml\nimport optuna\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport joblib\nimport yaml\nimport optuna\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT","@@ -270,7 +270,6 @@ class ContinuousStudySave:\n         self.tracking_method = tracking_method\n         \n     def __call__(self, study, trial):\n-        print('SAVIIIIING\n\n')\n         joblib.dump(study, opt.tracking_method + """"_study.pkl"""")\n \n     \n@@ -304,4 +303,4 @@ if __name__ == """"__main__"""":\n     save_plots(opt, study, opt.objectives)\n     print_best_trial_metric_results(study, opt.objectives)\n \n-        \n\ No newline at end of file\n+        \n",add,Added example annotation
b1e8a737a04700ab2e83dd233c92b6e171713a3e,fix bug showing bboxes and seg simultaneously,track.py,"import argparse\nimport cv2\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport loggi","import argparse\nimport cv2\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport loggi","@@ -224,6 +224,15 @@ def run(\n                 # draw boxes for visualization\n                 if len(outputs[i]) > 0:\n                     \n+                    if is_seg:\n+                        # Mask plotting\n+                        annotator.masks(\n+                            masks[i],\n+                            colors=[colors(x, True) for x in det[:, 5]],\n+                            im_gpu=torch.as_tensor(im0, dtype=torch.float16).to(device).permute(2, 0, 1).flip(0).contiguous() /\n+                            255 if retina_masks else im[i]\n+                        )\n+                    \n                     for j, (output) in enumerate(outputs[i]):\n                         \n                         bbox = output[0:4]\n@@ -249,14 +258,7 @@ def run(\n                                 (f'{id} {conf:.2f}' if hide_class else f'{id} {names[c]} {conf:.2f}'))\n                             color = colors(c, True)\n                             annotator.box_label(bbox, label, color=color)\n-                            if is_seg:\n-                                    # Mask plotting\n-                                annotator.masks(\n-                                    masks[i],\n-                                    colors=[colors(x, True) for x in det[:, 5]],\n-                                    im_gpu=torch.as_tensor(im0, dtype=torch.float16).to(device).permute(2, 0, 1).flip(0).contiguous() /\n-                                    255 if retina_masks else im[i]\n-                                )\n+                            \n                             if save_trajectories and tracking_method == 'strongsort':\n                                 q = output[7]\n                                 tracker_list[i].trajectory(im0, q, color=color)\n",add,Added the user for to extensions
8423d5dd18c41e6f98d05ff6d688c4ed33b2bc77,fix engine track bug,track.py,"import argparse\nimport cv2\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport loggi","import argparse\nimport cv2\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport loggi","@@ -106,6 +106,7 @@ def run(\n     imgsz = check_imgsz(imgsz, stride=stride)  # check image size\n \n     # Dataloader\n+    bs = 1\n     if webcam:\n         show_vid = check_imshow(warn=True)\n         dataset = LoadStreams(\n@@ -116,7 +117,7 @@ def run(\n             transforms=getattr(model.model, 'transforms', None),\n             vid_stride=vid_stride\n         )\n-        nr_sources = len(dataset)\n+        bs = len(dataset)\n     else:\n         dataset = LoadImages(\n             source,\n@@ -126,24 +127,23 @@ def run(\n             transforms=getattr(model.model, 'transforms', None),\n             vid_stride=vid_stride\n         )\n-        nr_sources = 1\n-    vid_path, vid_writer, txt_path = [None] * nr_sources, [None] * nr_sources, [None] * nr_sources\n+    vid_path, vid_writer, txt_path = [None] * bs, [None] * bs, [None] * bs\n     model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n \n     # Create as many strong sort instances as there are video sources\n     tracker_list = []\n-    for i in range(nr_sources):\n+    for i in range(bs):\n         tracker = create_tracker(tracking_method, tracking_config, reid_weights, device, half)\n         tracker_list.append(tracker, )\n         if hasattr(tracker_list[i], 'model'):\n             if hasattr(tracker_list[i].model, 'warmup'):\n                 tracker_list[i].model.warmup()\n-    outputs = [None] * nr_sources\n+    outputs = [None] * bs\n \n     # Run tracking\n     #model.warmup(imgsz=(1 if pt else nr_sources, 3, *imgsz))  # warmup\n     seen, windows, dt = 0, [], (Profile(), Profile(), Profile(), Profile())\n-    curr_frames, prev_frames = [None] * nr_sources, [None] * nr_sources\n+    curr_frames, prev_frames = [None] * bs, [None] * bs\n     for frame_idx, batch in enumerate(dataset):\n         path, im, im0s, vid_cap, s = batch\n         visualize = increment_path(save_dir / Path(path[0]).stem, mkdir=True) if visualize else False\n@@ -170,7 +170,7 @@ def run(\n         # P",fix,Add note about data volume to enable_metrics_collection
821872282ca3565bdff92120390d1573fc0f9016,fix engine track bug,track.py,"import argparse\nimport cv2\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport loggi","import argparse\nimport cv2\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport loggi","@@ -141,7 +141,7 @@ def run(\n     outputs = [None] * bs\n \n     # Run tracking\n-    #model.warmup(imgsz=(1 if pt else nr_sources, 3, *imgsz))  # warmup\n+    #model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))  # warmup\n     seen, windows, dt = 0, [], (Profile(), Profile(), Profile(), Profile())\n     curr_frames, prev_frames = [None] * bs, [None] * bs\n     for frame_idx, batch in enumerate(dataset):\n",add,Add note about data volume to enable_metrics_collection
9922d3532b21d436636d2ea98c7e92e5337bbc56,fix yolov8 compatibility,val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport yaml\nimport optuna\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport yaml\nimport optuna\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.","@@ -292,7 +292,7 @@ class Evaluator:\n \n def parse_opt():\n     parser = argparse.ArgumentParser()\n-    parser.add_argument('--yolo-weights', type=str, default=WEIGHTS / 'crowdhuman_yolov5m.pt', help='model.pt path(s)')\n+    parser.add_argument('--yolo-weights', type=str, default=WEIGHTS / 'yolov8n.pt', help='model.pt path(s)')\n     parser.add_argument('--reid-weights', type=str, default=WEIGHTS / 'osnet_x1_0_dukemtmcreid.pt')\n     parser.add_argument('--tracking-method', type=str, default='strongsort', help='strongsort, ocsort')\n     parser.add_argument('--tracking-config', type=Path, default=None)\n",add,Added information on how to set port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-02-FRCNN/MOT17-02-FRCNN/000001.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 8""""            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? ZZZ+ \n(PRÓ©)-RhÓ¨RRHbE6M@RP) 7) ZJ Ju%>Â(E%)i)(hb\nJGtOiP@GRQ k :|4+I Jî®^*EttÇ?W5ZSj@ mSZTOZ XT9Dkk 84Z uÚ_Ò©PKn ßµ]z |T? ß£OQ K JLdj&?"""" FERWUoOÃ® \n O g#FS/ai ç_vRlIeÆ AV?FxZjzB~!]Q	9 Ys*|g?Ü^WA?A Ø _W~Ð·}ZoEbx6ÊOhoE\O^`#Ô¹*<Ð§GJj:?cÌÃ¶yÇÏ½NK\n>K8Wìª²Þµq12 y+[TI|I h}F	9,ÌÞg5] cÛ¿r(]ÕeR'Ü?WGWODICX'm|5eÈgs+Ec>A}Ooj#""""f59<)s^h,Q</>o[_Tx<Þa,4pÅ¾(tVi  é~5it%Í~ÑµZO9W@O3]I×{;{Ëß±,N 55{fMÛ¤0g{]""""{[2/q1Ð{+_#]n *u[j i[So(È©Hu_ZÃ²]'{GB4M5Ú¾|8d$ß½6B{C^=ÍºyOJ_h{N=^9s+'J4m4;5}Z>._ÄE",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-02-FRCNN/MOT17-02-FRCNN/000001.jpg differ\n,add,Added the user group to the contributor list
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-02-FRCNN/MOT17-02-FRCNN/000002.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 8""""            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? ZZZ+ \n(PRÓª%>Z@:J)Z)(iRQA""""h1JJ:M@}2@LRS(`QI@-%QE0\nJGtOiP@GRQ k :|4+I Jî®^*EttÇ?W5ZSj@ mSZTOZ XT9Dkk 84Z uÚ_Ò©PKn ßµ]z |T? ß£OQ K JLdj&?"""" FERWUoOÃ® \n O g#FS/ai ç_vRlIeÆ AV?FxZjzB~!]Q	9 Ys*|g?Ü^WA?A Ø _W~Ð·}ZoEbx6ÊOhoE\O^`#Ô¹*< = ~<=3vhk#WuDcFk#@x5q12 Ø«AÉ¤ oq1yp8(5{] czR(ZuÕ¬| f _+SÓ¸mK%xaGc^SX:×¬\>?<O('2YnAQg/;}yqy_y\"""":- LVpI7 d c6È·Ú¿_ÂO{OYW""""> VA+_x5;}1DvGu0'S WG'I?7z""""9U,Nx>tRÚ¥8Ü¬qx[]M9!?5b B~j$ /Î®]xz+{x 6LRSP.mcOlx _×MN;BUKË_w'& g]XgoÞS GL=SÞI{<e]dxg|kkJD",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-02-FRCNN/MOT17-02-FRCNN/000002.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-02-FRCNN/MOT17-02-FRCNN/000003.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 8""""            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? ZZZ+ \n(PRJ}%:CJ)ZJ(QE!E:i):I@Å¦MS) QE(E%% -Q@%#'~  >Qt %DWO /\::cÎ+ -)\t6 -*'-Û¬*v9Dkk 84_K(Ôº Ot=*""""Û®7W_ ' >?C~ wz-o_] #VH W^ß£O)~òº~xG\/?<{ @  {6q(]|) ?1?,? |\\|4e<[ A=#qF'l^ \n3_Ò¿' SMkZkfy.+mg_nOWS~Ð¿WÔ \n.l _KCGQ&$ys75v^ /5.a{Ê¯:Dy6h16	:cGOJÎºs>%g4oy#Ux5Q1wAZi?kW8KxednQÞ¡M^Ø#[Û×%RZ	^ q~TNO&D_ÂUSX6ZÞ´?nHÌ-x?Ò}QEw?lØg""""ÝºN=/""""6\I""""\6 m{W$ vAd ÇÉ¿u;:KX_jnx""""2wy0zui/is$T?hq+`OÞ§{xwÊ¯*S)Ö°h1j2jt{%aqwTzÅµ'>ap LMfÚ¼	'Do6 \n^kQFT}(È¹K~E GC=w zÞ¿%r:M× ay_w'tÔ³kÙ±9O=PÆ\Ç© vE2|qÒ¼'Æ¾5]4m4;jO:VLS-d^yW< *KÓ¼5rif",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-02-FRCNN/MOT17-02-FRCNN/000003.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-02-FRCNN/MOT17-02-FRCNN/000004.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 8""""            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? ZZZ+ \n(R)ii`JZB@-SÔ´PE-ShJM(P!ÔmMLbh)Ô (`QI@E%% (P T  u3#NT â«\GLyz%s[ 6F?DuNF84u0. j]IuT wMÒu =)j < o s ~W  kt /^  Ft=S<_Y _ 'Õg!'ÂI 0hß¸OjoS>?,? |6Q4y?y&e>=!? \nWby~M_<U AM+ """"N5i Zgì¢½ <x*3 -5I <eF 'JN 9.ßW?Ko , R|JGuOxAtdÞ{y){]NHÓ¬~ÆpNQ\JP>qKY_uz_GIá«º>=qÂÝ´_i,Ò MR}:oD\K63Z+oc#Ý¹Öµmux	!DD/oFz,tw5DOCqKÞ¿^9[#Ò®%Sf=o(SF}OPFQ""""I8<$K6~k'.<Í¹7 ySJÈ'Í¿ÛsÞ¢Z×®-_Ù¢7Ãz|=mk/OwG~È¸KO)Þ§& r/,NH%[l?rvJ>:-titOHm^""""7yVpbkyRdÜ¿QDRQK""""{{{m}`q ^}SÉ¯K!QO*A5{t/  uk<_khog×l}sU}Oq<kX^G3?wÉò½tÇ",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-02-FRCNN/MOT17-02-FRCNN/000004.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-02-FRCNN/det/det.txt,,"69,-1,912.8,482.9,97.6,112.6,1\n69,-1,835.8,472.2,53.7,77,1\n69,-1,374.4,447.1,43.4,105.1,1\n69,-1,1261,447.7,34.9,100.6,1\n69,-1,419,458.4,42.7,85.3,1\n69,-1,501.4,440.4,129.3,324,1\n69,-1,1543.7,429.8,52,127.2,1\n69,-1,1088.7,482.7,35.6,117.1,1\n69,-1,1004.6,441.5,42.1,112,1\n69,-1,619.2,441.2,96.2,290,1\n69,-1,1052.2,482.2,39.6,112.4,1\n69,-1,796.3,475.5,56.9,62.8,1\n69,-1,696.6,454.2,76.1,219.9,0.998\n69,-1,1095.8,442.2,39.7,108.8,0.998\n69,-1,475.5,465.6,33.4,97.9,0.996\n517,-1,382.8,463.7,51.1,115.9,1\n517,-1,738.7,437.9,107.2,259.8,1\n517,-1,916,442.8,142.5,282,1\n517,-1,835.9,473.4,53.6,75.5,1\n517,-1,1556.7,370.3,307.6,710.7,1\n517,-1,689.5,448.5,39.9,124.9,1\n517,-1,1330.7,427.2,247.2,644,1\n517,-1,572.5,458,32.7,81.8,1\n517,-1,1179.3,447.9,35.3,86.5,1\n517,-1,1048.5,436.3,101.7,243.9,1\n517,-1,643.9,453.7,33.3,96.6,1\n517,-1,539.9,462.1,29.1,74.9,0.993\n517,-1,459.4,460.2,25.8,73.9,0.979\n517,-1,412.1,471.5,48.5,97.6,0.421\n436,-1,916.6,483.9,92.1,110.8,1\n436,-1,543.2,440,1","@@ -0,0 +1,8186 @@\n+69,-1,912.8,482.9,97.6,112.6,1\n+69,-1,835.8,472.2,53.7,77,1\n+69,-1,374.4,447.1,43.4,105.1,1\n+69,-1,1261,447.7,34.9,100.6,1\n+69,-1,419,458.4,42.7,85.3,1\n+69,-1,501.4,440.4,129.3,324,1\n+69,-1,1543.7,429.8,52,127.2,1\n+69,-1,1088.7,482.7,35.6,117.1,1\n+69,-1,1004.6,441.5,42.1,112,1\n+69,-1,619.2,441.2,96.2,290,1\n+69,-1,1052.2,482.2,39.6,112.4,1\n+69,-1,796.3,475.5,56.9,62.8,1\n+69,-1,696.6,454.2,76.1,219.9,0.998\n+69,-1,1095.8,442.2,39.7,108.8,0.998\n+69,-1,475.5,465.6,33.4,97.9,0.996\n+517,-1,382.8,463.7,51.1,115.9,1\n+517,-1,738.7,437.9,107.2,259.8,1\n+517,-1,916,442.8,142.5,282,1\n+517,-1,835.9,473.4,53.6,75.5,1\n+517,-1,1556.7,370.3,307.6,710.7,1\n+517,-1,689.5,448.5,39.9,124.9,1\n+517,-1,1330.7,427.2,247.2,644,1\n+517,-1,572.5,458,32.7,81.8,1\n+517,-1,1179.3,447.9,35.3,86.5,1\n+517,-1,1048.5,436.3,101.7,243.9,1\n+517,-1,643.9,453.7,33.3,96.6,1\n+517,-1,539.9,462.1,29.1,74.9,0.993\n+517,-1,459.4,460.2,25.8,73.9,0.979\n+517,-1,412.1,471.5,48.5,97.6,0.421\n+436,-1,916.6,483.9,92.1,110.8,1\n+436,-1,543.2,440,145.2,314.8,1\n+436,-1,421.5,435.7,125,312.3,1\n+436,-1,756,414.4,104.2,315.5,1\n+436,-1,1090.8,417.1,200.9,635.1,0.999\n+436,-1,1286.1,300.3,479.9,780.7,0.998\n+436,-1,696.2,429.5,72.8,285.6,0.996\n+436,-1,528.8,466.7,24.2,71.6,0.306\n+294,-1,752.6,445,65.1,198,1\n+294,-1,1517.6,430.2,241.1,461.2,1\n+294,-1,508.5,455.6,35.5,109.2,1\n+294,-1,1003.3,408,156.3,613,1\n+294,-1,1159.8,444.8,47,110,1\n+294,-1,856.1,432.9,168.6,495.6,1\n+294,-1,1359.1,441.9,51.6,109.5,1\n+294,-1,410.1,470.4,49.1,104.4,1\n+294,-1,659.9,460.9,25.8,72.7,1\n+294,-1,382.3,463.2,44.8,111.7,0.999\n+294,-1,1399.5,425.4,151.8,291.2,0.999\n+294,-1,574.7,463,26.6,59.4,0.99\n+294,-1,441.3,462.8,32.6,96.9,0.94\n+294,-1,557,462.1,26.2,62.5,0.899\n+294,-1,597,460,22.8,55.7,0.788\n+294,-1,828.1,481.5,53.4,67.1,0.295\n+294,-1,472,458,35.2,94.6,0.186\n+294,-1,534.9,455.6,37.4,70.6,0.177\n+385,-1,835.1,472.5,53.5,76.9,1\n+385,-1,1083.5,400.3,207.8,626.1,1\n+385,-1,3,438.1,204.6,",add,Added STORM - 862 to Changelog
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-02-FRCNN/gt/gt.txt,,"1,1,912,484,97,109,0,7,1.0\n2,1,912,484,97,109,0,7,1.0\n3,1,912,484,97,109,0,7,1.0\n4,1,912,484,97,109,0,7,1.0\n1,2,1338,418,167,379,1,1,1.0\n2,2,1342,417,168,380,1,1,1.0\n3,2,1346,417,170,380,1,1,1.0\n4,2,1351,417,171,381,1,1,1.0\n1,3,586,447,85,263,1,1,1.0\n2,3,586,446,85,264,1,1,1.0\n3,3,586,446,85,264,1,1,1.0\n4,3,586,446,85,264,1,1,1.0\n1,4,1585,-1,336,578,0,9,0.98153\n2,4,1585,-1,336,578,0,9,0.97777\n3,4,1585,-1,336,578,0,9,0.974\n4,4,1585,-1,336,578,0,9,0.97023\n1,5,1163,441,33,89,0,8,1.0\n2,5,1163,441,33,89,0,8,1.0\n3,5,1163,441,33,89,0,8,1.0\n4,5,1163,441,33,89,0,8,1.0\n1,6,1308,431,34,118,0,8,0.85714\n2,6,1308,431,34,118,0,8,0.97143\n3,6,1308,431,34,118,0,8,1.0\n4,6,1308,431,34,118,0,8,1.0\n1,8,1416,431,184,336,1,1,0.51351\n2,8,1422,431,183,337,1,1,0.5163\n3,8,1428,431,182,338,1,1,0.51366\n4,8,1434,431,181,339,1,1,0.51099\n1,9,1056,484,36,110,1,1,0.94595\n2,9,1055,483,36,110,1,1,0.94643\n3,9,1055,483,36,110,1,1,0.94643\n4,9,1055,483,36,110,1,1,0.94643\n1,10,1091,484,31,115,1,","@@ -0,0 +1,156 @@\n+1,1,912,484,97,109,0,7,1.0\n+2,1,912,484,97,109,0,7,1.0\n+3,1,912,484,97,109,0,7,1.0\n+4,1,912,484,97,109,0,7,1.0\n+1,2,1338,418,167,379,1,1,1.0\n+2,2,1342,417,168,380,1,1,1.0\n+3,2,1346,417,170,380,1,1,1.0\n+4,2,1351,417,171,381,1,1,1.0\n+1,3,586,447,85,263,1,1,1.0\n+2,3,586,446,85,264,1,1,1.0\n+3,3,586,446,85,264,1,1,1.0\n+4,3,586,446,85,264,1,1,1.0\n+1,4,1585,-1,336,578,0,9,0.98153\n+2,4,1585,-1,336,578,0,9,0.97777\n+3,4,1585,-1,336,578,0,9,0.974\n+4,4,1585,-1,336,578,0,9,0.97023\n+1,5,1163,441,33,89,0,8,1.0\n+2,5,1163,441,33,89,0,8,1.0\n+3,5,1163,441,33,89,0,8,1.0\n+4,5,1163,441,33,89,0,8,1.0\n+1,6,1308,431,34,118,0,8,0.85714\n+2,6,1308,431,34,118,0,8,0.97143\n+3,6,1308,431,34,118,0,8,1.0\n+4,6,1308,431,34,118,0,8,1.0\n+1,8,1416,431,184,336,1,1,0.51351\n+2,8,1422,431,183,337,1,1,0.5163\n+3,8,1428,431,182,338,1,1,0.51366\n+4,8,1434,431,181,339,1,1,0.51099\n+1,9,1056,484,36,110,1,1,0.94595\n+2,9,1055,483,36,110,1,1,0.94643\n+3,9,1055,483,36,110,1,1,0.94643\n+4,9,1055,483,36,110,1,1,0.94643\n+1,10,1091,484,31,115,1,1,1.0\n+2,10,1090,484,32,114,1,1,1.0\n+3,10,1090,484,32,114,1,1,1.0\n+4,10,1090,484,32,114,1,1,1.0\n+1,11,734,487,29,68,0,2,0.31884\n+2,11,733,487,30,68,0,2,0.31884\n+3,11,733,487,30,69,0,2,0.31429\n+4,11,732,487,31,69,0,2,0.31429\n+1,12,679,492,53,105,0,2,0.30573\n+2,12,679,492,52,105,0,2,0.29263\n+3,12,679,492,52,105,0,2,0.28622\n+4,12,679,492,52,105,0,2,0.28622\n+1,13,738,458,27,75,0,2,0.3985\n+2,13,737,457,27,75,0,2,0.40273\n+3,13,737,457,27,75,0,2,0.40273\n+4,13,737,457,27,75,0,2,0.40273\n+1,14,1255,447,33,100,1,1,1.0\n+2,14,1255,447,33,100,1,1,1.0\n+3,14,1255,447,33,100,1,1,1.0\n+4,14,1255,447,33,100,1,1,1.0\n+1,15,1016,430,40,116,1,1,0.98687\n+2,15,1015,430,40,116,1,1,0.98666\n+3,15,1015,430,40,116,1,1,0.98666\n+4,15,1015,431,40,116,1,1,0.98645\n+1,17,1101,441,38,108,1,1,0.65843\n+2,17,1100,440,38,108,1,1,0.64832\n+3,17,1100,440,38,108,1,1,0.64832\n+4,17,1100,440,38,108,1,1,0.64832\n+1,18,935,436,42,114,1,1,0.41739\n+2,18,934,4",add,Added STORM - 862 to Changelog
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-02-FRCNN/seqinfo.ini,,[Sequence]\nname=MOT17-02-FRCNN\nimDir=img1\nframeRate=30\nseqLength=600\nimWidth=1920\nimHeight=1080\nimExt=.jpg\n\n,"@@ -0,0 +1,9 @@\n+[Sequence]\n+name=MOT17-02-FRCNN\n+imDir=img1\n+frameRate=30\n+seqLength=600\n+imWidth=1920\n+imHeight=1080\n+imExt=.jpg\n+\n",add,Add note about data volume to enable
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000001.jpg,," Lavc55.69.100  C 					\n\n\n          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? },cOZBÙ­L?ST<ImO?kÌªc?Gl{pAm&1Õ°Ö¶|+ x5xaFsE*07|j	`æ¢»ajv#zW5|Ugu,2y <W=E#zTwf; \nc&&M*bF|oAUGoU7BI=R.u$Õµw*XÔ®M?xZy2K<Xv<C{F$8.crm[L$TmÜ[x1i}\n_`Ói+näª.(0ä¯z""""q.mO76EZZNi%2\p`n2Kj\pP7uckbl6`lV>iZE p~clhMA=DI17~r:q+lXÊÞ qG_Vu'ex8u8 3]}7Ì¡$wÇ®=+OZ/q#H)\n[?WPq-u#cn7ZÊ´]JÑ¥hj)~F50;+JÐ»A&Õ¤7""""nxlA4\n?Ñ®Ò½Z_ÃTy_Ä»?T ÍpImÍÅ¥~X p>XiyÝ½-IQÚMÍ®!;G&ÚJr\nrF?:enadZ@""""Kt-4AW$1la`%ysÔr^MvVam?r2z/KIUl O×jvZj	d_xb	ÃÒiJ7L~?kjtu&8çm\}^//\ÃµONC =\nm/""""V_R èeWî§·|U4e96cÖ°aÔÎw;Í`8>}JpKzK#GÄ©QOÒª:r1hv×³Am/Ç¥H#[SlN0S7*=H49 >fQç¶|'EÊ»Z#@ëtijë­=lJ/QhÌ£9",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000001.jpg differ\n,add,Added Type name for DFI ( # 3 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000002.jpg,," Lavc55.69.100  C 					\n\n\n          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? }+>X3LÙ­L?ST|IoMjLÆ­ F+\$Â/Pfz E^G>xmxZlJ ZUUcFu.iQ|i?=q=Va2""""Q>sz7Iz@<ÂjY%OxN m1Â\Þw.dBDvÒ¿(ã·¥T_9V8aÛ§OzUSI>StZX%]NÛ#v^Þ¥`-*r#8f wf>If'8hW=Ø¯i%2_Ã½z0ÎK]~@z=)6D\Ùµ?k?4rbp0wZHKH!sÂ\.^kmqICk4Õ}q?^`><1m:&61	x\}I~Yn?93+lXT.ToQWÕY^0nË­jp|@ nd#=qZ^Ôµ%/i]ÂaRx`>.N;ï·X7Irku+*J0Y5#]Lt(qsMmUi)/SKBx_}Mf#	×¯Óq ( F_?Ji>MS_VS3	?wG,b<aT {})6ÑZkt8N}&I*c,1ÒJRnÎucmt3-Y:Ë¸""""}-JYEHÂd zg=f(Weo:Þ»q=Dæª-|CFu=+SImV 9$~>?u <-5[anqM=PtB=8_(VszÙ0d] tg5V]SÍm5ç+d,jO9@Â«Ë²%`0q{z~.~!j_nIU~vZ<d%\n1#9å¬Kki6;\n]I/1,:KO|ziN~WW<m HUQÐ®^v.<2/9kyCqSjKo*Q*G"""":#i_JOqS[TzGRÛ¹",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000002.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000003.jpg,," Lavc55.69.100  C 					\n\n\n          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? }+>X3LÙ­L?ST|IoMjLÆ­ F+\,Â/Pfo[wWP~EF}ì¤Ö¶-`vV*zw1M4@Z~6D?Xq]pem(+aÚb9 E#jJ/S_n[lp7Ë¥$pHG8UG/Ê«l}>iUNU$NTQjWIkNccÂ¼_Z^^Û#Â»s7zb""""3)&å8âª{^\KbM-ß¼= WtCJ[.c#?z""""c?Î¹j~0~há½¤`î´jI)B ;t&\Rã51i@w\1u3YG|iaIm4vH9>Úµ&YÛ `OGN3mk*e8:JÖ¶]lKSc5AdÇ¥ixPÔµ=%/i]ÂaRx >$;@ï·X7Irk$j)\nç;.iZmJBj%%&p eG_0(Qp~}_C~$,@A'×<pAÂ¨RLuwmÝ;t>46qir2*|ÔªRwtÂ¬RC9Ñ¶>u7pÅ2}-JYp2:N TK =3Ú¶?XMlz#y{D-|3Fum+P[xUv]exxW /P& I O;Õ­&""""&l|pÅªNax;JÞ¾gPA9>íl{m9<VÇ±[$aOJ:2} \n~^Dè/x_= =~>+]Zì¤U=ØÂ{t	]I bXÔ°TNj}ËOOP_Ye\,\n(9*:=Ìv}$V-CqRz^'(n""""1KJOqJÔj='ÚÈÎ|,	s??55uAÂr_	_%Ò]p5]\JiEV|E*Ã¢3bÚ¼>æ>",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000003.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000004.jpg,," Lavc55.69.100  C 					\n\n\n          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? }+>1kiO5é§k7c3yp \n8MÙTo TÃ¶G3Ò·|r75xS1-*=7Ö¸qbX~j8XZa2#QÂ¹~Í©?XgÚ¨mVISk;m?[lp7É¥,pHG8UÇyR6B#JUbROTEE>vm6?xÇ/wcWkxSVU	$8,5>ÔX8â®PÈ¯.}]_=Uf9\74U×B8ã¿½z#c?\Ú9q8ohî´jLRJe\iÙe7mKX9FsÆs8ÔX?gFks7QÆ_P}SQ%=~s[g\nG)rÞ 38j88	^Ë®%@ì¦ºr8w2#?_jZaKv ~.NÐ½th13eV>Ò²m[T#aL|hA^m+^mrIHM?IIno	D/Iì°A ÆÃG5 QWKq=|:j=,rPA'×+kshy Â¨T3fx~\D1PEÅ¦oHSËº,1J)7~U:QLÊÖa>{uHi@8F@9UH#fq, lm% zuG\K/3j?#Ñ´SJ,{wVap_j|DgVUN>\ãQ&\VnSZ7ñ£""""?[ivQO)mkmfÕ­1jI;6IONLPs W%`0Q}e5#+d\npz	y>*ZOx#=Xv]ez?%ß.eq, \pOJ9ziN=>xUgieKn&2QO^IoFF?	 ×³à¿sTd6s^\nPR}-n5G{RHDsA.{YU ]lhp}p1\Y?_}(×Za]%Ä«VQEh",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000004.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000005.jpg,," Lavc55.69.100  C 					\n\n\n          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? }+AVksF;S L o0ÆºuU ??9ZH ?t>Oi2 kXz|:nG\F/1Xc}k =Tq]-63,0?/Â¹~nß½ rXdæ¹¨6)UTz w'V)5i7oØ¬pÝ´QUWÇ	BL}=AÊ¤]:QGO%qpÍ+Izw}+Cwio3 BÄ ,-$âªÈ­>}æ¶#hSQt;N\n9ÄAÒdN1AuÍG.'^HKH!sÂ\.^kmQICk5Íµ-ë""""txMÍ¥vv$tßjÑ¼=#_É©'|Û¿t'?Ò¶57VnRQWg0Ø]YÔ©grZ>kqedÇ¥i\nRÔ´v\n[;WPypviMMÖ²Rhiqc,#;+Í¥km.Pi)	F PdMn	$i8 \n\/uzCÃ¦7k>e91g8$*)%+iuiASkcFA7{%BE>MFÔiK//ÖJRnÎªu`c:@^&e@C[fî¢h=d%Å­QxÌ¸<.HistmGqiZsZ#:7n<c+W tC1jm%V$<Wj0DãfTcVnWZ7ñ""""?ki<ß­yÚ½F?-rN$ i8Su%EIbMA?4CÝ©hÍ´1 í-%Ngnl	qÓ¯KOJ9ziJ,ffWTifKd `zg'jK""""IaZá¶I>.(MJ\Ã¯[(r)>Æ·ZÔ`9 >/çµ	e^k5Æ\n|%|Ò]p5]lJiEV|E*Ã¢3bÚ¼BÏ«6?^é´G",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000005.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000006.jpg,," Lavc55.69.100  C 					\n\n\n          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? }+AVkwF;S L Oc0ÆºeWs1 H K(@ 9..e*7e?X=kso {{^hÂrcoIdT_ Qw7`Ì²6 Ð¿\n!Ú~2[kjY%OxN mÂ\Öv]	]OuU|y} *[JIÓ­tZiÒ¼_Ww{p#[Þ¥`2,X;uË¶o.u,zqU{Z|-MS\nO	DaÓ3O\GtU$A11GL× iI'  :iX/M$[ÙCqm;7ts49kt]Xî¸¸Ú½@>Ed^ñmy#DsØÑ¼=#_É©'|Û¿t'?Ò¶5jR8#bÎ¥H8 UG#Aì¦ºGXs(a;O_jZSc|id(QÔqjb4*!07ZÊ´ZÊPVÕ¸G1U3Ñ\niZmJBiZJKsL_}b>?(Qp~}_CÊ¾$Ý¬Í ×YuY * xÒvLr5:Kze%'pD `?#|EÅ¦mSËºG2{zRJMß¹N9lg/&?ï¡Ö´;Ûb Ö&5 F$j(kf([eaÌ?]vK4[Qh^6W5'eqz&éºb@DÔSÔ¯&`'_Ê£YOUp0y|M2 sUo<?>W;0$(Uv+/EE#+d_FVÔ¼e7^l<Â±-%OqÓ­~r>qs`I)G/M)S|]wlbÄ£*l.Fwxnkb):cu]6CxÇ½.a×­9a8SZGÑ³'tDtKg<Rçµ	e^mZ Bb/gOZë­0Ü®sbUO",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000006.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000007.jpg,," Lavc55.69.100  C 					\n\n\n          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? }+AVksF;S L _mT#]fU3NGV*sEVs!vS A5;]GxzurS^hÆ9Xc}k =Tq],3,0?/Â¹~nß½ Ú¨mVISl|;m?[lp5Ý¾d8Ò©_&\n4lØ\t<{ÒIÓ­uZi7p65#k9#5~ Ô¬FA%ëEs2ãª>Ô-æ¹oWÙz0'#Zuë²9${dtz#pprbË{Eu))%2.am;35Î°QÚ$k_K4Õ-ëb+'Jð´Ù­.G\[?7$Û¿v'?Ò¶O&MVWgp]iÕ©e""""â·}QjV^?:b;/ ZaKv<czidSÛ(GiU0BankÕ¬ZjÚ'LaÝm.Pi)	GG&p7Ù§} OaP+Ô¥85N|KLPsM&""""lX#ñ:|[%'!#M&qgLWe`:gÚ¦)7~M:QÌ¢@?*Ö´""""Í¹Òh?EuV7ED]Åk^(|Wc	YmH.qKÚCFuM+PkT{iK#Ê×ªh1ß«Z&?w'	È¯>Õ­.m;	ÛzT`ÕHt+H7sW×szx@""""NjP!Û}-Â1RT;9oOÂ©×#?^è¿sU4VpUn	y>*_Z>\nIsuyc+kIqÂ·;p,i>%^gS,IVUg?As-3L#:IW5iY>=y5OE""""m+Ê¯[(r)?(kuÔh~Zu*qe	sCË½Ö¢ >#|=%",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000007.jpg differ\n,add,Added Type name for DFI ( # 3 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000008.jpg,," Lavc55.69.100  C 					\n\n\n          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? }+AVW1BfÝ¦ZgMcÍ\[Q[2?G&=Æ«.qJ	R@SÒºoN_j~;NUlmiITc|8â»ZmAdmÂ¹z6%#-sPmTR6og}MoMv?mÂ\Öv]nBFvuQ|y} *Ñ9{RU$WrÖ:N-4Û8Ò¼_ww{poR[yd1\hg\OQ/~b%,BO.Þ¯\nO	DaÓ3O\GU]B#G{fJP?Î¹j~1~haIä¹oL:IGj\p9}.cxV6`;.6P;GÕ+~2cv[_?##~C~KÍ»hy ~s+dk\njÜ¥nU{{Ñ¿y1ugR}VN*Ô¬|}ru$un` \nî¥¨iq+&`;\nV7{rvH{7JP3MÖ³JÊmEEUpB6Ým.Pi(&:~4[iK	? 1op~}[C%][8&(cj9fÕ¦7	?q0Nñ¥u{-9T:?#Nm""""ERr. ]q<=)T&V\n63H&/.1wKQ 7i""""Ñ»I\n""""&@8×½lAy>.à¬¶<'Ì:6sÑ´SJ{Tx$W1<tT~'é·¹QV8Y1O^\jAD\gÒÃI|DIL2; RsUo> iC<#n>}b/,Gy$75I3A;z~%Ø_Yz_F_4PSÛ¿.RrXbp$X]6qqEgfWE9 ng9KOKiJ==YU&2*d*Ê£pneÔ´;Gw$ä¯¦kb| n#jMS{YÒ¼^\nPR~Q8Æ©mH",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-04-FRCNN/MOT17-04-FRCNN/000008.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-04-FRCNN/det/det.txt,,"375,-1,1222,31.4,61,118.8,1\n375,-1,686.4,206,79.7,113.1,1\n375,-1,1632.7,15.3,59.8,162.8,1\n375,-1,1079.3,447.2,68.5,202.7,1\n375,-1,358.8,110.2,55.7,176.5,1\n375,-1,445.8,308,73.6,214.7,1\n375,-1,285,130.4,54.7,168.9,1\n375,-1,828.5,1,55.8,113.1,1\n375,-1,109,350.7,53,189.7,1\n375,-1,455,545.4,71.9,233,1\n375,-1,1719.1,455,78.5,214.2,1\n375,-1,953.1,41.1,61.6,176.7,1\n375,-1,445.3,75.7,69.2,197,1\n375,-1,1590,632.3,83.1,243.1,1\n375,-1,795.9,150.3,59.3,171.1,1\n375,-1,348.2,529.4,88.2,246.3,1\n375,-1,508.4,86.8,51,168.9,1\n375,-1,215.8,139.5,58.3,158.4,1\n375,-1,1018.4,52.6,54.8,164.2,1\n375,-1,1434.3,2,52.3,97.2,1\n375,-1,562.9,566,99.4,253.7,1\n375,-1,1803.1,449.2,64.2,214,1\n375,-1,380.9,2.2,47.5,92.3,1\n375,-1,212.1,306.5,53.4,180.2,1\n375,-1,850.4,488.2,84.6,232.2,1\n375,-1,911.3,500.3,78.1,212.6,1\n375,-1,756.5,97.5,46.8,142.8,1\n375,-1,1767.7,104.3,53.6,154.4,0.999\n375,-1,1810.5,69.2,64.4,165.5,0.999\n375,-1,969.5,916.2,86.2,164.8,0.997\n242,-1,1220.8,30.7,63.6,119.6,1\n242,-","@@ -0,0 +1,28406 @@\n+375,-1,1222,31.4,61,118.8,1\n+375,-1,686.4,206,79.7,113.1,1\n+375,-1,1632.7,15.3,59.8,162.8,1\n+375,-1,1079.3,447.2,68.5,202.7,1\n+375,-1,358.8,110.2,55.7,176.5,1\n+375,-1,445.8,308,73.6,214.7,1\n+375,-1,285,130.4,54.7,168.9,1\n+375,-1,828.5,1,55.8,113.1,1\n+375,-1,109,350.7,53,189.7,1\n+375,-1,455,545.4,71.9,233,1\n+375,-1,1719.1,455,78.5,214.2,1\n+375,-1,953.1,41.1,61.6,176.7,1\n+375,-1,445.3,75.7,69.2,197,1\n+375,-1,1590,632.3,83.1,243.1,1\n+375,-1,795.9,150.3,59.3,171.1,1\n+375,-1,348.2,529.4,88.2,246.3,1\n+375,-1,508.4,86.8,51,168.9,1\n+375,-1,215.8,139.5,58.3,158.4,1\n+375,-1,1018.4,52.6,54.8,164.2,1\n+375,-1,1434.3,2,52.3,97.2,1\n+375,-1,562.9,566,99.4,253.7,1\n+375,-1,1803.1,449.2,64.2,214,1\n+375,-1,380.9,2.2,47.5,92.3,1\n+375,-1,212.1,306.5,53.4,180.2,1\n+375,-1,850.4,488.2,84.6,232.2,1\n+375,-1,911.3,500.3,78.1,212.6,1\n+375,-1,756.5,97.5,46.8,142.8,1\n+375,-1,1767.7,104.3,53.6,154.4,0.999\n+375,-1,1810.5,69.2,64.4,165.5,0.999\n+375,-1,969.5,916.2,86.2,164.8,0.997\n+242,-1,1220.8,30.7,63.6,119.6,1\n+242,-1,686.3,207.5,79.9,111.6,1\n+242,-1,1163.6,120.3,53,160.5,1\n+242,-1,795.4,150.9,59.8,176.4,1\n+242,-1,1592.6,2.1,57.8,145.1,1\n+242,-1,1483.3,59.7,52.9,151,1\n+242,-1,446.1,175.5,74.8,209.5,1\n+242,-1,1083.9,120.4,59.9,182.2,1\n+242,-1,1363.1,568,110.2,244.7,1\n+242,-1,288.8,127.8,55.1,172.2,1\n+242,-1,1720.8,457.1,77.9,212.3,1\n+242,-1,387.5,523.2,86.9,236.9,1\n+242,-1,109.3,350.3,49.7,191,1\n+242,-1,704.4,1,54.9,160.1,1\n+242,-1,355.5,104.7,54.2,179.6,1\n+242,-1,212.1,125.6,55.5,167.1,1\n+242,-1,929.4,120,69,185.2,1\n+242,-1,495.8,356.4,73.2,224.5,1\n+242,-1,222,475.7,62.3,204.3,1\n+242,-1,196.3,717.6,86,253.1,1\n+242,-1,844.9,345.4,79.5,193.6,1\n+242,-1,516.2,118.1,52.1,181,1\n+242,-1,589.8,366.6,61.8,194.9,1\n+242,-1,787.2,332.6,80.9,206,1\n+242,-1,111.2,563.6,87.4,243,0.999\n+242,-1,931,894.7,72.4,186.3,0.979\n+242,-1,156.7,667.7,61.7,198.4,0.089\n+583,-1,1221.4,30.7,61.8,119.6,1\n+583,-1,687,206.1,79.6,113.5,1\n+583,-1,1480.8,",add,Added STORM - 370 to Changelog
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-04-FRCNN/gt/gt.txt,,"1,1,1363,569,103,241,1,1,0.86014\n2,1,1362,568,103,241,1,1,0.86173\n3,1,1362,568,103,241,1,1,0.86173\n4,1,1362,568,103,241,1,1,0.86173\n5,1,1362,568,103,241,1,1,0.86173\n6,1,1362,568,103,241,1,1,0.86173\n7,1,1362,568,103,241,1,1,0.86173\n8,1,1362,568,103,241,1,1,0.86173\n1,2,371,410,80,239,1,1,1.0\n2,2,371,408,80,239,1,1,1.0\n3,2,372,407,80,239,1,1,1.0\n4,2,372,406,81,239,1,1,1.0\n5,2,373,405,81,239,1,1,1.0\n6,2,373,404,82,238,1,1,1.0\n7,2,374,403,81,238,1,1,1.0\n8,2,374,402,82,238,1,1,1.0\n1,3,103,549,83,251,1,1,1.0\n2,3,102,549,83,250,1,1,1.0\n3,3,102,549,83,250,1,1,1.0\n4,3,102,549,83,250,1,1,1.0\n5,3,102,549,83,250,1,1,1.0\n6,3,102,549,83,250,1,1,1.0\n7,3,102,549,83,250,1,1,1.0\n8,3,102,549,83,250,1,1,1.0\n1,4,1734,457,76,213,1,1,0.98386\n2,4,1733,457,76,212,1,1,0.97586\n3,4,1732,457,76,212,1,1,0.96781\n4,4,1731,457,76,212,1,1,0.95976\n5,4,1730,457,76,211,1,1,0.95185\n6,4,1730,457,75,211,1,1,0.95122\n7,4,1729,457,75,211,1,1,0.94309\n8,4,1728,457,75,210,1,1,0.93515\n1,5,1098,980,78,","@@ -0,0 +1,792 @@\n+1,1,1363,569,103,241,1,1,0.86014\n+2,1,1362,568,103,241,1,1,0.86173\n+3,1,1362,568,103,241,1,1,0.86173\n+4,1,1362,568,103,241,1,1,0.86173\n+5,1,1362,568,103,241,1,1,0.86173\n+6,1,1362,568,103,241,1,1,0.86173\n+7,1,1362,568,103,241,1,1,0.86173\n+8,1,1362,568,103,241,1,1,0.86173\n+1,2,371,410,80,239,1,1,1.0\n+2,2,371,408,80,239,1,1,1.0\n+3,2,372,407,80,239,1,1,1.0\n+4,2,372,406,81,239,1,1,1.0\n+5,2,373,405,81,239,1,1,1.0\n+6,2,373,404,82,238,1,1,1.0\n+7,2,374,403,81,238,1,1,1.0\n+8,2,374,402,82,238,1,1,1.0\n+1,3,103,549,83,251,1,1,1.0\n+2,3,102,549,83,250,1,1,1.0\n+3,3,102,549,83,250,1,1,1.0\n+4,3,102,549,83,250,1,1,1.0\n+5,3,102,549,83,250,1,1,1.0\n+6,3,102,549,83,250,1,1,1.0\n+7,3,102,549,83,250,1,1,1.0\n+8,3,102,549,83,250,1,1,1.0\n+1,4,1734,457,76,213,1,1,0.98386\n+2,4,1733,457,76,212,1,1,0.97586\n+3,4,1732,457,76,212,1,1,0.96781\n+4,4,1731,457,76,212,1,1,0.95976\n+5,4,1730,457,76,211,1,1,0.95185\n+6,4,1730,457,75,211,1,1,0.95122\n+7,4,1729,457,75,211,1,1,0.94309\n+8,4,1728,457,75,210,1,1,0.93515\n+1,5,1098,980,78,208,1,1,0.48325\n+2,5,1101,979,78,209,1,1,0.48571\n+3,5,1104,978,78,210,1,1,0.48815\n+4,5,1107,977,78,211,1,1,0.49057\n+5,5,1111,977,77,211,1,1,0.49057\n+6,5,1114,976,78,212,1,1,0.49296\n+7,5,1117,975,78,213,1,1,0.49533\n+8,5,1121,975,77,213,1,1,0.49533\n+1,6,632,761,100,251,1,1,0.31903\n+2,6,631,761,100,251,1,1,0.31903\n+3,6,631,761,100,251,1,1,0.31903\n+4,6,631,761,100,251,1,1,0.31903\n+5,6,631,761,100,251,1,1,0.31903\n+6,6,631,761,100,251,1,1,0.31903\n+7,6,631,761,100,251,1,1,0.31903\n+8,6,631,761,100,251,1,1,0.31903\n+1,7,623,901,144,123,0,11,1.0\n+2,7,623,901,144,123,0,11,1.0\n+3,7,623,901,144,123,0,11,1.0\n+4,7,623,901,144,123,0,11,1.0\n+5,7,623,901,144,123,0,11,1.0\n+6,7,623,901,144,123,0,11,1.0\n+7,7,623,901,144,123,0,11,1.0\n+8,7,623,901,144,123,0,11,1.0\n+1,8,671,427,42,652,0,10,0.80063\n+2,8,671,427,42,652,0,10,0.80063\n+3,8,671,427,42,652,0,10,0.80063\n+4,8,671,427,42,652,0,10,0.80063\n+5,8,671,427,42,652,0,10,0.80063\n+6,",add,Added STORM - 862 to Changelog
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-04-FRCNN/seqinfo.ini,,[Sequence]\nname=MOT17-04-FRCNN\nimDir=img1\nframeRate=30\nseqLength=1050\nimWidth=1920\nimHeight=1080\nimExt=.jpg\n\n,"@@ -0,0 +1,9 @@\n+[Sequence]\n+name=MOT17-04-FRCNN\n+imDir=img1\n+frameRate=30\n+seqLength=1050\n+imWidth=1920\n+imHeight=1080\n+imExt=.jpg\n+\n",add,Add note about data volume to enable
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-05-FRCNN/MOT17-05-FRCNN/000001.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 """"            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? v VeFÊºFF*a*Mh&4 c.yrpy>q~<V1GA`~QGÒ·Jh7G?	)Ï""""[ÙYj.~=;SC!@SZ=Li:*28?tÕ00QE\nP0( PEPEPEPEPEPEPEPEPEP\-5cu0L&{8\Ú¼drGJX~j1<.>HqV?-NxmN	CCEee<>Y@vWc#fGW%pGLw4ty4p+=?&-hm4Bk1Ì¼]ci['=?]fkwpZ\<Y%a""""Dk[\ST QÖ·$rjÂ=g%WÞ¬erszWHîÞ©Î!ky"""")E]#C\nÉ½yAsI%n2^J{$$Â¥+jÇ¹-ZV$cy|9$ u?V[3Qf-Ã³#\Z :~r6yQd]Uq.j/k$ y,?:ciY:,gMf8S_Ä¶\nÂ s*l--tx 	)$Dy_x_\nf@ t~^_i[Â£yGsP+#b/ WI'n!V%'sg`LFÛ»fÆ¡sWUc+)\]Ù= 4o2NsVq,>>fOz?E<Ke?_7+6!@""""{23hE;c((q%Y#C:wsJIm7P8#Q!.#/+B\""""4È¤9 Wu",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-05-FRCNN/MOT17-05-FRCNN/000001.jpg differ\n,add,Added Type name for DFI ( # 3 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-05-FRCNN/MOT17-05-FRCNN/000002.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 """"            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? XkB*/Ö¯QMqÞ´RG5Xl.XwY?\n8Ò¹ë¼U842=+2ké©zm{RR6#9%:!~S5>}ja|v5Z w/t;Ó5iV-A.áµº&Ö±zxO$`(QE QE QE QE QE QE Q@Q@Q@Q@%- U-O?d8jSN:=T@=[C8\AfljÚ³$D[9Ï-$D0=-^G<z3W_mhD^CÍVnÒ 7#$\nÒ²%e-PÖYM$/_Gq9Gh,iH<0jä·¹ULcY6""""ZE\n:f_;J4YLq[94; ^;ÖCI YX""""[OT2\""""5L,>	;b}+7bV5?E%;JHR1P*r|OAJKMAneay5R78XWZ98l-QE1ItsF~QL.#y\\v	rxë¹Î¡VÉ«vkjeÖ°9iW'5\c }TGO,1i64N[&E>c+5t8ÌÒ²Í¿E<n?;Ke9}> m  :4_3uSNj|P`GJAd\iÜ,Qâ5U3tÎ²<dsM7:xyÐ¬EsÒ<UwXv#sPjÓ½Ì	TOjb9j'GknXÔ½Kl6(mMM?]r~cP3S;R=aI}'m4'VH;:J>U(LBJ?|wRigeBs",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-05-FRCNN/MOT17-05-FRCNN/000002.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-05-FRCNN/MOT17-05-FRCNN/000003.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 """"            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? QÄ»iyD5tTAaA8XÄAhKf1f5U9'6_ÂpjÛ¨QÂU1Hnu6\ncZ}Õ­GÉ:pOwFzÒpÇµ7Gx^0GjnxPKU+-~)7QRV-XéºEPR\n(AaEPEPEPEPEPEPEPEPEPEPEPT1@:5XakcS`bcP@9sÒ¢[jt +][/Ú-cPbj]Ü¶( =}Qå¶¹h6fe-bF~Hauj}cDG n6-w&Ae[>-\Oji Iot&;yQ/{4Vip\n d5;]El5z+kXd5,]/& RF #51X"""" 1AB  V#?EPIAYL$oap057Zi-aYkKc~#Õ=ky	f]Õ~yU?H*$6zLzÝ fdt&mv5[#NgSSgZÌ¬SPjK0$æ¼Cn+([[ÊL1Ysp{d#P7H45F_OKi{F=8?T>Ñ§g?""""XÏS7qÊ¡:?xQjLÑr-h|Ya7/X|Î£:Å°j7XAEÃ45s'jjw'?*~TsW e:X$/9(PÍ¾BIh-i?a+bYR'""""XvÑ	H=SEQW[UU Zt,zSÖ@ # \nyÈ½TR",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-05-FRCNN/MOT17-05-FRCNN/000003.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-05-FRCNN/MOT17-05-FRCNN/000004.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 """"            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? pdxl*dH'A|3c\nxÌ)J:49PqX241]NÈHG*æ§v gÚC]eiC× R6c:l}b%6>Y6[gÞ§UnPÜ """"Ú£^&+f`]A""""T r+Fb(Q@E QE QE QE QE QE QE QE QE QE QE VN@GË¶3VhØ§bÊ~]yÏÊ] D60{O	8W<:U'MzFl.lq""""I0#Ælp!#\upIoZR85'YÅ\n'Ç¤f~uÜ>`Î³(&:wtÖ¥E?.XKf_B*w;EdixÓMa,xj`5tVW	W7Lc8'o>zÊ¨2N*NM2;i XpV.'Ñ«qpbBÔ¯flu5|G5kvv/`(kAQ.uD'Rk\nK\RIM~]J%%Õ Kßb#EA\nolTÞµ5'i.'gsx6xBS5GÏ±iEÖIcRi[$TmÞ¿Â 7l\non@r}^r?\ng GR$jC(hWa|3 FxÃWU9F1JèÂvV {4~~Ó©] yGQ?J.,kGir9Wly\nnWQkcPAU3Ë½B3J2KzW)GÌ{VvÈª=[k`<iG>H3K7ZbwG>LGCVOzoGk*z-=Õ´w1",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-05-FRCNN/MOT17-05-FRCNN/000004.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-05-FRCNN/MOT17-05-FRCNN/000005.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 """"            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? ;!3\nÆ³5nÙU-Tk*s=jo\IVc+QOÂ³E8,#9 }RMhCtÍ¶#4u$mÊÂ²-OAqwIZi ^;98AzFtqK[CcRXh#EP,RojR	XE:+#QABRÞ@Q@Q@Q@Q@	KE QE% -Q@Q@Q@Q@Q@- U+ |ivÅSYM+çÎ+]x}OiOR6d""""3GRN-0y<^#'\nXI$RÑ5 xTC7s{V$v[>,]\J%h|Q"""")fVïecS	5Â(H C$5K	5z$Ñ¬,ZÝ%flâPrj{F@\Nw6TZkSmOBãeMOQã±Þ±'	_sjler^w}K}u#Æ§xDbu(Ro5H""""2IjXyuuå²H7*=Î	\n\n1&{KO0	:Uß·X?iry,i^ksMpdQS}^l~U[UÚ¤|6|yMqzÃo)W#TOL#AEÃo[ )Ï½3NSxQq&r8o T"""">Ird4-2CÔ3o,5DORwÓ¸r%4uEjwpj&RJ(ZyÛ±Ú³.dZ=fÇ©\DEspÂt6_xU-6ONT#%C#lohÜ¶EA$9U^7-+jz:%01JXnqDEft r:",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-05-FRCNN/MOT17-05-FRCNN/000005.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-05-FRCNN/MOT17-05-FRCNN/000006.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 """"            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? XRYF{dSlJ5.	kJO\Æ·6mV1Xn?Õ:r\n{JCiX~})Ü¶37)o>eW&<7l43PpÞ5u6@?Ó~ÆN j~\SsKTG4R~PTHclPH+vT|#V)\njsÓ¥`ÕK(FEbJ(((()(h(((((e$`0kVabm?ÒØUuKmEsà«²+\n>á±|W6NXJ6s+ÌµK52VcBgv0uj×¶FEw oZÕ¹xÈ¬\EAoF\3u*##XÇX\% ?y-QNz-r:<Ò­E|.kÖ;WG3O2	4ak[gVJ4[è!Û¥j=ÄD	VgÔµ%'Kw,%'NÅ+~$)Â)/)3cU|>IR%% ;|Uija>lN{+O#v=ZuqyÜ¼zTfs<]Æ¾3""""B~k]jmC5ThÞ±eØ¤4[$ ysY,0R i]S(Oé¯UP#i u]p9#4ytk#ÆL{hXKÉ?J9/pYL]9:gQyx?}PQ&zqEÄ­u_n*C9<\nO""""#}N%-j	5Vn8Zij;#_-7b*æ¸ rj8gGGf~a[#X[?+m~Ð¸n4U[;wqT*\3LmlÔzQT)0=x&uu%H *r",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-05-FRCNN/MOT17-05-FRCNN/000006.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-05-FRCNN/det/det.txt,,"84,-1,313.8,1.2,278.4,479.8,1\n84,-1,1.8,1,237,480,1\n610,-1,208,1,234.4,480,1\n610,-1,90.9,120.1,91.8,225.8,1\n29,-1,58.6,86.4,127,346.6,1\n29,-1,233.5,100.5,126.1,314.8,1\n212,-1,282.5,4.3,238,476.7,1\n212,-1,3.5,1,203,480,1\n212,-1,520.5,185.7,29.6,84.8,0.999\n212,-1,160.5,53.2,108.6,362.8,0.356\n212,-1,544.6,197.1,22.3,55.3,0.069\n263,-1,97.7,10.1,213.5,470.9,1\n263,-1,317.1,122.6,99.1,262.2,1\n263,-1,451.2,13.4,189.8,467.6,0.896\n263,-1,381.8,157.3,51,184.2,0.103\n649,-1,274,138.4,72.4,224.5,1\n649,-1,46.5,1,184.2,480,1\n649,-1,386,5.2,255,475.8,0.998\n649,-1,202.1,138,69.1,219.7,0.988\n805,-1,161.9,178.9,41,114.7,1\n805,-1,288,39.7,217.3,424.1,1\n805,-1,209.4,185.9,30.6,88.5,0.999\n805,-1,25.6,144.1,49.2,156.8,0.999\n805,-1,235,125.5,90.8,221.4,0.999\n805,-1,64.5,156.7,36.9,136.7,0.987\n805,-1,82.6,166.9,42,119.8,0.946\n805,-1,441.7,10.3,99.5,470.7,0.481\n533,-1,205.6,17.5,158.6,463.5,1\n533,-1,77.8,149.6,67.2,176.9,1\n533,-1,366.6,101.1,102.6,316.9,0.999\n533,-1,487.1,1,153.9,48","@@ -0,0 +1,3848 @@\n+84,-1,313.8,1.2,278.4,479.8,1\n+84,-1,1.8,1,237,480,1\n+610,-1,208,1,234.4,480,1\n+610,-1,90.9,120.1,91.8,225.8,1\n+29,-1,58.6,86.4,127,346.6,1\n+29,-1,233.5,100.5,126.1,314.8,1\n+212,-1,282.5,4.3,238,476.7,1\n+212,-1,3.5,1,203,480,1\n+212,-1,520.5,185.7,29.6,84.8,0.999\n+212,-1,160.5,53.2,108.6,362.8,0.356\n+212,-1,544.6,197.1,22.3,55.3,0.069\n+263,-1,97.7,10.1,213.5,470.9,1\n+263,-1,317.1,122.6,99.1,262.2,1\n+263,-1,451.2,13.4,189.8,467.6,0.896\n+263,-1,381.8,157.3,51,184.2,0.103\n+649,-1,274,138.4,72.4,224.5,1\n+649,-1,46.5,1,184.2,480,1\n+649,-1,386,5.2,255,475.8,0.998\n+649,-1,202.1,138,69.1,219.7,0.988\n+805,-1,161.9,178.9,41,114.7,1\n+805,-1,288,39.7,217.3,424.1,1\n+805,-1,209.4,185.9,30.6,88.5,0.999\n+805,-1,25.6,144.1,49.2,156.8,0.999\n+805,-1,235,125.5,90.8,221.4,0.999\n+805,-1,64.5,156.7,36.9,136.7,0.987\n+805,-1,82.6,166.9,42,119.8,0.946\n+805,-1,441.7,10.3,99.5,470.7,0.481\n+533,-1,205.6,17.5,158.6,463.5,1\n+533,-1,77.8,149.6,67.2,176.9,1\n+533,-1,366.6,101.1,102.6,316.9,0.999\n+533,-1,487.1,1,153.9,480,0.831\n+533,-1,157.3,158.1,40.4,131.9,0.131\n+604,-1,186.3,1,241.4,480,1\n+604,-1,129.5,146.4,72.4,182.4,1\n+732,-1,145.3,71.6,122.4,383.1,1\n+732,-1,118.8,206.1,22,55,1\n+732,-1,73.5,193.5,31.4,81.4,1\n+732,-1,262.6,164.4,48,155.5,0.997\n+732,-1,319.5,126,104.5,291.2,0.981\n+732,-1,374.4,1,266.6,480,0.792\n+352,-1,330,1,209.5,480,1\n+352,-1,192.5,162,52.5,113.4,1\n+352,-1,124.6,120.6,71.7,178.1,1\n+352,-1,66.6,127.4,69.4,164.4,0.999\n+352,-1,508.6,63.9,106,340.8,0.998\n+487,-1,390.8,96.4,107.2,325.3,1\n+487,-1,18.8,4.5,270.8,476.5,0.999\n+487,-1,491.3,89.1,109.7,328.2,0.814\n+487,-1,236.8,35,105.5,411.6,0.144\n+295,-1,27.8,38.3,192.1,442.7,1\n+295,-1,243.3,58,127.5,417.1,1\n+295,-1,362.5,159.3,69,186.2,0.999\n+295,-1,451.9,12.2,145.9,448.4,0.885\n+393,-1,488.3,99,152.7,316.3,1\n+393,-1,66.4,214.4,26.4,67.5,1\n+393,-1,257.7,163.4,57.4,154.2,1\n+393,-1,210,164,53.4,151,1\n+393,-1,393.7,196.7,33.5,87.2,1\n+393,-1,328.2,220.3,14.1,37.4,",add,Added note about dates .
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-05-FRCNN/gt/gt.txt,,"1,1,17,150,77,191,1,1,1.0\n2,1,20,148,78,195,1,1,1.0\n3,1,23,147,79,199,1,1,1.0\n4,1,26,146,79,202,1,1,1.0\n5,1,29,144,80,206,1,1,1.0\n6,1,33,142,81,210,1,1,1.0\n1,2,20,136,69,190,1,1,0.073298\n2,2,24,134,69,191,1,1,0.072917\n3,2,29,132,68,192,1,1,0.07772\n4,2,34,131,68,196,1,1,0.076142\n5,2,37,130,68,199,1,1,0.07\n6,2,41,129,71,202,1,1,0.064039\n5,3,220,204,21,63,1,1,0.95455\n6,3,225,203,22,64,1,1,0.86957\n4,4,232,192,28,71,1,1,0.80747\n5,4,240,196,29,70,1,1,0.94085\n6,4,247,196,28,70,1,1,0.96892\n3,5,243,186,22,76,1,1,0.32919\n4,5,251,186,22,76,1,1,0.11462\n5,5,258,187,20,74,1,1,0.14857\n6,5,265,187,20,73,1,1,0.1538\n1,14,240,199,25,66,1,1,0.0\n2,14,244,198,25,66,1,1,0.46154\n3,14,248,197,25,66,1,1,1.0\n4,14,255,197,25,66,1,1,0.76923\n5,14,265,201,23,64,1,1,0.79167\n6,14,271,201,24,64,1,1,0.8\n1,112,566,181,228,98,0,3,0.32751\n2,112,555,182,240,98,0,3,0.35685\n3,112,545,183,252,98,0,3,0.37945\n4,112,535,184,264,98,0,3,0.4\n5,112,522,183,271,99,0,3,0.4375\n6,112,509,182,279,100,0,3,0.","@@ -0,0 +1,73 @@\n+1,1,17,150,77,191,1,1,1.0\n+2,1,20,148,78,195,1,1,1.0\n+3,1,23,147,79,199,1,1,1.0\n+4,1,26,146,79,202,1,1,1.0\n+5,1,29,144,80,206,1,1,1.0\n+6,1,33,142,81,210,1,1,1.0\n+1,2,20,136,69,190,1,1,0.073298\n+2,2,24,134,69,191,1,1,0.072917\n+3,2,29,132,68,192,1,1,0.07772\n+4,2,34,131,68,196,1,1,0.076142\n+5,2,37,130,68,199,1,1,0.07\n+6,2,41,129,71,202,1,1,0.064039\n+5,3,220,204,21,63,1,1,0.95455\n+6,3,225,203,22,64,1,1,0.86957\n+4,4,232,192,28,71,1,1,0.80747\n+5,4,240,196,29,70,1,1,0.94085\n+6,4,247,196,28,70,1,1,0.96892\n+3,5,243,186,22,76,1,1,0.32919\n+4,5,251,186,22,76,1,1,0.11462\n+5,5,258,187,20,74,1,1,0.14857\n+6,5,265,187,20,73,1,1,0.1538\n+1,14,240,199,25,66,1,1,0.0\n+2,14,244,198,25,66,1,1,0.46154\n+3,14,248,197,25,66,1,1,1.0\n+4,14,255,197,25,66,1,1,0.76923\n+5,14,265,201,23,64,1,1,0.79167\n+6,14,271,201,24,64,1,1,0.8\n+1,112,566,181,228,98,0,3,0.32751\n+2,112,555,182,240,98,0,3,0.35685\n+3,112,545,183,252,98,0,3,0.37945\n+4,112,535,184,264,98,0,3,0.4\n+5,112,522,183,271,99,0,3,0.4375\n+6,112,509,182,279,100,0,3,0.47143\n+1,113,-73,152,345,145,0,3,0.31554\n+2,113,-81,151,338,145,0,3,0.27019\n+3,113,-89,151,331,145,0,3,0.2259\n+4,113,-97,151,325,145,0,3,0.18405\n+5,113,-105,151,318,145,0,3,0.15361\n+6,113,-113,151,311,145,0,3,0.16987\n+5,114,281,199,36,54,0,2,0.37838\n+6,114,307,198,35,55,0,2,0.48214\n+5,115,268,225,63,36,0,4,0.67188\n+6,115,295,225,63,35,0,4,0.98438\n+1,117,307,213,12,32,1,1,1.0\n+2,117,312,213,12,32,1,1,1.0\n+3,117,317,213,12,32,1,1,1.0\n+4,117,322,213,12,32,1,1,1.0\n+5,117,329,213,12,32,1,1,0.85315\n+6,117,336,213,12,32,1,1,0.16783\n+1,118,493,187,24,49,0,8,0.0\n+2,118,501,187,25,49,0,8,0.0\n+3,118,510,187,25,49,0,8,0.0\n+4,118,519,187,25,50,0,8,0.0\n+5,118,528,187,25,50,0,8,0.0\n+6,118,537,187,25,50,0,8,0.0\n+1,125,114,154,85,176,1,1,1.0\n+2,125,117,152,86,180,1,1,1.0\n+3,125,121,151,86,184,1,1,1.0\n+4,125,126,150,87,186,1,1,1.0\n+5,125,131,149,89,189,1,1,1.0\n+6,125,136,148,91,192,1,1,1.0\n+1,146,325,-9,325,257,0,9,0.87112\n",add,Added STORM - 862 to Changelog
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-05-FRCNN/seqinfo.ini,,[Sequence]\nname=MOT17-05-FRCNN\nimDir=img1\nframeRate=14\nseqLength=837\nimWidth=640\nimHeight=480\nimExt=.jpg\n\n,"@@ -0,0 +1,9 @@\n+[Sequence]\n+name=MOT17-05-FRCNN\n+imDir=img1\n+frameRate=14\n+seqLength=837\n+imWidth=640\n+imHeight=480\n+imExt=.jpg\n+\n",add,Add note about data volume to enable
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-09-FRCNN/MOT17-09-FRCNN/000001.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 8""""            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? (\n(\nJZ(RU( (\n(;fpÌq^o_%<R,YÃ¨G/^ZEm}`ÇEbnRW[U/iÉ3jÎ>%Ü_nzàµ h\3V3`Í©OUv.y3L4r=+ +2T.æ´H[ZZ(97Õ§Sj\nÊ#Ö4QE -PJ(|1<#r7+~Z&Ë<+>|-5M^y#1Æ DP:FR8*]x5UÚ7_ðªz2 5)n>oÊ¸vOAj#\n>vÕ¼7h?U}SvjËxkW+'Iï¥%.J#^-Å¼)YPV=X5tQE!(78x4^8z9	^~ÑKU SÚ`CUdjUyR/sNÎ½p	QlZj[\_o0iáªjKêººyZ_â«lfK\\Z[T jé³®YÖ¬-rjJÕ£R'aÕ¨o.<Õ¬X?Ú©uÏ}mT<Ca}CzÓZÆ²Så¬¿6Í \Y]e 5[m!XD\Y]\nkRW`KS_:ÖUZLEF5@ÔQÚ¢gU1iß** noHi)hæ¤¤KEaÔ»PMLV$KQî¥ RVNZu""""lIKQP+î¢Xu=j*u-ZaSÓ©heaÔ´)7Rt@R{_jÂ» ouFH~DIE_\n`G?kâ³>fjjZ))(!I@Ú¨Si J)E6@\J(	EP Å¢BJ(ii 7ShcLCj&k",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-09-FRCNN/MOT17-09-FRCNN/000001.jpg differ\n,add,Added the user group to the contributor list
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-09-FRCNN/MOT17-09-FRCNN/000002.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 8""""            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? (\n(\n()((`\n(|+~Ç­+dbZ~j-eHEzFv+mB1`KZ^+)>huus,VZ+aj]KWv>p~.v.[XevN%iqSzBQE@N*lkE^ E-g%Æµ6ÔRTf4-QREPEPKI(WTE{Þ»=áhLqGzo?&FgHdcQÆ(H*\n%#1h]{*-O-i5<_BMp'~>F-SgeÐ¯]WÚ®.Bh+YF?V7\Q#{^?7ï¥.J%QÖ¥JjÇ«)+((javdq{9z6i|TLQWbUXÆ´b#a%ÑzÞ«I \DÓ\5;TfG]j[%x0Qj^e Ôw^sWj,b-=n Ú«qjR>ÍzÎ­V""""h WV*V3p;/6Cyqb{R5'Fa}ZTyY~m/L\Y]gkU.i1XD\]^OkM[m`jOuTU#_ÖT;jj\n#SÚ¢gUboÓh]UOKE)P+FÃ·RQA<wTT\n&Ä´nR&Äi.KKQP+EÔ´ZbJZj!êL,HêHD]:E KL@QQQw@tÇº?oACK/0V""""""""@ IwWß­>+8oOSSRPR(j(RRU QÔ´Æ¦""""*JZJZJ%%1hJ Z)(IE6IIEKM",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-09-FRCNN/MOT17-09-FRCNN/000002.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-09-FRCNN/MOT17-09-FRCNN/000003.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 8""""            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? (((!)hEU(V |+~É­3dm\W[Wh,LRE0+Ò´o×Q[jv5	ÔµrWW_Î¸ZG&$Î¾J?,Ovgm\UÔ®Õ¼U&(j.4iq)`(	BÍS hsU--%]M56ÔT RTfÅ¢*@( (c)T]z\n{wAk=r4Gzw?&B3lb$Ft `P)q.<^=U=K}5,	 *GÑ·m=yP*#R^ó¦« j4;)Î³}QÍ«[Ëp~32sVNwSm.J'QÖ¦Ht89fÕV2RWAERÝn*^(>fCyTb3Az*^vAU)QU{Dc,$3Y/[n-GR'sOÎ½pWn-KÔR	/n-@béo/Õ«Î¢Ô \MgV2GyÅ§-Un-SV'Ù³YÕªR-U~++VVbu|TZ<Õ v,3Oo,!c]nyWGL\Y]g 5[áXD\Y^OkM[m`jO(n.:tb-DaSÚ¢wU>w_Ö@	qR*})()P+QH,:u26QÑºXu.u.	%:HQnV$nVKMP\n}GK@h-ZZe-.Zi}-2u :F=iÛª:Z@IIIoCW.TouÞÈ_I_\nQET\n3ËWß­=,Oy=MMl6@%5SJcSLSii Sh ((`RQ@E%!ii(E%",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-09-FRCNN/MOT17-09-FRCNN/000003.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-09-FRCNN/MOT17-09-FRCNN/000004.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 8""""            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? (\n(\n()( (V Z6S\m6fQ3î¸¯y_KGxfYbb!Ê°+Ñ´m[}`Çk[`'77\OueKdlÚ¨'}3p;xFs5ob0IsjWXæ¥C+5e)cdQEfP4;nDWg%5NjESHbRRA!E%E QE)Â|Q#w1(9WsyWm.&J&Û£Ò½;4H-Z}kR4QqMBR8*~P\nÔ?jvÞ*?]=wr{)'ÎTÞw\n>yÕ¼7h2O?QNÅ/5hHf	~[c}yRä«¹S\[Kk;*pr*Ö²jÇ«&(c\nÚbh>ffB<,Ó¼TJ1W4^\nÝUR{SL\n}1]U Ú®pB^$};a*[R;}J4\n?0tGÔyoÕ«bÔ \M]JNjEZW jÅªEÖ¬f)kRU~++8WT7mb{R5ÂµRmêI;YhT7Ò¬!can×-eyXYyx~j-l""""ls,'5×­5'U$9ÖTi1Ø¨Q5Xj(Ô­Q1iMZr 7kâªihP()%-GKlIZA<]Ô&rC%S&SÃ©iH,KIMJA$Z`KKLZZBNnu :Q@QQ}w@Çº?oAAJ 4a*QEP(D3OK8oOSRR(Ä¦Ó¨C))ZÚP h )-%%PÂ))\nIE -P(J()",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-09-FRCNN/MOT17-09-FRCNN/000004.jpg differ\n,add,Added hypest who apparently actually wrote the C # port
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-09-FRCNN/det/det.txt,,"170,-1,1001.4,18.3,400,1062.7,1\n170,-1,1504.5,382.6,143.7,295.3,1\n170,-1,601.7,154.7,390.4,926.3,1\n170,-1,468.4,310.3,190.4,571.4,0.999\n170,-1,1376.3,448,92,234.1,0.998\n126,-1,1434.9,146.8,289.2,817.2,1\n126,-1,1137.8,271.8,293,694.4,1\n126,-1,332.3,434.7,109.2,301.4,1\n126,-1,451.4,409.6,117.5,317.1,1\n239,-1,1517.9,534.1,100.7,180.2,1\n239,-1,914.7,309.6,262.8,547.8,1\n239,-1,1276.6,354.8,163.3,398.9,1\n239,-1,1730.2,421.7,83.2,220.6,1\n239,-1,553.4,311.9,238.3,610,1\n239,-1,1169.4,420.3,113.3,312.5,1\n239,-1,737.3,318,199.4,495.7,0.264\n146,-1,929.2,230.4,353.4,819.8,1\n146,-1,443.1,394.7,136.7,351.2,1\n146,-1,1254.4,123.9,325.1,913.6,1\n146,-1,24.3,272.3,232.3,674.1,1\n146,-1,319.2,423.1,121.6,332.9,0.999\n95,-1,987.4,318.5,228.1,450.7,1\n95,-1,1373.4,319.9,239.5,548.6,1\n95,-1,1628.6,240.3,243.7,639,1\n95,-1,463,423.1,109.9,297.1,1\n95,-1,313.7,456.2,104.4,258.8,1\n227,-1,752.6,279.7,256.5,595.5,1\n227,-1,1492.1,425.2,113.1,272.5,1\n227,-1,300.9,296.1,298.8,681.5,1\n227,-1,14","@@ -0,0 +1,3049 @@\n+170,-1,1001.4,18.3,400,1062.7,1\n+170,-1,1504.5,382.6,143.7,295.3,1\n+170,-1,601.7,154.7,390.4,926.3,1\n+170,-1,468.4,310.3,190.4,571.4,0.999\n+170,-1,1376.3,448,92,234.1,0.998\n+126,-1,1434.9,146.8,289.2,817.2,1\n+126,-1,1137.8,271.8,293,694.4,1\n+126,-1,332.3,434.7,109.2,301.4,1\n+126,-1,451.4,409.6,117.5,317.1,1\n+239,-1,1517.9,534.1,100.7,180.2,1\n+239,-1,914.7,309.6,262.8,547.8,1\n+239,-1,1276.6,354.8,163.3,398.9,1\n+239,-1,1730.2,421.7,83.2,220.6,1\n+239,-1,553.4,311.9,238.3,610,1\n+239,-1,1169.4,420.3,113.3,312.5,1\n+239,-1,737.3,318,199.4,495.7,0.264\n+146,-1,929.2,230.4,353.4,819.8,1\n+146,-1,443.1,394.7,136.7,351.2,1\n+146,-1,1254.4,123.9,325.1,913.6,1\n+146,-1,24.3,272.3,232.3,674.1,1\n+146,-1,319.2,423.1,121.6,332.9,0.999\n+95,-1,987.4,318.5,228.1,450.7,1\n+95,-1,1373.4,319.9,239.5,548.6,1\n+95,-1,1628.6,240.3,243.7,639,1\n+95,-1,463,423.1,109.9,297.1,1\n+95,-1,313.7,456.2,104.4,258.8,1\n+227,-1,752.6,279.7,256.5,595.5,1\n+227,-1,1492.1,425.2,113.1,272.5,1\n+227,-1,300.9,296.1,298.8,681.5,1\n+227,-1,1469.1,534.8,74.2,175.9,1\n+227,-1,1707.7,397.6,109.5,249.5,1\n+227,-1,1186.6,341.1,153.3,426,1\n+227,-1,1066.2,409.8,143.2,353.1,1\n+227,-1,1650.1,420.3,99.2,230.2,0.999\n+227,-1,586.6,300.1,207.9,561.2,0.996\n+206,-1,1645.9,397.2,118.4,250.9,1\n+206,-1,1410,536.4,69.5,161.7,1\n+206,-1,974.2,337.3,201.2,471.8,1\n+206,-1,1526.3,425.1,98.9,230.8,1\n+206,-1,1429.1,431.8,102.2,261.4,1\n+206,-1,217.5,1,636.4,1080,1\n+206,-1,855.4,391.3,154.2,395.9,0.081\n+499,-1,570.9,331.3,188,508.6,1\n+499,-1,1281.5,450.2,86.2,199.6,1\n+499,-1,138.2,247,408.7,722,1\n+499,-1,1821,422.9,77.5,207.8,1\n+499,-1,1489.7,396.3,107.1,301.9,1\n+499,-1,1417.1,425.8,94.8,244.5,1\n+499,-1,1596,448.9,76.9,199,0.996\n+16,-1,1659.8,376,178.6,380.4,1\n+16,-1,291.2,447.3,110.2,262.9,1\n+16,-1,1319.5,453.5,78,209.5,1\n+16,-1,1291,535.1,53.2,132.9,1\n+16,-1,1,244.9,224.7,691.3,0.932\n+273,-1,1713.2,526.4,107.9,198.1,1\n+273,-1,1075.5,334.3,209.9,504.4,1\n+273,-1,1435.2,371.4,1",add,Added note about dates .
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-09-FRCNN/gt/gt.txt,,"1,1,260,450,102,262,1,1,1.0\n2,1,262,449,102,263,1,1,1.0\n3,1,264,449,102,263,1,1,1.0\n4,1,266,448,102,264,1,1,1.0\n1,19,1686,387,171,345,1,1,1.0\n2,19,1685,386,170,347,1,1,1.0\n3,19,1684,386,170,348,1,1,1.0\n4,19,1683,385,169,351,1,1,1.0\n1,20,1886,327,156,404,1,1,0.22293\n2,20,1883,326,157,406,1,1,0.24051\n3,20,1881,325,157,408,1,1,0.25316\n4,20,1878,325,158,410,1,1,0.27044\n1,21,1253,533,63,129,1,1,1.0\n2,21,1256,534,62,127,1,1,0.5873\n3,21,1259,535,61,125,1,1,0.58065\n4,21,1262,536,60,124,1,1,0.57377\n1,22,1292,459,70,202,1,1,0.77624\n2,22,1293,458,71,203,1,1,0.77342\n3,22,1295,457,72,204,1,1,1.0\n4,22,1297,456,73,205,1,1,1.0\n1,23,-348,235,477,695,1,1,0.26987\n2,23,-340,234,473,695,1,1,0.28059\n3,23,-331,234,469,694,1,1,0.29362\n4,23,-322,234,465,693,1,1,0.30687\n1,25,1035,174,136,532,0,9,1.0\n2,25,1035,174,136,532,0,9,1.0\n3,25,1035,174,136,532,0,9,1.0\n4,25,1035,174,136,532,0,9,1.0\n1,26,116,522,84,230,0,8,0.83529\n2,26,115,521,84,230,0,8,0.77647\n3,26,115,521,84,230,0,8,0.71765","@@ -0,0 +1,52 @@\n+1,1,260,450,102,262,1,1,1.0\n+2,1,262,449,102,263,1,1,1.0\n+3,1,264,449,102,263,1,1,1.0\n+4,1,266,448,102,264,1,1,1.0\n+1,19,1686,387,171,345,1,1,1.0\n+2,19,1685,386,170,347,1,1,1.0\n+3,19,1684,386,170,348,1,1,1.0\n+4,19,1683,385,169,351,1,1,1.0\n+1,20,1886,327,156,404,1,1,0.22293\n+2,20,1883,326,157,406,1,1,0.24051\n+3,20,1881,325,157,408,1,1,0.25316\n+4,20,1878,325,158,410,1,1,0.27044\n+1,21,1253,533,63,129,1,1,1.0\n+2,21,1256,534,62,127,1,1,0.5873\n+3,21,1259,535,61,125,1,1,0.58065\n+4,21,1262,536,60,124,1,1,0.57377\n+1,22,1292,459,70,202,1,1,0.77624\n+2,22,1293,458,71,203,1,1,0.77342\n+3,22,1295,457,72,204,1,1,1.0\n+4,22,1297,456,73,205,1,1,1.0\n+1,23,-348,235,477,695,1,1,0.26987\n+2,23,-340,234,473,695,1,1,0.28059\n+3,23,-331,234,469,694,1,1,0.29362\n+4,23,-322,234,465,693,1,1,0.30687\n+1,25,1035,174,136,532,0,9,1.0\n+2,25,1035,174,136,532,0,9,1.0\n+3,25,1035,174,136,532,0,9,1.0\n+4,25,1035,174,136,532,0,9,1.0\n+1,26,116,522,84,230,0,8,0.83529\n+2,26,115,521,84,230,0,8,0.77647\n+3,26,115,521,84,230,0,8,0.71765\n+4,26,115,521,84,230,0,8,0.65882\n+1,27,234,395,21,440,0,9,1.0\n+2,27,234,395,21,440,0,9,1.0\n+3,27,234,395,21,440,0,9,1.0\n+4,27,234,395,21,440,0,9,1.0\n+1,28,1682,470,65,122,0,8,0.060606\n+2,28,1682,470,64,121,0,8,0.046154\n+3,28,1682,470,64,121,0,8,0.030769\n+4,28,1682,470,64,121,0,8,0.015385\n+1,30,42,502,85,254,0,8,0.0\n+2,30,42,501,85,254,0,8,0.0\n+3,30,42,501,85,254,0,8,0.0\n+4,30,42,501,85,254,0,8,0.0\n+1,31,863,522,46,108,0,12,1.0\n+2,31,863,521,46,108,0,12,1.0\n+3,31,864,521,46,108,0,12,1.0\n+4,31,865,521,46,108,0,12,1.0\n+1,44,751,506,95,182,0,12,1.0\n+2,44,748,507,96,182,0,12,1.0\n+3,44,746,509,96,181,0,12,1.0\n+4,44,744,510,96,182,0,12,1.0\n",add,Added STORM - 862 to Changelog
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-09-FRCNN/seqinfo.ini,,[Sequence]\nname=MOT17-09-FRCNN\nimDir=img1\nframeRate=30\nseqLength=525\nimWidth=1920\nimHeight=1080\nimExt=.jpg\n\n,"@@ -0,0 +1,9 @@\n+[Sequence]\n+name=MOT17-09-FRCNN\n+imDir=img1\n+frameRate=30\n+seqLength=525\n+imWidth=1920\n+imHeight=1080\n+imExt=.jpg\n+\n",add,Add note about data volume to enable
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-10-FRCNN/MOT17-10-FRCNN/000001.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 8""""            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? .jKU-kTnk1OjUv""""&Vy}a-A/}M@ UL)U*B.-JKVb*ii/qT:Qo;A'{u 3qV  ]P fme-DjjTmVoWWa^QR=Y_Ui×BU8`OFo2ÌO-ovfh}ÓJ}/V/-Õqg%fgL5H'-;]R-:[q(JmKYPmÅª~}.q0Ò²USv7R>YnÚ¡{pfEVYkHV7WAEsK[ZSjr=W:dÑ¯5(ÔlP *)gvyk6w-TZQÊ·Y]2vQ9ZKVj-1=Yz@TPHPU3EiÔ#Ö¥jmfCjJ-5:NHÈjbQZ@ScTU~7-QKCiNJ6Z(JE	IKIH(IKI@	E-JJZJ J)vh ((BZJ ( ( ))hÞ©(J-%:H)m 2I@	BÑ¶ J)h*HeGRÄoÊ¿zVÔ²OÝ­k}o_ DzÛ¦*hAO(=WÝ|O49JÏm {x oM{\n|5eG|F>D>#+\.w?,DvlZZ*k%Õ®Ù±4a#[!q}VÆkÜ¼ Ç¼8g?ltK}-^!DXw5o-2(VGcÚ²#ntV7S\n^5 m Yk2o:N^Æ\ZN@QAX ZwDÛkr?5Ó¢*esYmÈ»~rORxf]._JQY+É'",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-10-FRCNN/MOT17-10-FRCNN/000001.jpg differ\n,add,Added the user group to the contributors
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-10-FRCNN/MOT17-10-FRCNN/000002.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 8""""            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? ç¥=2UYl1z[wj.dFeZ4lYZZm-0._Ie×µê²µBW}~]gF$w5F/6j[-47|Co_huÛºEÞµUj +U-~Ûª'Þ©WV0%WZjXFiIwUj]VÕR%Ú¿5fi#7\\n{]Cj?U%qPGe2]=pyÖ\n&u~/5t;Vu%|7q\&5l$R""""ÙªÜ¨JOBLvUVn2*[yR=+/U q+^R*Ô%-y}.]ÌMmZßT`h,f	kee]GuEÂ\njÕ)i ,Zil4Gm E,T'uyjL rÉVlVezFeiÚ¨KVj+P)jÕ©jUÚ¬=@ÔB3T-HhqUjVcmAC)Jå¥¡~L56*6*7d-M56eWQnDL&SJHFSZR@KIHÐu%E-% I@	EPIKIH%6E 6()J)i( ((i)z)hci)@u7m0QIIm 6FJ Z)VZ [T,FM *Úo@Xxz/I?Ã>TMu~m/æ²«R|=nMh)Vg[aÎmá¼±s_Ù m5kR.:<n_Sv_kU\3E}kÛ¢^Ä«GÖ²/]?e_&*VIf mzh2:Ð²FJ}EI$wH@;WmR]oVH#8	Y-MiÒ³)LOzuWoaHÓ <5Ñ´_.E~>kK;K#?r",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-10-FRCNN/MOT17-10-FRCNN/000002.jpg differ\n,add,Added Type name for DFI ( # 18480 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-10-FRCNN/MOT17-10-FRCNN/000003.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 8""""            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? &Zë ^'e]0yjZ%Rddjd3 US-AÝ©&{5g-h j`WR+DUgA?J k:L$*j;UmsU\\n6fZ QioZwguHT+OOLRN3SUIZ.ËeÊ4h#Ö¥3oá¬o}qz4;%Kum#>ß»V-U	-Ä»XT~5b0~E(t/5ErD6TrkE&l[Îµe]i :m7';Ó©V312bNiv.^[U~æÒ<RAÊ¯[S#gVYÚNbU*K6 +xSj:Í«+jrY~oZ%I""""Ö«Am[jZwn~:SÐuKQCzVlJZJk|5""""*ÕFfM,JWZÞ©cwZBVVKW%j+P2wÞ«15B3T-R-Un>Zjs4*RTkS)Le5Èµ%IKÚQE""""=@;@ZI@MYk]mmeZJu%2F6M1J JJZ)m%:Ä¤R))i( ) QE %MM PQE QE!	E-% QE QE :}Ú'Ý bKE 6Ó¶mOM@E: î´/@-+-5hYiSqK	95FSGSÄU}ÞO_ÑÉ¡>| /uÃ2wswj0vun?ë¯/ }M3_=mk:/S5\}j_TY{+(tHNÐ/6-\ 4~uK""""ZG\n>:ngFÜ«OWOÉ¯@g'r-6""""?*o2ë³*J@ñ¾®°X.~CÞ kÍN~",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-10-FRCNN/MOT17-10-FRCNN/000003.jpg differ\n,add,Added Type name for DFI ( # 18480 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-10-FRCNN/MOT17-10-FRCNN/000004.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 8""""            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? ÞykbNf<}ß½]9'>hT UW5% F-Q#ÖZ`i UwUÙª%WUJ""""W5Y J(Y~UÑ6_%Î¬6f }jb&OOvoi3S""""YFTSu^7|[wDvPã¸ flUÌY>Z_Þ²[61\Þ²xVhIÎ¿v:Xj\n}sTJ[Æ[""""o#F\n536_Öur2vMÇ§Gujk;J[&uQéªWQ Fo.],%v#m'Öq_joR'.wU+z;E2*S[ rV_9[5ZÂ°et jj-\nÝ¸fzlDGk(H^k]GQNnjdGO{6O/æªm')^e-uc+tRFW%j-ZL\n7RTzC jWZB+TMRQ^_NKZ5&MjmIbILJ}K)u6Æ§)=+Tt Æ¨jVFTë±ª*Ð7-Po6	IKI@Ä¤RRRR(FRPRRR( J(\n(E-% \nF)J(Ä¢\nJZ)J(Â( (STQT)) JJZkR)i)%:KKE -  Ú`w]Pm^< @TÛ¶ZÚµg YEOì»)=3~Û®$cC{YxÂ·j8V3+lÞ®;oq/Í»xUnÝ®aÎ3YrÖ,ÈZ;A""""ïºSMio_Y)!G;kPE}5W>w=tV	opKYWVp+R4)Kqa!8Y\dr~&×.-:>.",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-10-FRCNN/MOT17-10-FRCNN/000004.jpg differ\n,add,Added Type name for DFI ( # 18480 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-10-FRCNN/MOT17-10-FRCNN/000005.jpg,," JFIF       C 		\n $.' """",#(7),01444'9=82<.342 C			2!!22222222222222222222222222222222222222222222222222 8""""            	\n    } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz        	\n   w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz   ? çª©V-vÄµ""""T}>BzOP7Þ VVL@R*1y[~oL	ÛVvÛµvU6f*wZTW2=Z*=^#:51=iZVVÙ§_ÔOic#)E;a*q}j0wWC:5{X;U*Q:$ÑgZOZ+ARQ\);EpE`_Y=Û[WIMeSÖ:#0|SVIB(=jmbaOÊ¢+&zÖj^DrB<r+[kH;/5M,R;6f?9l|Y/Ofd_-Y/]pzI-liOsï¹<vEu`ÎoG)4)<uv_zhv}EQPXF~Hk&[eÒª)Kv#yê¤²Ë¿k7TZ9JFW)^	ZKV%j+SMÌ­UZf-PLTDRzRz MSjHeHzR)%:SR1PN GM)+N:je	LUJJZ(JCBSiRJZ) )i(((	EP16S QE  QE RQE!Q@Q@HK KIKEIKIH:L2MR@RQ@NO/@hjOyk5EHed_jhoN ^	me}Fy8 Ö¯_>=""""ewc5ÅvA{qÒ¼O 0^:Wx>,?YxÊ±2-S_>E6RM.&Zé tGKmU]75e$k4\nWk+Zn1[wLZk#^[tÏ¬ê¹x%vT^WÉ¾hz[vHUp+KÓE_0rkPzfqVB7×?o$~8C]LlF",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-10-FRCNN/MOT17-10-FRCNN/000005.jpg differ\n,add,Added Type name for DFI ( # 18480 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-10-FRCNN/det/det.txt,,"572,-1,1137.2,284.8,59.3,142.9,1\n572,-1,957.3,324.8,52.5,103.4,1\n572,-1,171,300.8,42.4,100.1,1\n572,-1,917.3,313.8,54.4,102.9,1\n572,-1,60.2,288.2,73.8,138.5,1\n572,-1,1227.7,281.2,56.5,129.8,1\n572,-1,760.9,273.8,62.5,177.5,1\n572,-1,853.9,289.6,57.9,161,1\n572,-1,1029.8,295.6,32.4,73.6,1\n572,-1,1528.2,175.1,148.2,538.7,0.999\n572,-1,725.8,296.1,45.6,110.9,0.997\n572,-1,610.4,304.7,25.2,55.5,0.994\n572,-1,1084,299.4,24.3,59.9,0.956\n572,-1,123.4,302.9,40.7,101.6,0.942\n572,-1,643.1,304.5,22.8,49.7,0.94\n572,-1,661,302.5,21.8,50.6,0.936\n572,-1,630.5,304.9,23.1,51.5,0.919\n572,-1,1103.4,302.1,24.9,56,0.731\n572,-1,1469.2,208.3,95.7,437,0.322\n572,-1,683.4,304.6,23.5,47.6,0.291\n614,-1,1166.3,337.7,58.7,144,1\n614,-1,1331.1,322.3,82.5,180.8,1\n614,-1,943.2,371.7,60.7,110.3,1\n614,-1,991.1,381.4,62.7,106.7,1\n614,-1,42.2,349.5,51.5,116.4,1\n614,-1,768.6,327.1,63.9,180.9,1\n614,-1,2.6,352.6,48.7,114.7,1\n614,-1,851.9,341.3,59.7,164.6,1\n614,-1,629.5,355.5,28.2,59.2,1\n614,-1,1060.1,351","@@ -0,0 +1,9701 @@\n+572,-1,1137.2,284.8,59.3,142.9,1\n+572,-1,957.3,324.8,52.5,103.4,1\n+572,-1,171,300.8,42.4,100.1,1\n+572,-1,917.3,313.8,54.4,102.9,1\n+572,-1,60.2,288.2,73.8,138.5,1\n+572,-1,1227.7,281.2,56.5,129.8,1\n+572,-1,760.9,273.8,62.5,177.5,1\n+572,-1,853.9,289.6,57.9,161,1\n+572,-1,1029.8,295.6,32.4,73.6,1\n+572,-1,1528.2,175.1,148.2,538.7,0.999\n+572,-1,725.8,296.1,45.6,110.9,0.997\n+572,-1,610.4,304.7,25.2,55.5,0.994\n+572,-1,1084,299.4,24.3,59.9,0.956\n+572,-1,123.4,302.9,40.7,101.6,0.942\n+572,-1,643.1,304.5,22.8,49.7,0.94\n+572,-1,661,302.5,21.8,50.6,0.936\n+572,-1,630.5,304.9,23.1,51.5,0.919\n+572,-1,1103.4,302.1,24.9,56,0.731\n+572,-1,1469.2,208.3,95.7,437,0.322\n+572,-1,683.4,304.6,23.5,47.6,0.291\n+614,-1,1166.3,337.7,58.7,144,1\n+614,-1,1331.1,322.3,82.5,180.8,1\n+614,-1,943.2,371.7,60.7,110.3,1\n+614,-1,991.1,381.4,62.7,106.7,1\n+614,-1,42.2,349.5,51.5,116.4,1\n+614,-1,768.6,327.1,63.9,180.9,1\n+614,-1,2.6,352.6,48.7,114.7,1\n+614,-1,851.9,341.3,59.7,164.6,1\n+614,-1,629.5,355.5,28.2,59.2,1\n+614,-1,1060.1,351.8,31.5,65.7,0.999\n+614,-1,1106.3,353.9,27.8,64.9,0.986\n+614,-1,743.5,357.2,34.9,69,0.956\n+614,-1,666.5,353.8,24.5,50.9,0.956\n+614,-1,1140.6,355.6,32.9,68.5,0.695\n+614,-1,696.7,351.2,23.5,49.6,0.683\n+614,-1,725.6,355.4,23.7,52.7,0.136\n+128,-1,679.3,383.1,38.4,112.5,1\n+128,-1,2.3,374,91.7,235.5,1\n+128,-1,1366.8,381.8,52.3,139.1,1\n+128,-1,603.3,405.8,34.4,52.5,1\n+128,-1,790.4,376.5,52.8,132.8,1\n+128,-1,894.2,384.4,72.2,168.6,1\n+128,-1,431.7,378.7,63.8,144.8,1\n+128,-1,960.1,369.1,50.5,148,1\n+128,-1,180.7,393.7,60.8,144.4,1\n+128,-1,866,380.8,31.5,112.4,0.996\n+128,-1,95.8,402.1,35.5,106.2,0.995\n+128,-1,1772.6,341.1,148.4,565.4,0.994\n+128,-1,388.1,406.2,25.2,77.8,0.09\n+128,-1,588.5,405.6,30.3,47.8,0.088\n+128,-1,418.4,401.9,29.7,98.5,0.057\n+315,-1,1499.7,225.6,235.4,751.7,1\n+315,-1,1098.8,278.3,66.2,164.2,1\n+315,-1,696.3,286,44.3,119.7,1\n+315,-1,19,279.4,35.5,96.4,1\n+315,-1,222,261,74.5,217.8,1\n+315,-1,905.9,283.1,48",add,Added STORM - 862 to Changelog
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-10-FRCNN/gt/gt.txt,,"1,1,1368,394,74,226,1,1,1.0\n2,1,1366,394,75,229,1,1,1.0\n3,1,1365,394,76,232,1,1,1.0\n4,1,1363,394,77,235,1,1,1.0\n5,1,1362,394,78,238,1,1,1.0\n1,2,1478,418,74,176,1,1,1.0\n2,2,1471,419,74,176,1,1,1.0\n3,2,1465,420,74,176,1,1,1.0\n4,2,1458,421,74,176,1,1,1.0\n5,2,1452,422,74,176,1,1,1.0\n1,3,680,407,67,199,1,1,1.0\n2,3,672,408,71,202,1,1,1.0\n3,3,664,410,75,205,1,1,1.0\n4,3,656,412,79,207,1,1,1.0\n5,3,648,414,84,210,1,1,1.0\n1,4,1232,412,36,112,1,1,1.0\n2,4,1223,411,39,115,1,1,1.0\n3,4,1214,411,42,117,1,1,1.0\n4,4,1205,410,45,120,1,1,1.0\n5,4,1196,410,48,122,1,1,1.0\n1,5,470,432,72,176,1,1,1.0\n2,5,464,431,73,177,1,1,1.0\n3,5,459,431,73,177,1,1,1.0\n4,5,453,430,74,178,1,1,1.0\n5,5,448,430,74,178,1,1,1.0\n1,6,730,421,55,173,1,1,0.67857\n2,6,721,421,58,176,1,1,0.61017\n3,6,713,421,60,179,1,1,0.55738\n4,6,704,421,63,182,1,1,0.5\n5,6,696,422,66,184,1,1,0.44776\n1,8,550,436,60,170,1,1,1.0\n2,8,544,436,61,170,1,1,1.0\n3,8,539,437,61,170,1,1,1.0\n4,8,533,437,62,170,1,1,0.66667\n5,8,528,438,6","@@ -0,0 +1,125 @@\n+1,1,1368,394,74,226,1,1,1.0\n+2,1,1366,394,75,229,1,1,1.0\n+3,1,1365,394,76,232,1,1,1.0\n+4,1,1363,394,77,235,1,1,1.0\n+5,1,1362,394,78,238,1,1,1.0\n+1,2,1478,418,74,176,1,1,1.0\n+2,2,1471,419,74,176,1,1,1.0\n+3,2,1465,420,74,176,1,1,1.0\n+4,2,1458,421,74,176,1,1,1.0\n+5,2,1452,422,74,176,1,1,1.0\n+1,3,680,407,67,199,1,1,1.0\n+2,3,672,408,71,202,1,1,1.0\n+3,3,664,410,75,205,1,1,1.0\n+4,3,656,412,79,207,1,1,1.0\n+5,3,648,414,84,210,1,1,1.0\n+1,4,1232,412,36,112,1,1,1.0\n+2,4,1223,411,39,115,1,1,1.0\n+3,4,1214,411,42,117,1,1,1.0\n+4,4,1205,410,45,120,1,1,1.0\n+5,4,1196,410,48,122,1,1,1.0\n+1,5,470,432,72,176,1,1,1.0\n+2,5,464,431,73,177,1,1,1.0\n+3,5,459,431,73,177,1,1,1.0\n+4,5,453,430,74,178,1,1,1.0\n+5,5,448,430,74,178,1,1,1.0\n+1,6,730,421,55,173,1,1,0.67857\n+2,6,721,421,58,176,1,1,0.61017\n+3,6,713,421,60,179,1,1,0.55738\n+4,6,704,421,63,182,1,1,0.5\n+5,6,696,422,66,184,1,1,0.44776\n+1,8,550,436,60,170,1,1,1.0\n+2,8,544,436,61,170,1,1,1.0\n+3,8,539,437,61,170,1,1,1.0\n+4,8,533,437,62,170,1,1,0.66667\n+5,8,528,438,62,170,1,1,0.66667\n+1,12,960,416,39,110,1,1,0.82342\n+2,12,955,417,39,110,1,1,0.82342\n+3,12,950,418,39,110,1,1,0.82342\n+4,12,945,419,40,111,1,1,0.80597\n+5,12,940,420,40,111,1,1,0.80597\n+1,13,1170,418,38,104,1,1,1.0\n+2,13,1164,420,38,104,1,1,1.0\n+3,13,1158,422,38,104,1,1,1.0\n+4,13,1152,424,38,104,1,1,1.0\n+5,13,1146,426,38,104,1,1,1.0\n+1,15,590,427,55,177,1,1,0.64396\n+2,15,585,427,55,178,1,1,0.64385\n+3,15,580,427,56,179,1,1,0.65205\n+4,15,575,427,57,180,1,1,0.65793\n+5,15,570,428,58,180,1,1,0.66373\n+1,18,1112,418,32,81,1,1,1.0\n+2,18,1108,418,30,81,1,1,1.0\n+3,18,1104,419,29,80,1,1,1.0\n+4,18,1100,419,30,81,1,1,1.0\n+5,18,1097,420,30,81,1,1,1.0\n+1,19,893,463,83,58,0,10,0.79762\n+2,19,888,464,84,58,0,10,0.78824\n+3,19,884,465,85,59,0,10,0.76744\n+4,19,880,466,86,59,0,10,0.74713\n+5,19,875,467,88,60,0,10,0.73034\n+1,20,-109,369,474,374,0,3,0.76842\n+2,20,-116,369,472,374,0,3,0.75264\n+3,20,-122,369,469,374,0,3,0.7383\n+4,20",add,Added STORM - 862 to Changelog
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-10-FRCNN/seqinfo.ini,,[Sequence]\nname=MOT17-10-FRCNN\nimDir=img1\nframeRate=30\nseqLength=654\nimWidth=1920\nimHeight=1080\nimExt=.jpg\n\n,"@@ -0,0 +1,9 @@\n+[Sequence]\n+name=MOT17-10-FRCNN\n+imDir=img1\n+frameRate=30\n+seqLength=654\n+imWidth=1920\n+imHeight=1080\n+imExt=.jpg\n+\n",add,Add note about data volume to enable
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-11-FRCNN/MOT17-11-FRCNN/000001.jpg,," Lavc54.35.0  C \n\n\n##$++3          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? qNJz&UjÒ­\niVÓ¶UQÄ²ÊQf7t+>Hx5×¾}IC6kcKLH) ))QuH3LehCiiaH?{[Â° [[t/ 4)ÂT'R9,0E<zHÛRÅR=SZ3\nj&MØB+\(trI\kSQ}tqG$C]1QQ_#e9Q]@YZP[0;Q#n*,h1XU*#ÜµÏ(4hp~Zkf>R×cÖ½\nPbWÔZEQ_I$Q9;MHnÖµTSj+Î¸UqH8QT)Ù³[c}+l^hGjÛN=""""jr+Ft#ZÜV+-kLLnr^zæ²¡'5nRzvWCk(uU%<H0?WFsÝ7`+""""dNQ[SV9/,LKH#'q\.Ú®Wtw("""" q+51/!, È®fdr+e+hq\n0Õ®)V8\F\1 sV]KW6\n,_Ä½E .Rh_pt`Ooc[Js\iÉWg$hQIKZKIE -Q@ QE nZÇ«p>+HM:e>\)Je4mPm:T!ÊR4@eR$m0é¨cibBU`y 4Í>Z$*8cHW$sK@-r+S4]|92TThU6{V)X	7UI!./BW`!sÉ¡0v\n&u\R(\n!VRBnTAÛ¶^N[[ZGl^+eU;m`508n1o4W}D PDV9W*ÚQ956#Ñå¨¶t:'V%/NkyÆ¡s5To#tM<ÕAEG35QH2M%--IchQ m%I1@MMVUUR1VH.",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-11-FRCNN/MOT17-11-FRCNN/000001.jpg differ\n,add,Added Type name for DFI ( # 3 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-11-FRCNN/MOT17-11-FRCNN/000002.jpg,," Lavc54.35.0  C \n\n\n##$++3          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? 3OI*<U0 j.ÅªZ5IØTÂbB58n7=:8CIjfcc×HWLu-6EE :E%0j3N4MFii,iyf-dÃk+kxytIN4IllcÐ¶lR1t@V)|iG#+BYEWXFnjsÆ3ONUWYjMÈ¥)hj[[æY#1NX8v&Ò²6CH;M7-vSÈ\nY&)r(ZD\noV%RUo""""Bc""""TRHNOZ<?Zh:IAW$N 4uLz8e85X_<Wh2Uc9Fç¨²Uk>PYÞ·3]*lJ>8""""faaU""""+#i3sÒ²`'5jVh]3NÒ¯\mO_=E]1$Xn}-ÝnVkJs~uqjc~Qß½iO%G !hDXd{ÓqË¹#B+:&6&F""""uXrZal2+5_GV/+r-*)kÔ¦bZS{VFPÔ8â³¢}QJZZ  ofjÕ¦KFæ¶2\nu6EPE%- - jk:0æ®NÙ¦ÒJ6i%2u6SiÆT!ÚT1JI4Ê¤HaP)c5b +BÑ§nIif0 !@ ND6 ^Ô·1b#6huZ9*rbCV""""`q7â³®/R1&oH^INF'9&*%6KGJYXYVH""""[H, ß¶K`kCqkUsJd0$P@Õ>j+iU?2tRb@bV4eN$ÎY\nÈPæ°Ùºi.f/y+SgfM2URÔm1@1K""""4Õ}(Hy=*CU6'&MF**Dt57}",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-11-FRCNN/MOT17-11-FRCNN/000002.jpg differ\n,add,Added Type name for DFI ( # 3 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-11-FRCNN/MOT17-11-FRCNN/000003.jpg,," Lavc54.35.0  C \n\n\n##$++3          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? jATÛ¨3bnV{5fmdVV.GW]Ë¼p5]n*lM`e^Ò©LETe(;a7JIqPc(5-GODu-2QMIIE14N4TGMSjcM (4hoc c>Õ]\n+>2MPxeVVËJ8q*ue>dpr2kr>n)G+HKSd\.j=i`Í³,qF#BTwFEZA4Y-hYØ¤.TZ%Z-'inemLVn^3=?*Ç¸~^g(; á¿½(Ö­?-RRS$]Ö²2Z5jbÔ½QqF)S\QFAPÕÚ´t~5Z'_ÆG3&8.SuGOCUY7=AVFJnPVs]J6.?}*h}@Ð\sxh&c -Ve>:j.W`};_$ Fz){8;H]kn[wi`Imd;<Y;Ù1<cY7V$RFsZrnGO4[rg-trW4v#TÂ¸f]Ö·+,c,_wF.(8JWiNqZA!vÈ  z6TÚTe*JtÜ©Ë«8CB=j%&&ME,bEdÒÏLVk[ÛkÉÅ$jRAN!h )'zu - lÞ²./S^Nm>LMMSj2RSqTiI$ReR$m0é¨cibBU`y-á¨JmRpOs[B3""""KU~B,Y+G45Ê­c`10uS9UF	' V)XuRHGb9'49FÜ±=ÔxYgBI!HMonemRBneBzVÅ¶$Vå½QvU;zV'4v*[YC LlV p0+)%g{Õ½f7en&/S.1FC",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-11-FRCNN/MOT17-11-FRCNN/000003.jpg differ\n,add,Added Type name for DFI ( # 3 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-11-FRCNN/MOT17-11-FRCNN/000004.jpg,," Lavc54.35.0  C \n\n\n##$++3          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? 5Kn7\pjHC30-[#S85M""""PqJrgBxicjSffNjV3+]rCSOeKe-!Ehcj3RÕ¢SM:VPÊ§} W*Í´7333M T ZYiZynÌ°J(q`Z5s{TX+B4TyShOØ´oÞv{y685C94yoFk0\?:8cG475MgSR|VdG$	A<Z=B_YSQgtR<}doL8lh?=>CkUÔªw=O9Zq+Ks7tØ®r~{Cs7msnR?SQÖ¢\nzUgP"""";y	Ú½.sq^p8pQsJ)6)VMÜ¨â¶¢o/ÖºL&\\+2u5ye1s[$C#rÈÆa%8Ic)3.vT\nÞ¹Lnx'*}+Re*9VEpbLã`é¬FX2oZJ{.b,G\YI_@F\nFV57 6XM<ORU~Ö¥ $`3 ucZk2Kwo^/:0wE_.Ë«f\n,PiÚ´*âJ\Uysò¢¯0'	""""HÓ¢+c0SihQE !QÞJZ JØVkÖ²./SRN46iILjGIO4Ê¡TJXSLD}0m6Um]w\n5-8QÓ¹;eÚ£æ·.a9B+;4^Å»C0Z1u ],se*H0e*2NbuQHGÉ¸#1cF-Ou$,x9gBI!m vmlIU$&VT'jiz`VQu>/N+eU;#r	zQY1.]-fuVLi@qE%Gâ£( Õ-EmÞµJK\sIÕT$.fsvMK5",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-11-FRCNN/MOT17-11-FRCNN/000004.jpg differ\n,add,Added Type name for DFI ( # 3 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-11-FRCNN/MOT17-11-FRCNN/000005.jpg,," Lavc54.35.0  C \n\n\n##$++3          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? Ñ±JdKSw×cÒ¹tRv?V@tt9ys-WB+]o3dPKMN}iÓRLhPHShRQ@vqL464XÐZi5eJ :&- xM>+jgQP\n\n/*hi-0_4sEevy(PSÌceWÖ±ê¥NÎL4zsO}jt*ÂVJb[&#?dÍ²xV_RQHÔ£D]-N\nj'KÖKcEj	P7N'cH^'Vx5v~S>SF2ks<gÂE4=j.S2*,P2	?Ò¹e_Ý·Ò¹5Rk\n#1kQmØFuÚ¦6IKM$?RRG\nY;Sr+Óµ #k JV3nz`jakÓµ#WnQê¹®[J,R:~5jwFA2tt5ITI\n{5 f8[`\>;m_jÒ±2NivDq6n`In w*yrW9M0Ñ¸+1;Zsn'_p8O 1É({U#Ø£}5ldGR%B9jacÛÕ69X8%dSÛ vQØ\nLR*-,h9C_Õ½Æ¡Z\S}fHGÕ§L05fM\YFQ[KIK@Q@Ó©)hi)hxzZÈ«Vïµ±YM]jS)\f4ÔM m>T2iHC)1ON4Ê¢D>Lci)jhg8&JakIGTx	$=nfYdj0ykW49Úº298Õ kUkH~Uå¿arÄ·)o^cpJIZC9MKÎi&Mb4W5}m%dÖ´T9-JrH\nØ¢9qSg$YMV UÕ*}D3QP:ÎQ>r4A5]/Rsspb$OÆ¡Slé¦½",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-11-FRCNN/MOT17-11-FRCNN/000005.jpg differ\n,add,Added Type name for DFI ( # 3 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-11-FRCNN/MOT17-11-FRCNN/000006.jpg,," Lavc54.35.0  C \n\n\n##$++3          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? ß´'y]t)tU:yZÕ°+[VKG!yYyKQkÅ±hæ6>3A#p	))7\nGMM/Uau0nÐ´MX'TÕ§1iWRO;_))M%AZp+BS#\nsKe-fr&ÃR-4ÙHk)ê³ZFy&F*UÚiFfD3SDQ{VGFÑX.7qÞ­U:.\nf""""}6Q\n^,Þ¦Í©IP m\~x\)j""""yÏO&+dT&5v<tu5B)Xw0/+)YÛ´yuØ§ÚºdTT-r0qK)QA?ZORj'Æ\nTOs:TU)XQ\nÍµ>p]DQak39>+>s3^G_Ý¹8y?c8Y(Zn$ thS(Y 0Z:r	5ra1Uec*QoLG[N>Z\2X4n5x1h<v`kBrî V/#Y v5t)Rl7s%_Zz;sr@^rk+QY"""",""""NVApzTÚ)WVhF""""Ç½X:b=x""""$W?QØMfQ6U/jHeR@fYW\nFÂZZ>66VDEVBREP\nu%- QE %mDV5\|EtiMmr4ÚiMMPiT2RVM0Ó2CiLi)\Â´Jb y""""Wuea9RZjWvu\nv9%>bJ-L25csZI]FÑTÙ­Iv""""r}MqOwoaC;~fysJj.t	i)i)%:M1z5tTZÓ 8mz4m.iyÝ¯_Êºh-.BzSO'#9,[.0}*ê (?}26j%D2Xsk(ê¹<\",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-11-FRCNN/MOT17-11-FRCNN/000006.jpg differ\n,add,Added Type name for DFI ( # 3 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-11-FRCNN/MOT17-11-FRCNN/000007.jpg,," Lavc54.35.0  C \n\n\n##$++3          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? y%E%}j|WIvUPJ	YcJ9jT3^%{z& 4kÇ uENÈ¢v$)7VwÑ¾X}7}c-0ec!SK7U1?Õ·Ò49CW3YAk&>'?Zjg&)(HÍVg#Fi8kB9VhAZv9+E4\A<\gO|p]*\VUXAjl.9n$M)VG&SÞOz}iD;Ö{~sz]5_D{RSð¨¦¹,1SÌm3qZ )Q61)j Q ^mnAUÄ¿345TU{Tu|J# \j\ÖghZiOIvG0\ntz[.Kx z/&pd4QEX?UOZ`K+zWR#FrUFb\n,/Mt%sXÜVe#qEmc$~|lÖ²7Zt-Â¢8#V5tH cnw!zV!Zc+Ûd=saOÄZØ®jxbj`n2	>Ø®iAdÞ¶'Rl\nTQQ(ej)bYTSÂª=+.:B$[wri~byoAVk3Â­=\nE"""")<0(4Ô¶ |dÌ¡I|?5ZT'ÖZ0j0\n)hdÆ­psX5o&FiDz(L@ â\n(NmnPkÖ*#H4l2iMe6IT""""3MeR1KEX4yUaS2<ÖM{aZ$Ckk%AnÂ»[H\n27sO;d}jz9NIKBnQ[Û+iPbcKB/XZJKÑ¹ÔyXf+yCb}2Qs0HCIKIH))Ø©BLEzP1Þ¯=u*LMgo1,zVm//][[Ç~=jiX\\n9.È¥	=*\â¾LJI",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-11-FRCNN/MOT17-11-FRCNN/000007.jpg differ\n,add,Added Type name for DFI ( # 3 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-11-FRCNN/det/det.txt,,"534,-1,1246.7,53.1,369.4,977.5,1\n534,-1,889.7,206.4,164.9,489.5,1\n534,-1,370.3,76.3,310.2,792,1\n534,-1,172.9,200.9,256.8,682,1\n534,-1,1138.8,274,133.8,319.3,0.999\n872,-1,804.1,161.1,167.6,477.3,1\n872,-1,1403,275.5,39.1,124.1,1\n872,-1,978.1,119.8,158.4,531.6,1\n872,-1,1860.9,272,40,99.2,1\n872,-1,1179.7,211.3,99.9,345.8,1\n872,-1,1801,266.9,38.9,117.9,1\n872,-1,1625.4,276.5,35.2,104.6,1\n872,-1,161.9,22.4,557.1,1058.6,0.999\n872,-1,1332.1,290.7,27.1,88.3,0.965\n872,-1,1459.2,293.3,27.1,69,0.965\n872,-1,1363.8,292.3,28.6,85.4,0.949\n872,-1,1424,279.4,35.4,104.3,0.098\n807,-1,1017.8,177.1,323.7,724.3,1\n807,-1,886.7,184.2,157.7,532.5,1\n807,-1,641.8,265.9,144.9,457.8,0.999\n807,-1,508,255.9,158.3,446,0.766\n807,-1,1,6.5,511.6,1074.5,0.65\n297,-1,1240.7,170.2,208.2,635.2,1\n297,-1,837.7,178.9,180.7,563.9,1\n297,-1,1063.9,205.3,187.9,554.2,1\n297,-1,379.1,123.1,274.4,765.4,1\n297,-1,33.8,114.3,305.4,821.9,1\n297,-1,1789.5,903.7,39.1,75.6,0.999\n297,-1,761.1,282,97.8,266.4,0.999\n297,","@@ -0,0 +1,6007 @@\n+534,-1,1246.7,53.1,369.4,977.5,1\n+534,-1,889.7,206.4,164.9,489.5,1\n+534,-1,370.3,76.3,310.2,792,1\n+534,-1,172.9,200.9,256.8,682,1\n+534,-1,1138.8,274,133.8,319.3,0.999\n+872,-1,804.1,161.1,167.6,477.3,1\n+872,-1,1403,275.5,39.1,124.1,1\n+872,-1,978.1,119.8,158.4,531.6,1\n+872,-1,1860.9,272,40,99.2,1\n+872,-1,1179.7,211.3,99.9,345.8,1\n+872,-1,1801,266.9,38.9,117.9,1\n+872,-1,1625.4,276.5,35.2,104.6,1\n+872,-1,161.9,22.4,557.1,1058.6,0.999\n+872,-1,1332.1,290.7,27.1,88.3,0.965\n+872,-1,1459.2,293.3,27.1,69,0.965\n+872,-1,1363.8,292.3,28.6,85.4,0.949\n+872,-1,1424,279.4,35.4,104.3,0.098\n+807,-1,1017.8,177.1,323.7,724.3,1\n+807,-1,886.7,184.2,157.7,532.5,1\n+807,-1,641.8,265.9,144.9,457.8,0.999\n+807,-1,508,255.9,158.3,446,0.766\n+807,-1,1,6.5,511.6,1074.5,0.65\n+297,-1,1240.7,170.2,208.2,635.2,1\n+297,-1,837.7,178.9,180.7,563.9,1\n+297,-1,1063.9,205.3,187.9,554.2,1\n+297,-1,379.1,123.1,274.4,765.4,1\n+297,-1,33.8,114.3,305.4,821.9,1\n+297,-1,1789.5,903.7,39.1,75.6,0.999\n+297,-1,761.1,282,97.8,266.4,0.999\n+297,-1,720.9,319.8,50.8,146.5,0.999\n+111,-1,688.5,394.2,70.4,172.1,1\n+111,-1,1086,301.5,192.9,468.5,1\n+111,-1,770.6,227.6,211.5,654.1,1\n+111,-1,423.1,348,108.1,366.6,1\n+111,-1,1589.9,364.6,50,139.4,1\n+111,-1,1278.8,389.8,48.7,126.9,1\n+111,-1,959.1,323.6,100.3,337.9,0.999\n+111,-1,1874.9,387.3,35.1,104.3,0.956\n+450,-1,1159.3,131.3,316.7,846.1,1\n+450,-1,704.2,198.5,176.4,534.9,1\n+450,-1,929.3,220.6,140,443.4,1\n+450,-1,596.6,306.8,109.1,261.9,1\n+450,-1,546.6,308.6,65.4,211.2,0.999\n+450,-1,1105.2,309.8,83.8,244.8,0.998\n+766,-1,969.5,232.7,163.7,401.3,1\n+766,-1,278.6,9.4,426.2,992.4,1\n+766,-1,836.2,168.1,158.3,517.1,1\n+766,-1,706.6,199.4,171.7,457.4,0.999\n+432,-1,1061.8,132.5,338.6,845.2,1\n+432,-1,692.4,207.6,178.3,542.5,1\n+432,-1,882.9,236.2,137.3,436.4,1\n+432,-1,1,1,506,1080,1\n+432,-1,594,324.1,102.3,233.5,1\n+432,-1,522.2,323.2,64.5,185,0.998\n+43,-1,921.5,265.5,208.7,690.3,1\n+43,-1,327.1,363.6,181.6,481.7,1\n+43,-1,570",add,Added note about dates .
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-11-FRCNN/gt/gt.txt,,"1,1,867,145,236,635,1,1,1.0\n2,1,870,146,228,633,1,1,1.0\n3,1,873,148,220,630,1,1,1.0\n4,1,877,150,212,627,1,1,1.0\n5,1,880,148,204,628,1,1,1.0\n6,1,884,147,196,628,1,1,1.0\n7,1,888,146,187,628,1,1,1.0\n1,2,-33,10,385,1122,1,1,0.86969\n2,2,-64,-5,408,1139,1,1,0.79681\n3,2,-95,-20,431,1156,1,1,0.72602\n4,2,-125,-35,453,1174,1,1,0.66405\n5,2,-136,-36,443,1178,1,1,0.63338\n6,2,-146,-37,432,1182,1,1,0.603\n7,2,-156,-38,421,1186,1,1,0.57136\n1,3,397,270,117,327,1,1,1.0\n2,3,397,270,117,327,1,1,1.0\n3,3,389,267,116,331,1,1,1.0\n4,3,381,265,116,334,1,1,1.0\n5,3,373,262,116,338,1,1,1.0\n6,3,365,260,116,341,1,1,1.0\n7,3,357,258,116,344,1,1,1.0\n1,4,546,272,103,317,1,1,1.0\n2,4,540,270,103,321,1,1,1.0\n3,4,535,268,103,325,1,1,1.0\n4,4,530,267,102,329,1,1,1.0\n5,4,524,265,103,333,1,1,1.0\n6,4,519,263,102,337,1,1,1.0\n7,4,514,262,102,341,1,1,1.0\n1,5,699,338,41,127,1,1,1.0\n2,5,696,337,41,127,1,1,1.0\n3,5,693,336,41,127,1,1,1.0\n4,5,690,336,41,127,1,1,1.0\n5,5,687,335,41,127,1,1,1.0\n6,5,684,334,4","@@ -0,0 +1,161 @@\n+1,1,867,145,236,635,1,1,1.0\n+2,1,870,146,228,633,1,1,1.0\n+3,1,873,148,220,630,1,1,1.0\n+4,1,877,150,212,627,1,1,1.0\n+5,1,880,148,204,628,1,1,1.0\n+6,1,884,147,196,628,1,1,1.0\n+7,1,888,146,187,628,1,1,1.0\n+1,2,-33,10,385,1122,1,1,0.86969\n+2,2,-64,-5,408,1139,1,1,0.79681\n+3,2,-95,-20,431,1156,1,1,0.72602\n+4,2,-125,-35,453,1174,1,1,0.66405\n+5,2,-136,-36,443,1178,1,1,0.63338\n+6,2,-146,-37,432,1182,1,1,0.603\n+7,2,-156,-38,421,1186,1,1,0.57136\n+1,3,397,270,117,327,1,1,1.0\n+2,3,397,270,117,327,1,1,1.0\n+3,3,389,267,116,331,1,1,1.0\n+4,3,381,265,116,334,1,1,1.0\n+5,3,373,262,116,338,1,1,1.0\n+6,3,365,260,116,341,1,1,1.0\n+7,3,357,258,116,344,1,1,1.0\n+1,4,546,272,103,317,1,1,1.0\n+2,4,540,270,103,321,1,1,1.0\n+3,4,535,268,103,325,1,1,1.0\n+4,4,530,267,102,329,1,1,1.0\n+5,4,524,265,103,333,1,1,1.0\n+6,4,519,263,102,337,1,1,1.0\n+7,4,514,262,102,341,1,1,1.0\n+1,5,699,338,41,127,1,1,1.0\n+2,5,696,337,41,127,1,1,1.0\n+3,5,693,336,41,127,1,1,1.0\n+4,5,690,336,41,127,1,1,1.0\n+5,5,687,335,41,127,1,1,1.0\n+6,5,684,334,41,127,1,1,1.0\n+7,5,681,334,41,127,1,1,1.0\n+1,6,739,323,51,131,1,1,0.96591\n+2,6,736,322,51,131,1,1,0.96591\n+3,6,733,322,51,131,1,1,0.96562\n+4,6,731,322,51,131,1,1,0.98281\n+5,6,728,321,51,131,1,1,0.98281\n+6,6,725,321,51,131,1,1,0.98266\n+7,6,723,321,51,131,1,1,1.0\n+1,8,1670,528,64,229,0,8,1.0\n+2,8,1670,528,64,229,0,8,1.0\n+3,8,1671,528,64,229,0,8,1.0\n+4,8,1672,528,64,229,0,8,1.0\n+5,8,1672,529,64,229,0,8,1.0\n+6,8,1673,529,64,229,0,8,1.0\n+7,8,1674,529,64,229,0,8,1.0\n+1,9,1736,531,128,225,0,8,1.0\n+2,9,1736,531,133,224,0,8,1.0\n+3,9,1736,531,138,224,0,8,1.0\n+4,9,1736,531,144,224,0,8,0.9931\n+5,9,1737,531,148,224,0,8,1.0\n+6,9,1737,531,154,224,0,8,0.99355\n+7,9,1737,531,159,224,0,8,0.9875\n+1,11,1123,374,810,88,0,9,0.96054\n+2,11,1121,374,810,88,0,9,0.96301\n+3,11,1120,375,810,88,0,9,0.96547\n+4,11,1119,375,810,88,0,9,0.96794\n+5,11,1118,376,810,88,0,9,0.97287\n+6,11,1117,376,810,88,0,9,0.97657\n+7,11,1116,377,810,88,0,9,0.9",add,Added STORM - 862 to Changelog
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-11-FRCNN/seqinfo.ini,,[Sequence]\nname=MOT17-11-FRCNN\nimDir=img1\nframeRate=30\nseqLength=900\nimWidth=1920\nimHeight=1080\nimExt=.jpg\n\n,"@@ -0,0 +1,9 @@\n+[Sequence]\n+name=MOT17-11-FRCNN\n+imDir=img1\n+frameRate=30\n+seqLength=900\n+imWidth=1920\n+imHeight=1080\n+imExt=.jpg\n+\n",add,Add note about data volume to enable
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-13-FRCNN/MOT17-13-FRCNN/000001.jpg,," Lavc54.35.0  C 					\n\n\n          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? TâDjC(E%()N$Q z--i)[%\npÓ!iÔ}KY?(f- p(0(\n~WKs* 	ÓJrMiKE*frI$eH8XjyÈ»NÂºwQS8QQJXÔ¶VsbI}8jfZTJ[PfP$=Fyu;ugÜµ+@#&ÝEE[yYk&p59b9SRÝ±Tå³	EVqBÄ{á¢Nte>W)'x	jre_#^UqR@N)+:m(7QEski1qÖº#X#5a8ÛGudÚj?{v^,7VrHlPkpécB^v:Ì¿=?\nvJk.Mé:5 G_YÒ±/ VMâKZ$pj""""(oUoXß³""""{|t)rØÌ·Z =)pFx'T$U0E-35kJNs} qz t'ULMNl.ec^/g}	\nIT0pyEmPO3""""tzpjÕ¬2EÉ\,B4É $x""""ßR.I ``^\#%Ö­sg2,<G ?:ê°¸VEs:\Ç;%A]O6(SH?Jm]È³*krAbZ?cXM96O`K{-NMrUT`~U}ÓZ|ganU;=x=ÉÑ¤""""KzG5<)wÊ#Õ|'uMSÌ""""aD[^Ü¸\æ¥¦i%VTÝ»V13Â¸hj[zfN$ê¥¤i1Üy+yGR(? ?d5?Ua ?E fI | K___U Ò$ ?Å=~>Ý¨z'*rO @ vO @Xuf9m",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-13-FRCNN/MOT17-13-FRCNN/000001.jpg differ\n,add,Added Type name for DFI ( # 3 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-13-FRCNN/MOT17-13-FRCNN/000002.jpg,," Lavc54.35.0  C 					\n\n\n          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? |m:IERRRSCB|=éz k@C@@1@$GEPÆ)=:EGDa-|(Z(P@Qp(0(Æ¢ge_JÑ)È¥I..MXIZcL;\{Z5fë¨«W*ÅATa\=U?Ý¿\nP ]×¡2Ñ°tÃªMo'2H[I<Y,	0ibß¡yÐªkIGp~<B|Q!&jm?Oy #\n-2JBeBux~ÍBOau)R$k#Þ¦Jn;#)]gÃ>Æ¹VU*Ã­DmNTh! M<gdV×<ÖÒ¼Å}c Û""""~=P<HVnä¦¹HÏµf^SZ,	qoÛt+CR cPB6Zz9Ú¦2lu2^{m5zvcTu5%_xY6Ij I;v>ZE*0f,;SO2N Ú§IkWEuAT`""""M`Ð cRLF]~Ð¹ 2z}Sv:`MHbbp@? xoWk(~^IÇ¿wOeR(UeBPn%T7</&== qms&[	Cd\)GGxgK\nD5\n9\[ÚlÒ²Ä¾<É³hqEoFsTToÖ¤$bÞ2DÊX!mbx7`qsJUA`Ò¤fa$?Zhkp""""_g9Ö;qK<NiW	WK h.å²¿vARvJK`,.kEc#+GÏ¹PeCÏ¼)dq?]Amr\ngK<7GZOkJZjsbiOOCR(h/er""""\np6#*j#B/ Åm&2lAÆ otgYI~LMê?j\}GÎ=CEÅ¹oD84%n'I#h=",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-13-FRCNN/MOT17-13-FRCNN/000002.jpg differ\n,add,Added Type name for DFI ( # 18480 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-13-FRCNN/MOT17-13-FRCNN/000003.jpg,," Lavc54.35.0  C 					\n\n\n          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? J( RJhhpE %Q@^""""ÒkuEGj\n|,?%(Z(0P(0(*9T*nc_#Z)ÔrMY!AiÙ=)W+^BZ[ [>oZZqo,âîº¯Ö¥Q\vÔ¶9;H& ÝªVWrF5|Ç»+<M%sM2O`@MZn*AW*~VjlnTâ²µE*JV})Xchb*KWN;L>mÒÂ¤jXP#"""".tYG7st!y-/mDoC#!dÄ¢+tQ@5Ç3Ð·5fMF1uuP2,^vx@]ax*!k,cQy[,ZlkÏ¶+h/tãµDdXRi4Ïµ21 tS}*3u6lLN+BrÄ¢t>kJßT>.b'Ô!GSdzW),3@\ÑQMsW$ `=VS""""mEH5AKá¤o ?U. }?*m  T!Ï/ P\%.n wk<,a)\F=|k_xZ[U)È£%FÞG^É­Tu0'Ó­ZZz962Hes	7{gvq ==Mym%s&PC_AMjP*)qMì§LV)k6d9*1UUe?SUjw55$kuO(ds#qLsF{N+Kk4|(!rq\n|+[Y\opT5Y+Jr7 :75h>8PÓ£ d=u?\nj>_""""iDw$wxZ /KWn""""Le q~oM>(_ 2zytl2|N   o |Æ 	4 ?3G$ Ï¨fzQ?1@hx?Zg9T>WZ|a. E)""""",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-13-FRCNN/MOT17-13-FRCNN/000003.jpg differ\n,add,Added Type name for DFI ( # 18480 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-13-FRCNN/MOT17-13-FRCNN/000004.jpg,," Lavc54.35.0  C 					\n\n\n          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? :JZJHh)E%(JSI@ >1R\nV)i- MIQRVUw:QEE(c(1E\n) QEG7OÂ¢Znz{# QUc\!lzV@'k^Ò²LF:S4|lRÛ¡omÓ/}{Æ«=Y \j3i""""l#^9 sMu'i#F=r.mHdw>IL305*sVxÌ¶Ô:0Dc^sHv?Þ¿nGj&Ú´,d`{T-N]634LT5oF7xaãj3\nV4c+7ROREk[-PÃ\Ì±6-*gEHÇ½y$ìdCÖ¼ÑH?\n%;]Û©O""""WA}""""U|j2\~]mO[8T:u'03%08L=r?CWmÄ?Þ«1]OdYËj'a	Âd49\@ÒÎÈ³xw/\nZ8GqGEØC\nPy{/^Ù®.]o,v1#9zï«H2wB8gO;fLm×¸43ÞgCf 23\n3Lg+s×~xS_Ka\n4eDZ""""EpRhqFV4g49;iTwiGS\n#l)Ô·[FsÖ¡	*?:lex&Å;;;gÒ¹3Z×to,Wz(u~(#-S|#PXWGf{t%O>%VGj=Üº-T \n*0uk|?i>oÊJ(á£Ùi<>qÐª(Ù@)ß¿<@Osm&Ã±/(Ä¿Ø|\n%Cuc4(|>SÚ2.Flx/R7WxØ¡s^x4&@>Px{q uy,åº¶*[qgß×¼E,1)cD^Í§jW.7`	n>{",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-13-FRCNN/MOT17-13-FRCNN/000004.jpg differ\n,add,Added Type name for DFI ( # 18480 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-13-FRCNN/MOT17-13-FRCNN/000005.jpg,," Lavc54.35.0  C 					\n\n\n          	\n       	\n   } !1AQa""""q2#BR$3br	\n%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz  w !1AQaq""""2B	#3Rbr\n$4%&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz 8""""      ? :JZJHh)E%(JSI@\njÓ¨ÂRzA@Z:UxXv:0(VV:P(Ð¢\nZAKIQE&S7Þ­(v>äojÊ¿-Öµ_e)4Ç\Ö¯jÊ² 2ÖµA\n?OÔ¶'e>Ç­3h{gSZ TSN[17mIFÚ61@*ZAT^ÆzO""""/Þ¹{3-h6.l2!C&iw8T4fS2Õ""""Ú¡ PXB Ã¨#Þ«Ü© 1 Õ¡MI7 cq~{imÜ¤TsS}F6pLiNqØ¢Z%Ö#ru\z4o\nAJ~%s+],c\f|U*0,gyo\~ r)+[?1Ú#L-{rc$gjmPSx<?&w4uU\DÍfÎ¨iOÆZx Æ«L? zZz1}&=i&);UWZ:nh+NwJgKq Kh9 ÖLÎ²-#jjs/50xQv*AINX')+Þ¸5FnSEgHEEs[WYyEÜ£qsz[]%~$nC/ZxKUE! zWWRAkkor\nRr <	GZlz8Ë·SWU1VfSLR0È¨As>G@73\+;Xu?::1iÍ»Æ,[e[p ÓRHrh!Ú­?Jm\Úy9=Hu{IF 3[>9ÔYDIaÛqfLnecWQII9OizGm>z0ÛZBkgx#5	kzSgXi Q5<iÌZc ~fyddE(9q'(!*?JO<'F|_r*zouosj[!g`\KeTWRWlW×xBÖ¯u	N",Binary files /dev/null and b/assets/MOT17-mini/train/MOT17-13-FRCNN/MOT17-13-FRCNN/000005.jpg differ\n,add,Added Type name for DFI ( # 18480 )
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-13-FRCNN/det/det.txt,,"219,-1,1338.8,554,51.5,135.7,1\n219,-1,1678.3,551.6,76.4,142.4,1\n219,-1,137.9,591.2,26.7,81,1\n219,-1,1010.6,529,32.3,71.5,0.999\n219,-1,877.3,531.5,23.4,60.5,0.999\n219,-1,824,519.8,17.1,49.2,0.998\n219,-1,27,575.1,29.2,91.5,0.988\n219,-1,7.6,576.9,32.3,92.3,0.983\n219,-1,1106.2,539.4,27.9,82.6,0.981\n219,-1,967.5,532.9,26.6,67.5,0.964\n219,-1,1081.6,540.4,29.6,78.6,0.94\n219,-1,995.5,532.8,25.4,65.4,0.817\n219,-1,1059.8,539.7,26.8,70,0.816\n219,-1,1041.5,530.4,27.8,69.7,0.805\n219,-1,982.3,532.6,24.6,69.5,0.775\n219,-1,953.7,534.4,24,62.8,0.093\n583,-1,209.1,597.1,83.6,170.4,1\n583,-1,1251.9,527.3,31.8,79.8,1\n583,-1,12.9,843.7,118,237.3,0.834\n316,-1,1700,543.9,36.7,104.6,1\n316,-1,1188.9,530.2,25.3,58.3,1\n316,-1,34.8,646.7,51.1,132.2,1\n316,-1,1390,599.4,53,136.1,0.999\n316,-1,257.3,628,38.2,135.1,0.999\n316,-1,1343.4,579,66.7,140.6,0.999\n316,-1,1265.7,521.3,24.2,61.7,0.999\n316,-1,222.5,656.6,33.4,121.7,0.998\n245,-1,1419.6,551.1,46.9,149,1\n245,-1,106.3,599.1,29.8,91.2,1\n245,","@@ -0,0 +1,8442 @@\n+219,-1,1338.8,554,51.5,135.7,1\n+219,-1,1678.3,551.6,76.4,142.4,1\n+219,-1,137.9,591.2,26.7,81,1\n+219,-1,1010.6,529,32.3,71.5,0.999\n+219,-1,877.3,531.5,23.4,60.5,0.999\n+219,-1,824,519.8,17.1,49.2,0.998\n+219,-1,27,575.1,29.2,91.5,0.988\n+219,-1,7.6,576.9,32.3,92.3,0.983\n+219,-1,1106.2,539.4,27.9,82.6,0.981\n+219,-1,967.5,532.9,26.6,67.5,0.964\n+219,-1,1081.6,540.4,29.6,78.6,0.94\n+219,-1,995.5,532.8,25.4,65.4,0.817\n+219,-1,1059.8,539.7,26.8,70,0.816\n+219,-1,1041.5,530.4,27.8,69.7,0.805\n+219,-1,982.3,532.6,24.6,69.5,0.775\n+219,-1,953.7,534.4,24,62.8,0.093\n+583,-1,209.1,597.1,83.6,170.4,1\n+583,-1,1251.9,527.3,31.8,79.8,1\n+583,-1,12.9,843.7,118,237.3,0.834\n+316,-1,1700,543.9,36.7,104.6,1\n+316,-1,1188.9,530.2,25.3,58.3,1\n+316,-1,34.8,646.7,51.1,132.2,1\n+316,-1,1390,599.4,53,136.1,0.999\n+316,-1,257.3,628,38.2,135.1,0.999\n+316,-1,1343.4,579,66.7,140.6,0.999\n+316,-1,1265.7,521.3,24.2,61.7,0.999\n+316,-1,222.5,656.6,33.4,121.7,0.998\n+245,-1,1419.6,551.1,46.9,149,1\n+245,-1,106.3,599.1,29.8,91.2,1\n+245,-1,1028.6,523.2,31.7,74.6,1\n+245,-1,904.1,529.2,26.7,66.1,0.999\n+245,-1,855.1,515.8,20.5,52.5,0.999\n+245,-1,385.2,534,17.5,38.5,0.999\n+245,-1,1182,520.5,26,78,0.997\n+245,-1,1157.8,522.5,22.6,74.8,0.207\n+664,-1,381.8,595.9,59.4,125.3,1\n+664,-1,366.4,529.9,31.2,75.9,1\n+664,-1,1031.9,509.3,21.6,63.4,0.999\n+664,-1,508.5,508.6,18.8,45.9,0.993\n+664,-1,494.9,507.9,20.8,47.2,0.984\n+664,-1,483.3,505.8,21.6,45.9,0.663\n+358,-1,1577.2,644.2,90.3,193.6,1\n+358,-1,1368.9,612.4,88.4,208.4,1\n+358,-1,1455.1,667.4,68.6,202.3,1\n+358,-1,106.6,675.4,48.4,169.6,1\n+358,-1,47.5,707,42.9,162.3,1\n+358,-1,1533.1,554.9,36.8,93.5,0.999\n+358,-1,1457.3,560.3,38,83.1,0.992\n+358,-1,1353.1,526.5,22.5,54.9,0.97\n+358,-1,1413.3,554.4,33.8,87.3,0.771\n+358,-1,1364.2,531,23,55,0.765\n+358,-1,1381.7,541.7,23.4,55.6,0.463\n+556,-1,667.9,584,57.5,123.3,1\n+556,-1,693.9,765.8,140.1,315.2,1\n+556,-1,750.9,534.2,35.5,95.6,1\n+556,-1,1480.2,533.5,27,72.2,1\n+556,",add,Added note about dates .
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-13-FRCNN/gt/gt.txt,,"1,1,1376,485,37,28,0,11,1.0\n2,1,1379,486,37,28,0,11,1.0\n3,1,1382,487,38,29,0,11,1.0\n4,1,1386,488,38,29,0,11,1.0\n5,1,1389,490,38,29,0,11,1.0\n1,2,1371,518,33,95,1,1,0.67647\n2,2,1375,516,32,94,1,1,0.66667\n3,2,1379,514,32,93,1,1,0.64829\n4,2,1383,513,33,99,1,1,0.61912\n5,2,1387,519,33,100,1,1,0.64123\n1,3,1391,474,10,148,0,10,0.80537\n2,3,1395,471,10,150,0,10,0.80795\n3,3,1399,468,10,152,0,10,0.80392\n4,3,1403,466,11,153,0,10,0.80519\n5,3,1406,471,11,154,0,10,0.80645\n1,4,1477,519,38,76,1,1,1.0\n2,4,1481,517,37,72,1,1,1.0\n3,4,1485,515,37,68,1,1,1.0\n4,4,1490,513,36,64,1,1,1.0\n5,4,1496,519,35,70,1,1,1.0\n1,5,1521,568,52,109,1,1,1.0\n2,5,1527,561,55,118,1,1,1.0\n3,5,1538,555,55,118,1,1,1.0\n4,5,1541,560,55,118,1,1,1.0\n5,5,1547,567,55,120,1,1,1.0\n1,6,1609,558,57,118,1,1,1.0\n2,6,1615,560,57,114,1,1,1.0\n3,6,1626,546,58,127,1,1,1.0\n4,6,1633,553,58,125,1,1,1.0\n5,6,1640,563,61,130,1,1,1.0\n1,7,1445,511,33,78,1,1,0.94713\n2,7,1448,504,33,79,1,1,0.97537\n3,7,1451,500,34,78,1,1,0.97685","@@ -0,0 +1,181 @@\n+1,1,1376,485,37,28,0,11,1.0\n+2,1,1379,486,37,28,0,11,1.0\n+3,1,1382,487,38,29,0,11,1.0\n+4,1,1386,488,38,29,0,11,1.0\n+5,1,1389,490,38,29,0,11,1.0\n+1,2,1371,518,33,95,1,1,0.67647\n+2,2,1375,516,32,94,1,1,0.66667\n+3,2,1379,514,32,93,1,1,0.64829\n+4,2,1383,513,33,99,1,1,0.61912\n+5,2,1387,519,33,100,1,1,0.64123\n+1,3,1391,474,10,148,0,10,0.80537\n+2,3,1395,471,10,150,0,10,0.80795\n+3,3,1399,468,10,152,0,10,0.80392\n+4,3,1403,466,11,153,0,10,0.80519\n+5,3,1406,471,11,154,0,10,0.80645\n+1,4,1477,519,38,76,1,1,1.0\n+2,4,1481,517,37,72,1,1,1.0\n+3,4,1485,515,37,68,1,1,1.0\n+4,4,1490,513,36,64,1,1,1.0\n+5,4,1496,519,35,70,1,1,1.0\n+1,5,1521,568,52,109,1,1,1.0\n+2,5,1527,561,55,118,1,1,1.0\n+3,5,1538,555,55,118,1,1,1.0\n+4,5,1541,560,55,118,1,1,1.0\n+5,5,1547,567,55,120,1,1,1.0\n+1,6,1609,558,57,118,1,1,1.0\n+2,6,1615,560,57,114,1,1,1.0\n+3,6,1626,546,58,127,1,1,1.0\n+4,6,1633,553,58,125,1,1,1.0\n+5,6,1640,563,61,130,1,1,1.0\n+1,7,1445,511,33,78,1,1,0.94713\n+2,7,1448,504,33,79,1,1,0.97537\n+3,7,1451,500,34,78,1,1,0.97685\n+4,7,1456,501,31,79,1,1,1.0\n+5,7,1461,509,30,80,1,1,1.0\n+1,8,-13,662,64,224,1,1,0.78462\n+2,8,-21,652,64,224,1,1,0.66154\n+1,9,122,711,49,97,1,1,1.0\n+2,9,111,698,49,97,1,1,1.0\n+3,9,105,726,49,97,1,1,1.0\n+4,9,94,724,49,97,1,1,1.0\n+5,9,82,717,38,104,1,1,1.0\n+1,10,195,575,46,154,1,1,1.0\n+2,10,188,563,46,154,1,1,1.0\n+3,10,178,584,46,154,1,1,1.0\n+4,10,173,581,46,154,1,1,1.0\n+5,10,165,581,46,154,1,1,1.0\n+1,11,273,585,46,148,1,1,0.76596\n+2,11,266,574,46,148,1,1,0.78723\n+3,11,258,592,46,148,1,1,0.65957\n+4,11,257,592,46,148,1,1,0.80851\n+5,11,247,586,46,148,1,1,0.76596\n+1,12,264,495,19,297,0,10,1.0\n+2,12,256,485,19,297,0,10,1.0\n+3,12,254,501,19,297,0,10,1.0\n+4,12,246,499,19,297,0,10,1.0\n+5,12,238,498,19,297,0,10,1.0\n+1,13,504,549,34,93,1,1,1.0\n+2,13,502,537,34,93,1,1,1.0\n+3,13,501,550,34,93,1,1,1.0\n+4,13,501,546,34,93,1,1,1.0\n+5,13,498,543,34,93,1,1,1.0\n+1,14,543,548,31,95,1,1,1.0\n+2,14,543,536,31,95,1,1,1.0\n+3,14",add,Added note about data volume to be able to Table .
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,assets/MOT17-mini/train/MOT17-13-FRCNN/seqinfo.ini,,[Sequence]\nname=MOT17-13-FRCNN\nimDir=img1\nframeRate=25\nseqLength=750\nimWidth=1920\nimHeight=1080\nimExt=.jpg\n\n,"@@ -0,0 +1,9 @@\n+[Sequence]\n+name=MOT17-13-FRCNN\n+imDir=img1\n+frameRate=25\n+seqLength=750\n+imWidth=1920\n+imHeight=1080\n+imExt=.jpg\n+\n",add,Add note about data volume to enable
27fbd8898d7a3366b585d808129f0c6e70792f39,eval on MOT17-mini by default,val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path.append(str","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path.append(str","@@ -36,6 +36,12 @@ if str(ROOT / 'strong_sort') not in sys.path:\n     sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\n ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n \n+import numpy as np\n+\n+from packaging import version\n+if version.parse(np.__version__) >= version.parse(""""1.24.0""""):\n+    np.float = np.float32\n+\n from yolov8.ultralytics.yolo.utils import LOGGER\n from yolov8.ultralytics.yolo.utils.checks import check_requirements, print_args\n from yolov8.ultralytics.yolo.utils.files import increment_path\n@@ -129,7 +135,7 @@ class Evaluator:\n         """"""""""""\n         \n         # set paths\n-        mot_seqs_path = val_tools_path / 'data' / opt.benchmark / opt.split\n+        mot_seqs_path = Path('./assets/MOT17-mini/train')\n         \n         if opt.benchmark == 'MOT17':\n             # each sequences is present 3 times, one for each detector\n@@ -242,7 +248,7 @@ class Evaluator:\n         p = subprocess.run(\n             args=[\n                 sys.executable,  val_tools_path / 'scripts' / 'run_mot_challenge.py',\n-                """"--GT_FOLDER"""", val_tools_path / 'data' / self.opt.benchmark / self.opt.split,\n+                """"--GT_FOLDER"""", './assets/MOT17-mini/train',\n                 """"--BENCHMARK"""", self.opt.benchmark,\n                 """"--TRACKERS_TO_EVAL"""",  self.opt.eval_existing if self.opt.eval_existing else self.opt.benchmark,\n                 """"--SPLIT_TO_EVAL"""", """"train"""",\n",add,Add forced default for text type as oCC
32f82eb0e08075fdcc47b9e1a1e6e71e331238c0,"Remove un controlled bounding box printing

removed printing of bboxes in StrongSort. It's extremely annoying when you get unexpected outputs in executing models.
It's a quick fix, just removed one line of code",trackers/strongsort/strong_sort.py,"import numpy as np\nimport torch\nimport sys\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists, join\nimport torchvision.transforms as transforms\n\nfrom sort.nn_matching import NearestNeighborDistanceMetric\nfrom sort.detection import Detection\nfrom sort.tracker import Tracker\n\nfrom reid_multibackend import ReIDDetectMultiBackend\n\nfrom yolov8.ultralytics.yolo.utils.ops import xyxy2xywh\n\n\nclass StrongSORT(object):\n    def __init__(self, \n                 model_weights,\n                 device,\n                 fp16,\n                 max_dist=0.2,\n                 max_iou_dist=0.7,\n                 max_age=70,\n                 max_unmatched_preds=7,\n                 n_init=3,\n                 nn_budget=100,\n                 mc_lambda=0.995,\n                 ema_alpha=0.9\n                ):\n\n        self.model = ReIDDetectMultiBackend(weights=model_weights, device=device, fp16=fp16)\n        \n        self.max_dist = max_dist\n        metric = Ne","import numpy as np\nimport torch\nimport sys\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists, join\nimport torchvision.transforms as transforms\n\nfrom sort.nn_matching import NearestNeighborDistanceMetric\nfrom sort.detection import Detection\nfrom sort.tracker import Tracker\n\nfrom reid_multibackend import ReIDDetectMultiBackend\n\nfrom yolov8.ultralytics.yolo.utils.ops import xyxy2xywh\n\n\nclass StrongSORT(object):\n    def __init__(self, \n                 model_weights,\n                 device,\n                 fp16,\n                 max_dist=0.2,\n                 max_iou_dist=0.7,\n                 max_age=70,\n                 max_unmatched_preds=7,\n                 n_init=3,\n                 nn_budget=100,\n                 mc_lambda=0.995,\n                 ema_alpha=0.9\n                ):\n\n        self.model = ReIDDetectMultiBackend(weights=model_weights, device=device, fp16=fp16)\n        \n        self.max_dist = max_dist\n        metric = Ne","@@ -50,7 +50,6 @@ class StrongSORT(object):\n         self.height, self.width = ori_img.shape[:2]\n         \n         # generate detections\n-        print(xywhs)\n         features = self._get_features(xywhs, ori_img)\n         bbox_tlwh = self._xywh_to_tlwh(xywhs)\n         detections = [Detection(bbox_tlwh[i], conf, features[i]) for i, conf in enumerate(\n",add,Added the necessary Harfbuzz scripts for rendering Japanese text correctly .
eb164fc2984ac9774a3bd67b2fe514ca886413b9,BoTSORT nwo default,track.py,"import argparse\nimport cv2\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov8'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport loggi","import argparse\nimport cv2\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov8'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport loggi","@@ -316,7 +316,7 @@ def parse_opt():\n     parser = argparse.ArgumentParser()\n     parser.add_argument('--yolo-weights', nargs='+', type=Path, default=WEIGHTS / 'yolov8s-seg.pt', help='model.pt path(s)')\n     parser.add_argument('--reid-weights', type=Path, default=WEIGHTS / 'osnet_x0_25_msmt17.pt')\n-    parser.add_argument('--tracking-method', type=str, default='bytetrack', help='strongsort, ocsort, bytetrack')\n+    parser.add_argument('--tracking-method', type=str, default='botsort', help='strongsort, ocsort, bytetrack, botsort')\n     parser.add_argument('--tracking-config', type=Path, default=None)\n     parser.add_argument('--source', type=str, default='0', help='file/dir/URL/glob, 0 for webcam')  \n     parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n",add,Added curContext . blendFunc to fake - dom .
fea3e85444c78c6061101551364c03e7b2445aed,deepocsort now default,track.py,"import argparse\nimport cv2\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov8'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport loggi","import argparse\nimport cv2\nimport os\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport platform\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport torch.backends.cudnn as cudnn\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov8'))  # add yolov5 ROOT to PATH\nif str(ROOT / 'trackers' / 'strongsort') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers' / 'strongsort'))  # add strong_sort ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport loggi","@@ -37,7 +37,7 @@ from yolov8.ultralytics.yolo.utils.checks import check_file, check_imgsz, check_\n from yolov8.ultralytics.yolo.utils.files import increment_path\n from yolov8.ultralytics.yolo.utils.torch_utils import select_device\n from yolov8.ultralytics.yolo.utils.ops import Profile, non_max_suppression, scale_boxes, process_mask, process_mask_native\n-from yolov8.ultralytics.yolo.utils.plotting import Annotator, colors, save_one_box\n+from yolov8.ultralytics.yolo.utils.plotting import Annotator, colors\n \n from trackers.multi_tracker_zoo import create_tracker\n \n@@ -316,7 +316,7 @@ def parse_opt():\n     parser = argparse.ArgumentParser()\n     parser.add_argument('--yolo-weights', nargs='+', type=Path, default=WEIGHTS / 'yolov8s-seg.pt', help='model.pt path(s)')\n     parser.add_argument('--reid-weights', type=Path, default=WEIGHTS / 'osnet_x0_25_msmt17.pt')\n-    parser.add_argument('--tracking-method', type=str, default='botsort', help='strongsort, ocsort, bytetrack, botsort')\n+    parser.add_argument('--tracking-method', type=str, default='deepocsort', help='deepocsort, botsort, strongsort, ocsort, bytetrack')\n     parser.add_argument('--tracking-config', type=Path, default=None)\n     parser.add_argument('--source', type=str, default='0', help='file/dir/URL/glob, 0 for webcam')  \n     parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n",add,Fix bytesPerPixel for SurfaceTexture
0536c322f9177d77a0d858689475a31db55e5432,"Fixed the issue with returning ""XXX.zip is corrupted"" even though it downloaded and unzipped successfully",val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","@@ -46,36 +46,29 @@ from yolov8.ultralytics.yolo.utils.files import increment_path\n from torch.utils.tensorboard import SummaryWriter\n \n from track import run\n-    \n+\n \n class Evaluator:\n     """"""""""""Evaluates a specific benchmark (MOT16, MOT17, MOT20) and split (train, val, test)\n-    \n-    This object provides interfaces to download: the official tools for MOT evaluation and the\n-    official MOT datasets. It also provides setup functionality to select which devices to run\n-    sequences on and configuration to enable evaluation on different MOT datasets.\n-\n-    Args:\n-        opt: the parsed script arguments\n \n-    Attributes:\n-        opt: the parsed script arguments\n-\n-    """"""""""""\n-    def __init__(self, opts):  \n+        This object provides interfaces to download: the official tools for MOT evaluation and the\n+        official MOT datasets. It also provides setup functionality to select which devices to run\n+        sequences on and configuration to enable evaluation on different MOT datasets.\n+        Args:\n+            opt: the parsed script arguments\n+        Attributes:\n+            opt: the parsed script arguments\n+        """"""""""""\n+    def __init__(self, opts):\n         self.opt = opts\n-        \n \n     def download_mot_eval_tools(self, val_tools_path):\n         """"""""""""Download officail evaluation tools for MOT metrics\n-\n         Args:\n             val_tools_path (pathlib.Path): path to the val tool folder destination\n-\n         Returns:\n             None\n         """"""""""""\n-        # source: https://github.com/JonathonLuiten/TrackEval#official-evaluation-code\n         val_tools_url = """"https://github.com/JonathonLuiten/TrackEval""""\n         try:\n             Repo.clone_from(val_tools_url, val_tools_path)\n@@ -83,45 +76,37 @@ class Evaluator:\n         except git.exc.GitError as err:\n             LOGGER.info('Eval repo already downloaded')\n \n-\n     def download_mot_dataset(self, val_tools_path, benchmark):\n       ",add,Add warning about data volume to enable_metrics_collection
0e97907cefdac5c8499f8adb94f6ef94de692d81,fix https://github.com/mikel-brostrom/yolov8_tracking/issues/823,trackers/deepocsort/ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom __future__ import print_function\n\nimport pdb\nimport pickle\n\nimport cv2\nimport torch\nimport torchvision\n\nimport numpy as np\nfrom .association import *\nfrom .embedding import EmbeddingComputer\nfrom .cmc import CMCComputer\nfrom reid_multibackend import ReIDDetectMultiBackend\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom __future__ import print_function\n\nimport pdb\nimport pickle\n\nimport cv2\nimport torch\nimport torchvision\n\nimport numpy as np\nfrom .association import *\nfrom .embedding import EmbeddingComputer\nfrom .cmc import CMCComputer\nfrom reid_multibackend import ReIDDetectMultiBackend\nfrom yolov8.ultralytics.yolo.utils.ops import xyxy2xywh\n\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect r","@@ -15,6 +15,8 @@ from .association import *\n from .embedding import EmbeddingComputer\n from .cmc import CMCComputer\n from reid_multibackend import ReIDDetectMultiBackend\n+from yolov8.ultralytics.yolo.utils.ops import xyxy2xywh\n+\n \n \n def k_previous_obs(observations, cur_age, k):\n@@ -531,10 +533,10 @@ class OCSort(object):\n         y2 = min(int(y + h / 2), self.height - 1)\n         return x1, y1, x2, y2\n     \n-    def _get_features(self, bbox_xywh, ori_img):\n+    def _get_features(self, bbox_xyxy, ori_img):\n         im_crops = []\n-        for box in bbox_xywh:\n-            x1, y1, x2, y2 = self._xywh_to_xyxy(box)\n+        for box in bbox_xyxy:\n+            x1, y1, x2, y2 = box.astype(int)\n             im = ori_img[y1:y2, x1:x2]\n             im_crops.append(im)\n         if im_crops:\n",add,Add note about data volume to enable_metrics_collection
9f993b74fd5bc0bcd3bd507b7f15e085f0fdfc1f,update default reid model,val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","@@ -311,8 +311,8 @@ class Evaluator:\n def parse_opt():\n     parser = argparse.ArgumentParser()\n     parser.add_argument('--yolo-weights', type=str, default=WEIGHTS / 'yolov8n.pt', help='model.pt path(s)')\n-    parser.add_argument('--reid-weights', type=str, default=WEIGHTS / 'osnet_x1_0_dukemtmcreid.pt')\n-    parser.add_argument('--tracking-method', type=str, default='strongsort', help='strongsort, ocsort')\n+    parser.add_argument('--reid-weights', type=str, default=WEIGHTS / 'lmbn_n_cuhk03_d.pt')\n+    parser.add_argument('--tracking-method', type=str, default='deepocsort', help='strongsort, ocsort')\n     parser.add_argument('--tracking-config', type=Path, default=None)\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n     parser.add_argument('--project', default=ROOT / 'runs' / 'val', help='save results to project/name')\n",add,Add KHR_gl_texture_2D_image extension string .
a9a33fe929f70c92d977e458ece540ceb8f1e155,update default reid model,evolve.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport yaml\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov8'))  # add yolov5 RO","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport yaml\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov8'))  # add yolov5 RO","@@ -281,9 +281,9 @@ def write_best_HOTA_params_to_config(opt, study):\n     \n def parse_opt():\n     parser = argparse.ArgumentParser()\n-    parser.add_argument('--yolo-weights', type=str, default=WEIGHTS / 'yolov8m.pt', help='model.pt path(s)')\n-    parser.add_argument('--reid-weights', type=str, default=WEIGHTS / 'osnet_x1_0_dukemtmcreid.pt')\n-    parser.add_argument('--tracking-method', type=str, default='strongsort', help='strongsort, ocsort')\n+    parser.add_argument('--yolo-weights', type=str, default=WEIGHTS / 'yolov8n.pt', help='model.pt path(s)')\n+    parser.add_argument('--reid-weights', type=str, default=WEIGHTS / 'lmbn_n_cuhk03_d.pt')\n+    parser.add_argument('--tracking-method', type=str, default='deepocsort', help='strongsort, ocsort')\n     parser.add_argument('--tracking-config', type=Path, default=None)\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n     parser.add_argument('--project', default=ROOT / 'runs' / 'evolve', help='save results to project/name')\n",add,Add note about data volume to enable EGL
e0616bf7689762ee262ac8fa87b67c049dd114c9,fix https://github.com/mikel-brostrom/yolov8_tracking/issues/856,trackers/strongsort/deep/reid_model_factory.py,"import torch\nimport time\nimport sys\nfrom collections import OrderedDict\n\n\n__model_types = [\n    'resnet50', 'mlfn', 'hacnn', 'mobilenetv2_x1_0', 'mobilenetv2_x1_4',\n    'osnet_x1_0', 'osnet_x0_75', 'osnet_x0_5', 'osnet_x0_25',\n    'osnet_ibn_x1_0', 'osnet_ain_x1_0', 'lmbn_n']\n\n__trained_urls = {\n\n    # market1501 models ########################################################\n    'resnet50_market1501.pt':\n    'https://drive.google.com/uc?id=1dUUZ4rHDWohmsQXCRe2C_HbYkzz94iBV',\n    'resnet50_dukemtmcreid.pt':\n    'https://drive.google.com/uc?id=17ymnLglnc64NRvGOitY3BqMRS9UWd1wg',\n    'resnet50_msmt17.pt':\n    'https://drive.google.com/uc?id=1ep7RypVDOthCRIAqDnn4_N-UhkkFHJsj',\n\n    'resnet50_fc512_market1501.pt':\n    'https://drive.google.com/uc?id=1kv8l5laX_YCdIGVCetjlNdzKIA3NvsSt',\n    'resnet50_fc512_dukemtmcreid.pt':\n    'https://drive.google.com/uc?id=13QN8Mp3XH81GK4BPGXobKHKyTGH50Rtx',\n    'resnet50_fc512_msmt17.pt':\n    'https://drive.google.com/uc?id=1fDJ","import torch\nimport time\nimport sys\nfrom collections import OrderedDict\n\n\n__model_types = [\n    'resnet50', 'mlfn', 'hacnn', 'mobilenetv2_x1_0', 'mobilenetv2_x1_4',\n    'osnet_x1_0', 'osnet_x0_75', 'osnet_x0_5', 'osnet_x0_25',\n    'osnet_ibn_x1_0', 'osnet_ain_x1_0', 'lmbn_n']\n\n__trained_urls = {\n\n    # market1501 models ########################################################\n    'resnet50_market1501.pt':\n    'https://drive.google.com/uc?id=1dUUZ4rHDWohmsQXCRe2C_HbYkzz94iBV',\n    'resnet50_dukemtmcreid.pt':\n    'https://drive.google.com/uc?id=17ymnLglnc64NRvGOitY3BqMRS9UWd1wg',\n    'resnet50_msmt17.pt':\n    'https://drive.google.com/uc?id=1ep7RypVDOthCRIAqDnn4_N-UhkkFHJsj',\n\n    'resnet50_fc512_market1501.pt':\n    'https://drive.google.com/uc?id=1kv8l5laX_YCdIGVCetjlNdzKIA3NvsSt',\n    'resnet50_fc512_dukemtmcreid.pt':\n    'https://drive.google.com/uc?id=13QN8Mp3XH81GK4BPGXobKHKyTGH50Rtx',\n    'resnet50_fc512_msmt17.pt':\n    'https://drive.google.com/uc?id=1fDJ","@@ -180,12 +180,17 @@ def load_pretrained_weights(model, weight_path):\n         >>> weight_path = 'log/my_model/model-best.pth.tar'\n         >>> load_pretrained_weights(model, weight_path)\n     """"""""""""\n+    \n     if not torch.cuda.is_available():\n         checkpoint = torch.load(weight_path, map_location=torch.device('cpu'))\n+    else:\n+        checkpoint = torch.load(weight_path)\n+        \n     if 'state_dict' in checkpoint:\n         state_dict = checkpoint['state_dict']\n     else:\n         state_dict = checkpoint\n+\n     model_dict = model.state_dict()\n \n     if 'lmbn' in str(weight_path):\n",fix,Add note about data volume to enable_metrics_collection
2fc281ec88f1b111333f02177d37c33e456bdd82,default model now N,track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\nfrom ultralytics.yolo.engine.predictor import BasePredictor, STREAM_WARNING\n\nfrom ultralytics.yolo.utils import DEFAULT_CFG, LOGGER, SETTINGS, callbacks, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_imgsz, check_imshow, print_args, check_requirements\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.utils.torch_utils import select_device, smart_inference_mode\nfrom ultralytics.yolo.data import load_inference_source\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWE","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n        opt.tracking_method /\\n       ","@@ -8,13 +8,10 @@ import cv2\n \n from trackers.multi_tracker_zoo import create_tracker\n from ultralytics.yolo.engine.model import YOLO, TASK_MAP\n-from ultralytics.yolo.engine.predictor import BasePredictor, STREAM_WARNING\n \n-from ultralytics.yolo.utils import DEFAULT_CFG, LOGGER, SETTINGS, callbacks, colorstr, ops\n-from ultralytics.yolo.utils.checks import check_imgsz, check_imshow, print_args, check_requirements\n+from ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir\n+from ultralytics.yolo.utils.checks import check_imgsz, print_args\n from ultralytics.yolo.utils.files import increment_path\n-from ultralytics.yolo.utils.torch_utils import select_device, smart_inference_mode\n-from ultralytics.yolo.data import load_inference_source\n from ultralytics.yolo.engine.results import Boxes\n from ultralytics.yolo.data.utils import VID_FORMATS\n \n@@ -66,10 +63,9 @@ def write_MOT_results(txt_path, results, frame_idx, i):\n \n @torch.no_grad()\n def run(\n-    yolo_model=WEIGHTS / 'yolov8s.pt',  # model.pt path(s),\n+    yolo_model=WEIGHTS / 'yolov8n.pt',  # model.pt path(s),\n     reid_model=WEIGHTS / 'osnet_x0_25_msmt17.pt',  # model.pt path,\n     tracking_method='strongsort',\n-    tracking_config=None,\n     source = '0',\n     imgsz = [640, 640],\n     save_dir=False,\n@@ -228,7 +224,7 @@ def run(\n         t = tuple(x.t / predictor.seen * 1E3 for x in predictor.profilers)  # speeds per image\n         LOGGER.info(f'Speed: %.1fms preprocess, %.1fms inference, %.1fms postprocess, %.1fms tracking per image at shape '\n                     f'{(1, 3, *imgsz)}' % t)\n-    if save or args.save_txt or args.save_crop:\n+    if save or predictor.args.save_txt or predictor.args.save_crop:\n         nl = len(list(predictor.save_dir.glob('labels/*.txt')))  # number of labels\n         s = f""""\n{nl} label{'s' * (nl > 1)} saved to {predictor.save_dir / 'labels'}"""" if predictor.args.save_txt else ''\n         LOGGER.info(f""""Results saved to {colorstr('b",add,added meteor
ae02cb6d267e77eda34dfe0e20eb260a296e38a3,fix import,trackers/botsort/bot_sort.py,"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import deque\n\nfrom trackers.botsort import  matching\nfrom trackers.botsort.gmc import GMC\nfrom trackers.botsort.basetrack import BaseTrack, TrackState\nfrom trackers.botsort.kalman_filter import KalmanFilter\n\n# from fast_reid.fast_reid_interfece import FastReIDInterface\n\nfrom reid_multibackend import ReIDDetectMultiBackend\nfrom yolov8.ultralytics.yolo.utils.ops import xyxy2xywh, xywh2xyxy\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        self.update_cls(cls, score)\n\n        self.score = score\n        self.tracklet_len = 0\n\n   ","import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import deque\n\nfrom trackers.botsort import  matching\nfrom trackers.botsort.gmc import GMC\nfrom trackers.botsort.basetrack import BaseTrack, TrackState\nfrom trackers.botsort.kalman_filter import KalmanFilter\n\n# from fast_reid.fast_reid_interfece import FastReIDInterface\n\nfrom trackers.deep.reid_multibackend import ReIDDetectMultiBackend\nfrom ultralytics.yolo.utils.ops import xyxy2xywh, xywh2xyxy\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        self.update_cls(cls, score)\n\n        self.score = score\n        self.tracklet_len = 0","@@ -10,8 +10,8 @@ from trackers.botsort.kalman_filter import KalmanFilter\n \n # from fast_reid.fast_reid_interfece import FastReIDInterface\n \n-from reid_multibackend import ReIDDetectMultiBackend\n-from yolov8.ultralytics.yolo.utils.ops import xyxy2xywh, xywh2xyxy\n+from trackers.deep.reid_multibackend import ReIDDetectMultiBackend\n+from ultralytics.yolo.utils.ops import xyxy2xywh, xywh2xyxy\n \n \n class STrack(BaseTrack):\n",add,Add note about data volume to enable
59ab5f12bf819eef3b764c184a80adcdcf09c7d3,fix import,trackers/strongsort/reid_multibackend.py,"import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport torchvision.transforms as T\nfrom collections import OrderedDict, namedtuple\nimport gdown\nfrom os.path import exists as file_exists\n\n\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\nfrom ultralytics.yolo.utils import LOGGER\nfrom trackers.strongsort.deep.reid_model_factory import (show_downloadeable_models, get_model_url, get_model_name,\n                                                          download_url, load_pretrained_weights)\nfrom trackers.strongsort.deep.models import build_model\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in",,"@@ -1,234 +0,0 @@\n-import torch.nn as nn\n-import torch\n-from pathlib import Path\n-import numpy as np\n-import torchvision.transforms as T\n-from collections import OrderedDict, namedtuple\n-import gdown\n-from os.path import exists as file_exists\n-\n-\n-from ultralytics.yolo.utils.checks import check_requirements, check_version\n-from ultralytics.yolo.utils import LOGGER\n-from trackers.strongsort.deep.reid_model_factory import (show_downloadeable_models, get_model_url, get_model_name,\n-                                                          download_url, load_pretrained_weights)\n-from trackers.strongsort.deep.models import build_model\n-\n-\n-def check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n-    # Check file(s) for acceptable suffix\n-    if file and suffix:\n-        if isinstance(suffix, str):\n-            suffix = [suffix]\n-        for f in file if isinstance(file, (list, tuple)) else [file]:\n-            s = Path(f).suffix.lower()  # file suffix\n-            if len(s):\n-                assert s in suffix, f""""{msg}{f} acceptable suffix is {suffix}""""\n-\n-\n-class ReIDDetectMultiBackend(nn.Module):\n-    # ReID models MultiBackend class for python inference on various backends\n-    def __init__(self, weights='osnet_x0_25_msmt17.pt', device=torch.device('cpu'), fp16=False):\n-        super().__init__()\n-\n-        w = weights[0] if isinstance(weights, list) else weights\n-        self.pt, self.jit, self.onnx, self.xml, self.engine, self.tflite = self.model_type(w)  # get backend\n-        self.fp16 = fp16\n-        self.fp16 &= self.pt or self.jit or self.engine  # FP16\n-\n-        # Build transform functions\n-        self.device = device\n-        self.image_size=(256, 128)\n-        self.pixel_mean=[0.485, 0.456, 0.406]\n-        self.pixel_std=[0.229, 0.224, 0.225]\n-        self.transforms = []\n-        self.transforms += [T.Resize(self.image_size)]\n-        self.transforms += [T.ToTensor()]\n-        self.transforms += [T.Nor",add,Add note about data volume to enable
59ab5f12bf819eef3b764c184a80adcdcf09c7d3,fix import,trackers/strongsort/strong_sort.py,"import numpy as np\nimport torch\nimport sys\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists, join\nimport torchvision.transforms as transforms\n\nfrom sort.nn_matching import NearestNeighborDistanceMetric\nfrom sort.detection import Detection\nfrom sort.tracker import Tracker\n\nfrom reid_multibackend import ReIDDetectMultiBackend\n\nfrom yolov8.ultralytics.yolo.utils.ops import xyxy2xywh\n\n\nclass StrongSORT(object):\n    def __init__(self, \n                 model_weights,\n                 device,\n                 fp16,\n                 max_dist=0.2,\n                 max_iou_dist=0.7,\n                 max_age=70,\n                 max_unmatched_preds=7,\n                 n_init=3,\n                 nn_budget=100,\n                 mc_lambda=0.995,\n                 ema_alpha=0.9\n                ):\n\n        self.model = ReIDDetectMultiBackend(weights=model_weights, device=device, fp16=fp16)\n        \n        self.max_dist = max_dist\n        metric = Ne","import numpy as np\nimport torch\nimport sys\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists, join\nimport torchvision.transforms as transforms\n\nfrom sort.nn_matching import NearestNeighborDistanceMetric\nfrom sort.detection import Detection\nfrom sort.tracker import Tracker\n\nfrom trackers.deep.reid_multibackend import ReIDDetectMultiBackend\n\nfrom ultralytics.yolo.utils.ops import xyxy2xywh\n\n\nclass StrongSORT(object):\n    def __init__(self, \n                 model_weights,\n                 device,\n                 fp16,\n                 max_dist=0.2,\n                 max_iou_dist=0.7,\n                 max_age=70,\n                 max_unmatched_preds=7,\n                 n_init=3,\n                 nn_budget=100,\n                 mc_lambda=0.995,\n                 ema_alpha=0.9\n                ):\n\n        self.model = ReIDDetectMultiBackend(weights=model_weights, device=device, fp16=fp16)\n        \n        self.max_dist = max_dist\n        metr","@@ -10,9 +10,9 @@ from sort.nn_matching import NearestNeighborDistanceMetric\n from sort.detection import Detection\n from sort.tracker import Tracker\n \n-from reid_multibackend import ReIDDetectMultiBackend\n+from trackers.deep.reid_multibackend import ReIDDetectMultiBackend\n \n-from yolov8.ultralytics.yolo.utils.ops import xyxy2xywh\n+from ultralytics.yolo.utils.ops import xyxy2xywh\n \n \n class StrongSORT(object):\n",add,Add note about data volume to enable
0735c6ff967f8e090d36251ad647fcd465cc5412,fix import,trackers/ocsort/ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom __future__ import print_function\n\nimport numpy as np\nfrom .association import *\nfrom yolov8.ultralytics.yolo.utils.ops import xywh2xyxy\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age-dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w/2.\n    y = bbox[1] + h/2.\n    s = w * h  # scale is just area\n    r = w / float(h+1e-6)\n    return np.array([x,","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom __future__ import print_function\n\nimport numpy as np\nfrom .association import *\nfrom ultralytics.yolo.utils.ops import xywh2xyxy\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age-dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w/2.\n    y = bbox[1] + h/2.\n    s = w * h  # scale is just area\n    r = w / float(h+1e-6)\n    return np.array([x, y, s, ","@@ -5,7 +5,7 @@ from __future__ import print_function\n \n import numpy as np\n from .association import *\n-from yolov8.ultralytics.yolo.utils.ops import xywh2xyxy\n+from ultralytics.yolo.utils.ops import xywh2xyxy\n \n \n def k_previous_obs(observations, cur_age, k):\n",add,Addobfbuzz scripts
4d82e71461848e38d23e928a6337ac824b3b5bf1,fix imports,.github/workflows/ci-testing.yml,"# name of the workflow, what it is doing (optional)\nname: CI CPU testing\n\n# events that trigger the workflow (required)\non:\n  push:\n    branches: [master, CIdebug]\n  pull_request:\n    # pull request where master is target\n    branches: [master]\n\n# the workflow that gets triggerd\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, windows-latest]  # Error: Container action is only supported on Linux\n        python-version: [3.9]\n        model: ['yolov5n']  # models to test\n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      # Check out the repository recursively, updated to v3\n      - uses: actions/checkout@v3\n        with:\n          submodules: recursive\n      # Prepare environment with python 3.9\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Install requirmen","# name of the workflow, what it is doing (optional)\nname: CI CPU testing\n\n# events that trigger the workflow (required)\non:\n  push:\n    branches: [master, CIdebug]\n  pull_request:\n    # pull request where master is target\n    branches: [master]\n\n# the workflow that gets triggerd\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, windows-latest]  # Error: Container action is only supported on Linux\n        python-version: [3.9]\n        model: ['yolov8n']  # models to test\n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      # Check out the repository recursively, updated to v3\n      - uses: actions/checkout@v3\n        with:\n          submodules: recursive\n      # Prepare environment with python 3.9\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Install requirmen","@@ -18,7 +18,7 @@ jobs:\n       matrix:\n         os: [ubuntu-latest, windows-latest]  # Error: Container action is only supported on Linux\n         python-version: [3.9]\n-        model: ['yolov5n']  # models to test\n+        model: ['yolov8n']  # models to test\n \n     # Timeout: https://stackoverflow.com/a/59076067/4521646\n     timeout-minutes: 50\n",add,Added hypest who apparently actually wrote the C # port
4d82e71461848e38d23e928a6337ac824b3b5bf1,fix imports,trackers/bytetrack/byte_tracker.py,"import numpy as np\nfrom collections import deque\nimport os\nimport os.path as osp\nimport copy\nimport torch\nimport torch.nn.functional as F\n\nfrom yolov8.ultralytics.yolo.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nfrom trackers.bytetrack.kalman_filter import KalmanFilter\nfrom trackers.bytetrack import matching\nfrom trackers.bytetrack.basetrack import BaseTrack, TrackState\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n    def __init__(self, tlwh, score, cls):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n        self.cls = cls\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(mean_state, self","import numpy as np\nfrom collections import deque\nimport os\nimport os.path as osp\nimport copy\nimport torch\nimport torch.nn.functional as F\n\nfrom ultralytics.yolo.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nfrom trackers.bytetrack.kalman_filter import KalmanFilter\nfrom trackers.bytetrack import matching\nfrom trackers.bytetrack.basetrack import BaseTrack, TrackState\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n    def __init__(self, tlwh, score, cls):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n        self.cls = cls\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covari","@@ -6,7 +6,7 @@ import copy\n import torch\n import torch.nn.functional as F\n \n-from yolov8.ultralytics.yolo.utils.ops import xywh2xyxy, xyxy2xywh\n+from ultralytics.yolo.utils.ops import xywh2xyxy, xyxy2xywh\n \n \n from trackers.bytetrack.kalman_filter import KalmanFilter\n",add,Add note about data volume to enable
2e5107e0ed35da6a7fd393aef5c919b3e44d045c,fix arg,trackers/reid_export.py,"import argparse\n\nimport os\n\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom ultralytics.yolo.utils.t","import argparse\n\nimport os\n\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov5') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov5'))  # add yolov5 ROOT to PATH\n\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nimport logging\nfrom ultralytics.yolo.utils.t","@@ -240,7 +240,7 @@ if __name__ == """"__main__"""":\n     parser.add_argument('--opset', type=int, default=12, help='ONNX: opset version')\n     parser.add_argument('--workspace', type=int, default=4, help='TensorRT: workspace size (GB)')\n     parser.add_argument('--verbose', action='store_true', help='TensorRT: verbose log')\n-    parser.add_argument('--weights', nargs='+', type=str, default=WEIGHTS / 'osnet_x0_25_msmt17.pt', help='model.pt path(s)')\n+    parser.add_argument('--weights', nargs='+', type=str, default=WEIGHTS / 'mobilenetv2_x1_4_dukemtmcreid.pt', help='model.pt path(s)')\n     parser.add_argument('--half', action='store_true', help='FP16 half-precision export')\n     parser.add_argument('--include',\n                         nargs='+',\n",add,Add warning about data volume to enable_metrics_collection
d3c4f7d1159c0addb3bfbcf0120712580507abd1,fix weights path,reid_export.py,,"import argparse\n\nimport os\n\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\nfrom trackers.deep.models import build_model\nfrom trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weig","@@ -0,0 +1,304 @@\n+import argparse\n+\n+import os\n+\n+# limit the number of cpus used by high performance libraries\n+os.environ[""""OMP_NUM_THREADS""""] = """"1""""\n+os.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\n+os.environ[""""MKL_NUM_THREADS""""] = """"1""""\n+os.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\n+os.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n+\n+import sys\n+import numpy as np\n+from pathlib import Path\n+import torch\n+import time\n+import platform\n+import pandas as pd\n+import subprocess\n+import torch.backends.cudnn as cudnn\n+from torch.utils.mobile_optimizer import optimize_for_mobile\n+import logging\n+\n+from ultralytics.yolo.utils.torch_utils import select_device\n+from ultralytics.yolo.utils import LOGGER, colorstr, ops\n+from ultralytics.yolo.utils.checks import check_requirements, check_version\n+from trackers.deep.models import build_model\n+from trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n+\n+FILE = Path(__file__).resolve()\n+ROOT = FILE.parents[0]  # root dir\n+WEIGHTS = ROOT / 'weights'\n+\n+\n+def file_size(path):\n+    # Return file/dir size (MB)\n+    path = Path(path)\n+    if path.is_file():\n+        return path.stat().st_size / 1E6\n+    elif path.is_dir():\n+        return sum(f.stat().st_size for f in path.glob('**/*') if f.is_file()) / 1E6\n+    else:\n+        return 0.0\n+\n+\n+def export_formats():\n+    # YOLOv5 export formats\n+    x = [\n+        ['PyTorch', '-', '.pt', True, True],\n+        ['TorchScript', 'torchscript', '.torchscript', True, True],\n+        ['ONNX', 'onnx', '.onnx', True, True],\n+        ['OpenVINO', 'openvino', '_openvino_model', True, False],\n+        ['TensorRT', 'engine', '.engine', False, True],\n+        ['TensorFlow Lite', 'tflite', '.tflite', True, False],\n+    ]\n+    return pd.DataFrame(x, columns=['Format', 'Argument', 'Suffix', 'CPU', 'GPU'])\n+\n+\n+def export_torchscript(model, im, file, optimize, prefix=colorstr('TorchScript:')):\n+    # YOLOv5 TorchScript mo",add,Add note about data volume to enable_metrics_collection
0be87c062563ad6112039dd1589938b00fb16203,fix import,trackers/deep/reid_multibackend.py,"import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nfrom itertools import islice\nimport torchvision.transforms as transforms\nimport cv2\nimport sys\nimport torchvision.transforms as T\nfrom collections import OrderedDict, namedtuple\nimport gdown\nfrom os.path import exists as file_exists\n\n\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\nfrom ultralytics.yolo.utils import LOGGER\nfrom trackers.deep.reid_model_factory import (show_downloadeable_models, get_model_url, get_model_name,\n                                                          download_url, load_pretrained_weights)\nfrom trackers.deep.models import build_model\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffi","import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nfrom itertools import islice\nimport torchvision.transforms as transforms\nimport cv2\nimport sys\nimport torchvision.transforms as T\nfrom collections import OrderedDict, namedtuple\nimport gdown\nfrom os.path import exists as file_exists\n\n\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\nfrom ultralytics.yolo.utils import LOGGER\nfrom trackers.deep.reid_model_factory import (show_downloadeable_models, get_model_url, get_model_name,\n                                                          download_url, load_pretrained_weights)\nfrom trackers.deep.models import build_model\n\n\ndef check_suffix(file='yolov5s.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffi","@@ -164,7 +164,7 @@ class ReIDDetectMultiBackend(nn.Module):\n     @staticmethod\n     def model_type(p='path/to/model.pt'):\n         # Return model type from model path, i.e. path='path/to/model.onnx' -> type=onnx\n-        from trackers.reid_export import export_formats\n+        from .reid_export import export_formats\n         sf = list(export_formats().Suffix)  # export suffixes\n         check_suffix(p, sf)  # checks\n         types = [s in Path(p).name for s in sf]\n",add,Add note about data volume to enable_metrics_collection
a7ea1d784096d70c645a6b05a1ab07136b0f8514,fix imports,trackers/deep/reid_export.py,"import argparse\n\nimport os\n\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\nfrom trackers.deep.models import build_model\nfrom trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weig","import argparse\n\nimport os\n\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\nfrom .models import build_model\nfrom .reid_model_factory import get_model_name, load_pretrained_weights\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef file_size(pa","@@ -24,8 +24,8 @@ import logging\n from ultralytics.yolo.utils.torch_utils import select_device\n from ultralytics.yolo.utils import LOGGER, colorstr, ops\n from ultralytics.yolo.utils.checks import check_requirements, check_version\n-from trackers.deep.models import build_model\n-from trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n+from .models import build_model\n+from .reid_model_factory import get_model_name, load_pretrained_weights\n \n FILE = Path(__file__).resolve()\n ROOT = FILE.parents[0]  # root dir\n",fix,Add note about data volume to enable
eef669f223ce51e3ba32ba794d840d05c5289529,fix imports,trackers/deep/reid_export.py,"import argparse\n\nimport os\n\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\nfrom .models import build_model\nfrom .reid_model_factory import get_model_name, load_pretrained_weights\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef file_size(pa","import argparse\n\nimport os\n\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\nfrom trackers.deep.models import build_model\nfrom trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weig","@@ -24,8 +24,8 @@ import logging\n from ultralytics.yolo.utils.torch_utils import select_device\n from ultralytics.yolo.utils import LOGGER, colorstr, ops\n from ultralytics.yolo.utils.checks import check_requirements, check_version\n-from .models import build_model\n-from .reid_model_factory import get_model_name, load_pretrained_weights\n+from trackers.deep.models import build_model\n+from trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n \n FILE = Path(__file__).resolve()\n ROOT = FILE.parents[0]  # root dir\n",fix,Add note about data volume to enable
6be2a590ef9ba8836cefafaa8c470e1d1880c75f,fix imports,trackers/bytetrack/byte_tracker.py,"import numpy as np\nfrom collections import deque\nimport os\nimport os.path as osp\nimport copy\nimport torch\nimport torch.nn.functional as F\n\nfrom ultralytics.yolo.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nfrom trackers.bytetrack.kalman_filter import KalmanFilter\nfrom trackers.bytetrack import matching\nfrom trackers.bytetrack.basetrack import BaseTrack, TrackState\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n    def __init__(self, tlwh, score, cls):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n        self.cls = cls\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covari","import numpy as np\nfrom collections import deque\nimport os\nimport os.path as osp\nimport copy\nimport torch\nimport torch.nn.functional as F\n\nfrom trackers.bytetrack.kalman_filter import KalmanFilter\nfrom trackers.bytetrack import matching\nfrom trackers.bytetrack.basetrack import BaseTrack, TrackState\n\nfrom ultralytics.yolo.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n    def __init__(self, tlwh, score, cls):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n        self.cls = cls\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covari","@@ -6,13 +6,13 @@ import copy\n import torch\n import torch.nn.functional as F\n \n-from ultralytics.yolo.utils.ops import xywh2xyxy, xyxy2xywh\n-\n-\n from trackers.bytetrack.kalman_filter import KalmanFilter\n from trackers.bytetrack import matching\n from trackers.bytetrack.basetrack import BaseTrack, TrackState\n \n+from ultralytics.yolo.utils.ops import xywh2xyxy, xyxy2xywh\n+\n+\n class STrack(BaseTrack):\n     shared_kalman = KalmanFilter()\n     def __init__(self, tlwh, score, cls):\n",add,Add note about data volume to enable
22565069ebbf6300ce7f6cb7e524c00577c53ea2,fix imports,trackers/deep/reid_export.py,"import argparse\n\nimport os\n\n# limit the number of cpus used by high performance libraries\nos.environ[""""OMP_NUM_THREADS""""] = """"1""""\nos.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\nos.environ[""""MKL_NUM_THREADS""""] = """"1""""\nos.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\nos.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\nfrom trackers.deep.models import build_model\nfrom trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weig","import argparse\nimport os\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom .models import build_model\nfrom .reid_model_factory import get_model_name, load_pretrained_weights\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1E6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob('**/*') if f.is_file()) / 1E6\n    else:\n        return 0.0\n\n\ndef export_f","@@ -1,14 +1,5 @@\n import argparse\n-\n import os\n-\n-# limit the number of cpus used by high performance libraries\n-os.environ[""""OMP_NUM_THREADS""""] = """"1""""\n-os.environ[""""OPENBLAS_NUM_THREADS""""] = """"1""""\n-os.environ[""""MKL_NUM_THREADS""""] = """"1""""\n-os.environ[""""VECLIB_MAXIMUM_THREADS""""] = """"1""""\n-os.environ[""""NUMEXPR_NUM_THREADS""""] = """"1""""\n-\n import sys\n import numpy as np\n from pathlib import Path\n@@ -21,11 +12,13 @@ import torch.backends.cudnn as cudnn\n from torch.utils.mobile_optimizer import optimize_for_mobile\n import logging\n \n+from .models import build_model\n+from .reid_model_factory import get_model_name, load_pretrained_weights\n+\n from ultralytics.yolo.utils.torch_utils import select_device\n from ultralytics.yolo.utils import LOGGER, colorstr, ops\n from ultralytics.yolo.utils.checks import check_requirements, check_version\n-from trackers.deep.models import build_model\n-from trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n+\n \n FILE = Path(__file__).resolve()\n ROOT = FILE.parents[0]  # root dir\n",add,Add key to image components inside of MovieCell .
1be46f692d73c15ac54699d1d6365e3e5fb355f6,fix imports,trackers/deep/reid_export.py,"import argparse\nimport os\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom .models import build_model\nfrom .reid_model_factory import get_model_name, load_pretrained_weights\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1E6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob('**/*') if f.is_file()) / 1E6\n    else:\n        return 0.0\n\n\ndef export_f","import argparse\nimport os\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom trackers.deep.models import build_model\nfrom trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1E6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob('**/*') if f.is_file()) / 1E6\n    else:\n        re","@@ -12,8 +12,8 @@ import torch.backends.cudnn as cudnn\n from torch.utils.mobile_optimizer import optimize_for_mobile\n import logging\n \n-from .models import build_model\n-from .reid_model_factory import get_model_name, load_pretrained_weights\n+from trackers.deep.models import build_model\n+from trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n \n from ultralytics.yolo.utils.torch_utils import select_device\n from ultralytics.yolo.utils import LOGGER, colorstr, ops\n",fix,Add Rossen
810b78127e830770c05e5a4a6f4db80aced079db,fix imports,trackers/deep/reid_export.py,"import argparse\nimport os\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom trackers.deep.models import build_model\nfrom trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1E6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob('**/*') if f.is_file()) / 1E6\n    else:\n        re","import argparse\nimport os\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0].parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'trackers') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers'))  # add yolov5 ROOT to PATH\n\nfrom trackers.deep.models import build_model\nfrom trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\n\n\n\ndef file_size(path):\n    # Return file/dir size (MB)","@@ -12,6 +12,15 @@ import torch.backends.cudnn as cudnn\n from torch.utils.mobile_optimizer import optimize_for_mobile\n import logging\n \n+FILE = Path(__file__).resolve()\n+ROOT = FILE.parents[0].parents[0].parents[0]  # root dir\n+WEIGHTS = ROOT / 'weights'\n+\n+if str(ROOT) not in sys.path:\n+    sys.path.append(str(ROOT))  # add ROOT to PATH\n+if str(ROOT / 'trackers') not in sys.path:\n+    sys.path.append(str(ROOT / 'trackers'))  # add yolov5 ROOT to PATH\n+\n from trackers.deep.models import build_model\n from trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n \n@@ -20,10 +29,6 @@ from ultralytics.yolo.utils import LOGGER, colorstr, ops\n from ultralytics.yolo.utils.checks import check_requirements, check_version\n \n \n-FILE = Path(__file__).resolve()\n-ROOT = FILE.parents[0]  # root dir\n-WEIGHTS = ROOT / 'weights'\n-\n \n def file_size(path):\n     # Return file/dir size (MB)\n",add,Add Rossen
a1931497c0984cec72decc396c274b22e506dee6,fix yolo-model argument,track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nprint(ROOT)\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n        opt.tracking_method /\\n        'configs' ","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nprint(ROOT)\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n        opt.tracking_method /\\n        'configs' ","@@ -242,7 +242,7 @@ def run(\n \n def parse_opt():\n     parser = argparse.ArgumentParser()\n-    parser.add_argument('--yolo-model', nargs='+', type=str, default=WEIGHTS / 'yolov8n.pt', help='model.pt path(s)')\n+    parser.add_argument('--yolo-model', type=str, default=WEIGHTS / 'yolov8n.pt', help='model.pt path(s)')\n     parser.add_argument('--reid-model', type=Path, default=WEIGHTS / 'mobilenetv2_x1_4_dukemtmcreid.pt')\n     parser.add_argument('--tracking-method', type=str, default='deepocsort', help='deepocsort, botsort, strongsort, ocsort, bytetrack')\n     parser.add_argument('--source', type=str, default='0', help='file/dir/URL/glob, 0 for webcam')  \n@@ -250,32 +250,20 @@ def parse_opt():\n     parser.add_argument('--conf', type=float, default=0.5, help='confidence threshold')\n     parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n     parser.add_argument('--show', action='store_true', help='display tracking video results')\n-    # parser.add_argument('--save-vid', action='store_true', help='save video tracking results')\n+    parser.add_argument('--save', action='store_true', help='save video tracking results')\n     # # class 0 is person, 1 is bycicle, 2 is car... 79 is oven\n     parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')\n-    # parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n-    # parser.add_argument('--augment', action='store_true', help='augmented inference')\n-    # parser.add_argument('--visualize', action='store_true', help='visualize features')\n-    # parser.add_argument('--update', action='store_true', help='update all models')\n     parser.add_argument('--project', default=ROOT / 'runs' / 'track', help='save results to project/name')\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n     parser.add_argument('--exists-ok', action='store_true', help='existin",add,Added STORM - 236 to Changelog
eb144c43de58edd305068e3893ba99122f99512e,fix subprocess track arguments,val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","@@ -45,7 +45,7 @@ from ultralytics.yolo.utils.files import increment_path\n \n from torch.utils.tensorboard import SummaryWriter\n \n-from track_v2 import run\n+from track import run\n \n \n class Evaluator:\n@@ -202,18 +202,18 @@ class Evaluator:\n \n                 p = subprocess.Popen([\n                     sys.executable, """"track_v2.py"""",\n-                    # """"--yolo-weights"""", self.opt.yolo_weights,\n-                    # """"--reid-weights"""", self.opt.reid_weights,\n-                    # """"--tracking-method"""", self.opt.tracking_method,\n-                    # """"--conf-thres"""", str(self.opt.conf_thres),\n+                    """"--yolo-model"""", self.opt.yolo_weights,\n+                    """"--reid-model"""", self.opt.reid_weights,\n+                    """"--tracking-method"""", self.opt.tracking_method,\n+                    """"--conf"""", str(self.opt.conf_thres),\n                     """"--imgsz"""", str(self.opt.imgsz[0]),\n-                    # """"--classes"""", str(0),\n+                    """"--classes"""", str(0),\n                     """"--name"""", save_dir.name,\n                     """"--project"""", self.opt.project,\n-                    # """"--device"""", str(tracking_subprocess_device),\n+                    """"--device"""", str(tracking_subprocess_device),\n                     """"--source"""", dst_seq_path,\n                     """"--exists-ok"""",\n-                    # """"--save-txt"""",\n+                    """"--save"""",\n                 ])\n                 processes.append(p)\n \n@@ -312,7 +312,7 @@ class Evaluator:\n def parse_opt():\n     parser = argparse.ArgumentParser()\n     parser.add_argument('--yolo-weights', type=str, default=WEIGHTS / 'yolov8n.pt', help='model.pt path(s)')\n-    parser.add_argument('--reid-weights', type=str, default=WEIGHTS / 'lmbn_n_cuhk03_d.pt')\n+    parser.add_argument('--reid-weights', type=str, default=WEIGHTS / 'mobilenetv2_x1_4_dukemtmcreid.pt')\n     parser.add_argument('--tracking-method', type=str, default='deepocsort', help='stron",add,Add warning about data volume to enable_metrics_collection
ae1811fa7407476a6e5a650830212ae6338d330c,fix arguments,val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","@@ -202,8 +202,8 @@ class Evaluator:\n \n                 p = subprocess.Popen([\n                     sys.executable, """"track.py"""",\n-                    """"--yolo-model"""", self.opt.yolo_weights,\n-                    """"--reid-model"""", self.opt.reid_weights,\n+                    """"--yolo-model"""", self.opt.yolo_model,\n+                    """"--reid-model"""", self.opt.reid_model,\n                     """"--tracking-method"""", self.opt.tracking_method,\n                     """"--conf"""", str(self.opt.conf_thres),\n                     """"--imgsz"""", str(self.opt.imgsz[0]),\n",add,Add note about data volume to enable_metrics_collection
a99b6259f4271db0b0239e177aac973bac894759,fix args,evolve.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport yaml\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path.append(str(ROOT / 'yolov8'))  # add yolov5 RO","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport yaml\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nfrom ultralytics.yolo.utils import LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom track import run\n\n\nclass Objective(Evaluator):\n    """"""""""""","@@ -24,16 +24,8 @@ FILE = Path(__file__).resolve()\n ROOT = FILE.parents[0]  # yolov5 strongsort root directory\n WEIGHTS = ROOT / 'weights'\n \n-if str(ROOT) not in sys.path:\n-    sys.path.append(str(ROOT))  # add ROOT to PATH\n-if str(ROOT / 'yolov8') not in sys.path:\n-    sys.path.append(str(ROOT / 'yolov8'))  # add yolov5 ROOT to PATH\n-if str(ROOT / 'strong_sort') not in sys.path:\n-    sys.path.append(str(ROOT / 'strong_sort'))  # add strong_sort ROOT to PATH\n-ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n-\n-from yolov8.ultralytics.yolo.utils import LOGGER\n-from yolov8.ultralytics.yolo.utils.checks import check_requirements, print_args\n+from ultralytics.yolo.utils import LOGGER\n+from ultralytics.yolo.utils.checks import check_requirements, print_args\n from track import run\n \n \n@@ -287,7 +279,7 @@ def parse_opt():\n     parser.add_argument('--tracking-config', type=Path, default=None)\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n     parser.add_argument('--project', default=ROOT / 'runs' / 'evolve', help='save results to project/name')\n-    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n+    parser.add_argument('--exists-ok', action='store_true', help='existing project/name ok, do not increment')\n     parser.add_argument('--benchmark', type=str,  default='MOT17', help='MOT16, MOT17, MOT20')\n     parser.add_argument('--split', type=str,  default='train', help='existing project/name ok, do not increment')\n     parser.add_argument('--eval-existing', type=str, default='', help='evaluate existing tracker results under mot_callenge/MOTXX-YY/...')\n",add,added meteor
a2fe49b1b919f925a0d637e6a45fcfd5a81e8484,fix arg names,evolve.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport yaml\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nfrom ultralytics.yolo.utils import LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom track import run\n\n\nclass Objective(Evaluator):\n    """"""""""""","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport yaml\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nfrom ultralytics.yolo.utils import LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom track import run\n\n\nclass Objective(Evaluator):\n    """"""""""""","@@ -273,8 +273,8 @@ def write_best_HOTA_params_to_config(opt, study):\n     \n def parse_opt():\n     parser = argparse.ArgumentParser()\n-    parser.add_argument('--yolo-weights', type=str, default=WEIGHTS / 'yolov8n.pt', help='model.pt path(s)')\n-    parser.add_argument('--reid-weights', type=str, default=WEIGHTS / 'lmbn_n_cuhk03_d.pt')\n+    parser.add_argument('--yolo-model', type=str, default=WEIGHTS / 'yolov8n.pt', help='model.pt path(s)')\n+    parser.add_argument('--reid-model', type=str, default=WEIGHTS / 'lmbn_n_cuhk03_d.pt')\n     parser.add_argument('--tracking-method', type=str, default='deepocsort', help='strongsort, ocsort')\n     parser.add_argument('--tracking-config', type=Path, default=None)\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n",add,Added information on how to set port
fc67c4ec22b4d532c66fbf0d061bffac29bde987,fix for pose models,track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nprint(ROOT)\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n        opt.tracking_method /\\n        'configs' ","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nprint(ROOT)\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n        opt.tracking_method /\\n        'configs' ","@@ -52,8 +52,8 @@ def write_MOT_results(txt_path, results, frame_idx, i):\n     i = torch.full((nr_dets, 1), i)\n     mot = torch.cat([\n         frame_idx,\n-        results.boxes.id.unsqueeze(1),\n-        ops.xyxy2ltwh(results.boxes.xyxy),\n+        results.boxes.id.unsqueeze(1).to('cpu'),\n+        ops.xyxy2ltwh(results.boxes.xyxy).to('cpu'),\n         dont_care,\n         i\n     ], dim=1)\n@@ -185,7 +185,7 @@ def run(\n             # overwrite bbox results with tracker predictions\n             if predictor.tracker_outputs[i].size != 0:\n                 predictor.results[i].boxes = Boxes(\n-                    boxes=torch.from_numpy(predictor.tracker_outputs[i]),\n+                    boxes=torch.from_numpy(predictor.tracker_outputs[i]).to(dets.device),\n                     orig_shape=im0.shape[:2],  # (height, width)\n                 )\n             \n@@ -206,7 +206,7 @@ def run(\n                         predictor.MOT_txt_path,\n                         predictor.results[i],\n                         frame_idx,\n-                        i\n+                        i,\n                     )\n \n             # display an image in a window using OpenCV imshow()\n",add,Added STORM - 132 to Changelog
68816861497f8ab9c206c63148ea0bacf82f5cfb,fix custom evaluation,val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","@@ -134,7 +134,12 @@ class Evaluator:\n             gt_folder = Path('./assets') / self.opt.benchmark / self.opt.split\n             seq_paths = [p / 'img1' for p in Path(mot_seqs_path).iterdir() if Path(p).is_dir()]\n \n-        save_dir = increment_path(Path(opt.project) / opt.name, exist_ok=opt.exists_ok)\n+        if opt.eval_existing and (Path(opt.project) / opt.name).exists():\n+            save_dir = Path(opt.project) / opt.name\n+            if not (Path(opt.project) / opt.name).exists():\n+                LOGGER.error(f'{save_dir} does not exist')\n+        else:\n+            save_dir = increment_path(Path(opt.project) / opt.name, exist_ok=opt.exists_ok)\n         MOT_results_folder = val_tools_path / 'data' / 'trackers' / 'mot_challenge' / opt.benchmark / save_dir.name / 'data'\n         (MOT_results_folder).mkdir(parents=True, exist_ok=True)  # make\n         return seq_paths, save_dir, MOT_results_folder, gt_folder\n@@ -222,30 +227,23 @@ class Evaluator:\n \n         print_args(vars(self.opt))\n \n-        results = (save_dir.parent / self.opt.eval_existing / 'tracks' if self.opt.eval_existing else save_dir / 'tracks').glob('*.txt')\n-        for src in results:\n-            if self.opt.eval_existing:\n-                dst = MOT_results_folder.parent.parent / self.opt.eval_existing / 'data' / Path(src.stem + '.txt')\n-            else:\n-                dst = MOT_results_folder / Path(src.stem + '.txt')\n-            dst.parent.mkdir(parents=True, exist_ok=True)  # make\n-            shutil.copyfile(src, dst)\n         # run the evaluation on the generated txts\n         d = [seq_path.parent.name for seq_path in seq_paths]\n         p = subprocess.run(\n             args=[\n-                     sys.executable, val_tools_path / 'scripts' / 'run_mot_challenge.py',\n-                     """"--GT_FOLDER"""", gt_folder,\n-                     """"--BENCHMARK"""", """""""",\n-                     """"--TRACKERS_FOLDER"""", save_dir,\n-                     """"--TRACKERS_TO_",add,Added Type name for DFI ( # 57480 )
a7e774c75557671561d9a2f34fa5a32769d88476,fix missing seq_path https://github.com/mikel-brostrom/yolov8_tracking/issues/883,val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","@@ -126,13 +126,13 @@ class Evaluator:\n             # (DPM, FRCNN, SDP). Keep only sequences from  one of them\n             seq_paths = sorted([str(p / 'img1') for p in Path(mot_seqs_path).iterdir() if Path(p).is_dir()])\n             seq_paths = [Path(p) for p in seq_paths if 'FRCNN' in p]\n-        elif opt.benchmark == 'MOT16' or opt.benchmark == 'MOT20':\n-            # this is not the case for MOT16, MOT20 or your custom dataset\n-            seq_paths = [p / 'img1' for p in Path(mot_seqs_path).iterdir() if Path(p).is_dir()]\n         elif opt.benchmark == 'MOT17-mini':\n             mot_seqs_path = Path('./assets') / self.opt.benchmark / self.opt.split\n             gt_folder = Path('./assets') / self.opt.benchmark / self.opt.split\n             seq_paths = [p / 'img1' for p in Path(mot_seqs_path).iterdir() if Path(p).is_dir()]\n+        else:\n+            # this is not the case for MOT16, MOT20 or your custom dataset\n+            seq_paths = [p / 'img1' for p in Path(mot_seqs_path).iterdir() if Path(p).is_dir()]\n \n         save_dir = increment_path(Path(opt.project) / opt.name, exist_ok=opt.exists_ok)\n         MOT_results_folder = val_tools_path / 'data' / 'trackers' / 'mot_challenge' / opt.benchmark / save_dir.name / 'data'\n",add,added ehcache back into defaults dependencies for war ' ing
4ed562534ea600b3b7e36719e4890e2276f010e0,fix --show bug,track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nprint(ROOT)\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n        opt.tracking_method /\\n        'configs' ","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nprint(ROOT)\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n        opt.tracking_method /\\n        'configs' ","@@ -84,7 +84,9 @@ def run(\n     device = '',\n     show = False,\n     half = True,\n-    classes = None\n+    classes = None,\n+    hide_label = False,\n+    hide_conf = False,\n ):\n     if source is None:\n         source = ROOT / 'assets' if is_git_dir() else 'https://ultralytics.com/images/bus.jpg'\n@@ -105,13 +107,16 @@ def run(\n     predictor.args.conf = 0.5\n     predictor.args.project = project\n     predictor.args.name = name\n+    predictor.args.show = show\n     predictor.args.conf = conf\n     predictor.args.half = half\n     predictor.args.classes = classes\n     predictor.args.imgsz = imgsz\n     predictor.args.vid_stride = vid_stride\n-    predictor.args.save_txt = True\n-    predictor.args.save = True\n+    predictor.args.save_txt = save_txt\n+    predictor.args.save = save\n+    predictor.args.hide_labels = hide_label\n+    predictor.args.hide_conf = hide_conf\n     predictor.write_MOT_results = write_MOT_results\n     if not predictor.model:\n         predictor.setup_model(model=model.model, verbose=False)\n@@ -212,7 +217,7 @@ def run(\n \n             # display an image in a window using OpenCV imshow()\n             if show and plotted_img is not None:\n-                predictor.show(p)\n+                predictor.show(p.parent)\n \n             # save video predictions\n             if save and plotted_img is not None:\n@@ -259,6 +264,8 @@ def parse_opt():\n     parser.add_argument('--exists-ok', action='store_true', help='existing project/name ok, do not increment')\n     parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n     parser.add_argument('--vid-stride', type=int, default=1, help='video frame-rate stride')\n+    parser.add_argument('--hide-label', action='store_true', help='hide labels when show')\n+    parser.add_argument('--hide-conf', action='store_true', help='hide confidences when show')\n     opt = parser.parse_args()\n     print_args(vars(opt))\n     return opt\n",add,Added the UNSTARTED state to the YouTube PlayerState enum
6f5fb38a985934f9fcfde2d9a0f2fb322e22374e,delete debug print,track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nprint(ROOT)\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n        opt.tracking_method /\\n        'configs' ","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n        opt.tracking_method /\\n        'configs' /\\n        (","@@ -18,7 +18,6 @@ from ultralytics.yolo.data.utils import VID_FORMATS\n WEIGHTS = Path(SETTINGS['weights_dir'])\n FILE = Path(__file__).resolve()\n ROOT = FILE.parents[0]  # root dir\n-print(ROOT)\n WEIGHTS = ROOT / 'weights'\n \n \n",fix,added metedp size
bdd819a23ec5ac63aac4c6b916fbe27d3cccc987,fix https://github.com/mikel-brostrom/yolov8_tracking/issues/888,track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n        opt.tracking_method /\\n        'configs' /\\n        (","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n        opt.tracking_method /\\n        'configs' /\\n        (","@@ -34,7 +34,7 @@ def on_predict_start(predictor):\n             predictor.args.tracking_method,\n             predictor.args.tracking_config,\n             predictor.args.reid_model,\n-            predictor.args.device,\n+            predictor.device,\n             predictor.args.half\n         )\n         predictor.trackers.append(tracker)\n@@ -88,7 +88,6 @@ def run(\n         source = ROOT / 'assets' if is_git_dir() else 'https://ultralytics.com/images/bus.jpg'\n         LOGGER.warning(f""""WARNING â ï¸ 'source' is missing. Using 'source={source}'."""")\n     \n-    print(yolo_model)\n     model = YOLO(yolo_model)\n     overrides = model.overrides.copy()\n     model.predictor = TASK_MAP[model.task][3](overrides=overrides, _callbacks=model.callbacks)\n@@ -97,7 +96,6 @@ def run(\n \n     # https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/engine/model.py\n     #model.predictor.setup_model(model=model.model, verbose=False)\n-    \n     predictor.args.reid_model = reid_model\n     predictor.args.tracking_method = tracking_method\n     predictor.args.conf = 0.5\n",add,Added Cython typedefs
00bd8b07269899ef383d83e611e24be082fdf32b,merge default args with custom,track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n        opt.tracking_method /\\n        'configs' /\\n        (","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n   ","@@ -5,11 +5,12 @@ import torch\n import argparse\n import numpy as np\n import cv2\n+from types import SimpleNamespace\n \n from trackers.multi_tracker_zoo import create_tracker\n from ultralytics.yolo.engine.model import YOLO, TASK_MAP\n \n-from ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir\n+from ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\n from ultralytics.yolo.utils.checks import check_imgsz, print_args\n from ultralytics.yolo.utils.files import increment_path\n from ultralytics.yolo.engine.results import Boxes\n@@ -59,73 +60,26 @@ def write_MOT_results(txt_path, results, frame_idx, i):\n \n \n @torch.no_grad()\n-def run(\n-    yolo_model=WEIGHTS / 'yolov8n.pt',  # model.pt path(s),\n-    reid_model=WEIGHTS / 'osnet_x0_25_msmt17.pt',  # model.pt path,\n-    tracking_method='strongsort',\n-    source = '0',\n-    imgsz = [640, 640],\n-    save_dir=False,\n-    vid_stride = 1,\n-    verbose = True,\n-    project = None,\n-    exists_ok = False,\n-    name = None,\n-    save = True,\n-    save_txt = True,\n-    visualize=False,\n-    plotted_img = False,\n-    augment = False,\n-    conf = 0.5,\n-    device = '',\n-    show = False,\n-    half = True,\n-    classes = None,\n-    hide_label = False,\n-    hide_conf = False,\n-):\n-    if source is None:\n-        source = ROOT / 'assets' if is_git_dir() else 'https://ultralytics.com/images/bus.jpg'\n-        LOGGER.warning(f""""WARNING â ï¸ 'source' is missing. Using 'source={source}'."""")\n+def run(args):\n     \n-    model = YOLO(yolo_model)\n+    model = YOLO(args['yolo_model'])\n     overrides = model.overrides.copy()\n     model.predictor = TASK_MAP[model.task][3](overrides=overrides, _callbacks=model.callbacks)\n     \n+    # extract task predictor\n     predictor = model.predictor\n \n-    # https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/engine/model.py\n-    #model.predictor.setup_model(model=model.model, ve",add,added meteor
cb7409220ea2025d83327380ef90af49640c5cbe,overwrite default ultralytic args by custom,track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n   ","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        Path('trackers') /\\n   ","@@ -70,7 +70,8 @@ def run(args):\n     predictor = model.predictor\n \n     # combine default predictor args with custom, preferring custom\n-    combined_args = {**predictor.args.__dict__, **args} \n+    combined_args = {**predictor.args.__dict__, **args}\n+    # overwrite default args\n     predictor.args = IterableSimpleNamespace(**combined_args)\n \n     # setup source and model\n@@ -182,11 +183,11 @@ def run(args):\n         predictor.vid_writer[-1].release()  # release final video writer\n \n     # Print results\n-    if verbose and predictor.seen:\n+    if predictor.args.verbose and predictor.seen:\n         t = tuple(x.t / predictor.seen * 1E3 for x in predictor.profilers)  # speeds per image\n         LOGGER.info(f'Speed: %.1fms preprocess, %.1fms inference, %.1fms postprocess, %.1fms tracking per image at shape '\n                     f'{(1, 3, *predictor.args.imgsz)}' % t)\n-    if save or predictor.args.save_txt or predictor.args.save_crop:\n+    if predictor.args.save or predictor.args.save_txt or predictor.args.save_crop:\n         nl = len(list(predictor.save_dir.glob('labels/*.txt')))  # number of labels\n         s = f""""\n{nl} label{'s' * (nl > 1)} saved to {predictor.save_dir / 'labels'}"""" if predictor.args.save_txt else ''\n         LOGGER.info(f""""Results saved to {colorstr('bold', predictor.save_dir)}{s}"""")\n@@ -214,6 +215,7 @@ def parse_opt():\n     parser.add_argument('--vid-stride', type=int, default=1, help='video frame-rate stride')\n     parser.add_argument('--hide-label', action='store_true', help='hide labels when show')\n     parser.add_argument('--hide-conf', action='store_true', help='hide confidences when show')\n+    parser.add_argument('--save-txt', action='store_true', help='save tracking results in a txt file')\n     opt = parser.parse_args()\n     print_args(vars(opt))\n     return opt\n",add,Add forced default for text type in ot requirements
cb7409220ea2025d83327380ef90af49640c5cbe,overwrite default ultralytic args by custom,val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","@@ -139,7 +139,7 @@ class Evaluator:\n             if not (Path(opt.project) / opt.name).exists():\n                 LOGGER.error(f'{save_dir} does not exist')\n         else:\n-            save_dir = increment_path(Path(opt.project) / opt.name, exist_ok=opt.exists_ok)\n+            save_dir = increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok)\n         MOT_results_folder = val_tools_path / 'data' / 'trackers' / 'mot_challenge' / opt.benchmark / save_dir.name / 'data'\n         (MOT_results_folder).mkdir(parents=True, exist_ok=True)  # make\n         return seq_paths, save_dir, MOT_results_folder, gt_folder\n@@ -214,10 +214,11 @@ class Evaluator:\n                     """"--imgsz"""", str(self.opt.imgsz[0]),\n                     """"--classes"""", str(0),\n                     """"--name"""", save_dir.name,\n+                    """"--save-txt"""",\n                     """"--project"""", self.opt.project,\n                     """"--device"""", str(tracking_subprocess_device),\n                     """"--source"""", dst_seq_path,\n-                    """"--exists-ok"""",\n+                    """"--exist-ok"""",\n                     """"--save"""",\n                 ])\n                 processes.append(p)\n@@ -319,7 +320,7 @@ def parse_opt():\n     parser.add_argument('--tracking-method', type=str, default='deepocsort', help='strongsort, ocsort')\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n     parser.add_argument('--project', default=ROOT / 'runs' / 'val', help='save results to project/name')\n-    parser.add_argument('--exists-ok', action='store_true', help='existing project/name ok, do not increment')\n+    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n     parser.add_argument('--benchmark', type=str, default='MOT17-mini', help='MOT16, MOT17, MOT20')\n     parser.add_argument('--split', type=str, default='train', help='existing project/name ok, do not increment')\n     parser.",add,Added Type name for DFI ( # 57480 )
cf2564449a857f8aefcf081cb8326afed8aeeeb8,fix flag name,evolve.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport yaml\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nfrom ultralytics.yolo.utils import LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom track import run\n\n\nclass Objective(Evaluator):\n    """"""""""""","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport yaml\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nfrom ultralytics.yolo.utils import LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom track import run\n\n\nclass Objective(Evaluator):\n    """"""""""""","@@ -279,7 +279,7 @@ def parse_opt():\n     parser.add_argument('--tracking-config', type=Path, default=None)\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n     parser.add_argument('--project', default=ROOT / 'runs' / 'evolve', help='save results to project/name')\n-    parser.add_argument('--exists-ok', action='store_true', help='existing project/name ok, do not increment')\n+    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n     parser.add_argument('--benchmark', type=str,  default='MOT17', help='MOT16, MOT17, MOT20')\n     parser.add_argument('--split', type=str,  default='train', help='existing project/name ok, do not increment')\n     parser.add_argument('--eval-existing', type=str, default='', help='evaluate existing tracker results under mot_callenge/MOTXX-YY/...')\n",add,Added curContext . blendFunc to fake - dom .
8ad367abb43bfce4b81a68a3890012653a9fdfcb,Fix np float attribute deprecation in NumPy>=1.20,trackers/strongsort/strong_sort.py,"import numpy as np\nimport torch\nimport sys\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists, join\nimport torchvision.transforms as transforms\n\nfrom .sort.nn_matching import NearestNeighborDistanceMetric\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\n\nfrom trackers.deep.reid_multibackend import ReIDDetectMultiBackend\n\nfrom ultralytics.yolo.utils.ops import xyxy2xywh\n\n\nclass StrongSORT(object):\n    def __init__(self, \n                 model_weights,\n                 device,\n                 fp16,\n                 max_dist=0.2,\n                 max_iou_dist=0.7,\n                 max_age=70,\n                 max_unmatched_preds=7,\n                 n_init=3,\n                 nn_budget=100,\n                 mc_lambda=0.995,\n                 ema_alpha=0.9\n                ):\n\n        self.model = ReIDDetectMultiBackend(weights=model_weights, device=device, fp16=fp16)\n        \n        self.max_dist = max_dist\n        m","import numpy as np\nimport torch\nimport sys\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists, join\nimport torchvision.transforms as transforms\n\nfrom .sort.nn_matching import NearestNeighborDistanceMetric\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\n\nfrom trackers.deep.reid_multibackend import ReIDDetectMultiBackend\n\nfrom ultralytics.yolo.utils.ops import xyxy2xywh\n\n\nclass StrongSORT(object):\n    def __init__(self, \n                 model_weights,\n                 device,\n                 fp16,\n                 max_dist=0.2,\n                 max_iou_dist=0.7,\n                 max_age=70,\n                 max_unmatched_preds=7,\n                 n_init=3,\n                 nn_budget=100,\n                 mc_lambda=0.995,\n                 ema_alpha=0.9\n                ):\n\n        self.model = ReIDDetectMultiBackend(weights=model_weights, device=device, fp16=fp16)\n        \n        self.max_dist = max_dist\n        m","@@ -76,7 +76,7 @@ class StrongSORT(object):\n             class_id = track.class_id\n             conf = track.conf\n             queue = track.q\n-            outputs.append(np.array([x1, y1, x2, y2, track_id, conf, class_id], dtype=np.float))\n+            outputs.append(np.array([x1, y1, x2, y2, track_id, conf, class_id], dtype=np.float64))\n         outputs = np.asarray(outputs)\n         return outputs\n \n",add,Added hypest who apparently actually wrote the C # port
eb4521238294c622fdbf2cc48a7931e93722b471,fix imports,tools/__init__.py,,,,add,Added TypeSpec . xml
eb4521238294c622fdbf2cc48a7931e93722b471,fix imports,tools/export.py,"import argparse\nimport os\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0].parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'trackers') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers'))  # add yolov5 ROOT to PATH\n\nfrom trackers.deep.models import build_model\nfrom trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\n\n\n\ndef file_size(path):\n    # Return file/dir size (MB)","import argparse\nimport os\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0].parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'trackers') not in sys.path:\n    sys.path.append(str(ROOT / 'trackers'))  # add yolov5 ROOT to PATH\n\nfrom boxmot.deep.models import build_model\nfrom boxmot.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\n\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n  ","@@ -21,8 +21,8 @@ if str(ROOT) not in sys.path:\n if str(ROOT / 'trackers') not in sys.path:\n     sys.path.append(str(ROOT / 'trackers'))  # add yolov5 ROOT to PATH\n \n-from trackers.deep.models import build_model\n-from trackers.deep.reid_model_factory import get_model_name, load_pretrained_weights\n+from boxmot.deep.models import build_model\n+from boxmot.deep.reid_model_factory import get_model_name, load_pretrained_weights\n \n from ultralytics.yolo.utils.torch_utils import select_device\n from ultralytics.yolo.utils import LOGGER, colorstr, ops\n",fix,Add Rossen
eb4521238294c622fdbf2cc48a7931e93722b471,fix imports,tools/track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom trackers.multi_tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        ROOT /\\n        Path('t","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        ROOT /\\n        'box","@@ -7,7 +7,7 @@ import numpy as np\n import cv2\n from types import SimpleNamespace\n \n-from trackers.multi_tracker_zoo import create_tracker\n+from boxmot.tracker_zoo import create_tracker\n from ultralytics.yolo.engine.model import YOLO, TASK_MAP\n \n from ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\n@@ -18,7 +18,7 @@ from ultralytics.yolo.data.utils import VID_FORMATS\n \n WEIGHTS = Path(SETTINGS['weights_dir'])\n FILE = Path(__file__).resolve()\n-ROOT = FILE.parents[0]  # root dir\n+ROOT = FILE.parents[0].parents[0]  # root dir\n WEIGHTS = ROOT / 'weights'\n \n \n@@ -27,7 +27,7 @@ def on_predict_start(predictor):\n     predictor.tracker_outputs = [None] * predictor.dataset.bs\n     predictor.args.tracking_config = \\n         ROOT /\\n-        Path('trackers') /\\n+        'boxmot' /\\n         opt.tracking_method /\\n         'configs' /\\n         (opt.tracking_method + '.yaml')\n",add,added metedp size
eb4521238294c622fdbf2cc48a7931e93722b471,fix imports,tools/val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # yolov5 strongsort root directory\nWEIGHTS = ROOT / 'weights'\n\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif str(ROOT / 'yolov8') not in sys.path:\n    sys.path","@@ -45,7 +45,7 @@ from ultralytics.yolo.utils.files import increment_path\n \n from torch.utils.tensorboard import SummaryWriter\n \n-from track import run\n+from .track import run\n \n \n class Evaluator:\n",add,Add debug flag
a459db1725b6cefd20f8e8ef78ba1d66b18c606a,fix imports,boxmot/tracker_zoo.py,"from trackers.strongsort.utils.parser import get_config\n\ndef create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n    \n    cfg = get_config()\n    cfg.merge_from_file(tracker_config)\n    \n    if tracker_type == 'strongsort':\n        from trackers.strongsort.strong_sort import StrongSORT\n        strongsort = StrongSORT(\n            reid_weights,\n            device,\n            half,\n            max_dist=cfg.strongsort.max_dist,\n            max_iou_dist=cfg.strongsort.max_iou_dist,\n            max_age=cfg.strongsort.max_age,\n            max_unmatched_preds=cfg.strongsort.max_unmatched_preds,\n            n_init=cfg.strongsort.n_init,\n            nn_budget=cfg.strongsort.nn_budget,\n            mc_lambda=cfg.strongsort.mc_lambda,\n            ema_alpha=cfg.strongsort.ema_alpha,\n\n        )\n        return strongsort\n    \n    elif tracker_type == 'ocsort':\n        from trackers.ocsort.ocsort import OCSort\n        ocsort = OCSort(\n            det_t","from boxmot.strongsort.utils.parser import get_config\n\ndef create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n    \n    cfg = get_config()\n    cfg.merge_from_file(tracker_config)\n    \n    if tracker_type == 'strongsort':\n        from boxmot.strongsort.strong_sort import StrongSORT\n        strongsort = StrongSORT(\n            reid_weights,\n            device,\n            half,\n            max_dist=cfg.strongsort.max_dist,\n            max_iou_dist=cfg.strongsort.max_iou_dist,\n            max_age=cfg.strongsort.max_age,\n            max_unmatched_preds=cfg.strongsort.max_unmatched_preds,\n            n_init=cfg.strongsort.n_init,\n            nn_budget=cfg.strongsort.nn_budget,\n            mc_lambda=cfg.strongsort.mc_lambda,\n            ema_alpha=cfg.strongsort.ema_alpha,\n\n        )\n        return strongsort\n    \n    elif tracker_type == 'ocsort':\n        from boxmot.ocsort.ocsort import OCSort\n        ocsort = OCSort(\n            det_thresh=","@@ -1,4 +1,4 @@\n-from trackers.strongsort.utils.parser import get_config\n+from boxmot.strongsort.utils.parser import get_config\n \n def create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n     \n@@ -6,7 +6,7 @@ def create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n     cfg.merge_from_file(tracker_config)\n     \n     if tracker_type == 'strongsort':\n-        from trackers.strongsort.strong_sort import StrongSORT\n+        from boxmot.strongsort.strong_sort import StrongSORT\n         strongsort = StrongSORT(\n             reid_weights,\n             device,\n@@ -24,7 +24,7 @@ def create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n         return strongsort\n     \n     elif tracker_type == 'ocsort':\n-        from trackers.ocsort.ocsort import OCSort\n+        from boxmot.ocsort.ocsort import OCSort\n         ocsort = OCSort(\n             det_thresh=cfg.ocsort.det_thresh,\n             max_age=cfg.ocsort.max_age,\n@@ -38,7 +38,7 @@ def create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n         return ocsort\n     \n     elif tracker_type == 'bytetrack':\n-        from trackers.bytetrack.byte_tracker import BYTETracker\n+        from boxmot.bytetrack.byte_tracker import BYTETracker\n         bytetracker = BYTETracker(\n             track_thresh=cfg.bytetrack.track_thresh,\n             match_thresh=cfg.bytetrack.match_thresh,\n@@ -48,7 +48,7 @@ def create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n         return bytetracker\n     \n     elif tracker_type == 'botsort':\n-        from trackers.botsort.bot_sort import BoTSORT\n+        from boxmot.botsort.bot_sort import BoTSORT\n         botsort = BoTSORT(\n             reid_weights,\n             device,\n@@ -65,7 +65,7 @@ def create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n         )\n         return botsort\n     elif tracker_type == 'deepocsort':\n-        from t",add,Added the necessary Harfbuzz scripts for rendering Japanese text correctly .
1cab21ad55b5b0e84e37be3ce1bed01b434a1bac,fix paths to resources,examples/val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # examples absolute path\nEXAMPLES_ROOT = FILE.parents[0]  # examples absolute path\nprint(EXAMPLES_ROOT)\nWEIGHTS = EXAMPLES_ROOT / 'weights'\n\n\nif str(EXAMPLES_ROOT) not in sys.path:\n    ","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # examples absolute path\nEXAMPLES_ROOT = FILE.parents[0]  # examples absolute path\nprint(EXAMPLES_ROOT)\nWEIGHTS = EXAMPLES_ROOT / 'weights'\n\n\nif str(EXAMPLES_ROOT) not in sys.path:\n    ","@@ -125,8 +125,8 @@ class Evaluator:\n             seq_paths = sorted([str(p / 'img1') for p in Path(mot_seqs_path).iterdir() if Path(p).is_dir()])\n             seq_paths = [Path(p) for p in seq_paths if 'FRCNN' in p]\n         elif opt.benchmark == 'MOT17-mini':\n-            mot_seqs_path = Path('../assets') / self.opt.benchmark / self.opt.split\n-            gt_folder = Path('../assets') / self.opt.benchmark / self.opt.split\n+            mot_seqs_path = ROOT / 'assets' / self.opt.benchmark / self.opt.split\n+            gt_folder = ROOT / 'assets' / self.opt.benchmark / self.opt.split\n             seq_paths = [p / 'img1' for p in Path(mot_seqs_path).iterdir() if Path(p).is_dir()]\n         else:\n             # this is not the case for MOT16, MOT20 or your custom dataset\n@@ -204,7 +204,7 @@ class Evaluator:\n                     shutil.move(str(src_seq_path), str(dst_seq_path))\n \n                 p = subprocess.Popen([\n-                    sys.executable, """"track.py"""",\n+                    sys.executable, str(EXAMPLES_ROOT / """"track.py""""),\n                     """"--yolo-model"""", self.opt.yolo_model,\n                     """"--reid-model"""", self.opt.reid_model,\n                     """"--tracking-method"""", self.opt.tracking_method,\n",add,Add _binomial_double_trees by default
600047496376fb3245a8f000892033e078e3b3bc,fix tracker config path,boxmot/tracker_zoo.py,"from boxmot.strongsort.utils.parser import get_config\n\ndef create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n    \n    cfg = get_config()\n    cfg.merge_from_file(tracker_config)\n    \n    if tracker_type == 'strongsort':\n        from boxmot.strongsort.strong_sort import StrongSORT\n        strongsort = StrongSORT(\n            reid_weights,\n            device,\n            half,\n            max_dist=cfg.strongsort.max_dist,\n            max_iou_dist=cfg.strongsort.max_iou_dist,\n            max_age=cfg.strongsort.max_age,\n            max_unmatched_preds=cfg.strongsort.max_unmatched_preds,\n            n_init=cfg.strongsort.n_init,\n            nn_budget=cfg.strongsort.nn_budget,\n            mc_lambda=cfg.strongsort.mc_lambda,\n            ema_alpha=cfg.strongsort.ema_alpha,\n\n        )\n        return strongsort\n    \n    elif tracker_type == 'ocsort':\n        from boxmot.ocsort.ocsort import OCSort\n        ocsort = OCSort(\n            det_thresh=","from pathlib import Path\nfrom boxmot.strongsort.utils.parser import get_config\n\n\ndef get_tracker_config(tracker_type):\n    tracking_config = \\n        Path('./boxmot') /\\n        tracker_type /\\n        'configs' /\\n        (tracker_type + '.yaml')\n    return tracking_config\n    \n\ndef create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n    \n    cfg = get_config()\n    cfg.merge_from_file(tracker_config)\n    \n    if tracker_type == 'strongsort':\n        from boxmot.strongsort.strong_sort import StrongSORT\n        strongsort = StrongSORT(\n            reid_weights,\n            device,\n            half,\n            max_dist=cfg.strongsort.max_dist,\n            max_iou_dist=cfg.strongsort.max_iou_dist,\n            max_age=cfg.strongsort.max_age,\n            max_unmatched_preds=cfg.strongsort.max_unmatched_preds,\n            n_init=cfg.strongsort.n_init,\n            nn_budget=cfg.strongsort.nn_budget,\n            mc_lambda=cfg.strongsort.mc_","@@ -1,5 +1,16 @@\n+from pathlib import Path\n from boxmot.strongsort.utils.parser import get_config\n \n+\n+def get_tracker_config(tracker_type):\n+    tracking_config = \\n+        Path('./boxmot') /\\n+        tracker_type /\\n+        'configs' /\\n+        (tracker_type + '.yaml')\n+    return tracking_config\n+    \n+\n def create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n     \n     cfg = get_config()\n",add,Add warning about data volume to enable_metrics_collection
009bc595551bc97113a7ee68bd286b953cb546f9,fix imports,boxmot/botsort/bot_sort.py,"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import deque\n\nfrom .matching import iou_distance, fuse_score, linear_assignment, embedding_distance\nfrom .gmc import GMC\nfrom .basetrack import BaseTrack, TrackState\nfrom .kalman_filter import KalmanFilter\n\n# from fast_reid.fast_reid_interfece import FastReIDInterface\n\nfrom ..deep.reid_multibackend import ReIDDetectMultiBackend\nfrom ultralytics.yolo.utils.ops import xyxy2xywh, xywh2xyxy\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        self.update_cls(cls, score)\n\n        self.score = score\n        self.tracklet_len = 0\n\n    ","import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import deque\n\nfrom .matching import iou_distance, fuse_score, linear_assignment, embedding_distance\nfrom .gmc import GMC\nfrom .basetrack import BaseTrack, TrackState\nfrom .kalman_filter import KalmanFilter\n\n# from fast_reid.fast_reid_interfece import FastReIDInterface\n\nfrom ..deep.reid_multibackend import ReIDDetectMultiBackend\nfrom ultralytics.yolo.utils.ops import xyxy2xywh, xywh2xyxy\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        self.update_cls(cls, score)\n\n        self.score = score\n        self.tracklet_len = 0\n\n    ","@@ -333,8 +333,8 @@ class BoTSORT(object):\n         STrack.multi_gmc(unconfirmed, warp)\n \n         # Associate with high score detection boxes\n-        raw_emb_dists = matching.embedding_distance(strack_pool, detections)\n-        dists = matching.fuse_motion(self.kalman_filter, raw_emb_dists, strack_pool, detections, only_position=False, lambda_=self.lambda_)\n+        raw_emb_dists = embedding_distance(strack_pool, detections)\n+        dists = fuse_motion(self.kalman_filter, raw_emb_dists, strack_pool, detections, only_position=False, lambda_=self.lambda_)\n \n         # ious_dists = matching.iou_distance(strack_pool, detections)\n         # ious_dists_mask = (ious_dists > self.proximity_thresh)\n@@ -520,7 +520,7 @@ def sub_stracks(tlista, tlistb):\n \n \n def remove_duplicate_stracks(stracksa, stracksb):\n-    pdist = matching.iou_distance(stracksa, stracksb)\n+    pdist = iou_distance(stracksa, stracksb)\n     pairs = np.where(pdist < 0.15)\n     dupa, dupb = list(), list()\n     for p, q in zip(*pairs):\n",add,Added the UNSTARTED state to the YouTube PlayerState enum
30e282e6b3af3592dc29c135bf8bc2e9bf36ef5e,fix imports,boxmot/botsort/bot_sort.py,"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import deque\n\nfrom .matching import iou_distance, fuse_score, linear_assignment, embedding_distance\nfrom .gmc import GMC\nfrom .basetrack import BaseTrack, TrackState\nfrom .kalman_filter import KalmanFilter\n\n# from fast_reid.fast_reid_interfece import FastReIDInterface\n\nfrom ..deep.reid_multibackend import ReIDDetectMultiBackend\nfrom ultralytics.yolo.utils.ops import xyxy2xywh, xywh2xyxy\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        self.update_cls(cls, score)\n\n        self.score = score\n        self.tracklet_len = 0\n\n    ","import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import deque\n\nfrom .matching import iou_distance, fuse_score, linear_assignment, embedding_distance, fuse_motion\nfrom .gmc import GMC\nfrom .basetrack import BaseTrack, TrackState\nfrom .kalman_filter import KalmanFilter\n\n# from fast_reid.fast_reid_interfece import FastReIDInterface\n\nfrom ..deep.reid_multibackend import ReIDDetectMultiBackend\nfrom ultralytics.yolo.utils.ops import xyxy2xywh, xywh2xyxy\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        self.update_cls(cls, score)\n\n        self.score = score\n        self.tracklet_le","@@ -3,7 +3,7 @@ import matplotlib.pyplot as plt\n import numpy as np\n from collections import deque\n \n-from .matching import iou_distance, fuse_score, linear_assignment, embedding_distance\n+from .matching import iou_distance, fuse_score, linear_assignment, embedding_distance, fuse_motion\n from .gmc import GMC\n from .basetrack import BaseTrack, TrackState\n from .kalman_filter import KalmanFilter\n",add,Added note about dates .
2280aa7a5b609e11ff2cd3b9f2c0d0b4ee8d8a1e,fix imports,.github/workflows/ci.yml,"# name of the workflow, what it is doing (optional)\nname: CI CPU testing\n\n# events that trigger the workflow (required)\non:\n  push:\n    branches: [master, CIdebug]\n  pull_request:\n    # pull request where master is target\n    branches: [master]\n\n# the workflow that gets triggerd\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, windows-latest]  # Error: Container action is only supported on Linux\n        python-version: [3.9]\n        model: ['yolov8n']  # models to test\n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      # Check out the repository recursively, updated to v3\n      - uses: actions/checkout@v3\n        with:\n          submodules: recursive\n      # Prepare environment with python 3.9\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Install requireme","# name of the workflow, what it is doing (optional)\nname: CI CPU testing\n\n# events that trigger the workflow (required)\non:\n  push:\n    branches: [master, CIdebug]\n  pull_request:\n    # pull request where master is target\n    branches: [master]\n\n# the workflow that gets triggerd\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, windows-latest]  # Error: Container action is only supported on Linux\n        python-version: [3.9]\n        model: ['yolov8n']  # models to test\n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      # Check out the repository recursively, updated to v3\n      - uses: actions/checkout@v3\n        with:\n          submodules: recursive\n      # Prepare environment with python 3.9\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Install requireme","@@ -34,9 +34,10 @@ jobs:\n       - name: Install requirements\n         run: |\n           python -m pip install --upgrade pip setuptools wheel\n-          pip install numpy\n           # If not importing this prior to installing requirements...\n           # ImportError: lap requires numpy, please """"pip install numpy"""". Workaround...\n+          pip install numpy\n+          # now lap installation goes through\n           pip install -e '.[export]' --extra-index-url https://download.pytorch.org/whl/cpu\n           python --version\n           pip --version\n",add,Parallelizing the build
2280aa7a5b609e11ff2cd3b9f2c0d0b4ee8d8a1e,fix imports,examples/track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nWEIGHTS = Path(SETTINGS['weights_dir'])\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # root dir\nWEIGHTS = ROOT / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \\n        ROOT /\\n        'box","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # examples absolute path\nEXAMPLES = FILE.parents[0]  # examples absolute path\nWEIGHTS = EXAMPLES / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = \","@@ -16,10 +16,10 @@ from ultralytics.yolo.utils.files import increment_path\n from ultralytics.yolo.engine.results import Boxes\n from ultralytics.yolo.data.utils import VID_FORMATS\n \n-WEIGHTS = Path(SETTINGS['weights_dir'])\n FILE = Path(__file__).resolve()\n-ROOT = FILE.parents[0].parents[0]  # root dir\n-WEIGHTS = ROOT / 'weights'\n+ROOT = FILE.parents[0].parents[0]  # examples absolute path\n+EXAMPLES = FILE.parents[0]  # examples absolute path\n+WEIGHTS = EXAMPLES / 'weights'\n \n \n def on_predict_start(predictor):\n",add,Add Rossen
4cd1277e39c56035c4c014874f5ff752d906c627,fix paths,boxmot/deep/reid_export.py,"import argparse\nimport os\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom boxmot.deep.models import build_model\nfrom boxmot.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # examples absolute path\nEXAMPLES = FILE.parents[0]  # examples absolute path\nWEIGHTS = EXAMPLES / 'weights'\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1E6\n    elif path.is_dir():\n        return sum(f.stat().st","import argparse\nimport os\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom boxmot.deep.models import build_model\nfrom boxmot.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0].parents[0]  # root absolute path\nEXAMPLES = ROOT / 'examples'  # examples absolute path\nWEIGHTS = EXAMPLES / 'weights'  # weights absolute path\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1E6\n    elif path.is_dir(","@@ -20,9 +20,9 @@ from ultralytics.yolo.utils import LOGGER, colorstr, ops\n from ultralytics.yolo.utils.checks import check_requirements, check_version\n \n FILE = Path(__file__).resolve()\n-ROOT = FILE.parents[0].parents[0]  # examples absolute path\n-EXAMPLES = FILE.parents[0]  # examples absolute path\n-WEIGHTS = EXAMPLES / 'weights'\n+ROOT = FILE.parents[0].parents[0].parents[0]  # root absolute path\n+EXAMPLES = ROOT / 'examples'  # examples absolute path\n+WEIGHTS = EXAMPLES / 'weights'  # weights absolute path\n \n \n def file_size(path):\n",add,Add Rossen
1837a3c80e72e5402f207e65119a8842f5f2d7d0,fix non-existent tags,setup.py,"# AGPL-3.0 license\n\nimport re\nfrom pathlib import Path\n\nimport pkg_resources as pkg\nfrom setuptools import find_packages, setup\n\n# Settings\nFILE = Path(__file__).resolve()\nPARENT = FILE.parent  # root directory\nREADME = (PARENT / 'README.md').read_text(encoding='utf-8')\nREQUIREMENTS = [f'{x.name}{x.specifier}' for x in pkg.parse_requirements((PARENT / 'requirements.txt').read_text())]\n\n\ndef get_version():\n    file = PARENT / 'boxmot/__init__.py'\n    version = re.search(r'__version__\s*=\s*[\'\""""](.+?)[\'\""""]', file.read_text(encoding='utf-8')).group(1)\n    return version\n\n\nsetup(\n    name='box-mot',  # name of pypi package\n    version=get_version(),  # version of pypi package\n    python_requires='>=3.7',\n    license='AGPL-3.0',\n    description=('SOTA tracking methods for detection, segmentation and pose estimation models.'),\n    long_description=README,\n    long_description_content_type='text/markdown',\n    url='https://github.com/mikel-brostrom/yolov8_trac","# AGPL-3.0 license\n\nimport re\nfrom pathlib import Path\n\nimport pkg_resources as pkg\nfrom setuptools import find_packages, setup\n\n# Settings\nFILE = Path(__file__).resolve()\nPARENT = FILE.parent  # root directory\nREADME = (PARENT / 'README.md').read_text(encoding='utf-8')\nREQUIREMENTS = [f'{x.name}{x.specifier}' for x in pkg.parse_requirements((PARENT / 'requirements.txt').read_text())]\n\n\ndef get_version():\n    file = PARENT / 'boxmot/__init__.py'\n    version = re.search(r'__version__\s*=\s*[\'\""""](.+?)[\'\""""]', file.read_text(encoding='utf-8')).group(1)\n    return version\n\n\nsetup(\n    name='box-mot',  # name of pypi package\n    version=get_version(),  # version of pypi package\n    python_requires='>=3.7',\n    license='AGPL-3.0',\n    description=('SOTA tracking methods for detection, segmentation and pose estimation models.'),\n    long_description=README,\n    long_description_content_type='text/markdown',\n    url='https://github.com/mikel-brostrom/yolov8_trac","@@ -38,7 +38,7 @@ setup(\n     install_requires=REQUIREMENTS,\n     platforms=[""""linux"""", """"windows""""],\n     classifiers=[\n-        'Development Status :: Beta',\n+        'Development Status :: 3 - Alpha',\n         'Intended Audience :: Developers',\n         'Intended Audience :: Education',\n         'Intended Audience :: Science/Research',\n@@ -52,7 +52,8 @@ setup(\n         'Topic :: Software Development',\n         'Topic :: Scientific/Engineering',\n         'Topic :: Scientific/Engineering :: Artificial Intelligence',\n-        'Topic :: Scientific/Engineering :: Computer Vision',\n+        'Topic :: Scientific/Engineering :: Image Recognition',\n+        'Topic :: Scientific/Engineering :: Image Processing',\n     ],\n     keywords='machine-learning, deep-learning, vision, ML, DL, AI, YOLO',\n )\n\ No newline at end of file\n",add,Parallelizing the build
35ea70f4545a46bc9b7467b044a8b465f3492bbe,fix weigths path,boxmot/deep/reid_export.py,"import argparse\nimport os\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom boxmot.deep.models import build_model\nfrom boxmot.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0].parents[0]  # root absolute path\nEXAMPLES = ROOT / 'examples'  # examples absolute path\nWEIGHTS = EXAMPLES / 'weights'  # weights absolute path\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1E6\n    elif path.is_dir(","import argparse\nimport os\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom boxmot.deep.models import build_model\nfrom boxmot.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0].parents[0]  # root absolute path\nEXAMPLES = ROOT / 'examples'  # examples absolute path\nWEIGHTS = EXAMPLES / 'weights'  # weights absolute path\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1E6\n    elif path.is_dir(","@@ -54,7 +54,7 @@ def export_torchscript(model, im, file, optimize, prefix=colorstr('TorchScript:'\n     try:\n         LOGGER.info(f'\n{prefix} starting export with torch {torch.__version__}...')\n         f = file.with_suffix('.torchscript')\n-\n+        print(f)\n         ts = torch.jit.trace(model, im, strict=False)\n         if optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n             optimize_for_mobile(ts)._save_for_lite_interpreter(str(f))\n@@ -247,9 +247,6 @@ if __name__ == """"__main__"""":\n         assert args.device.type != 'cpu', '--half only compatible with GPU export, i.e. use --device 0'\n         # assert not args.dynamic, '--half not compatible with --dynamic, i.e. use either --half or --dynamic but not both'\n \n-    if type(args.weights) is list:\n-        args.weights = Path(args.weights[0])\n-\n     model = build_model(\n         get_model_name(args.weights),\n         num_classes=1,\n",add,Added STORM - 1270 to Changelog
7d511d5bda0e19042ef39a97808d9e8adeb83b68,fix weigths path,boxmot/deep/reid_export.py,"import argparse\nimport os\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom boxmot.deep.models import build_model\nfrom boxmot.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0].parents[0]  # root absolute path\nEXAMPLES = ROOT / 'examples'  # examples absolute path\nWEIGHTS = EXAMPLES / 'weights'  # weights absolute path\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1E6\n    elif path.is_dir(","import argparse\nimport os\nimport sys\nimport numpy as np\nfrom pathlib import Path\nimport torch\nimport time\nimport platform\nimport pandas as pd\nimport subprocess\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nimport logging\n\nfrom boxmot.deep.models import build_model\nfrom boxmot.deep.reid_model_factory import get_model_name, load_pretrained_weights\n\nfrom ultralytics.yolo.utils.torch_utils import select_device\nfrom ultralytics.yolo.utils import LOGGER, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_requirements, check_version\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0].parents[0]  # root absolute path\nEXAMPLES = ROOT / 'examples'  # examples absolute path\nWEIGHTS = EXAMPLES / 'weights'  # weights absolute path\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1E6\n    elif path.is_dir(","@@ -247,6 +247,9 @@ if __name__ == """"__main__"""":\n         assert args.device.type != 'cpu', '--half only compatible with GPU export, i.e. use --device 0'\n         # assert not args.dynamic, '--half not compatible with --dynamic, i.e. use either --half or --dynamic but not both'\n \n+    if type(args.weights) is list:\n+        args.weights = Path(args.weights[0])\n+\n     model = build_model(\n         get_model_name(args.weights),\n         num_classes=1,\n",add,Added STORM - 1270 to Changelog
736c95c164b7c56d4416e509a37e84fc918cff83,fix paths,examples/evolve.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport yaml\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # examples absolute path\nEXAMPLES = FILE.parents[0]  # examples absolute path\nWEIGHTS = EXAMPLES_ROOT / 'weights'\n\nfrom ultralytics.yolo.utils import LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfr","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport yaml\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # examples absolute path\nEXAMPLES = FILE.parents[0]  # examples absolute path\nWEIGHTS = EXAMPLES / 'weights'\n\nfrom ultralytics.yolo.utils import LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom tr","@@ -23,7 +23,7 @@ from val import Evaluator\n FILE = Path(__file__).resolve()\n ROOT = FILE.parents[0].parents[0]  # examples absolute path\n EXAMPLES = FILE.parents[0]  # examples absolute path\n-WEIGHTS = EXAMPLES_ROOT / 'weights'\n+WEIGHTS = EXAMPLES / 'weights'\n \n from ultralytics.yolo.utils import LOGGER\n from ultralytics.yolo.utils.checks import check_requirements, print_args\n",add,added meteor
55e7d8b778125a2ef00928c6fa6b767df52e3fc6,delete debug prints,examples/track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # repo root absolute path\nEXAMPLES = FILE.parents[0]  # examples absolute path\nWEIGHTS = EXAMPLES / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = ","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # repo root absolute path\nEXAMPLES = FILE.parents[0]  # examples absolute path\nWEIGHTS = EXAMPLES / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = ","@@ -125,10 +125,8 @@ def run(args):\n             with predictor.profilers[3]:\n                 # get raw bboxes tensor\n                 dets = predictor.results[i].boxes.data\n-                print(im0.shape)\n-                # get predictions\n+                # get tracker predictions\n                 predictor.tracker_outputs[i] = predictor.trackers[i].update(dets.cpu().detach(), im0)\n-                print(predictor.tracker_outputs[i].shape)\n             predictor.results[i].speed = {\n                 'preprocess': predictor.profilers[0].dt * 1E3 / n,\n                 'inference': predictor.profilers[1].dt * 1E3 / n,\n",update,Add note about data volume to enable EGL
c10476091f58754f7078bd076bcf6aba7f31fe4e,fix paths,examples/evolve.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport yaml\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # examples absolute path\nEXAMPLES = FILE.parents[0]  # examples absolute path\nWEIGHTS = EXAMPLES / 'weights'\n\nfrom ultralytics.yolo.utils import LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom tr","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport yaml\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # examples absolute path\nEXAMPLES = FILE.parents[0]  # examples absolute path\nWEIGHTS = EXAMPLES / 'weights'\n\nfrom ultralytics.yolo.utils import LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom tr","@@ -277,7 +277,6 @@ def parse_opt():\n     parser.add_argument('--yolo-model', type=str, default=WEIGHTS / 'yolov8n.pt', help='model.pt path(s)')\n     parser.add_argument('--reid-model', type=str, default=WEIGHTS / 'lmbn_n_cuhk03_d.pt')\n     parser.add_argument('--tracking-method', type=str, default='deepocsort', help='strongsort, ocsort')\n-    parser.add_argument('--tracking-config', type=Path, default=None)\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n     parser.add_argument('--project', default=ROOT / 'runs' / 'evolve', help='save results to project/name')\n     parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n@@ -293,7 +292,7 @@ def parse_opt():\n     parser.add_argument('--objectives', type=str, default='HOTA,MOTA,IDF1', help='set of objective metrics: HOTA,MOTA,IDF1')\n     \n     opt = parser.parse_args()\n-    opt.tracking_config = ROOT / 'trackers' / opt.tracking_method / 'configs' / (opt.tracking_method + '.yaml')\n+    opt.tracking_config = ROOT / 'boxmot' / opt.tracking_method / 'configs' / (opt.tracking_method + '.yaml')\n     opt.objectives = opt.objectives.split("""","""")\n \n     device = []\n",add,Added hypest who apparently actually wrote the C # port
fd8b839c24e184da048bcaa3d099209400da639d,lap fix,boxmot/bytetrack/matching.py,"import cv2\nimport numpy as np\nimport scipy\nimport lap\nfrom scipy.spatial.distance import cdist\n\nfrom .kalman_filter import chi2inv95\nimport time\n\ntry:\n    import lap  # for linear_assignment\n\n    assert lap.__version__  # verify package is not directory\nexcept (ImportError, AssertionError, AttributeError):\n    from ultralytics.yolo.utils.checks import check_requirements\n\n    check_requirements('lap>=0.4')  # install\n    import lap\n\ndef merge_matches(m1, m2, shape):\n    O,P,Q = shape\n    m1 = np.asarray(m1)\n    m2 = np.asarray(m2)\n\n    M1 = scipy.sparse.coo_matrix((np.ones(len(m1)), (m1[:, 0], m1[:, 1])), shape=(O, P))\n    M2 = scipy.sparse.coo_matrix((np.ones(len(m2)), (m2[:, 0], m2[:, 1])), shape=(P, Q))\n\n    mask = M1*M2\n    match = mask.nonzero()\n    match = list(zip(match[0], match[1]))\n    unmatched_O = tuple(set(range(O)) - set([i for i, j in match]))\n    unmatched_Q = tuple(set(range(Q)) - set([j for i, j in match]))\n\n    return match, unmatched_","import cv2\nimport numpy as np\nimport scipy\nfrom scipy.spatial.distance import cdist\n\nfrom .kalman_filter import chi2inv95\nimport time\n\ntry:\n    import lap  # for linear_assignment\n\n    assert lap.__version__  # verify package is not directory\nexcept (ImportError, AssertionError, AttributeError):\n    from ultralytics.yolo.utils.checks import check_requirements\n\n    check_requirements('lap>=0.4')  # install\n    import lap\n\ndef merge_matches(m1, m2, shape):\n    O,P,Q = shape\n    m1 = np.asarray(m1)\n    m2 = np.asarray(m2)\n\n    M1 = scipy.sparse.coo_matrix((np.ones(len(m1)), (m1[:, 0], m1[:, 1])), shape=(O, P))\n    M2 = scipy.sparse.coo_matrix((np.ones(len(m2)), (m2[:, 0], m2[:, 1])), shape=(P, Q))\n\n    mask = M1*M2\n    match = mask.nonzero()\n    match = list(zip(match[0], match[1]))\n    unmatched_O = tuple(set(range(O)) - set([i for i, j in match]))\n    unmatched_Q = tuple(set(range(Q)) - set([j for i, j in match]))\n\n    return match, unmatched_O, unmatched","@@ -1,7 +1,6 @@\n import cv2\n import numpy as np\n import scipy\n-import lap\n from scipy.spatial.distance import cdist\n \n from .kalman_filter import chi2inv95\n",add,Add warning about data volume to enable_metrics_collection
bbb482233c9c4a9208eba01a9a5aacc7be6d663c,delete debug prints,examples/yolo_nas_track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes, Results\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom super_gradients.training.models.detection_models.yolo_base import YoloPostPredictionCallback\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # repo root absolute path\nEXAMPLES = FILE.parents[0]  # examples a","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes, Results\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom super_gradients.training.models.detection_models.yolo_base import YoloPostPredictionCallback\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # repo root absolute path\nEXAMPLES = FILE.parents[0]  # examples a","@@ -114,16 +114,13 @@ def run(args):\n         path, im0s, vid_cap, s = batch\n         visualize = increment_path(save_dir / Path(path[0]).stem, exist_ok=True, mkdir=True) if predictor.args.visualize and (not predictor.dataset.source_type.tensor) else False\n \n-        print('im0s', im0s[0].shape)\n         # Preprocess\n         with predictor.profilers[0]:\n             im = predictor.preprocess(im0s)\n \n         # Inference\n         with predictor.profilers[1]:\n-            \n             prediction = next(iter(yolo_nas_model.predict(im0s, iou=0.5, conf=0.7))).prediction # Returns a generator of the batch, which here is 1\n-            print(type(prediction))\n             preds = np.concatenate(\n                 [\n                     prediction.bboxes_xyxy,\n",add,Added <STARTED state to the YouTube PlayerState enum
4281fa7d1aa343104b438397cbca95365a1f8f0f,fix id visualizations,examples/yolo_nas_track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes, Results\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom super_gradients.training.models.detection_models.yolo_base import YoloPostPredictionCallback\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # repo root absolute path\nEXAMPLES = FILE.parents[0]  # examples a","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes, Results\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom super_gradients.training.models.detection_models.yolo_base import YoloPostPredictionCallback\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # repo root absolute path\nEXAMPLES = FILE.parents[0]  # examples a","@@ -120,7 +120,7 @@ def run(args):\n \n         # Inference\n         with predictor.profilers[1]:\n-            prediction = next(iter(yolo_nas_model.predict(im0s, iou=0.5, conf=0.7))).prediction # Returns a generator of the batch, which here is 1\n+            prediction = next(iter(yolo_nas_model.predict(im0s, iou=0.7, conf=0.3))).prediction # Returns a generator of the batch, which here is 1\n             preds = np.concatenate(\n                 [\n                     prediction.bboxes_xyxy,\n@@ -129,10 +129,10 @@ def run(args):\n                 ], axis=1\n             )\n             preds = torch.from_numpy(preds)\n-\n+        predictor.results = [None]\n         # # Postprocess\n         with predictor.profilers[2]:\n-            predictor.results = Results(path=path, boxes=preds, orig_img=im0s[0], names=model.names)\n+            predictor.results[0] = Results(path=path, boxes=preds, orig_img=im0s[0], names=model.names)\n         predictor.run_callbacks('on_predict_postprocess_end')\n         \n         # Visualize, save, write results\n@@ -149,6 +149,8 @@ def run(args):\n                 dets = predictor.results[i].boxes.data\n                 # get tracker predictions\n                 predictor.tracker_outputs[i] = predictor.trackers[i].update(dets, im0)\n+                print(predictor.tracker_outputs[i].shape)\n+                print(predictor.tracker_outputs[i])\n             predictor.results[i].speed = {\n                 'preprocess': predictor.profilers[0].dt * 1E3 / n,\n                 'inference': predictor.profilers[1].dt * 1E3 / n,\n",add,Added the user tile to be able to .
9e55180dc6b56bdb6bc176511f7780fff2bee16b,fix ValueError: tile cannot extend outside image,examples/yolo_nas_track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes, Results\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom super_gradients.training.models.detection_models.yolo_base import YoloPostPredictionCallback\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # repo root absolute path\nEXAMPLES = FILE.parents[0]  # examples a","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes, Results\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom super_gradients.training.models.detection_models.yolo_base import YoloPostPredictionCallback\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # repo root absolute path\nEXAMPLES = FILE.parents[0]  # examples a","@@ -128,7 +128,7 @@ def run(args):\n                     prediction.labels[:, np.newaxis]\n                 ], axis=1\n             )\n-            preds = torch.from_numpy(preds)\n+            preds = torch.from_numpy(preds).int()\n         predictor.results = [None]\n         # # Postprocess\n         with predictor.profilers[2]:\n",fix,Added Type name for DFI ( # 3 )
4e3bcea03eec2b30b86336019413e8e3cf8a51a6,fix conf not getting updated bug,boxmot/deepocsort/ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom __future__ import print_function\n\n\nimport numpy as np\nfrom .association import *\nfrom .cmc import CMCComputer\nfrom ..deep.reid_multibackend import ReIDDetectMultiBackend\nfrom ultralytics.yolo.utils.ops import xyxy2xywh\n\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + ","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom __future__ import print_function\n\n\nimport numpy as np\nfrom .association import *\nfrom .cmc import CMCComputer\nfrom ..deep.reid_multibackend import ReIDDetectMultiBackend\nfrom ultralytics.yolo.utils.ops import xyxy2xywh\n\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + ","@@ -104,6 +104,7 @@ class KalmanBoxTracker(object):\n         else:\n             from filterpy.kalman import KalmanFilter\n         self.cls = cls\n+        \n         self.conf = bbox[-1]\n         self.new_kf = new_kf\n         if new_kf:\n@@ -200,6 +201,7 @@ class KalmanBoxTracker(object):\n         if bbox is not None:\n             self.frozen = False\n             self.cls = cls\n+            self.conf = bbox[-1]\n             if self.last_observation.sum() >= 0:  # no previous observation\n                 previous_box = None\n                 for dt in range(self.delta_t, 0, -1):\n",add,Added STORM - 374 to Changelog
45a2e3006c2d3e0655bd6e3f808756731cd2556f,fix https://github.com/mikel-brostrom/yolo_tracking/issues/877,examples/track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # repo root absolute path\nEXAMPLES = FILE.parents[0]  # examples absolute path\nWEIGHTS = EXAMPLES / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = ","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # repo root absolute path\nEXAMPLES = FILE.parents[0]  # examples absolute path\nWEIGHTS = EXAMPLES / 'weights'\n\n\ndef on_predict_start(predictor):\n    predictor.trackers = []\n    predictor.tracker_outputs = [None] * predictor.dataset.bs\n    predictor.args.tracking_config = ","@@ -128,7 +128,6 @@ def run(args):\n                 dets = predictor.results[i].boxes.data\n                 # get tracker predictions\n                 predictor.tracker_outputs[i] = predictor.trackers[i].update(dets.cpu().detach(), im0)\n-                predictor.tracker_outputs[i] = predictor.tracker_outputs[i][predictor.tracker_outputs[i][:, 5].argsort()[::-1]]\n             predictor.results[i].speed = {\n                 'preprocess': predictor.profilers[0].dt * 1E3 / n,\n                 'inference': predictor.profilers[1].dt * 1E3 / n,\n@@ -136,20 +135,22 @@ def run(args):\n                 'tracking': predictor.profilers[3].dt * 1E3 / n\n             }\n \n-            # filter boxes masks and pose results by tracking results\n-            yolo_confs = predictor.results[i].boxes.conf.cpu().numpy()\n-            tracker_confs = predictor.tracker_outputs[i][:, 5]\n-            mask = np.in1d(yolo_confs, tracker_confs)\n-            \n-            if predictor.results[i].masks is not None:\n-                predictor.results[i].masks = predictor.results[i].masks[mask]\n-                predictor.results[i].boxes = predictor.results[i].boxes[mask]\n-            elif predictor.results[i].keypoints is not None:\n-                predictor.results[i].boxes = predictor.results[i].boxes[mask]\n-                predictor.results[i].keypoints = predictor.results[i].keypoints[mask]\n-\n-            # overwrite bbox results with tracker predictions\n             if predictor.tracker_outputs[i].size != 0:\n+                \n+                # filter boxes masks and pose results by tracking results\n+                predictor.tracker_outputs[i] = predictor.tracker_outputs[i][predictor.tracker_outputs[i][:, 5].argsort()[::-1]]\n+                yolo_confs = predictor.results[i].boxes.conf.cpu().numpy()\n+                tracker_confs = predictor.tracker_outputs[i][:, 5]\n+                mask = np.in1d(yolo_confs, tracker_confs)\n+                \n+                if p",add,Add note about data volume to enable_metrics_collection
e4f424d7adcd1ad676fa517d61c4c278708c7983,fix yolo-nas int bug: confs always zero,examples/yolo_nas_track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes, Results\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom super_gradients.training.models.detection_models.yolo_base import YoloPostPredictionCallback\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # repo root absolute path\nEXAMPLES = FILE.parents[0]  # examples a","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import LOGGER, SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes, Results\nfrom ultralytics.yolo.data.utils import VID_FORMATS\n\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom super_gradients.training.models.detection_models.yolo_base import YoloPostPredictionCallback\n\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0].parents[0]  # repo root absolute path\nEXAMPLES = FILE.parents[0]  # examples a","@@ -30,9 +30,8 @@ try:\n     import super_gradients  # for linear_assignment\n except (ImportError, AssertionError, AttributeError):\n     from ultralytics.yolo.utils.checks import check_requirements\n-\n     check_requirements('super_gradients')  # install\n-    import lap\n+    import super_gradients\n \n \n def on_predict_start(predictor):\n@@ -128,7 +127,8 @@ def run(args):\n                     prediction.labels[:, np.newaxis]\n                 ], axis=1\n             )\n-            preds = torch.from_numpy(preds).int()\n+            preds = torch.from_numpy(preds)\n+            preds[:, 0:4] = preds[:, 0:4].int()\n         predictor.results = [None]\n         # # Postprocess\n         with predictor.profilers[2]:\n@@ -149,14 +149,11 @@ def run(args):\n                 dets = predictor.results[i].boxes.data\n                 # get tracker predictions\n                 predictor.tracker_outputs[i] = predictor.trackers[i].update(dets, im0)\n-                print(predictor.tracker_outputs[i].shape)\n-                print(predictor.tracker_outputs[i])\n             predictor.results[i].speed = {\n                 'preprocess': predictor.profilers[0].dt * 1E3 / n,\n                 'inference': predictor.profilers[1].dt * 1E3 / n,\n                 'postprocess': predictor.profilers[2].dt * 1E3 / n,\n                 'tracking': predictor.profilers[3].dt * 1E3 / n\n-            \n             }\n \n             # overwrite bbox results with tracker predictions\n",add,added llvm dependency for device .
af8188c5cd94137cfb51436f4eb169506e6ce569,fix path bug,examples/multi_yolo_backend.py,"from pathlib import Path\nimport numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass MultiYolo():\n    def __init__(self, model, device):\n        self.device = device\n        self.model_name = str(model.stem).lower()\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = 'yolov8'\n            self.model = model\n        if 'yolo_nas' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n            self.model_type = 'yolo_nas'\n            self.model = models.get(\n                self.model_name,\n                pretrained_weights=""""coco""""\n            ).to(self.device)\n        elif 'yolox' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n ","from pathlib import Path\nimport numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass MultiYolo():\n    def __init__(self, model, device):\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = 'yolov8'\n            self.model = model\n        else:\n            self.model_name = str(model.stem).lower()\n\n        if 'yolo_nas' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n            self.model_type = 'yolo_nas'\n            self.model = models.get(\n                self.model_name,\n                pretrained_weights=""""coco""""\n            ).to(self.device)\n        elif 'yolox' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.trai","@@ -7,10 +7,12 @@ from ultralytics.yolo.engine.results import Boxes, Results\n class MultiYolo():\n     def __init__(self, model, device):\n         self.device = device\n-        self.model_name = str(model.stem).lower()\n         if not (isinstance(model, str) or isinstance(model, Path)):\n             self.model_name = 'yolov8'\n             self.model = model\n+        else:\n+            self.model_name = str(model.stem).lower()\n+\n         if 'yolo_nas' in self.model_name:\n             self.try_sg_import()\n             from super_gradients.common.object_names import Models\n",add,Fix Snackbar theme on sw600dp
0cbbb93cd032e63d28c74acab3408f2a250f6069,fix sg pkg name,examples/multi_yolo_backend.py,"from pathlib import Path\nimport numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass MultiYolo():\n    def __init__(self, model, device):\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = 'yolov8'\n            self.model = model\n        else:\n            self.model_name = str(model.stem).lower()\n\n        if 'yolo_nas' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n            self.model_type = 'yolo_nas'\n            self.model = models.get(\n                self.model_name,\n                pretrained_weights=""""coco""""\n            ).to(self.device)\n        elif 'yolox' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.trai","from pathlib import Path\nimport numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass MultiYolo():\n    def __init__(self, model, device):\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = 'yolov8'\n            self.model = model\n        else:\n            self.model_name = str(model.stem).lower()\n\n        if 'yolo_nas' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n            self.model_type = 'yolo_nas'\n            self.model = models.get(\n                self.model_name,\n                pretrained_weights=""""coco""""\n            ).to(self.device)\n        elif 'yolox' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.trai","@@ -40,7 +40,7 @@ class MultiYolo():\n             import super_gradients  # for linear_assignment\n         except (ImportError, AssertionError, AttributeError):\n             from ultralytics.yolo.utils.checks import check_requirements\n-            check_requirements('super_gradients', cmds='--user')  # install\n+            check_requirements('super-gradients', cmds='--user')  # install\n             \n     def __call__(self, im, im0s):\n         if 'yolo_nas' in self.model_name or 'yolox' in self.model_name:\n",add,Add note about data volume to enable_metrics_collection
57e0e67238cc780c94919819044485119e1d69c8,fix sg pkg name,examples/multi_yolo_backend.py,"from pathlib import Path\nimport numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass MultiYolo():\n    def __init__(self, model, device):\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = 'yolov8'\n            self.model = model\n        else:\n            self.model_name = str(model.stem).lower()\n\n        if 'yolo_nas' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n            self.model_type = 'yolo_nas'\n            self.model = models.get(\n                self.model_name,\n                pretrained_weights=""""coco""""\n            ).to(self.device)\n        elif 'yolox' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.trai","from pathlib import Path\nimport numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass MultiYolo():\n    def __init__(self, model, device):\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = 'yolov8'\n            self.model = model\n        else:\n            self.model_name = str(model.stem).lower()\n\n        if 'yolo_nas' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n            self.model_type = 'yolo_nas'\n            self.model = models.get(\n                self.model_name,\n                pretrained_weights=""""coco""""\n            ).to(self.device)\n        elif 'yolox' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.trai","@@ -40,7 +40,7 @@ class MultiYolo():\n             import super_gradients  # for linear_assignment\n         except (ImportError, AssertionError, AttributeError):\n             from ultralytics.yolo.utils.checks import check_requirements\n-            check_requirements('super-gradients', cmds='--user')  # install\n+            check_requirements('super-gradients==3.1.1', cmds='--user')  # install\n             \n     def __call__(self, im, im0s):\n         if 'yolo_nas' in self.model_name or 'yolox' in self.model_name:\n",add,Add note about data volume to enable
36826aa803d9cd124c77f55a5d0cbebf369b8234,fix sg pkg name,examples/multi_yolo_backend.py,"from pathlib import Path\nimport numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass MultiYolo():\n    def __init__(self, model, device):\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = 'yolov8'\n            self.model = model\n        else:\n            self.model_name = str(model.stem).lower()\n\n        if 'yolo_nas' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n            self.model_type = 'yolo_nas'\n            self.model = models.get(\n                self.model_name,\n                pretrained_weights=""""coco""""\n            ).to(self.device)\n        elif 'yolox' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.trai","from pathlib import Path\nimport numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass MultiYolo():\n    def __init__(self, model, device):\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = 'yolov8'\n            self.model = model\n        else:\n            self.model_name = str(model.stem).lower()\n\n        if 'yolo_nas' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n            self.model_type = 'yolo_nas'\n            self.model = models.get(\n                self.model_name,\n                pretrained_weights=""""coco""""\n            ).to(self.device)\n        elif 'yolox' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.trai","@@ -40,7 +40,7 @@ class MultiYolo():\n             import super_gradients  # for linear_assignment\n         except (ImportError, AssertionError, AttributeError):\n             from ultralytics.yolo.utils.checks import check_requirements\n-            check_requirements('super-gradients==3.1.1', cmds='--user')  # install\n+            check_requirements('super-gradients==3.1.1')  # install\n             \n     def __call__(self, im, im0s):\n         if 'yolo_nas' in self.model_name or 'yolox' in self.model_name:\n",add,Add note about data volume to enable
05d6b4a8d61bec14226a40e596e83ace10e64662,delete dubug print,boxmot/botsort/bot_sort.py,"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import deque\n\nfrom .matching import iou_distance, fuse_score, linear_assignment, embedding_distance, fuse_motion\nfrom .gmc import GMC\nfrom .basetrack import BaseTrack, TrackState\nfrom .kalman_filter import KalmanFilter\nimport torch\n\n# from fast_reid.fast_reid_interfece import FastReIDInterface\n\nfrom ..deep.reid_multibackend import ReIDDetectMultiBackend\nfrom ultralytics.yolo.utils.ops import xyxy2xywh, xywh2xyxy\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        self.update_cls(cls, score)\n\n        self.score = score\n        se","import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import deque\n\nfrom .matching import iou_distance, fuse_score, linear_assignment, embedding_distance, fuse_motion\nfrom .gmc import GMC\nfrom .basetrack import BaseTrack, TrackState\nfrom .kalman_filter import KalmanFilter\nimport torch\n\n# from fast_reid.fast_reid_interfece import FastReIDInterface\n\nfrom ..deep.reid_multibackend import ReIDDetectMultiBackend\nfrom ultralytics.yolo.utils.ops import xyxy2xywh, xywh2xyxy\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        self.update_cls(cls, score)\n\n        self.score = score\n        se","@@ -264,7 +264,6 @@ class BoTSORT(object):\n         self.proximity_thresh = proximity_thresh\n         self.appearance_thresh = appearance_thresh\n         self.match_thresh = match_thresh\n-        print(device)\n \n         self.model = ReIDDetectMultiBackend(weights=model_weights, device=device, fp16=fp16)\n \n",add,Add note about data volume to enable_metrics_collection
e9d966cd32f73072f40f1948cde46118ad05f9b5,delete debug print,examples/multi_yolo_backend.py,"from pathlib import Path\nimport numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass MultiYolo():\n    def __init__(self, model, device, args):\n        self.args = args\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = 'yolov8'\n            self.model = model\n        else:\n            self.model_name = str(model.stem).lower()\n\n        if 'yolo_nas' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n            self.model_type = 'yolo_nas'\n            self.model = models.get(\n                self.model_name,\n                pretrained_weights=""""coco""""\n            ).to(self.device)\n        elif 'yolox' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n     ","from pathlib import Path\nimport numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass MultiYolo():\n    def __init__(self, model, device, args):\n        self.args = args\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = 'yolov8'\n            self.model = model\n        else:\n            self.model_name = str(model.stem).lower()\n\n        if 'yolo_nas' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n            self.model_type = 'yolo_nas'\n            self.model = models.get(\n                self.model_name,\n                pretrained_weights=""""coco""""\n            ).to(self.device)\n        elif 'yolox' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n     ","@@ -45,7 +45,6 @@ class MultiYolo():\n             \n     def __call__(self, im, im0s):\n         if 'yolo_nas' in self.model_name or 'yolox' in self.model_name:\n-            print(self.args)\n             prediction = next(iter(\n                 self.model.predict(im0s,\n                                    iou=self.args.iou,\n",add,Added the user group to the contributors
9666ab587d62d8ceb7fdacfdc094bf4ddc0cdfa9,fix auto SG pip install,examples/multi_yolo_backend.py,"from pathlib import Path\nimport numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass MultiYolo():\n    def __init__(self, model, device, args):\n        self.args = args\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = 'yolov8'\n            self.model = model\n        else:\n            self.model_name = str(model.stem).lower()\n\n        if 'yolo_nas' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n            self.model_type = 'yolo_nas'\n            self.model = models.get(\n                self.model_name,\n                pretrained_weights=""""coco""""\n            ).to(self.device)\n        elif 'yolox' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n     ","from pathlib import Path\nimport numpy as np\nimport torch\n\nfrom boxmot.utils.checks import TestRequirements\ntr = TestRequirements()\n\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass MultiYolo():\n    def __init__(self, model, device, args):\n        self.args = args\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = 'yolov8'\n            self.model = model\n        else:\n            self.model_name = str(model.stem).lower()\n\n        if 'yolo_nas' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n            self.model_type = 'yolo_nas'\n            self.model = models.get(\n                self.model_name,\n                pretrained_weights=""""coco""""\n            ).to(self.device)\n        elif 'yolox' in self.model_name:\n            self.try_sg_impo","@@ -1,6 +1,10 @@\n from pathlib import Path\n import numpy as np\n import torch\n+\n+from boxmot.utils.checks import TestRequirements\n+tr = TestRequirements()\n+\n from ultralytics.yolo.engine.results import Boxes, Results\n \n \n@@ -40,8 +44,7 @@ class MultiYolo():\n         try:\n             import super_gradients  # for linear_assignment\n         except (ImportError, AssertionError, AttributeError):\n-            from ultralytics.yolo.utils.checks import check_requirements\n-            check_requirements('super-gradients==3.1.1')  # install\n+            tr.check_packages(('super-gradients==3.1.1',))  # install\n             \n     def __call__(self, im, im0s):\n         if 'yolo_nas' in self.model_name or 'yolox' in self.model_name:\n",add,Don ' t define the right side title
3b67012f77b91082174729d466810cb895209e09,set default det thresh value,boxmot/deepocsort/ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom __future__ import print_function\n\nimport torch\nimport numpy as np\nfrom .association import *\nfrom .cmc import CMCComputer\nfrom boxmot.deep.reid_multibackend import ReIDDetectMultiBackend\n\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    s = w * h  # scale i","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom __future__ import print_function\n\nimport torch\nimport numpy as np\nfrom .association import *\nfrom .cmc import CMCComputer\nfrom boxmot.deep.reid_multibackend import ReIDDetectMultiBackend\n\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    s = w * h  # scale i","@@ -320,7 +320,7 @@ class OCSort(object):\n         model_weights,\n         device,\n         fp16,\n-        det_thresh,\n+        det_thresh=0.3,\n         max_age=30,\n         min_hits=3,\n         iou_threshold=0.3,\n",add,Added net . kano . joustsim . oscar . os
e5edeff07330ea6471f6a30289c88b5b6d8b17a1,debug prints,boxmot/tracker_zoo.py,"from pathlib import Path\nimport yaml\nfrom types import SimpleNamespace\nfrom boxmot.utils import BOXMOT\n\n\ndef get_tracker_config(tracker_type):\n    tracking_config = \\n        BOXMOT /\\n        tracker_type /\\n        'configs' /\\n        (tracker_type + '.yaml')\n    return tracking_config\n    \n\ndef create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n\n    with open(tracker_config, """"r"""") as f:\n        cfg = yaml.load(f.read(), Loader=yaml.FullLoader)\n    cfg = SimpleNamespace(**cfg)  # easier dict acces by dot, instead of ['']\n    print(cfg)\n    print(cfg.max_age)\n    print(cfg.min_hits)\n    print(cfg.iou_thresh)\n    print(cfg.delta_t)\n    print(cfg.asso_func)\n    print(cfg.inertia)\n    print(cfg.det_thresh)\n    \n    if tracker_type == 'strongsort':\n        from boxmot.strongsort.strong_sort import StrongSORT\n        strongsort = StrongSORT(\n            reid_weights,\n            device,\n            half,\n            max_dist=cfg.m","from pathlib import Path\nimport yaml\nfrom types import SimpleNamespace\nfrom boxmot.utils import BOXMOT\n\n\ndef get_tracker_config(tracker_type):\n    tracking_config = \\n        BOXMOT /\\n        tracker_type /\\n        'configs' /\\n        (tracker_type + '.yaml')\n    return tracking_config\n    \n\ndef create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n\n    with open(tracker_config, """"r"""") as f:\n        cfg = yaml.load(f.read(), Loader=yaml.FullLoader)\n    cfg = SimpleNamespace(**cfg)  # easier dict acces by dot, instead of ['']\n    print(cfg)\n    \n    if tracker_type == 'strongsort':\n        from boxmot.strongsort.strong_sort import StrongSORT\n        strongsort = StrongSORT(\n            reid_weights,\n            device,\n            half,\n            max_dist=cfg.max_dist,\n            max_iou_dist=cfg.max_iou_dist,\n            max_age=cfg.max_age,\n            max_unmatched_preds=cfg.max_unmatched_preds,\n            n_init=cfg.n_init,\","@@ -19,13 +19,6 @@ def create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n         cfg = yaml.load(f.read(), Loader=yaml.FullLoader)\n     cfg = SimpleNamespace(**cfg)  # easier dict acces by dot, instead of ['']\n     print(cfg)\n-    print(cfg.max_age)\n-    print(cfg.min_hits)\n-    print(cfg.iou_thresh)\n-    print(cfg.delta_t)\n-    print(cfg.asso_func)\n-    print(cfg.inertia)\n-    print(cfg.det_thresh)\n     \n     if tracker_type == 'strongsort':\n         from boxmot.strongsort.strong_sort import StrongSORT\n@@ -88,6 +81,7 @@ def create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n         return botsort\n     elif tracker_type == 'deepocsort':\n         from boxmot.deepocsort.ocsort import OCSort\n+        print(cfg)\n         deepocsort = OCSort(\n             reid_weights,\n             device,\n",add,Add note about data volume to enable_metrics_collection
52f2ee1c87a2e19f64a55ace1c70b0bd42a62b98,fix config writing,examples/evolve.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport yaml\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom track import run\n\nfrom ultralytics.yolo.utils import LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\n\n\n\nclass Objective(Evaluator):\n    """"""""""""Objective function to evolve best set of hyperparams for\n    \n    This object","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport os\nimport sys\nimport logging\nimport argparse\nimport yaml\nimport re\nfrom pathlib import Path\nfrom val import Evaluator\n\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom track import run\n\nfrom boxmot.utils import logger\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\n\n\n\nclass Objective(Evaluator):\n    """"""""""""Objective function to evolve best set of hyperparams for\n    \n    This object is passed","@@ -23,7 +23,7 @@ from val import Evaluator\n from boxmot.utils import ROOT, WEIGHTS\n from track import run\n \n-from ultralytics.yolo.utils import LOGGER\n+from boxmot.utils import logger\n from ultralytics.yolo.utils.checks import check_requirements, print_args\n \n \n@@ -182,6 +182,7 @@ class Objective(Evaluator):\n             }\n                         \n         # overwrite existing config for tracker\n+        logger.info(f""""Writing newly generated config for trial"""")\n         with open(self.opt.tracking_config, 'w') as f:\n             data = yaml.dump(d, f)   \n     \n@@ -216,11 +217,10 @@ def print_best_trial_metric_results(study, objectives):\n     """"""""""""\n     for ob in enumerate(objectives):  \n         trial_with_highest_ob = max(study.best_trials, key=lambda t: t.values[0])\n-        print(f""""Trial with highest {ob}: """")\n-        print(f""""\tnumber: {trial_with_highest_ob.number}"""")\n-        print(f""""\tparams: {trial_with_highest_ob.params}"""")\n-        print(f""""\tvalues: {trial_with_highest_ob.values}"""")\n-\n+        logger.info(f""""Trial with highest {ob}: """")\n+        logger.info(f""""\tnumber: {trial_with_highest_ob.number}"""")\n+        logger.info(f""""\tvalues: {trial_with_highest_ob.values}"""")\n+        logger.info(f""""\tparams: {trial_with_highest_ob.params}"""")\n     \n     \n def save_plots(opt, study, objectives):\n@@ -258,7 +258,7 @@ def write_best_HOTA_params_to_config(opt, study):\n         None\n     """"""""""""\n     trial_with_highest_HOTA = max(study.best_trials, key=lambda t: t.values[0])\n-    d = {opt.tracking_method: trial_with_highest_HOTA.params}\n+    d = trial_with_highest_HOTA.params\n     with open(opt.tracking_config, 'w') as f:\n         f.write(f'# Trial number:      {trial_with_highest_HOTA.number}\n')\n         f.write(f'# HOTA, MOTA, IDF1:  {trial_with_highest_HOTA.values}\n')\n@@ -335,7 +335,6 @@ if __name__ == """"__main__"""":\n         with open(opt.tracking_config, 'r') as f:\n             params = yaml.load(f, Loader=yaml.",add,Don ' t set the application title to the library
c8a874ea3f0ff622765c09501bb37fdc424e2603,delete debug prints,boxmot/tracker_zoo.py,"from pathlib import Path\nimport yaml\nfrom types import SimpleNamespace\nfrom boxmot.utils import BOXMOT\n\n\ndef get_tracker_config(tracker_type):\n    tracking_config = \\n        BOXMOT /\\n        tracker_type /\\n        'configs' /\\n        (tracker_type + '.yaml')\n    return tracking_config\n    \n\ndef create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n\n    with open(tracker_config, """"r"""") as f:\n        cfg = yaml.load(f.read(), Loader=yaml.FullLoader)\n    cfg = SimpleNamespace(**cfg)  # easier dict acces by dot, instead of ['']\n    print(cfg)\n    \n    if tracker_type == 'strongsort':\n        from boxmot.strongsort.strong_sort import StrongSORT\n        strongsort = StrongSORT(\n            reid_weights,\n            device,\n            half,\n            max_dist=cfg.max_dist,\n            max_iou_dist=cfg.max_iou_dist,\n            max_age=cfg.max_age,\n            max_unmatched_preds=cfg.max_unmatched_preds,\n            n_init=cfg.n_init,\","from pathlib import Path\nimport yaml\nfrom types import SimpleNamespace\nfrom boxmot.utils import BOXMOT\n\n\ndef get_tracker_config(tracker_type):\n    tracking_config = \\n        BOXMOT /\\n        tracker_type /\\n        'configs' /\\n        (tracker_type + '.yaml')\n    return tracking_config\n    \n\ndef create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n\n    with open(tracker_config, """"r"""") as f:\n        cfg = yaml.load(f.read(), Loader=yaml.FullLoader)\n    cfg = SimpleNamespace(**cfg)  # easier dict acces by dot, instead of ['']\n    \n    if tracker_type == 'strongsort':\n        from boxmot.strongsort.strong_sort import StrongSORT\n        strongsort = StrongSORT(\n            reid_weights,\n            device,\n            half,\n            max_dist=cfg.max_dist,\n            max_iou_dist=cfg.max_iou_dist,\n            max_age=cfg.max_age,\n            max_unmatched_preds=cfg.max_unmatched_preds,\n            n_init=cfg.n_init,\n            nn_","@@ -18,7 +18,6 @@ def create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n     with open(tracker_config, """"r"""") as f:\n         cfg = yaml.load(f.read(), Loader=yaml.FullLoader)\n     cfg = SimpleNamespace(**cfg)  # easier dict acces by dot, instead of ['']\n-    print(cfg)\n     \n     if tracker_type == 'strongsort':\n         from boxmot.strongsort.strong_sort import StrongSORT\n@@ -81,7 +80,6 @@ def create_tracker(tracker_type, tracker_config, reid_weights, device, half):\n         return botsort\n     elif tracker_type == 'deepocsort':\n         from boxmot.deepocsort.ocsort import OCSort\n-        print(cfg)\n         deepocsort = OCSort(\n             reid_weights,\n             device,\n",add,Added the UNSTARTED state to the YouTube PlayerState enum
6a2ceb7b554c7f1a083ae0b54353aae034359ec7,fix logging messages,examples/val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom boxmot.utils import logger as LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom ultralytics.yolo.utils.files import increment_path\n\nfrom boxmot.u","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom boxmot.utils import logger as LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom ultralytics.yolo.utils.files import increment_path\n\nfrom boxmot.u","@@ -91,7 +91,7 @@ class Evaluator:\n                                 zip_file.extract(member, val_tools_path / 'data')\n                 LOGGER.info(f'{benchmark}.zip unzipped successfully')\n             except Exception as e:\n-                print(f'{benchmark}.zip is corrupted. Try deleting the file and run the script again')\n+                LOGGER.error(f'{benchmark}.zip is corrupted. Try deleting the file and run the script again')\n                 sys.exit()\n \n     def eval_setup(self, opt, val_tools_path):\n@@ -191,6 +191,8 @@ class Evaluator:\n                 if not dst_seq_path.is_dir():\n                     src_seq_path = seq_path\n                     shutil.move(str(src_seq_path), str(dst_seq_path))\n+                else:\n+                    print('dfadf\n\n\n')\n \n                 LOGGER.info(f""""Staring evaluation process on {dst_seq_path}"""")\n                 p = subprocess.Popen(\n@@ -220,8 +222,8 @@ class Evaluator:\n                 \n                 # Check the return code of the subprocess\n                 if p.returncode != 0:\n-                    LOGGER.error(""""Subprocess failed with return code:"""", p.returncode)\n                     LOGGER.error(stderr)\n+                    LOGGER.error(stdout)\n                     sys.exit(1)\n                 else:\n                     LOGGER.success(f""""{dst_seq_path} evaluation succeeded"""")\n@@ -257,10 +259,11 @@ class Evaluator:\n \n         # Check the return code of the subprocess\n         if p.returncode != 0:\n-            LOGGER.error(""""Subprocess failed with return code:"""", p.returncode)\n+            LOGGER.error(stderr)\n+            LOGGER.error(stdout)\n             sys.exit(1)\n \n-        print(stdout)\n+        LOGGER.info(stdout)\n \n         # save MOT results in txt \n         with open(save_dir / 'MOT_results.txt', 'w') as f:\n",add,always construct RRD settings for 0 . 2
ba633fd670bb9c6b0ab4034e023d78a8a1f46abd,delete dubug logging,examples/val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom boxmot.utils import logger as LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom ultralytics.yolo.utils.files import increment_path\n\nfrom boxmot.u","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom boxmot.utils import logger as LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom ultralytics.yolo.utils.files import increment_path\n\nfrom boxmot.u","@@ -191,8 +191,6 @@ class Evaluator:\n                 if not dst_seq_path.is_dir():\n                     src_seq_path = seq_path\n                     shutil.move(str(src_seq_path), str(dst_seq_path))\n-                else:\n-                    print('dfadf\n\n\n')\n \n                 LOGGER.info(f""""Staring evaluation process on {dst_seq_path}"""")\n                 p = subprocess.Popen(\n",add,Add forced default for text type as oCC
6ae77333bec15cfb8aab2eb4208a06ce90c51003,set default det_thresh value,boxmot/ocsort/ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom __future__ import print_function\n\nimport numpy as np\nfrom .association import *\nfrom boxmot.utils.ops import xywh2xyxy\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age-dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w/2.\n    y = bbox[1] + h/2.\n    s = w * h  # scale is just area\n    r = w / float(h+1e-6)\n    return np.array([x, y, s, r]).reshap","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom __future__ import print_function\n\nimport numpy as np\nfrom .association import *\nfrom boxmot.utils.ops import xywh2xyxy\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age-dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w/2.\n    y = bbox[1] + h/2.\n    s = w * h  # scale is just area\n    r = w / float(h+1e-6)\n    return np.array([x, y, s, r]).reshap","@@ -179,7 +179,7 @@ ASSO_FUNCS = {  """"iou"""": iou_batch,\n \n \n class OCSort(object):\n-    def __init__(self, det_thresh, max_age=30, min_hits=3, \n+    def __init__(self, det_thresh=0.2, max_age=30, min_hits=3, \n         iou_threshold=0.3, delta_t=3, asso_func=""""iou"""", inertia=0.2, use_byte=False):\n         """"""""""""\n         Sets key parameters for SORT\n",add,Added KHR_gl_texture_2D_image extension string
3c9ef84dd952869bc2024cc6ffaabab23eefb5ca,fix dtype bug,boxmot/strongsort/sort/track.py,"# vim: expandtab:ts=4:sw=4\nimport cv2\nimport numpy as np\nfrom .kalman_filter import KalmanFilter\nfrom collections import deque\n\n\nclass TrackState:\n    """"""""""""\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n\n    """"""""""""\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    """"""""""""\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initial state distribution.\n    tr","# vim: expandtab:ts=4:sw=4\nimport cv2\nimport numpy as np\nfrom .kalman_filter import KalmanFilter\nfrom collections import deque\n\n\nclass TrackState:\n    """"""""""""\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n\n    """"""""""""\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    """"""""""""\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initial state distribution.\n    tr","@@ -276,7 +276,7 @@ class Track:\n             The associated detection.\n         """"""""""""\n         self.conf = conf\n-        self.class_id = class_id.int()\n+        self.class_id = class_id.astype('int64')\n         self.mean, self.covariance = self.kf.update(self.mean, self.covariance, detection.to_xyah(), detection.confidence)\n \n         feature = detection.feature / np.linalg.norm(detection.feature)\n",add,Add note about data volume to enable_metrics_collection
eb0955792a3a20f15a441ca45cc69f9716a6523b,fix var name,boxmot/strongsort/strong_sort.py,"import numpy as np\nimport torch\nimport sys\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists, join\nimport torchvision.transforms as transforms\n\nfrom .sort.nn_matching import NearestNeighborDistanceMetric\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\n\nfrom boxmot.deep.reid_multibackend import ReIDDetectMultiBackend\n\nfrom boxmot.utils.ops import xyxy2xywh\n\n\nclass StrongSORT(object):\n    def __init__(self, \n                 model_weights,\n                 device,\n                 fp16,\n                 max_dist=0.2,\n                 max_iou_dist=0.7,\n                 max_age=70,\n                 max_unmatched_preds=7,\n                 n_init=3,\n                 nn_budget=100,\n                 mc_lambda=0.995,\n                 ema_alpha=0.9\n                ):\n\n        self.model = ReIDDetectMultiBackend(weights=model_weights, device=device, fp16=fp16)\n        \n        self.max_dist = max_dist\n        metric = Near","import numpy as np\nimport torch\nimport sys\nimport cv2\nimport gdown\nfrom os.path import exists as file_exists, join\nimport torchvision.transforms as transforms\n\nfrom .sort.nn_matching import NearestNeighborDistanceMetric\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\n\nfrom boxmot.deep.reid_multibackend import ReIDDetectMultiBackend\n\nfrom boxmot.utils.ops import xyxy2xywh\n\n\nclass StrongSORT(object):\n    def __init__(self, \n                 model_weights,\n                 device,\n                 fp16,\n                 max_dist=0.2,\n                 max_iou_dist=0.7,\n                 max_age=70,\n                 max_unmatched_preds=7,\n                 n_init=3,\n                 nn_budget=100,\n                 mc_lambda=0.995,\n                 ema_alpha=0.9\n                ):\n\n        self.model = ReIDDetectMultiBackend(weights=model_weights, device=device, fp16=fp16)\n        \n        self.max_dist = max_dist\n        metric = Near","@@ -38,10 +38,10 @@ class StrongSORT(object):\n         self.tracker = Tracker(\n             metric, max_iou_dist=max_iou_dist, max_age=max_age, n_init=n_init, max_unmatched_preds=max_unmatched_preds, mc_lambda=mc_lambda, ema_alpha=ema_alpha)\n \n-    def update(self, dets,  ori_img):\n+    def update(self, dets,  img):\n \n         assert isinstance(dets, np.ndarray), f""""Unsupported 'dets' input format '{type(dets)}', valid format is np.ndarray""""\n-        assert isinstance(img_numpy, np.ndarray), f""""Unsupported 'img_numpy' input format '{type(ori_img)}', valid format is np.ndarray""""\n+        assert isinstance(img, np.ndarray), f""""Unsupported 'img' input format '{type(img)}', valid format is np.ndarray""""\n         assert len(dets.shape) == 2, f""""Unsupported 'dets' dimensions, valid number of dimensions is two""""\n         assert dets.shape[1] == 6, f""""Unsupported 'dets' 2nd dimension lenght, valid lenghts is 6""""\n         \n@@ -52,10 +52,10 @@ class StrongSORT(object):\n         classes = clss\n         xywhs = xyxy2xywh(xyxys)\n         confs = confs\n-        self.height, self.width = ori_img.shape[:2]\n+        self.height, self.width = img.shape[:2]\n         \n         # generate detections\n-        features = self._get_features(xywhs, ori_img)\n+        features = self._get_features(xywhs, img)\n         bbox_tlwh = self._xywh_to_tlwh(xywhs)\n         detections = [Detection(bbox_tlwh[i], conf, features[i]) for i, conf in enumerate(\n             confs)]\n@@ -134,11 +134,11 @@ class StrongSORT(object):\n         return t, l, w, h\n \n     @torch.no_grad()\n-    def _get_features(self, bbox_xywh, ori_img):\n+    def _get_features(self, bbox_xywh, img):\n         im_crops = []\n         for box in bbox_xywh:\n             x1, y1, x2, y2 = self._xywh_to_xyxy(box)\n-            im = ori_img[y1:y2, x1:x2]\n+            im = img[y1:y2, x1:x2]\n             im_crops.append(im)\n         if im_crops:\n             features = self.model(im_crops)\n",add,Don ' t define the dependency on real_commad twice
6616e29bcf3435abca9e3c4b6ef5428f55941236,fix saving problem when using track.py,examples/track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.torch_utils import select_device\n\ntr = TestRequirements()\ntr.check_packages(('ultralytics',))  # install\n\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\nfrom ultralytics.yolo.utils.plotting import save_one_box\n\nfrom multi_yolo_backend import Multi","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.torch_utils import select_device\n\ntr = TestRequirements()\ntr.check_packages(('ultralytics',))  # install\n\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\nfrom ultralytics.yolo.utils.plotting import save_one_box\n\nfrom multi_yolo_backend import Multi","@@ -28,6 +28,8 @@ from ultralytics.yolo.utils.plotting import save_one_box\n from multi_yolo_backend import MultiYolo\n from utils import write_MOT_results\n \n+from boxmot.utils import EXAMPLES\n+\n \n def on_predict_start(predictor):\n     predictor.trackers = []\n@@ -152,7 +154,6 @@ def run(args):\n                     \n                 if predictor.tracker_outputs[i].size != 0 and predictor.args.save_mot:\n                     # needed if txt save is not activated, otherwise redundant\n-                    predictor.MOT_txt_path.mkdir(parents=True, exist_ok=predictor.args.exist_ok)\n                     write_MOT_results(\n                         predictor.MOT_txt_path,\n                         predictor.results[i],\n@@ -214,7 +215,7 @@ def parse_opt():\n     parser.add_argument('--save', action='store_true', help='save video tracking results')\n     # # class 0 is person, 1 is bycicle, 2 is car... 79 is oven\n     parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')\n-    parser.add_argument('--project', default=ROOT / 'runs' / 'track', help='save results to project/name')\n+    parser.add_argument('--project', default=EXAMPLES / 'runs' / 'track', help='save results to project/name')\n     parser.add_argument('--name', default='exp', help='save results to project/name')\n     parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n     parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n",add,Added STORM - 236 to Changelog
6616e29bcf3435abca9e3c4b6ef5428f55941236,fix saving problem when using track.py,examples/utils.py,"import torch\nimport numpy as np\nfrom ultralytics.yolo.utils import ops\n\ndef write_MOT_results(txt_path, results, frame_idx, i):\n    nr_dets = len(results.boxes)\n    frame_idx = torch.full((1, 1), frame_idx + 1)\n    frame_idx = frame_idx.repeat(nr_dets, 1)\n    dont_care = torch.full((nr_dets, 1), -1)\n    i = torch.full((nr_dets, 1), i)\n    mot = torch.cat([\n        frame_idx,\n        results.boxes.id.unsqueeze(1).to('cpu'),\n        ops.xyxy2ltwh(results.boxes.xyxy).to('cpu'),\n        results.boxes.conf.unsqueeze(1).to('cpu'),\n        results.boxes.cls.unsqueeze(1).to('cpu'),\n        dont_care\n    ], dim=1)\n\n    with open(str(txt_path) + '.txt', 'ab') as f:  # append binary mode\n        np.savetxt(f, mot.numpy(), fmt='%d')  # save as ints instead of scientific notation","import torch\nimport numpy as np\nfrom ultralytics.yolo.utils import ops\n\ndef write_MOT_results(txt_path, results, frame_idx, i):\n    nr_dets = len(results.boxes)\n    frame_idx = torch.full((1, 1), frame_idx + 1)\n    frame_idx = frame_idx.repeat(nr_dets, 1)\n    dont_care = torch.full((nr_dets, 1), -1)\n    i = torch.full((nr_dets, 1), i)\n    mot = torch.cat([\n        frame_idx,\n        results.boxes.id.unsqueeze(1).to('cpu'),\n        ops.xyxy2ltwh(results.boxes.xyxy).to('cpu'),\n        results.boxes.conf.unsqueeze(1).to('cpu'),\n        results.boxes.cls.unsqueeze(1).to('cpu'),\n        dont_care\n    ], dim=1)\n\n    # create parent folder\n    txt_path.parent.mkdir(parents=False, exist_ok=True)\n    # create mot txt file\n    txt_path.with_suffix('.txt').touch(exist_ok=True)\n\n    with open(str(txt_path) + '.txt', 'ab+') as f:  # append binary mode\n        np.savetxt(f, mot.numpy(), fmt='%d')  # save as ints instead of scientific notation","@@ -17,5 +17,10 @@ def write_MOT_results(txt_path, results, frame_idx, i):\n         dont_care\n     ], dim=1)\n \n-    with open(str(txt_path) + '.txt', 'ab') as f:  # append binary mode\n+    # create parent folder\n+    txt_path.parent.mkdir(parents=False, exist_ok=True)\n+    # create mot txt file\n+    txt_path.with_suffix('.txt').touch(exist_ok=True)\n+\n+    with open(str(txt_path) + '.txt', 'ab+') as f:  # append binary mode\n         np.savetxt(f, mot.numpy(), fmt='%d')  # save as ints instead of scientific notation\n\ No newline at end of file\n",add,Add warning about data volume to enable_metrics_collection
954b5644a818becbee9e2fbe50495539b11d9bdd,fix saving problem when using track.py,examples/track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.torch_utils import select_device\n\ntr = TestRequirements()\ntr.check_packages(('ultralytics',))  # install\n\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\nfrom ultralytics.yolo.utils.plotting import save_one_box\n\nfrom multi_yolo_backend import Multi","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport numpy as np\nimport cv2\nfrom types import SimpleNamespace\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.torch_utils import select_device\n\ntr = TestRequirements()\ntr.check_packages(('ultralytics',))  # install\n\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import SETTINGS, colorstr, ops, is_git_dir, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz, print_args\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.engine.results import Boxes\nfrom ultralytics.yolo.data.utils import VID_FORMATS\nfrom ultralytics.yolo.utils.plotting import save_one_box\n\nfrom multi_yolo_backend import Multi","@@ -153,7 +153,6 @@ def run(args):\n                     predictor.MOT_txt_path = predictor.txt_path.parent / p.parent.name\n                     \n                 if predictor.tracker_outputs[i].size != 0 and predictor.args.save_mot:\n-                    # needed if txt save is not activated, otherwise redundant\n                     write_MOT_results(\n                         predictor.MOT_txt_path,\n                         predictor.results[i],\n",add,Added Type name for DFI ( # 3 )
8e25085fe486be5dcc63266fac3ea04ade5eea8a,Fixed so osnet weights file is put in the right location when running test,tests/test_python.py,"# pytest tests/test_python.py\n\nimport numpy as np\nimport torch\nfrom pathlib import Path\n\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.trackers import StrongSORT\nfrom boxmot.trackers import OCSort as OCSORT\nfrom boxmot.trackers import BYTETracker\nfrom boxmot.trackers import BoTSORT\nfrom boxmot.trackers import DeepOCSort as DeepOCSORT\nfrom boxmot.tracker_zoo import create_tracker, get_tracker_config\n\n\ndef test_tracker_output():\n    tracker_conf = get_tracker_config('deepocsort')\n    tracker = create_tracker(\n        'deepocsort',\n        tracker_conf,\n        WEIGHTS / 'mobilenetv2_x1_4_dukemtmcreid.pt',\n        'cpu',\n        False\n    )\n    rgb = np.random.randint(255, size=(640, 640, 3),dtype=np.uint8)\n    det = np.array([[144, 212, 578, 480, 0.82, 0],\n                    [425, 281, 576, 472, 0.56, 65]])\n    output = tracker.update(det, rgb)\n    assert output.shape == (2, 7)  # two inputs should give two outputs\n\n\ndef test_strongsort_instantiation():\n  ","# pytest tests/test_python.py\n\nimport numpy as np\nimport torch\nfrom pathlib import Path\n\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.trackers import StrongSORT\nfrom boxmot.trackers import OCSort as OCSORT\nfrom boxmot.trackers import BYTETracker\nfrom boxmot.trackers import BoTSORT\nfrom boxmot.trackers import DeepOCSort as DeepOCSORT\nfrom boxmot.tracker_zoo import create_tracker, get_tracker_config\n\n\ndef test_tracker_output():\n    tracker_conf = get_tracker_config('deepocsort')\n    tracker = create_tracker(\n        'deepocsort',\n        tracker_conf,\n        WEIGHTS / 'mobilenetv2_x1_4_dukemtmcreid.pt',\n        'cpu',\n        False\n    )\n    rgb = np.random.randint(255, size=(640, 640, 3),dtype=np.uint8)\n    det = np.array([[144, 212, 578, 480, 0.82, 0],\n                    [425, 281, 576, 472, 0.56, 65]])\n    output = tracker.update(det, rgb)\n    assert output.shape == (2, 7)  # two inputs should give two outputs\n\n\ndef test_strongsort_instantiation():\n  ","@@ -31,7 +31,7 @@ def test_tracker_output():\n \n def test_strongsort_instantiation():\n     ss = StrongSORT(\n-        model_weights=Path('osnet_x0_25_msmt17.pt'),\n+        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n         device='cpu',\n         fp16=True,\n     )\n@@ -39,7 +39,7 @@ def test_strongsort_instantiation():\n \n def test_botsort_instantiation():\n     bs = BoTSORT(\n-        model_weights=Path('osnet_x0_25_msmt17.pt'),\n+        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n         device='cpu',\n         fp16=True,\n     )\n@@ -47,7 +47,7 @@ def test_botsort_instantiation():\n \n def test_deepocsort_instantiation():\n     dos = DeepOCSORT(\n-        model_weights=Path('osnet_x0_25_msmt17.pt'),\n+        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n         device='cpu',\n         fp16=True,\n     )\n",add,Added the UNSTARTED state to the YouTube PlayerState enum
3f537ece74b43b3b16f23000479ac090b8aa411e,Fixed some mismatch in paths in example scripts,examples/track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport cv2\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.torch_utils import select_device\n\ntry:\n    from ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\n    from ultralytics.yolo.utils import colorstr, ops, IterableSimpleNamespace\n    from ultralytics.yolo.utils.checks import check_imgsz\n    from ultralytics.yolo.utils.files import increment_path\n    from ultralytics.yolo.data.utils import VID_FORMATS\n    from ultralytics.yolo.utils.plotting import save_one_box\nexcept ImportError:\n    LOGGER.error(\n        'Running track.py requires the following packages to be installed:\n'\n        '$ pip install ultralytics==8.0.124\n'\n    )\n\n\nfrom multi_yolo_backend import MultiYolo\nfrom utils import write_MOT_results\n\nfrom boxmot.ut","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport cv2\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.torch_utils import select_device\n\ntry:\n    from ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\n    from ultralytics.yolo.utils import colorstr, ops, IterableSimpleNamespace\n    from ultralytics.yolo.utils.checks import check_imgsz\n    from ultralytics.yolo.utils.files import increment_path\n    from ultralytics.yolo.data.utils import VID_FORMATS\n    from ultralytics.yolo.utils.plotting import save_one_box\nexcept ImportError:\n    LOGGER.error(\n        'Running track.py requires the following packages to be installed:\n'\n        '$ pip install ultralytics==8.0.124\n'\n    )\n\n\nfrom multi_yolo_backend import MultiYolo\nfrom utils import write_MOT_results\n\nfrom boxmot.ut","@@ -37,7 +37,6 @@ def on_predict_start(predictor):\n     predictor.args.tracking_config = \\n         ROOT /\\n         'boxmot' /\\n-        opt.tracking_method /\\n         'configs' /\\n         (opt.tracking_method + '.yaml')\n     for i in range(predictor.dataset.bs):\n",fix,Add warning about data volume to enable_metrics_collection
3f537ece74b43b3b16f23000479ac090b8aa411e,Fixed some mismatch in paths in example scripts,examples/val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom boxmot.utils import logger as LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom ultralytics.yolo.utils.files import increment_path\n\nfrom boxmot.u","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport os\nimport sys\nimport torch\nimport subprocess\nfrom subprocess import Popen\nimport argparse\nimport git\nimport re\nimport yaml\nfrom git import Repo\nimport zipfile\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom boxmot.utils import logger as LOGGER\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom ultralytics.yolo.utils.files import increment_path\n\nfrom boxmot.u","@@ -270,7 +270,6 @@ class Evaluator:\n         tracking_config = \\n             ROOT /\\n             'boxmot' /\\n-            opt.tracking_method /\\n             'configs' /\\n             (opt.tracking_method + '.yaml')\n         shutil.copyfile(tracking_config, save_dir / Path(tracking_config).name)\n",add,Add note about data volume to enable_metrics_collection
477e2eb87dbd02b79133db06359f2ce7374bd361,Fixed some issues with kalman filter adapters,boxmot/motion/adapters/botsort_kf_adapter.py,"import numpy as np\nfrom ..kalman_filter import KalmanFilter, multi_predict\n\n\nclass BotSortKalmanFilterAdapter(KalmanFilter):\n\n    ndim = 4\n\n    def __init__(self, dt=1):\n        super().__init__(dim_x=2 * self.ndim, dim_z=self.ndim)\n\n        # Set transition matrix\n        for i in range(self.ndim):\n            self.F[i, self.ndim + i] = dt\n\n        # Set observation matrix\n        self.H = np.eye(self.ndim, 2 * self.ndim)\n\n        # Motion and observation uncertainty are chosen relative to the current\n        # state estimate. These weights control the amount of uncertainty in\n        # the model. This is a bit hacky.\n        self._std_weight_position = 1. / 20\n        self._std_weight_velocity = 1. / 160\n\n    def initiate(self, measurement):\n        """"""""""""Create track from unassociated measurement.\n\n        Parameters\n        ----------\n        measurement : ndarray\n            Bounding box coordinates (x, y, w, h) with center position (x, y),\n         ","import numpy as np\nfrom ..kalman_filter import KalmanFilter, multi_predict\n\n\nclass BotSortKalmanFilterAdapter(KalmanFilter):\n\n    ndim = 4\n\n    def __init__(self, dt=1):\n        super().__init__(dim_x=2 * self.ndim, dim_z=self.ndim)\n\n        # Set transition matrix\n        for i in range(self.ndim):\n            self.F[i, self.ndim + i] = dt\n\n        # Set observation matrix\n        self.H = np.eye(self.ndim, 2 * self.ndim)\n\n        # Motion and observation uncertainty are chosen relative to the current\n        # state estimate. These weights control the amount of uncertainty in\n        # the model. This is a bit hacky.\n        self._std_weight_position = 1. / 20\n        self._std_weight_velocity = 1. / 160\n\n    def initiate(self, measurement):\n        """"""""""""Create track from unassociated measurement.\n\n        Parameters\n        ----------\n        measurement : ndarray\n            Bounding box coordinates (x, y, w, h) with center position (x, y),\n         ","@@ -41,7 +41,7 @@ class BotSortKalmanFilterAdapter(KalmanFilter):\n         """"""""""""\n         mean_pos = measurement\n         mean_vel = np.zeros_like(mean_pos)\n-        self.X = np.r_[mean_pos, mean_vel]\n+        self.x = (np.r_[mean_pos, mean_vel]).T\n \n         std = [\n             2 * self._std_weight_position * measurement[2],\n@@ -54,7 +54,7 @@ class BotSortKalmanFilterAdapter(KalmanFilter):\n             10 * self._std_weight_velocity * measurement[3]]\n         self.P = np.diag(np.square(std))\n \n-        return self.x, self.P\n+        return self.x.T, self.P\n \n     def predict(self, mean, covariance):\n         """"""""""""Run Kalman filter prediction step.\n@@ -89,7 +89,7 @@ class BotSortKalmanFilterAdapter(KalmanFilter):\n \n         super().predict(Q=motion_cov)\n \n-        return self.X, self.P\n+        return self.x.T, self.P\n \n     def update(self, mean, covariance, measurement):\n         """"""""""""Run Kalman filter correction step.\n@@ -111,7 +111,7 @@ class BotSortKalmanFilterAdapter(KalmanFilter):\n             Returns the measurement-corrected state distribution.\n \n         """"""""""""\n-        self.X = mean\n+        self.x = mean.T\n         self.P = covariance\n \n         std = [\n@@ -123,7 +123,7 @@ class BotSortKalmanFilterAdapter(KalmanFilter):\n \n         super().update(measurement, R=innovation_cov)\n \n-        return self.X, self.P\n+        return self.x.T, self.P\n \n     def multi_predict(self, mean, covariance):\n         """"""""""""Run Kalman filter prediction step (Vectorized version).\n",add,Added note about data volume to enable_metrics_collection
477e2eb87dbd02b79133db06359f2ce7374bd361,Fixed some issues with kalman filter adapters,boxmot/motion/adapters/bytetrack_kf_adapter.py,"import numpy as np\nfrom ..kalman_filter import KalmanFilter, multi_predict\n\n\nclass ByteTrackKalmanFilterAdapter(KalmanFilter):\n\n    ndim = 4\n\n    def __init__(self, dt=1):\n        super().__init__(dim_x=2 * self.ndim, dim_z=self.ndim)\n\n        # Set transition matrix\n        for i in range(self.ndim):\n            self.F[i, self.ndim + i] = dt\n\n        # Set observation matrix\n        self.H = np.eye(self.ndim, 2 * self.ndim)\n\n        # Motion and observation uncertainty are chosen relative to the current\n        # state estimate. These weights control the amount of uncertainty in\n        # the model. This is a bit hacky.\n        self._std_weight_position = 1. / 20\n        self._std_weight_velocity = 1. / 160\n\n    def initiate(self, measurement):\n        """"""""""""Create track from unassociated measurement.\n\n        Parameters\n        ----------\n        measurement : ndarray\n            Bounding box coordinates (x, y, w, h) with center position (x, y),\n       ","import numpy as np\nfrom ..kalman_filter import KalmanFilter, multi_predict\n\n\nclass ByteTrackKalmanFilterAdapter(KalmanFilter):\n\n    ndim = 4\n\n    def __init__(self, dt=1):\n        super().__init__(dim_x=2 * self.ndim, dim_z=self.ndim)\n\n        # Set transition matrix\n        for i in range(self.ndim):\n            self.F[i, self.ndim + i] = dt\n\n        # Set observation matrix\n        self.H = np.eye(self.ndim, 2 * self.ndim)\n\n        # Motion and observation uncertainty are chosen relative to the current\n        # state estimate. These weights control the amount of uncertainty in\n        # the model. This is a bit hacky.\n        self._std_weight_position = 1. / 20\n        self._std_weight_velocity = 1. / 160\n\n    def initiate(self, measurement):\n        """"""""""""Create track from unassociated measurement.\n\n        Parameters\n        ----------\n        measurement : ndarray\n            Bounding box coordinates (x, y, w, h) with center position (x, y),\n       ","@@ -41,7 +41,7 @@ class ByteTrackKalmanFilterAdapter(KalmanFilter):\n         """"""""""""\n         mean_pos = measurement\n         mean_vel = np.zeros_like(mean_pos)\n-        self.X = np.r_[mean_pos, mean_vel]\n+        self.x = (np.r_[mean_pos, mean_vel]).T\n \n         std = [\n             2 * self._std_weight_position * measurement[3],\n@@ -54,7 +54,7 @@ class ByteTrackKalmanFilterAdapter(KalmanFilter):\n             10 * self._std_weight_velocity * measurement[3]]\n         self.P = np.diag(np.square(std))\n \n-        return self.x, self.P\n+        return self.x.T, self.P\n \n     def predict(self, mean, covariance):\n         """"""""""""Run Kalman filter prediction step.\n@@ -89,7 +89,7 @@ class ByteTrackKalmanFilterAdapter(KalmanFilter):\n \n         super().predict(Q=motion_cov)\n \n-        return self.X, self.P\n+        return self.x.T, self.P\n \n     def update(self, mean, covariance, measurement):\n         """"""""""""Run Kalman filter correction step.\n@@ -111,7 +111,7 @@ class ByteTrackKalmanFilterAdapter(KalmanFilter):\n             Returns the measurement-corrected state distribution.\n \n         """"""""""""\n-        self.X = mean\n+        self.x = mean.T\n         self.P = covariance\n \n         std = [\n@@ -123,7 +123,7 @@ class ByteTrackKalmanFilterAdapter(KalmanFilter):\n \n         super().update(measurement, R=innovation_cov)\n \n-        return self.X, self.P\n+        return self.x.T, self.P\n \n     def multi_predict(self, mean, covariance):\n         """"""""""""Run Kalman filter prediction step (Vectorized version).\n",add,Added note about data volume to enable_metrics_collection
477e2eb87dbd02b79133db06359f2ce7374bd361,Fixed some issues with kalman filter adapters,boxmot/motion/adapters/strongsort_kf_adapter.py,"import numpy as np\nfrom ..kalman_filter import KalmanFilter\n\n\nclass StrongSortKalmanFilterAdapter(KalmanFilter):\n\n    ndim = 4\n\n    def __init__(self, dt=1):\n        super().__init__(dim_x=2 * self.ndim, dim_z=self.ndim)\n\n        # Set transition matrix\n        for i in range(self.ndim):\n            self.F[i, self.ndim + i] = dt\n\n        # Set observation matrix\n        self.H = np.eye(self.ndim, 2 * self.ndim)\n\n        # Motion and observation uncertainty are chosen relative to the current\n        # state estimate. These weights control the amount of uncertainty in\n        # the model. This is a bit hacky.\n        self._std_weight_position = 1. / 20\n        self._std_weight_velocity = 1. / 160\n\n    def initiate(self, measurement):\n        """"""""""""Create track from unassociated measurement.\n\n        Parameters\n        ----------\n        measurement : ndarray\n            Bounding box coordinates (x, y, w, h) with center position (x, y),\n            width w, ","import numpy as np\nfrom ..kalman_filter import KalmanFilter\n\n\nclass StrongSortKalmanFilterAdapter(KalmanFilter):\n\n    ndim = 4\n\n    def __init__(self, dt=1):\n        super().__init__(dim_x=2 * self.ndim, dim_z=self.ndim)\n\n        # Set transition matrix\n        for i in range(self.ndim):\n            self.F[i, self.ndim + i] = dt\n\n        # Set observation matrix\n        self.H = np.eye(self.ndim, 2 * self.ndim)\n\n        # Motion and observation uncertainty are chosen relative to the current\n        # state estimate. These weights control the amount of uncertainty in\n        # the model. This is a bit hacky.\n        self._std_weight_position = 1. / 20\n        self._std_weight_velocity = 1. / 160\n\n    def initiate(self, measurement):\n        """"""""""""Create track from unassociated measurement.\n\n        Parameters\n        ----------\n        measurement : ndarray\n            Bounding box coordinates (x, y, w, h) with center position (x, y),\n            width w, ","@@ -41,7 +41,7 @@ class StrongSortKalmanFilterAdapter(KalmanFilter):\n         """"""""""""\n         mean_pos = measurement\n         mean_vel = np.zeros_like(mean_pos)\n-        self.X = np.r_[mean_pos, mean_vel]\n+        self.x = np.r_[mean_pos, mean_vel]\n \n         std = [\n             2 * self._std_weight_position * measurement[0],   # the center point x\n@@ -89,7 +89,7 @@ class StrongSortKalmanFilterAdapter(KalmanFilter):\n \n         super().predict(Q=motion_cov)\n \n-        return self.X, self.P\n+        return self.x, self.P\n \n     def update(self, mean, covariance, measurement, confidence=.0):\n         """"""""""""Run Kalman filter correction step.\n@@ -113,7 +113,7 @@ class StrongSortKalmanFilterAdapter(KalmanFilter):\n             Returns the measurement-corrected state distribution.\n \n         """"""""""""\n-        self.X = mean\n+        self.x = mean\n         self.P = covariance\n \n         std = [\n@@ -127,7 +127,7 @@ class StrongSortKalmanFilterAdapter(KalmanFilter):\n \n         super().update(measurement, R=innovation_cov)\n \n-        return self.X, self.P\n+        return self.x, self.P\n \n     def gating_distance(self, measurements):\n         """"""""""""Compute gating distance between state distribution and measurements.\n",add,Add note about data volume to enable_metrics_collection
477e2eb87dbd02b79133db06359f2ce7374bd361,Fixed some issues with kalman filter adapters,boxmot/utils/association.py,"import numpy as np\n\n\ndef iou_batch(bboxes1, bboxes2):\n    """"""""""""\n    From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n    """"""""""""\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n    yy2 = np.minimum(bboxes1[..., 3], bboxes2[..., 3])\n    w = np.maximum(0.0, xx2 - xx1)\n    h = np.maximum(0.0, yy2 - yy1)\n    wh = w * h\n    o = wh / (\n        (bboxes1[..., 2] - bboxes1[..., 0]) * (bboxes1[..., 3] - bboxes1[..., 1])\n        + (bboxes2[..., 2] - bboxes2[..., 0]) * (bboxes2[..., 3] - bboxes2[..., 1])\n        - wh\n    )\n    return o\n\n\ndef giou_batch(bboxes1, bboxes2):\n    """"""""""""\n    :param bbox_p: predict of bbox(N,4)(x1,y1,x2,y2)\n    :param bbox_g: groundtruth of bbox(N,4)(x1,y1,x2,y2)\n    :return:\n    """"""""""""\n    # for details should go to https:/","import numpy as np\n\n\ndef iou_batch(bboxes1, bboxes2):\n    """"""""""""\n    From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n    """"""""""""\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n    yy2 = np.minimum(bboxes1[..., 3], bboxes2[..., 3])\n    w = np.maximum(0.0, xx2 - xx1)\n    h = np.maximum(0.0, yy2 - yy1)\n    wh = w * h\n    o = wh / (\n        (bboxes1[..., 2] - bboxes1[..., 0]) * (bboxes1[..., 3] - bboxes1[..., 1])\n        + (bboxes2[..., 2] - bboxes2[..., 0]) * (bboxes2[..., 3] - bboxes2[..., 1])\n        - wh\n    )\n    return o\n\n\ndef giou_batch(bboxes1, bboxes2):\n    """"""""""""\n    :param bbox_p: predict of bbox(N,4)(x1,y1,x2,y2)\n    :param bbox_g: groundtruth of bbox(N,4)(x1,y1,x2,y2)\n    :return:\n    """"""""""""\n    # for details should go to https:/","@@ -285,7 +285,16 @@ def compute_aw_max_metric(emb_cost, w_association_emb, bottom=0.5):\n \n \n def associate(\n-    detections, trackers, iou_threshold, velocities, previous_obs, vdc_weight, emb_cost, w_assoc_emb, aw_off, aw_param\n+    detections,\n+    trackers,\n+    iou_threshold,\n+    velocities,\n+    previous_obs,\n+    vdc_weight,\n+    emb_cost=None,\n+    w_assoc_emb=None,\n+    aw_off=None,\n+    aw_param=None\n ):\n     if len(trackers) == 0:\n         return (\n",add,Added the UNSTARTED state to the YouTube PlayerState enum
d6195a541c55176d3a45ede80f468d20bfbd6292,Fixed paths in evolve.py,boxmot/configs/deepocsort.yaml,alpha_fixed_emb: 0.9217945107294758\nasso_func: iou\naw_off: true\naw_param: 0.431336052467765\ncmc_off: true\ndelta_t: 3\ndet_thresh: 0.1\nembedding_off: false\ninertia: 0.11282583500843651\niou_thresh: 0.21251708690170096\nmax_age: 40\nmin_hits: 4\nnew_kf_off: false\nw_association_emb: 0.5677451432405688\n,alpha_fixed_emb: 0.9217945107294758\nasso_func: iou\naw_off: true\naw_param: 0.431336052467765\ncmc_off: true\ndelta_t: 3\ndet_thresh: 0\nembedding_off: false\ninertia: 0.11282583500843651\niou_thresh: 0.21251708690170096\nmax_age: 40\nmin_hits: 4\nnew_kf_off: false\nw_association_emb: 0.5677451432405688\n,"@@ -4,7 +4,7 @@ aw_off: true\n aw_param: 0.431336052467765\n cmc_off: true\n delta_t: 3\n-det_thresh: 0.1\n+det_thresh: 0\n embedding_off: false\n inertia: 0.11282583500843651\n iou_thresh: 0.21251708690170096\n",add,Added STORM - 1270 to Changelog
d6195a541c55176d3a45ede80f468d20bfbd6292,Fixed paths in evolve.py,examples/evolve.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport argparse\nimport yaml\nfrom val import Evaluator\n\nfrom boxmot.utils import ROOT, WEIGHTS\n\nfrom boxmot.utils import logger\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\n\n\nclass Objective(Evaluator):\n    """"""""""""Objective function to evolve best set of hyperparams for\n    \n    This object is passed to an objective function and provides interfaces to overwrite\n    a tracker's config yaml file and ","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport argparse\nimport yaml\nfrom val import Evaluator\n\nfrom boxmot.utils import ROOT, WEIGHTS\n\nfrom boxmot.utils import logger\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\n\n\nclass Objective(Evaluator):\n    """"""""""""Objective function to evolve best set of hyperparams for\n    \n    This object is passed to an objective function and provides interfaces to overwrite\n    a tracker's config yaml file and ","@@ -279,7 +279,7 @@ def parse_opt():\n     parser.add_argument('--objectives', type=str, default='HOTA,MOTA,IDF1', help='set of objective metrics: HOTA,MOTA,IDF1')\n     \n     opt = parser.parse_args()\n-    opt.tracking_config = ROOT / 'boxmot' / opt.tracking_method / 'configs' / (opt.tracking_method + '.yaml')\n+    opt.tracking_config = ROOT / 'boxmot' / 'configs' / (opt.tracking_method + '.yaml')\n     opt.objectives = opt.objectives.split("""","""")\n \n     device = []\n",add,Add note about data volume to enable_metrics_collection
88ec480fdb0be58c85d988bef0573ccc54cd69a4,Fixed comments and reintroduced TestRequirements,README.md,"# Real-time multi-object, segmentation and pose tracking using Yolov8 | Yolo-NAS | YOLOX with DeepOCSORT and LightMBN\n\n\n<div align=""""center"""">\n  <p>\n  <img src=""""images/track_all_seg_1280_025conf.gif"""" width=""""400""""/>\n  </p>\n  <br>\n  <div>\n  <a href=""""https://github.com/mikel-brostrom/yolov8_tracking/actions/workflows/ci.yml""""><img src=""""https://github.com/mikel-brostrom/yolov8_tracking/actions/workflows/ci.yml/badge.svg"""" alt=""""CI CPU testing""""></a>\n  <a href=""""https://pepy.tech/project/boxmot""""><img src=""""https://static.pepy.tech/badge/boxmot""""></a>\n  <br>  \n  <a href=""""https://colab.research.google.com/drive/18nIqkBr68TkK8dHdarxTco6svHUJGggY?usp=sharing""""><img src=""""https://colab.research.google.com/assets/colab-badge.svg"""" alt=""""Open In Colab""""></a>\n<a href=""""https://doi.org/10.5281/zenodo.7629840""""><img src=""""https://zenodo.org/badge/DOI/10.5281/zenodo.7629840.svg"""" alt=""""DOI""""></a>\n  </div>\n</div>\n\n\n## Introduction\n\nThis repo contains a collections of state-of","# Real-time multi-object, segmentation and pose tracking using Yolov8 | Yolo-NAS | YOLOX with DeepOCSORT and LightMBN\n\n\n<div align=""""center"""">\n  <p>\n  <img src=""""assets/images/track_all_seg_1280_025conf.gif"""" width=""""400""""/>\n  </p>\n  <br>\n  <div>\n  <a href=""""https://github.com/mikel-brostrom/yolov8_tracking/actions/workflows/ci.yml""""><img src=""""https://github.com/mikel-brostrom/yolov8_tracking/actions/workflows/ci.yml/badge.svg"""" alt=""""CI CPU testing""""></a>\n  <a href=""""https://pepy.tech/project/boxmot""""><img src=""""https://static.pepy.tech/badge/boxmot""""></a>\n  <br>  \n  <a href=""""https://colab.research.google.com/drive/18nIqkBr68TkK8dHdarxTco6svHUJGggY?usp=sharing""""><img src=""""https://colab.research.google.com/assets/colab-badge.svg"""" alt=""""Open In Colab""""></a>\n<a href=""""https://doi.org/10.5281/zenodo.7629840""""><img src=""""https://zenodo.org/badge/DOI/10.5281/zenodo.7629840.svg"""" alt=""""DOI""""></a>\n  </div>\n</div>\n\n\n## Introduction\n\nThis repo contains a collections of s","@@ -3,7 +3,7 @@\n \n <div align=""""center"""">\n   <p>\n-  <img src=""""images/track_all_seg_1280_025conf.gif"""" width=""""400""""/>\n+  <img src=""""assets/images/track_all_seg_1280_025conf.gif"""" width=""""400""""/>\n   </p>\n   <br>\n   <div>\n",add,Add note about data volume to enable_metrics_collection
88ec480fdb0be58c85d988bef0573ccc54cd69a4,Fixed comments and reintroduced TestRequirements,assets/images/output_04.gif,,,,add,Added TypeSpec . xml
88ec480fdb0be58c85d988bef0573ccc54cd69a4,Fixed comments and reintroduced TestRequirements,assets/images/output_th025.gif,,,,add,Added TypeSpec . xml
88ec480fdb0be58c85d988bef0573ccc54cd69a4,Fixed comments and reintroduced TestRequirements,assets/images/track_all_1280_025conf.gif,,,,add,Added TypeSpec . xml
88ec480fdb0be58c85d988bef0573ccc54cd69a4,Fixed comments and reintroduced TestRequirements,assets/images/track_all_seg_1280_025conf.gif,,,,add,Added TypeSpec . xml
88ec480fdb0be58c85d988bef0573ccc54cd69a4,Fixed comments and reintroduced TestRequirements,assets/images/track_pedestrians_1280_05conf.gif,,,,add,Added TypeSpec . xml
88ec480fdb0be58c85d988bef0573ccc54cd69a4,Fixed comments and reintroduced TestRequirements,boxmot/appearance/reid_multibackend.py,"import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport torchvision.transforms as T\nfrom collections import OrderedDict, namedtuple\nimport gdown\nfrom os.path import exists as file_exists\n\n\nfrom ..utils import logger as LOGGER\nfrom .reid_model_factory import (\n    show_downloadable_models,\n    get_model_url,\n    get_model_name,\n    load_pretrained_weights\n)\nfrom .backbones import build_model\n\n\ndef check_suffix(file='osnet_x0_25_msmt17.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                try:\n                    assert s in suffix\n                except AssertionError as err:\n                    LOGGER.error(f""""{err}{f} acceptable suffix is {suffix}"""")\n\n\ncl","import torch.nn as nn\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport torchvision.transforms as T\nfrom collections import OrderedDict, namedtuple\nimport gdown\nfrom os.path import exists as file_exists\n\n\nfrom ..utils import logger as LOGGER\nfrom .reid_model_factory import (\n    show_downloadable_models,\n    get_model_url,\n    get_model_name,\n    load_pretrained_weights\n)\nfrom .backbones import build_model\n\nfrom ..utils.checks import TestRequirements\n__tr = TestRequirements()\n\n\ndef check_suffix(file='osnet_x0_25_msmt17.pt', suffix=('.pt',), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                try:\n                    assert s in suffix\n                except AssertionError as err:\n           ","@@ -17,6 +17,9 @@ from .reid_model_factory import (\n )\n from .backbones import build_model\n \n+from ..utils.checks import TestRequirements\n+__tr = TestRequirements()\n+\n \n def check_suffix(file='osnet_x0_25_msmt17.pt', suffix=('.pt',), msg=''):\n     # Check file(s) for acceptable suffix\n@@ -88,31 +91,14 @@ class ReIDDetectMultiBackend(nn.Module):\n         elif self.onnx:  # ONNX Runtime\n             LOGGER.info(f'Loading {w} for ONNX Runtime inference...')\n             cuda = torch.cuda.is_available() and device.type != 'cpu'\n-            try:\n-                import onnxruntime\n-            except ImportError:\n-                LOGGER.error(\n-                    f'Running {self.__class__} with the specified ONNX weights\n{w.name}\n'\n-                    'requires onnx pip packages to be installed!\n'\n-                    (\n-                        'For CPU:\n'\n-                        '$ pip install onnx>=1.12.0 onnxruntime\n'\n-                        'For GPU:\n'\n-                        '$ pip install onnx>=1.12.0 onnxruntime-gpu\n'\n-                    ) if cuda else '$ pip install onnx onnxruntime\n'\n-                )\n+            __tr.check_packages(['onnx', 'onnxruntime-gpu' if cuda else 'onnxruntime'])\n+            import onnxruntime\n             providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if cuda else ['CPUExecutionProvider']\n             self.session = onnxruntime.InferenceSession(str(w), providers=providers)\n         elif self.engine:  # TensorRT\n             LOGGER.info(f'Loading {w} for TensorRT inference...')\n-            try:\n-                import tensorrt as trt  # https://developer.nvidia.com/nvidia-tensorrt-download\n-            except ImportError:\n-                LOGGER.error(\n-                    f'Running {self.__class__} with the specified TensorRT weights\n{w.name}\n'\n-                    'requires tensorrt pip package to be installed!\n'\n-                    '$ pip install nvidia-tenso",add,Add note about data volume to enable
88ec480fdb0be58c85d988bef0573ccc54cd69a4,Fixed comments and reintroduced TestRequirements,boxmot/utils/checks.py,,"from pathlib import Path\nimport subprocess\nimport pkg_resources\n\nfrom boxmot.utils import REQUIREMENTS\nfrom boxmot.utils import logger\n\n\nclass TestRequirements():\n    \n    def check_requirements(self):\n        requirements = pkg_resources.parse_requirements(REQUIREMENTS.open())\n        self.check_packages(requirements)\n        \n    def check_packages(self, requirements, cmds=''):\n        """"""""""""Test that each required package is available.""""""""""""\n        # Ref: https://stackoverflow.com/a/45474387/\n        \n        s = '' # missing packages\n        for r in requirements:\n            r = str(r)\n            try:\n                pkg_resources.require(r)\n            except Exception as e:\n                s += f'""""{r}"""" '\n        if s:\n            logger.warning(f'\nMissing packages: {s}\nAtempting installation...')\n            try:\n                subprocess.check_output(f'pip install --no-cache {s} {cmds}', shell=True, stderr=subprocess.STDOUT)\n            excep","@@ -0,0 +1,33 @@\n+from pathlib import Path\n+import subprocess\n+import pkg_resources\n+\n+from boxmot.utils import REQUIREMENTS\n+from boxmot.utils import logger\n+\n+\n+class TestRequirements():\n+    \n+    def check_requirements(self):\n+        requirements = pkg_resources.parse_requirements(REQUIREMENTS.open())\n+        self.check_packages(requirements)\n+        \n+    def check_packages(self, requirements, cmds=''):\n+        """"""""""""Test that each required package is available.""""""""""""\n+        # Ref: https://stackoverflow.com/a/45474387/\n+        \n+        s = '' # missing packages\n+        for r in requirements:\n+            r = str(r)\n+            try:\n+                pkg_resources.require(r)\n+            except Exception as e:\n+                s += f'""""{r}"""" '\n+        if s:\n+            logger.warning(f'\nMissing packages: {s}\nAtempting installation...')\n+            try:\n+                subprocess.check_output(f'pip install --no-cache {s} {cmds}', shell=True, stderr=subprocess.STDOUT)\n+            except Exception as e:\n+                logger.error(e)\n+                exit()\n+            logger.success('All the missing packages were installed successfully')\n\ No newline at end of file\n",add,Added note about dates .
88ec480fdb0be58c85d988bef0573ccc54cd69a4,Fixed comments and reintroduced TestRequirements,examples/multi_yolo_backend.py,"from pathlib import Path\nimport numpy as np\nimport torch\n\nfrom ultralytics.yolo.engine.results import Boxes, Results\nfrom boxmot.utils import logger as LOGGER\n\n\nclass MultiYolo():\n    def __init__(self, model, device, args):\n        self.args = args\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = 'yolov8'\n            self.model = model\n        else:\n            self.model_name = str(model.stem).lower()\n\n        if 'yolo_nas' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n            self.model_type = 'yolo_nas'\n            self.model = models.get(\n                self.model_name,\n                pretrained_weights=""""coco""""\n            ).to(self.device)\n        elif 'yolox' in self.model_name:\n            self.try_sg_import()\n            from super_gradi","from pathlib import Path\nimport numpy as np\nimport torch\n\nfrom ultralytics.yolo.engine.results import Boxes, Results\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.utils.checks import TestRequirements\n\n__tr = TestRequirements()\n\n\nclass MultiYolo():\n    def __init__(self, model, device, args):\n        self.args = args\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = 'yolov8'\n            self.model = model\n        else:\n            self.model_name = str(model.stem).lower()\n\n        if 'yolo_nas' in self.model_name:\n            self.try_sg_import()\n            from super_gradients.common.object_names import Models\n            from super_gradients.training import models\n            self.model_type = 'yolo_nas'\n            self.model = models.get(\n                self.model_name,\n                pretrained_weights=""""coco""""\n            ).to(self.device)\n        elif 'yolox' in","@@ -5,6 +5,10 @@ import torch\n from ultralytics.yolo.engine.results import Boxes, Results\n from boxmot.utils import logger as LOGGER\n \n+from boxmot.utils.checks import TestRequirements\n+\n+__tr = TestRequirements()\n+\n \n class MultiYolo():\n     def __init__(self, model, device, args):\n@@ -41,11 +45,8 @@ class MultiYolo():\n     def try_sg_import(self):\n         try:\n             import super_gradients  # for linear_assignment\n-        except ImportError:\n-            LOGGER.error(\n-                f'Running {self._class_} requires the following packages to be installed:\n'\n-                '$ pip install super-gradients==3.1.1\n'\n-            )\n+        except (ImportError, AssertionError, AttributeError):\n+            __tr.check_packages(('super-gradients==3.1.1',))  # install\n \n     def __call__(self, im, im0s):\n         if 'yolo_nas' in self.model_name or 'yolox' in self.model_name:\n",add,Fix Snackbar theme on sw600dp
88ec480fdb0be58c85d988bef0573ccc54cd69a4,Fixed comments and reintroduced TestRequirements,examples/reid_export.py,"import argparse\nimport os\nfrom pathlib import Path\nimport torch\nimport time\nimport subprocess\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import get_model_name, load_pretrained_weights\nfrom boxmot.utils import WEIGHTS, logger\nfrom boxmot.utils.torch_utils import select_device\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1E6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob('**/*') if f.is_file()) / 1E6\n    else:\n        return 0.0\n\n\ndef export_torchscript(model, im, file, optimize):\n    try:\n        logger.info(f'\nStarting export with torch {torch.__version__}...')\n        f = file.with_suffix('.torchscript')\n        print(f)\n        ts = torch.jit.trace(model, im, strict=False)\n ","import argparse\nimport os\nfrom pathlib import Path\nimport torch\nimport time\nimport subprocess\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import get_model_name, load_pretrained_weights\nfrom boxmot.utils import WEIGHTS, logger\nfrom boxmot.utils.torch_utils import select_device\n\nfrom boxmot.utils.checks import TestRequirements\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1E6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob('**/*') if f.is_file()) / 1E6\n    else:\n        return 0.0\n\n\ndef export_torchscript(model, im, file, optimize):\n    try:\n        logger.info(f'\nStarting export with torch {torch.__version__}...')\n        f = file.with_suffix('.torchscrip","@@ -12,6 +12,9 @@ from boxmot.appearance.reid_model_factory import get_model_name, load_pretrained\n from boxmot.utils import WEIGHTS, logger\n from boxmot.utils.torch_utils import select_device\n \n+from boxmot.utils.checks import TestRequirements\n+__tr = TestRequirements()\n+\n \n def file_size(path):\n     # Return file/dir size (MB)\n@@ -44,13 +47,8 @@ def export_torchscript(model, im, file, optimize):\n def export_onnx(model, im, file, opset, dynamic, fp16, simplify):\n     # ONNX export\n     try:\n-        try:\n-            import onnx\n-        except ImportError:\n-            logger.error(\n-                'Running export_onnx(...) requires the following packages to be installed:\n'\n-                '$ pip install onnx>=1.12.0\n'\n-            )\n+        __tr.check_packages(('onnx',))\n+        import onnx\n \n         f = file.with_suffix('.onnx')\n         logger.info(f'\nStarting export with onnx {onnx.__version__}...')\n@@ -79,19 +77,8 @@ def export_onnx(model, im, file, opset, dynamic, fp16, simplify):\n         if simplify:\n             try:\n                 cuda = torch.cuda.is_available()\n-                try:\n-                    import onnxsim\n-                except ImportError:\n-                    logger.error(\n-                        'Running export_onnx(..., simplify=True) requires the following'\n-                        ' packages to be installed:\n'\n-                        (\n-                            'For CPU:\n'\n-                            '$ pip install onnxruntime onnx-simplifier>=0.4.1\n'\n-                            'For GPU:\n'\n-                            '$ pip install onnxruntime-gpu onnx-simplifier>=0.4.1\n'\n-                        ) if cuda else '$ pip install onnx onnxruntime onnx-simplifier>=0.4.1\n'\n-                    )\n+                __tr.check_packages(('onnxruntime-gpu' if cuda else 'onnxruntime', 'onnx-simplifier>=0.4.1'))\n+                import onnxsim\n \n                 logger.info(f'",add,Don ' t define the right services twice
88ec480fdb0be58c85d988bef0573ccc54cd69a4,Fixed comments and reintroduced TestRequirements,examples/track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport cv2\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.torch_utils import select_device\n\ntry:\n    from ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\n    from ultralytics.yolo.utils import colorstr, ops, IterableSimpleNamespace\n    from ultralytics.yolo.utils.checks import check_imgsz\n    from ultralytics.yolo.utils.files import increment_path\n    from ultralytics.yolo.data.utils import VID_FORMATS\n    from ultralytics.yolo.utils.plotting import save_one_box\nexcept ImportError:\n    LOGGER.error(\n        'Running track.py requires the following packages to be installed:\n'\n        '$ pip install ultralytics==8.0.124\n'\n    )\n\n\nfrom multi_yolo_backend import MultiYolo\nfrom utils import write_MOT_results\n\nfrom boxmot.ut","# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nfrom pathlib import Path\nimport torch\nimport argparse\nimport cv2\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.torch_utils import select_device\n\nfrom boxmot.utils.checks import TestRequirements\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics==8.0.124',))  # install\n\nfrom ultralytics.yolo.engine.model import YOLO, TASK_MAP\n\nfrom ultralytics.yolo.utils import colorstr, ops, IterableSimpleNamespace\nfrom ultralytics.yolo.utils.checks import check_imgsz\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.data.utils import VID_FORMATS\nfrom ultralytics.yolo.utils.plotting import save_one_box\n\nfrom multi_yolo_backend import MultiYolo\nfrom utils import write_MOT_results\n\nfrom boxmot.utils import EXAMPLES\n\n\ndef on_predict_start(predictor):\n    predi","@@ -10,20 +10,17 @@ from boxmot.utils import ROOT, WEIGHTS\n from boxmot.utils import logger as LOGGER\n from boxmot.utils.torch_utils import select_device\n \n-try:\n-    from ultralytics.yolo.engine.model import YOLO, TASK_MAP\n-\n-    from ultralytics.yolo.utils import colorstr, ops, IterableSimpleNamespace\n-    from ultralytics.yolo.utils.checks import check_imgsz\n-    from ultralytics.yolo.utils.files import increment_path\n-    from ultralytics.yolo.data.utils import VID_FORMATS\n-    from ultralytics.yolo.utils.plotting import save_one_box\n-except ImportError:\n-    LOGGER.error(\n-        'Running track.py requires the following packages to be installed:\n'\n-        '$ pip install ultralytics==8.0.124\n'\n-    )\n+from boxmot.utils.checks import TestRequirements\n+__tr = TestRequirements()\n+__tr.check_packages(('ultralytics==8.0.124',))  # install\n+\n+from ultralytics.yolo.engine.model import YOLO, TASK_MAP\n \n+from ultralytics.yolo.utils import colorstr, ops, IterableSimpleNamespace\n+from ultralytics.yolo.utils.checks import check_imgsz\n+from ultralytics.yolo.utils.files import increment_path\n+from ultralytics.yolo.data.utils import VID_FORMATS\n+from ultralytics.yolo.utils.plotting import save_one_box\n \n from multi_yolo_backend import MultiYolo\n from utils import write_MOT_results\n",add,added metedp size
431b665e1e647f551deb7425528ca72d56bf0921,fix class filtering for yolox,examples/multi_yolo_backend.py,"from pathlib import Path\nimport numpy as np\nimport torch\nimport gdown\n\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.ops import xywh2xyxy\n\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\ntr = TestRequirements()\n\nYOLOX_ZOO = {\n    'yolox_n': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass MultiYolo():\n    def __init__(self, model, device, args):\n        self.args = args\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = '","from pathlib import Path\nimport numpy as np\nimport torch\nimport gdown\n\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.ops import xywh2xyxy\n\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\ntr = TestRequirements()\n\nYOLOX_ZOO = {\n    'yolox_n': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass MultiYolo():\n    def __init__(self, model, device, args):\n        self.args = args\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = '","@@ -127,10 +127,9 @@ class MultiYolo():\n             preds[:, [1, 3]] = preds[:, [1, 3]] * h_r\n \n             preds = torch.clip(preds, min=0)\n-            preds.detach().cpu().numpy()\n \n             if self.args.classes:  # Filter boxes by classes\n-                preds = preds[np.isin(preds[:, 5], self.args.classes)]\n+                preds = preds[torch.isin(preds[:, 5].cpu(), torch.as_tensor(self.args.classes))]\n \n         elif 'yolov8' in self.model_name:\n             preds = self.model(\n",add,Added Type name for DFI ( # 51 )
9b0409507e5ffb00d1bafc06e9fd9199861250d2,fix no yolox detections,examples/multi_yolo_backend.py,"from pathlib import Path\nimport numpy as np\nimport torch\nimport gdown\n\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.ops import xywh2xyxy\n\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\ntr = TestRequirements()\n\nYOLOX_ZOO = {\n    'yolox_n': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass MultiYolo():\n    def __init__(self, model, device, args):\n        self.args = args\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = '","from pathlib import Path\nimport numpy as np\nimport torch\nimport gdown\n\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.ops import xywh2xyxy\n\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\ntr = TestRequirements()\n\nYOLOX_ZOO = {\n    'yolox_n': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass MultiYolo():\n    def __init__(self, model, device, args):\n        self.args = args\n        self.device = device\n        if not (isinstance(model, str) or isinstance(model, Path)):\n            self.model_name = '","@@ -110,26 +110,29 @@ class MultiYolo():\n                 nms_thre=0.45, class_agnostic=True\n             )[0]\n \n-            # (x, y, x, y, conf, obj, cls) --> (x, y, x, y, conf, cls)\n-            preds[:, 4] = preds[:, 4] * preds[:, 5]\n-            preds = preds[:, [0, 1, 2, 3, 4, 6]]\n-\n-            # calculate factor for predictions\n-            im0_w = im0s[0].shape[1]\n-            im0_h = im0s[0].shape[0]\n-            im_w = im[0].shape[2]\n-            im_h = im[0].shape[1]\n-            w_r = im0_w / im_w\n-            h_r = im0_h / im_h\n-\n-            # scale to original image\n-            preds[:, [0, 2]] = preds[:, [0, 2]] * w_r\n-            preds[:, [1, 3]] = preds[:, [1, 3]] * h_r\n+            if preds is None:\n+                return torch.empty(0, 6)\n+            else:\n+                # (x, y, x, y, conf, obj, cls) --> (x, y, x, y, conf, cls)\n+                preds[:, 4] = preds[:, 4] * preds[:, 5]\n+                preds = preds[:, [0, 1, 2, 3, 4, 6]]\n \n-            preds = torch.clip(preds, min=0)\n+                # calculate factor for predictions\n+                im0_w = im0s[0].shape[1]\n+                im0_h = im0s[0].shape[0]\n+                im_w = im[0].shape[2]\n+                im_h = im[0].shape[1]\n+                w_r = im0_w / im_w\n+                h_r = im0_h / im_h\n \n-            if self.args.classes:  # Filter boxes by classes\n-                preds = preds[torch.isin(preds[:, 5].cpu(), torch.as_tensor(self.args.classes))]\n+                # scale to original image\n+                preds[:, [0, 2]] = preds[:, [0, 2]] * w_r\n+                preds[:, [1, 3]] = preds[:, [1, 3]] * h_r\n+\n+                preds = torch.clip(preds, min=0)\n+\n+                if self.args.classes:  # Filter boxes by classes\n+                    preds = preds[torch.isin(preds[:, 5].cpu(), torch.as_tensor(self.args.classes))]\n \n         elif 'yolov8' in self.model_name:\n             preds = self.model(\n@@ -183,4 +186,4 ",add,Fix version of gpg plugin .
20a79599e6b6e2744c8b6192d689107d75c899be,delete debug print,boxmot/utils/__init__.py,"import sys\nimport numpy as np\nfrom pathlib import Path\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / 'boxmot'\nEXAMPLES = ROOT / 'examples'\nWEIGHTS = ROOT / 'examples' / 'weights'\nREQUIREMENTS = ROOT / 'requirements.txt'\n\n# global logger\nfrom loguru import logger\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        self.update = method\n        print(self.update)\n    def __get__(self, instance, owner):\n        def wrapper(*args, **kwargs):\n            modified_args = list(args)\n            dets = modified_args[0]\n            im = modified_args[1]\n\n            # input one class of detections at a time in order to not mix them up\n            if instance.per_class is True and dets.size != 0:\n                dets_dict = {class_id: np.array([det for det in dets if det[5] == class_id]) for class_id in set(det[5] for det in dets)}\n        ","import sys\nimport numpy as np\nfrom pathlib import Path\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / 'boxmot'\nEXAMPLES = ROOT / 'examples'\nWEIGHTS = ROOT / 'examples' / 'weights'\nREQUIREMENTS = ROOT / 'requirements.txt'\n\n# global logger\nfrom loguru import logger\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        self.update = method\n    def __get__(self, instance, owner):\n        def wrapper(*args, **kwargs):\n            modified_args = list(args)\n            dets = modified_args[0]\n            im = modified_args[1]\n\n            # input one class of detections at a time in order to not mix them up\n            if instance.per_class is True and dets.size != 0:\n                dets_dict = {class_id: np.array([det for det in dets if det[5] == class_id]) for class_id in set(det[5] for det in dets)}\n                # get unique classes","@@ -18,7 +18,6 @@ logger.add(sys.stderr, colorize=True, level=""""INFO"""")\n class PerClassDecorator:\n     def __init__(self, method):\n         self.update = method\n-        print(self.update)\n     def __get__(self, instance, owner):\n         def wrapper(*args, **kwargs):\n             modified_args = list(args)\n",add,Add forced default for HADOOP - 794
8f1a080d3d3f6d157f08c409a88be0187867a7cb,Fixed missing import of from PerClassDecorator,boxmot/trackers/deepocsort/deep_ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport torch\nimport numpy as np\nfrom ...motion.adapters import OCSortKalmanFilterAdapter\nfrom ...utils.association import *\nfrom ...utils.cmc import CameraMotionCompensation\nfrom ...appearance.reid_multibackend import ReIDDetectMultiBackend\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n   ","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport torch\nimport numpy as np\nfrom ...motion.adapters import OCSortKalmanFilterAdapter\nfrom ...utils.association import *\nfrom ...utils.cmc import CameraMotionCompensation\nfrom ...appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.utils import PerClassDecorator\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[","@@ -8,6 +8,7 @@ from ...motion.adapters import OCSortKalmanFilterAdapter\n from ...utils.association import *\n from ...utils.cmc import CameraMotionCompensation\n from ...appearance.reid_multibackend import ReIDDetectMultiBackend\n+from boxmot.utils import PerClassDecorator\n \n \n def k_previous_obs(observations, cur_age, k):\n",add,Added string type to outerHTML .
779ec8476f8aa06581ea25832d30b436522be398,Fixed strongsort test to include gating metric in coverage,boxmot/motion/adapters/strongsort_kf_adapter.py,"import numpy as np\nfrom ..kalman_filter import KalmanFilter\n\n\nclass StrongSortKalmanFilterAdapter(KalmanFilter):\n    ndim = 4\n\n    def __init__(self, dt=1):\n        super().__init__(dim_x=2 * self.ndim, dim_z=self.ndim)\n\n        # Set transition matrix\n        for i in range(self.ndim):\n            self.F[i, self.ndim + i] = dt\n\n        # Set observation matrix\n        self.H = np.eye(self.ndim, 2 * self.ndim)\n\n        # Motion and observation uncertainty are chosen relative to the current\n        # state estimate. These weights control the amount of uncertainty in\n        # the model. This is a bit hacky.\n        self._std_weight_position = 1.0 / 20\n        self._std_weight_velocity = 1.0 / 160\n\n    def initiate(self, measurement):\n        """"""""""""Create track from unassociated measurement.\n\n        Parameters\n        ----------\n        measurement : ndarray\n            Bounding box coordinates (x, y, w, h) with center position (x, y),\n            width w, ","import numpy as np\nfrom filterpy.common import reshape_z\nfrom ..kalman_filter import KalmanFilter\n\n\nclass StrongSortKalmanFilterAdapter(KalmanFilter):\n    ndim = 4\n\n    def __init__(self, dt=1):\n        super().__init__(dim_x=2 * self.ndim, dim_z=self.ndim)\n\n        # Set transition matrix\n        for i in range(self.ndim):\n            self.F[i, self.ndim + i] = dt\n\n        # Set observation matrix\n        self.H = np.eye(self.ndim, 2 * self.ndim)\n\n        # Motion and observation uncertainty are chosen relative to the current\n        # state estimate. These weights control the amount of uncertainty in\n        # the model. This is a bit hacky.\n        self._std_weight_position = 1.0 / 20\n        self._std_weight_velocity = 1.0 / 160\n\n    def initiate(self, measurement):\n        """"""""""""Create track from unassociated measurement.\n\n        Parameters\n        ----------\n        measurement : ndarray\n            Bounding box coordinates (x, y, w, h) with center ","@@ -1,4 +1,5 @@\n import numpy as np\n+from filterpy.common import reshape_z\n from ..kalman_filter import KalmanFilter\n \n \n@@ -132,7 +133,7 @@ class StrongSortKalmanFilterAdapter(KalmanFilter):\n \n         return self.x, self.P\n \n-    def gating_distance(self, measurements):\n+    def gating_distance(self, measurements, only_position=False):\n         """"""""""""Compute gating distance between state distribution and measurements.\n         A suitable distance threshold can be obtained from `chi2inv95`. If\n         `only_position` is False, the chi-square distribution has 4 degrees of\n@@ -143,6 +144,9 @@ class StrongSortKalmanFilterAdapter(KalmanFilter):\n             An Nx4 dimensional matrix of N measurements, each in\n             format (x, y, a, h) where (x, y) is the bounding box center\n             position, a the aspect ratio, and h the height.\n+        only_position : bool\n+            Whether to use only the positional attributes of the track for\n+            calculating the gating distance\n         Returns\n         -------\n         ndarray\n@@ -153,5 +157,11 @@ class StrongSortKalmanFilterAdapter(KalmanFilter):\n \n         squared_maha = np.zeros((measurements.shape[0],))\n         for i, measurement in enumerate(measurements):\n-            squared_maha[i] = super().md_for_measurement(measurement)\n+            if not only_position:\n+                squared_maha[i] = super().md_for_measurement(measurement)\n+            else:\n+                z = reshape_z(z, self.dim_z, 2)\n+                H = self.H[:2,:2]\n+                y = z - np.dot(H, self.x[:2])\n+                squared_maha[i] = np.sqrt(float(np.dot(np.dot(y.T, self.SI[:2,:2]), y)))\n         return squared_maha\n",add,Add note about data volume to enable_metrics_collection
779ec8476f8aa06581ea25832d30b436522be398,Fixed strongsort test to include gating metric in coverage,boxmot/trackers/strongsort/sort/linear_assignment.py,"# vim: expandtab:ts=4:sw=4\nfrom __future__ import absolute_import\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nfrom ....utils.matching import chi2inv95\n\n\nINFTY_COST = 1e5\n\n\ndef min_cost_matching(\n    distance_metric,\n    max_distance,\n    tracks,\n    detections,\n    track_indices=None,\n    detection_indices=None,\n):\n    """"""""""""Solve linear assignment problem.\n    Parameters\n    ----------\n    distance_metric : Callable[List[Track], List[Detection], List[int], List[int]) -> ndarray\n        The distance metric is given a list of tracks and detections as well as\n        a list of N track indices and M detection indices. The metric should\n        return the NxM dimensional cost matrix, where element (i, j) is the\n        association cost between the i-th track in the given track indices and\n        the j-th detection in the given detection_indices.\n    max_distance : float\n        Gating threshold. Associations with cost larger than this va","# vim: expandtab:ts=4:sw=4\nfrom __future__ import absolute_import\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nfrom ....utils.matching import chi2inv95\n\n\nINFTY_COST = 1e5\n\n\ndef min_cost_matching(\n    distance_metric,\n    max_distance,\n    tracks,\n    detections,\n    track_indices=None,\n    detection_indices=None,\n):\n    """"""""""""Solve linear assignment problem.\n    Parameters\n    ----------\n    distance_metric : Callable[List[Track], List[Detection], List[int], List[int]) -> ndarray\n        The distance metric is given a list of tracks and detections as well as\n        a list of N track indices and M detection indices. The metric should\n        return the NxM dimensional cost matrix, where element (i, j) is the\n        association cost between the i-th track in the given track indices and\n        the j-th detection in the given detection_indices.\n    max_distance : float\n        Gating threshold. Associations with cost larger than this va","@@ -189,9 +189,7 @@ def gate_cost_matrix(\n     measurements = np.asarray([detections[i].to_xyah() for i in detection_indices])\n     for row, track_idx in enumerate(track_indices):\n         track = tracks[track_idx]\n-        gating_distance = track.kf.gating_distance(\n-            track.mean, track.covariance, measurements, only_position\n-        )\n+        gating_distance = track.kf.gating_distance(measurements, only_position)\n         cost_matrix[row, gating_distance > gating_threshold] = gated_cost\n         cost_matrix[row] = (\n             mc_lambda * cost_matrix[row] + (1 - mc_lambda) * gating_distance\n",add,Add note about data volume to enable_metrics_collection
779ec8476f8aa06581ea25832d30b436522be398,Fixed strongsort test to include gating metric in coverage,tests/test_python.py,"# pytest tests/test_python.py\n\nimport numpy as np\nfrom pathlib import Path\nfrom numpy.testing import assert_allclose\n\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.trackers import StrongSORT\nfrom boxmot.trackers import OCSort as OCSORT\nfrom boxmot.trackers import BYTETracker\nfrom boxmot.trackers import BoTSORT\nfrom boxmot.trackers import DeepOCSort as DeepOCSORT\nfrom boxmot.tracker_zoo import create_tracker, get_tracker_config\n\n\ndef test_strongsort_instantiation():\n    ss = StrongSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\ndef test_botsort_instantiation():\n    bs = BoTSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n    \n\ndef test_deepocsort_instantiation():\n    dos = DeepOCSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n        per_class=False\n    )\","# pytest tests/test_python.py\n\nimport numpy as np\nfrom pathlib import Path\nfrom numpy.testing import assert_allclose\n\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.trackers import StrongSORT\nfrom boxmot.trackers import OCSort as OCSORT\nfrom boxmot.trackers import BYTETracker\nfrom boxmot.trackers import BoTSORT\nfrom boxmot.trackers import DeepOCSort as DeepOCSORT\nfrom boxmot.tracker_zoo import create_tracker, get_tracker_config\n\n\ndef test_strongsort_instantiation():\n    ss = StrongSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\ndef test_botsort_instantiation():\n    bs = BoTSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n    \n\ndef test_deepocsort_instantiation():\n    dos = DeepOCSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n        per_class=False\n    )\","@@ -87,6 +87,8 @@ def test_deepocsort_output():\n     assert output.size == 0\n     output = tracker.update(det, rgb)\n     assert output.shape == (2, 7)  # two inputs should give two outputs\n+    output = tracker.update(det, rgb)\n+    assert output.shape == (2, 7)  # two inputs should give two outputs\n     output = np.flip(np.delete(output, 4, axis=1), axis=0)\n     assert_allclose(det, output, atol=1, rtol=7e-3, verbose=True)\n \n@@ -130,6 +132,8 @@ def test_ocsort_output():\n     assert output.size == 0\n     output = tracker.update(det, rgb)\n     assert output.shape == (2, 7)  # two inputs should give two outputs\n+    output = tracker.update(det, rgb)\n+    assert output.shape == (2, 7)  # two inputs should give two outputs\n     output = np.flip(np.delete(output, 4, axis=1), axis=0)\n     assert_allclose(det, output, atol=1, rtol=7e-3, verbose=True)\n \n@@ -151,6 +155,8 @@ def test_botsort_output():\n     assert output.shape == (2, 7)  # two inputs should give two outputs\n     output = tracker.update(det, rgb)\n     assert output.shape == (2, 7)  # two inputs should give two outputs\n+    output = tracker.update(det, rgb)\n+    assert output.shape == (2, 7)  # two inputs should give two outputs\n     output = np.delete(output, 4, axis=1)\n     assert_allclose(det, output, atol=1, rtol=7e-3, verbose=True)\n \n@@ -172,6 +178,8 @@ def test_bytetrack_output():\n     assert output.shape == (2, 7)  # two inputs should give two outputs\n     output = tracker.update(det, rgb)\n     assert output.shape == (2, 7)  # two inputs should give two outputs\n+    output = tracker.update(det, rgb)\n+    assert output.shape == (2, 7)  # two inputs should give two outputs\n     output = np.delete(output, 4, axis=1)\n     assert_allclose(det, output, atol=1, rtol=7e-3, verbose=True)\n \n@@ -196,5 +204,7 @@ def test_strongsort_output():\n     assert output.size == 0\n     output = tracker.update(det, rgb)\n     assert output.shape == (2, 7)  # two inputs should give two output",add,Added STORM - 370 to Changelog
579ad98cf7fd0aeeee682cd36cc93f92fe790255,fix imports,tests/test_python.py,"# pytest tests/test_python.py\n\nfrom pathlib import Path\n\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom boxmot.tracker_zoo import create_tracker, get_tracker_config\nfrom boxmot.trackers import BoTSORT, BYTETracker\nfrom boxmot.trackers import DeepOCSort as DeepOCSORT\nfrom boxmot.trackers import OCSort as OCSORT\nfrom boxmot.trackers import StrongSORT\nfrom boxmot.utils import WEIGHTS\n\n\ndef test_strongsort_instantiation():\n    StrongSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\ndef test_botsort_instantiation():\n    BoTSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\ndef test_deepocsort_instantiation():\n    DeepOCSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n        per_class=False\n    )\n\n\ndef test_ocsort_instantiation():\n    OCS","# pytest tests/test_python.py\n\nfrom pathlib import Path\n\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom boxmot import BoTSORT, BYTETracker\nfrom boxmot import DeepOCSort as DeepOCSORT\nfrom boxmot import OCSort as OCSORT\nfrom boxmot import StrongSORT, create_tracker, get_tracker_config\nfrom boxmot.utils import WEIGHTS\n\n\ndef test_strongsort_instantiation():\n    StrongSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\ndef test_botsort_instantiation():\n    BoTSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\ndef test_deepocsort_instantiation():\n    DeepOCSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n        per_class=False\n    )\n\n\ndef test_ocsort_instantiation():\n    OCSORT(\n        per_class=False\n    )\n\n\ndef test_bytetrack_instan","@@ -5,11 +5,10 @@ from pathlib import Path\n import numpy as np\n from numpy.testing import assert_allclose\n \n-from boxmot.tracker_zoo import create_tracker, get_tracker_config\n-from boxmot.trackers import BoTSORT, BYTETracker\n-from boxmot.trackers import DeepOCSort as DeepOCSORT\n-from boxmot.trackers import OCSort as OCSORT\n-from boxmot.trackers import StrongSORT\n+from boxmot import BoTSORT, BYTETracker\n+from boxmot import DeepOCSort as DeepOCSORT\n+from boxmot import OCSort as OCSORT\n+from boxmot import StrongSORT, create_tracker, get_tracker_config\n from boxmot.utils import WEIGHTS\n \n \n",add,Added note about dates .
a41dbaa753d113cb9fbf6bf95d9e0c04b67d8a81,fix imports,tests/test_python.py,"# pytest tests/test_python.py\n\nfrom pathlib import Path\n\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom boxmot import BoTSORT, BYTETracker\nfrom boxmot import DeepOCSort as DeepOCSORT\nfrom boxmot import OCSort as OCSORT\nfrom boxmot import StrongSORT, create_tracker, get_tracker_config\nfrom boxmot.utils import WEIGHTS\n\n\ndef test_strongsort_instantiation():\n    StrongSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\ndef test_botsort_instantiation():\n    BoTSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\ndef test_deepocsort_instantiation():\n    DeepOCSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n        per_class=False\n    )\n\n\ndef test_ocsort_instantiation():\n    OCSORT(\n        per_class=False\n    )\n\n\ndef test_bytetrack_instan","# pytest tests/test_python.py\n\nfrom pathlib import Path\n\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom boxmot import (OCSORT, BoTSORT, BYTETracker, DeepOCSORT, StrongSORT,\n                    create_tracker, get_tracker_config)\nfrom boxmot.utils import WEIGHTS\n\n\ndef test_strongsort_instantiation():\n    StrongSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\ndef test_botsort_instantiation():\n    BoTSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\ndef test_deepocsort_instantiation():\n    DeepOCSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n        per_class=False\n    )\n\n\ndef test_ocsort_instantiation():\n    OCSORT(\n        per_class=False\n    )\n\n\ndef test_bytetrack_instantiation():\n    BYTETracker()\n\n\ndef test_deepocsort_out","@@ -5,10 +5,8 @@ from pathlib import Path\n import numpy as np\n from numpy.testing import assert_allclose\n \n-from boxmot import BoTSORT, BYTETracker\n-from boxmot import DeepOCSort as DeepOCSORT\n-from boxmot import OCSort as OCSORT\n-from boxmot import StrongSORT, create_tracker, get_tracker_config\n+from boxmot import (OCSORT, BoTSORT, BYTETracker, DeepOCSORT, StrongSORT,\n+                    create_tracker, get_tracker_config)\n from boxmot.utils import WEIGHTS\n \n \n",add,Added note about dates .
87bae3b4fcfb9402739ed029637cfc1febbdaa1a,fix pip install bug,boxmot/utils/checks.py,"import subprocess\n\nimport pkg_resources\n\nfrom boxmot.utils import REQUIREMENTS, logger\n\n\nclass TestRequirements:\n    def check_requirements(self):\n        requirements = pkg_resources.parse_requirements(REQUIREMENTS.open())\n        self.check_packages(requirements)\n\n    def check_packages(self, requirements, cmds=""""""""):\n        """"""""""""Test that each required package is available.""""""""""""\n        # Ref: https://stackoverflow.com/a/45474387/\n\n        s = """"""""  # missing packages\n        for r in requirements:\n            r = str(r)\n            try:\n                pkg_resources.require(r)\n            except Exception as e:\n                s += f'""""{r}"""" {e}'\n        if s:\n            logger.warning(f""""\nMissing packages: {s}\nAtempting installation..."""")\n            try:\n                subprocess.check_output(\n                    f""""pip install --no-cache {s} {cmds}"""",\n                    shell=True,\n                    stderr=subprocess.STDOUT,\n              ","import subprocess\n\nimport pkg_resources\n\nfrom boxmot.utils import REQUIREMENTS, logger\n\n\nclass TestRequirements():\n\n    def check_requirements(self):\n        requirements = pkg_resources.parse_requirements(REQUIREMENTS.open())\n        self.check_packages(requirements)\n\n    def check_packages(self, requirements, cmds=''):\n        """"""""""""Test that each required package is available.""""""""""""\n        # Ref: https://stackoverflow.com/a/45474387/\n\n        s = ''  # missing packages\n        for r in requirements:\n            r = str(r)\n            try:\n                pkg_resources.require(r)\n            except Exception as e:\n                logger.error(f'{e}')\n                s += f'""""{r}"""" '\n        if s:\n            logger.warning(f'\nMissing packages: {s}\nAtempting installation...')\n            try:\n                subprocess.check_output(f'pip install --no-cache {s} {cmds}', shell=True, stderr=subprocess.STDOUT)\n            except Exception as e:\n           ","@@ -5,31 +5,29 @@ import pkg_resources\n from boxmot.utils import REQUIREMENTS, logger\n \n \n-class TestRequirements:\n+class TestRequirements():\n+\n     def check_requirements(self):\n         requirements = pkg_resources.parse_requirements(REQUIREMENTS.open())\n         self.check_packages(requirements)\n \n-    def check_packages(self, requirements, cmds=""""""""):\n+    def check_packages(self, requirements, cmds=''):\n         """"""""""""Test that each required package is available.""""""""""""\n         # Ref: https://stackoverflow.com/a/45474387/\n \n-        s = """"""""  # missing packages\n+        s = ''  # missing packages\n         for r in requirements:\n             r = str(r)\n             try:\n                 pkg_resources.require(r)\n             except Exception as e:\n-                s += f'""""{r}"""" {e}'\n+                logger.error(f'{e}')\n+                s += f'""""{r}"""" '\n         if s:\n-            logger.warning(f""""\nMissing packages: {s}\nAtempting installation..."""")\n+            logger.warning(f'\nMissing packages: {s}\nAtempting installation...')\n             try:\n-                subprocess.check_output(\n-                    f""""pip install --no-cache {s} {cmds}"""",\n-                    shell=True,\n-                    stderr=subprocess.STDOUT,\n-                )\n+                subprocess.check_output(f'pip install --no-cache {s} {cmds}', shell=True, stderr=subprocess.STDOUT)\n             except Exception as e:\n                 logger.error(e)\n                 exit()\n-            logger.success(""""All the missing packages were installed successfully"""")\n+            logger.success('All the missing packages were installed successfully')\n",add,Don ' t define the right crlf formatting when
ec317e24f149f83f5f5315fb574e30bbf22e6f2f,enbale ECC in StrongSROT by default,boxmot/trackers/strongsort/sort/tracker.py,"# vim: expandtab:ts=4:sw=4\nfrom __future__ import absolute_import\n\nimport numpy as np\n\nfrom ....utils.matching import chi2inv95\nfrom . import detection, iou_matching, linear_assignment\nfrom .track import Track\n\n\nclass Tracker:\n    """"""""""""\n    This is the multi-target tracker.\n    Parameters\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        A distance metric for measurement-to-track association.\n    max_age : int\n        Maximum number of missed misses before a track is deleted.\n    n_init : int\n        Number of consecutive detections before the track is confirmed. The\n        track state is set to `Deleted` if a miss occurs within the first\n        `n_init` frames.\n    Attributes\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        The distance metric used for measurement to track association.\n    max_age : int\n        Maximum number of missed misses before a track is deleted.\n    n_init : int\n        Nu","# vim: expandtab:ts=4:sw=4\nfrom __future__ import absolute_import\n\nimport numpy as np\n\nfrom ....utils.matching import chi2inv95\nfrom . import detection, iou_matching, linear_assignment\nfrom .track import Track\n\n\nclass Tracker:\n    """"""""""""\n    This is the multi-target tracker.\n    Parameters\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        A distance metric for measurement-to-track association.\n    max_age : int\n        Maximum number of missed misses before a track is deleted.\n    n_init : int\n        Number of consecutive detections before the track is confirmed. The\n        track state is set to `Deleted` if a miss occurs within the first\n        `n_init` frames.\n    Attributes\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        The distance metric used for measurement to track association.\n    max_age : int\n        Maximum number of missed misses before a track is deleted.\n    n_init : int\n        Nu","@@ -73,7 +73,7 @@ class Tracker:\n \n     def camera_update(self, previous_img, current_img):\n         if len(self.tracks) > 0:\n-            warp_matrix, src_aligned = self.tracks[0].ECC(previous_frame, next_frame)\n+            warp_matrix, src_aligned = self.tracks[0].ECC(previous_img, current_img)\n         for track in self.tracks:\n             track.camera_update(previous_img, current_img, ecc_results=(warp_matrix, src_aligned))\n \n",add,Added Type name for DFI ( # 3 )
ec317e24f149f83f5f5315fb574e30bbf22e6f2f,enbale ECC in StrongSROT by default,boxmot/trackers/strongsort/strong_sort.py,"import cv2\nimport numpy as np\nimport torch\n\nfrom ...appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom ...utils.matching import NearestNeighborDistanceMetric\nfrom ...utils.ops import xyxy2xywh\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\n\n\nclass StrongSORT(object):\n    def __init__(\n        self,\n        model_weights,\n        device,\n        fp16,\n        max_dist=0.2,\n        max_iou_dist=0.7,\n        max_age=70,\n        max_unmatched_preds=7,\n        n_init=3,\n        nn_budget=100,\n        mc_lambda=0.995,\n        ema_alpha=0.9,\n    ):\n        self.model = ReIDDetectMultiBackend(\n            weights=model_weights, device=device, fp16=fp16\n        )\n\n        self.max_dist = max_dist\n        metric = NearestNeighborDistanceMetric(""""cosine"""", self.max_dist, nn_budget)\n        self.tracker = Tracker(\n            metric,\n            max_iou_dist=max_iou_dist,\n            max_age=max_age,\n            n_init=n_ini","import cv2\nimport numpy as np\nimport torch\n\nfrom ...appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom ...utils.matching import NearestNeighborDistanceMetric\nfrom ...utils.ops import xyxy2xywh\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\n\n\nclass StrongSORT(object):\n    def __init__(\n        self,\n        model_weights,\n        device,\n        fp16,\n        max_dist=0.2,\n        max_iou_dist=0.7,\n        max_age=70,\n        max_unmatched_preds=7,\n        n_init=3,\n        nn_budget=100,\n        mc_lambda=0.995,\n        ema_alpha=0.9,\n    ):\n        self.model = ReIDDetectMultiBackend(\n            weights=model_weights, device=device, fp16=fp16\n        )\n\n        self.max_dist = max_dist\n        metric = NearestNeighborDistanceMetric(""""cosine"""", self.max_dist, nn_budget)\n        self.tracker = Tracker(\n            metric,\n            max_iou_dist=max_iou_dist,\n            max_age=max_age,\n            n_init=n_ini","@@ -39,6 +39,7 @@ class StrongSORT(object):\n             mc_lambda=mc_lambda,\n             ema_alpha=ema_alpha,\n         )\n+        self.previous_img = None\n \n     def update(self, dets, img):\n         assert isinstance(\n@@ -54,6 +55,9 @@ class StrongSORT(object):\n             dets.shape[1] == 6\n         ), """"Unsupported 'dets' 2nd dimension lenght, valid lenghts is 6""""\n \n+        self.tracker.camera_update(previous_img=self.previous_img, current_img=img)\n+        self.previous_img = img\n+\n         xyxys = dets[:, 0:4]\n         confs = dets[:, 4]\n         clss = dets[:, 5]\n",add,Added the UNSTARTED state to the YouTube PlayerState enum
f8ac4bd58d019308785ebc8789cef3fa2a758660,fix orb output,boxmot/motion/cmc/orb.py,"import copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\n\n\nclass ORBStrategy(CMCInterface):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        align=False,\n        grayscale=True\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [int, int]\n            scale_ratio: float\n            scale_size: [W, H]\n        align: bool\n   ","import copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\n\n\nclass ORBStrategy(CMCInterface):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        align=False,\n        grayscale=True\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [int, int]\n            scale_ratio: float\n            scale_size: [W, H]\n        align: bool\n   ","@@ -96,11 +96,11 @@ class ORBStrategy(CMCInterface):\n         mask = np.zeros_like(frame)\n         # mask[int(0.05 * height): int(0.95 * height), int(0.05 * width): int(0.95 * width)] = 255\n \n-        mask[int(0.02 * height): int(0.98 * height), int(0.02 * width): int(0.98 * width)] = 0\n+        mask[int(0.02 * height): int(0.98 * height), int(0.02 * width): int(0.98 * width)] = 255\n         if detections is not None:\n             for det in detections:\n                 tlbr = np.multiply(det, self.scale).astype(int)\n-                mask[tlbr[1]:tlbr[3], tlbr[0]:tlbr[2]] = 255\n+                mask[tlbr[1]:tlbr[3], tlbr[0]:tlbr[2]] = 0\n \n         # cv2.imshow('prev_img_aligned', mask)\n         # cv2.waitKey(0)\n@@ -172,7 +172,7 @@ class ORBStrategy(CMCInterface):\n         currPoints = np.array(currPoints)\n \n         # Draw the keypoint matches on the output image\n-        if False:\n+        if True:\n             self.prevFrame[:, :][mask == True] = 0  # noqa:E712\n             matches_img = np.hstack((self.prevFrame, frame))\n             matches_img = cv2.cvtColor(matches_img, cv2.COLOR_GRAY2BGR)\n@@ -216,7 +216,7 @@ class ORBStrategy(CMCInterface):\n         self.prevKeyPoints = copy.copy(keypoints)\n         self.prevDescriptors = copy.copy(descriptors)\n \n-        return H, matches_img\n+        return H\n \n \n def main():\n",add,Added STORM - 860 to Changelog
40d3e2a76bde7f3eb2c12f380e11a84d20c2008e,fix orb output,boxmot/motion/cmc/orb.py,"import copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\n\n\nclass ORBStrategy(CMCInterface):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        align=False,\n        grayscale=True\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [int, int]\n            scale_ratio: float\n            scale_size: [W, H]\n        align: bool\n   ","import copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\n\n\nclass ORBStrategy(CMCInterface):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        align=False,\n        grayscale=True\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [int, int]\n            scale_ratio: float\n            scale_size: [W, H]\n        align: bool\n   ","@@ -172,7 +172,7 @@ class ORBStrategy(CMCInterface):\n         currPoints = np.array(currPoints)\n \n         # Draw the keypoint matches on the output image\n-        if True:\n+        if False:\n             self.prevFrame[:, :][mask == True] = 0  # noqa:E712\n             matches_img = np.hstack((self.prevFrame, frame))\n             matches_img = cv2.cvtColor(matches_img, cv2.COLOR_GRAY2BGR)\n",add,Added example for MAP type in documentation
9dce7df2e338e33795b0fe3c42180fd1d29c977f,fix orb output,boxmot/motion/cmc/orb.py,"import copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\n\n\nclass ORBStrategy(CMCInterface):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        align=False,\n        grayscale=True\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [int, int]\n            scale_ratio: float\n            scale_size: [W, H]\n        align: bool\n   ","import copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\n\n\nclass ORBStrategy(CMCInterface):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        align=False,\n        grayscale=True\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [int, int]\n            scale_ratio: float\n            scale_size: [W, H]\n        align: bool\n   ","@@ -58,9 +58,10 @@ class ORBStrategy(CMCInterface):\n             self.warp_matrix = np.eye(2, 3, dtype=np.float32)\n \n         self.detector = cv2.FastFeatureDetector_create(threshold=20)\n-        self.extractor = cv2.ORB_create(nfeatures=5)\n+        self.extractor = cv2.ORB_create()\n         self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING)\n-        self.initializedFirstFrame = False\n+\n+        self.prev_img = None\n \n     def preprocess(self, img):\n \n@@ -111,10 +112,11 @@ class ORBStrategy(CMCInterface):\n         keypoints, descriptors = self.extractor.compute(frame, keypoints)\n \n         # Handle first frame\n-        if not self.initializedFirstFrame:\n+        if self.prev_img is None:\n             # Initialize data\n             self.prevDetections = detections.copy()\n             self.prevFrame = frame.copy()\n+            self.prev_img = frame.copy()\n             self.prevKeyPoints = copy.copy(keypoints)\n             self.prevDescriptors = copy.copy(descriptors)\n \n",add,Added hypest who apparently actually wrote the C # port
3371cea36f3959e2ab4a4c0c323f5bc1e74ff65e,set sparse optical flow as default cmc,boxmot/trackers/deepocsort/deep_ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.association import (associate, associate_kitti, ciou_batch,\n                                      ct_dist, diou_batch, giou_batch,\n                                      iou_batch, linear_assignment)\nfrom boxmot.utils.cmc import CameraMotionCompensation\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.association import (associate, associate_kitti, ciou_batch,\n                                      ct_dist, diou_batch, giou_batch,\n                                      iou_batch, linear_assignment)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2]","@@ -7,11 +7,11 @@ import torch\n \n from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n from boxmot.motion.adapters import OCSortKalmanFilterAdapter\n+from boxmot.motion.cmc import get_cmc_method\n from boxmot.utils import PerClassDecorator\n from boxmot.utils.association import (associate, associate_kitti, ciou_batch,\n                                       ct_dist, diou_batch, giou_batch,\n                                       iou_batch, linear_assignment)\n-from boxmot.utils.cmc import CameraMotionCompensation\n \n \n def k_previous_obs(observations, cur_age, k):\n@@ -356,7 +356,7 @@ class DeepOCSort(object):\n         KalmanBoxTracker.count = 0\n \n         self.embedder = ReIDDetectMultiBackend(weights=model_weights, device=device, fp16=fp16)\n-        self.cmc = CameraMotionCompensation()\n+        self.cmc = get_cmc_method('sof')()\n         self.embedding_off = embedding_off\n         self.cmc_off = cmc_off\n         self.aw_off = aw_off\n@@ -397,7 +397,7 @@ class DeepOCSort(object):\n \n         # CMC\n         if not self.cmc_off:\n-            transform = self.cmc.compute_affine(img, dets[:, :4], tag)\n+            transform = self.cmc.apply(img, dets[:, :4])\n             for trk in self.trackers:\n                 trk.apply_affine_correction(transform)\n \n",add,Added STORM - 360 to CHANGELOG
f9c63d1be309aeed8259a7c4d068b52bd1a1d507,fix evolve crash,examples/evolve.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport argparse\n\nimport yaml\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom val import Evaluator\n\nfrom boxmot.utils import ROOT, EXAMPLES, WEIGHTS, logger\n\n\nclass Objective(Evaluator):\n    """"""""""""Objective function to evolve best set of hyperparams for\n\n    This object is passed to an objective function and provides interfaces to overwrite\n    a tracker's config yaml file and the call to the obj","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport argparse\n\nimport yaml\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom val import Evaluator\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, logger\n\n\nclass Objective(Evaluator):\n    """"""""""""Objective function to evolve best set of hyperparams for\n\n    This object is passed to an objective function and provides interfaces to overwrite\n    a tracker's config yaml file and the call to the obj","@@ -17,7 +17,7 @@ import yaml\n from ultralytics.yolo.utils.checks import check_requirements, print_args\n from val import Evaluator\n \n-from boxmot.utils import ROOT, EXAMPLES, WEIGHTS, logger\n+from boxmot.utils import EXAMPLES, ROOT, WEIGHTS, logger\n \n \n class Objective(Evaluator):\n@@ -261,7 +261,7 @@ def parse_opt():\n     parser = argparse.ArgumentParser()\n     parser.add_argument('--yolo-model', type=str, default=WEIGHTS / 'yolov8n.pt',\n                         help='model.pt path(s)')\n-    parser.add_argument('--reid-model', type=str, default=WEIGHTS / 'lmbn_n_cuhk03_d.pt')\n+    parser.add_argument('--reid-model', type=str, default=WEIGHTS / 'mobilenetv2_x1_4_dukemtmcreid.pt')\n     parser.add_argument('--tracking-method', type=str, default='deepocsort',\n                         help='strongsort, ocsort, bytetrack, deepocsort, botsort')\n     parser.add_argument('--name', default='exp',\n",add,Add note about data volume to enable_metrics_collection
f9c63d1be309aeed8259a7c4d068b52bd1a1d507,fix evolve crash,examples/val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport argparse\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport zipfile\nfrom pathlib import Path\n\nimport git\nfrom git import Repo\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom ultralytics.yolo.utils.files import increment_path\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS\nfrom boxmot.utils import logger as LOGGER\n\n\nclass Evaluator:\n    """"""""""""Evaluate","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport argparse\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport zipfile\nfrom pathlib import Path\n\nimport git\nfrom git import Repo\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom ultralytics.yolo.utils.files import increment_path\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS\nfrom boxmot.utils import logger as LOGGER\n\n\nclass Evaluator:\n    """"""""""""Evaluate","@@ -171,7 +171,6 @@ class Evaluator:\n             processes = []\n \n             busy_devices = []\n-            print(seq_paths)\n             for i, seq_path in enumerate(seq_paths):\n                 # spawn one subprocess per GPU in increasing order.\n                 # When max devices are reached start at 0 again\n@@ -206,13 +205,12 @@ class Evaluator:\n                     ],\n                 )\n                 processes.append(p)\n-                # Wait for the subprocess to complete and capture output\n-                \n \n+            # Wait for the subprocess to complete and capture output\n             for p in processes:\n                 p.wait()\n-                \n-            LOGGER.success(f""""Evaluation succeeded"""")\n+\n+            LOGGER.success(""""Evaluation succeeded"""")\n \n         print_args(vars(self.opt))\n \n",add,Added Type name for DFI ( # 573 )
ab4c5cfb5947113bf632d848f56c72fd31875b2c,fix save paths,boxmot/motion/cmc/ecc.py,"import time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\nfrom boxmot.utils import BOXMOT\n\n\nclass ECC(CMCInterface):\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        align=False,\n        grayscale=True\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [int, int]\n            scale_ratio: float\n            scale_size: [W, H]\n        align:","import time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\nfrom boxmot.utils import BOXMOT\n\n\nclass ECC(CMCInterface):\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        align=False,\n        grayscale=True\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [int, int]\n            scale_ratio: float\n            scale_size: [W, H]\n        align:","@@ -137,7 +137,7 @@ def main():\n         weighted_img = cv2.addWeighted(curr_img, 0.5, ecc.prev_img_aligned, 0.5, 0)\n         cv2.imshow('prev_img_aligned', weighted_img)\n         cv2.waitKey(0)\n-        cv2.imwrite(str(BOXMOT / 'motion/cmc/orb_aligned.jpg'), weighted_img)\n+        cv2.imwrite(str(BOXMOT / 'motion/cmc/ecc_aligned.jpg'), weighted_img)\n \n \n if __name__ == """"__main__"""":\n",fix,Added STORM - 1270 to Changelog
ab4c5cfb5947113bf632d848f56c72fd31875b2c,fix save paths,boxmot/motion/cmc/sift.py,"import copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\nfrom boxmot.utils import BOXMOT\n\n\nclass SIFT(CMCInterface):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        grayscale=True,\n        draw_keypoint_matches=False,\n        align=False\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [int, int]\n            scale_ratio:","import copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\nfrom boxmot.utils import BOXMOT\n\n\nclass SIFT(CMCInterface):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        grayscale=True,\n        draw_keypoint_matches=False,\n        align=False\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [int, int]\n            scale_ratio:","@@ -265,7 +265,7 @@ def main():\n         weighted_img = cv2.addWeighted(curr_img, 0.5, sift.prev_img_aligned, 0.5, 0)\n         cv2.imshow('prev_img_aligned', weighted_img)\n         cv2.waitKey(0)\n-        cv2.imwrite(str(BOXMOT / 'motion/cmc/orb_aligned.jpg'), weighted_img)\n+        cv2.imwrite(str(BOXMOT / 'motion/cmc/sift_aligned.jpg'), weighted_img)\n \n \n if __name__ == """"__main__"""":\n",fix,Fix max stacksize on Potions .
ab4c5cfb5947113bf632d848f56c72fd31875b2c,fix save paths,boxmot/motion/cmc/sof.py,"import copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\nfrom boxmot.utils import BOXMOT\n\n\nclass SparseOptFlow(CMCInterface):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        align=False,\n        grayscale=True\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [int, int]\n            scale_ratio: float\n            scale_siz","import copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\nfrom boxmot.utils import BOXMOT\n\n\nclass SparseOptFlow(CMCInterface):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        align=False,\n        grayscale=True\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [int, int]\n            scale_ratio: float\n            scale_siz","@@ -207,7 +207,7 @@ def main():\n         weighted_img = cv2.addWeighted(curr_img, 0.5, sof.prev_img_aligned, 0.5, 0)\n         cv2.imshow('prev_img_aligned', weighted_img)\n         cv2.waitKey(0)\n-        cv2.imwrite(str(BOXMOT / 'motion/cmc/orb_aligned.jpg'), weighted_img)\n+        cv2.imwrite(str(BOXMOT / 'motion/cmc/sof_aligned.jpg'), weighted_img)\n \n \n if __name__ == """"__main__"""":\n",fix,Fix max stacksize on Potions .
275e2532fc975841df9625eaa45c21552088d015,fix case when no cmc keypoints are found,boxmot/motion/cmc/sof.py,"import copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\nfrom boxmot.utils import BOXMOT\n\n\nclass SparseOptFlow(CMCInterface):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        align=False,\n        grayscale=True\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [int, int]\n            scale_ratio: float\n            scale_siz","import copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\nfrom boxmot.utils import BOXMOT\n\n\nclass SparseOptFlow(CMCInterface):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        align=False,\n        grayscale=True\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [int, int]\n            scale_ratio: float\n            scale_siz","@@ -109,47 +109,53 @@ class SparseOptFlow(CMCInterface):\n \n             return H\n \n-        # sparse otical flow for sparse features using Lucas-Kanade with pyramids\n-        matchedKeypoints, status, err = cv2.calcOpticalFlowPyrLK(\n-            self.prev_img, img, self.prevKeyPoints, None\n-        )\n+        if keypoints is not None and self.prevKeyPoints is not None:\n \n-        # leave good correspondences only\n-        prevPoints = []\n-        currPoints = []\n+            # sparse otical flow for sparse features using Lucas-Kanade with pyramids\n+            matchedKeypoints, status, err = cv2.calcOpticalFlowPyrLK(\n+                self.prev_img, img, self.prevKeyPoints, None\n+            )\n \n-        for i in range(len(status)):\n-            if status[i]:\n-                prevPoints.append(self.prevKeyPoints[i])\n-                currPoints.append(matchedKeypoints[i])\n+            # leave good correspondences only\n+            prevPoints = []\n+            currPoints = []\n \n-        prevPoints = np.array(prevPoints)\n-        currPoints = np.array(currPoints)\n+            for i in range(len(status)):\n+                if status[i]:\n+                    prevPoints.append(self.prevKeyPoints[i])\n+                    currPoints.append(matchedKeypoints[i])\n \n-        # Find rigid matrix\n-        if (np.size(prevPoints, 0) > 4) and (\n-            np.size(prevPoints, 0) == np.size(prevPoints, 0)\n-        ):\n-            H, inliesrs = cv2.estimateAffinePartial2D(\n-                prevPoints, currPoints, cv2.RANSAC\n-            )\n+            prevPoints = np.array(prevPoints)\n+            currPoints = np.array(currPoints)\n \n-            # Handle downscale\n-            if self.scale < 1:\n-                H[0, 2] /= self.scale\n-                H[1, 2] /= self.scale\n+            # Find rigid matrix\n+            if (np.size(prevPoints, 0) > 4) and (\n+                np.size(prevPoints, 0) == np.size(prevPoints, 0)\n+            ):\",add,Added STORM - 360 to CHANGELOG
cdc13ca3a2a27366f38400f74d872b403af89a92,fix deepocsort asso bug,boxmot/utils/association.py,"import numpy as np\n\n\ndef iou_batch(bboxes1, bboxes2):\n    """"""""""""\n    From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n    """"""""""""\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n    yy2 = np.minimum(bboxes1[..., 3], bboxes2[..., 3])\n    w = np.maximum(0.0, xx2 - xx1)\n    h = np.maximum(0.0, yy2 - yy1)\n    wh = w * h\n    o = wh / (\n        (bboxes1[..., 2] - bboxes1[..., 0]) * (bboxes1[..., 3] - bboxes1[..., 1]) +\n        (bboxes2[..., 2] - bboxes2[..., 0]) * (bboxes2[..., 3] - bboxes2[..., 1]) -\n        wh\n    )\n    return o\n\n\ndef giou_batch(bboxes1, bboxes2):\n    """"""""""""\n    :param bbox_p: predict of bbox(N,4)(x1,y1,x2,y2)\n    :param bbox_g: groundtruth of bbox(N,4)(x1,y1,x2,y2)\n    :return:\n    """"""""""""\n    # for details should go to https:/","import numpy as np\n\n\ndef iou_batch(bboxes1, bboxes2):\n    """"""""""""\n    From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n    """"""""""""\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n    yy2 = np.minimum(bboxes1[..., 3], bboxes2[..., 3])\n    w = np.maximum(0.0, xx2 - xx1)\n    h = np.maximum(0.0, yy2 - yy1)\n    wh = w * h\n    o = wh / (\n        (bboxes1[..., 2] - bboxes1[..., 0]) * (bboxes1[..., 3] - bboxes1[..., 1]) +\n        (bboxes2[..., 2] - bboxes2[..., 0]) * (bboxes2[..., 3] - bboxes2[..., 1]) -\n        wh\n    )\n    return o\n\n\ndef giou_batch(bboxes1, bboxes2):\n    """"""""""""\n    :param bbox_p: predict of bbox(N,4)(x1,y1,x2,y2)\n    :param bbox_g: groundtruth of bbox(N,4)(x1,y1,x2,y2)\n    :return:\n    """"""""""""\n    # for details should go to https:/","@@ -334,6 +334,7 @@ def associate(\n             if emb_cost is None:\n                 emb_cost = 0\n             else:\n+                emb_cost = emb_cost.numpy()\n                 emb_cost[iou_matrix <= 0] = 0\n                 if not aw_off:\n                     emb_cost = compute_aw_max_metric(emb_cost, w_assoc_emb, bottom=aw_param)\n@@ -345,6 +346,9 @@ def associate(\n     else:\n         matched_indices = np.empty(shape=(0, 2))\n \n+    if matched_indices.size == 0:\n+        matched_indices = np.empty(shape=(0, 2))\n+\n     unmatched_detections = []\n     for d, det in enumerate(detections):\n         if d not in matched_indices[:, 0]:\n",add,Added STORM - 236 to Changelog
c2502c0c358776adc472c15ddded3623b2f0fa09,fix deepocsort asso bug,boxmot/utils/association.py,"import numpy as np\n\n\ndef iou_batch(bboxes1, bboxes2):\n    """"""""""""\n    From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n    """"""""""""\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n    yy2 = np.minimum(bboxes1[..., 3], bboxes2[..., 3])\n    w = np.maximum(0.0, xx2 - xx1)\n    h = np.maximum(0.0, yy2 - yy1)\n    wh = w * h\n    o = wh / (\n        (bboxes1[..., 2] - bboxes1[..., 0]) * (bboxes1[..., 3] - bboxes1[..., 1]) +\n        (bboxes2[..., 2] - bboxes2[..., 0]) * (bboxes2[..., 3] - bboxes2[..., 1]) -\n        wh\n    )\n    return o\n\n\ndef giou_batch(bboxes1, bboxes2):\n    """"""""""""\n    :param bbox_p: predict of bbox(N,4)(x1,y1,x2,y2)\n    :param bbox_g: groundtruth of bbox(N,4)(x1,y1,x2,y2)\n    :return:\n    """"""""""""\n    # for details should go to https:/","import numpy as np\n\n\ndef iou_batch(bboxes1, bboxes2):\n    """"""""""""\n    From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n    """"""""""""\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n    yy2 = np.minimum(bboxes1[..., 3], bboxes2[..., 3])\n    w = np.maximum(0.0, xx2 - xx1)\n    h = np.maximum(0.0, yy2 - yy1)\n    wh = w * h\n    o = wh / (\n        (bboxes1[..., 2] - bboxes1[..., 0]) * (bboxes1[..., 3] - bboxes1[..., 1]) +\n        (bboxes2[..., 2] - bboxes2[..., 0]) * (bboxes2[..., 3] - bboxes2[..., 1]) -\n        wh\n    )\n    return o\n\n\ndef giou_batch(bboxes1, bboxes2):\n    """"""""""""\n    :param bbox_p: predict of bbox(N,4)(x1,y1,x2,y2)\n    :param bbox_g: groundtruth of bbox(N,4)(x1,y1,x2,y2)\n    :return:\n    """"""""""""\n    # for details should go to https:/","@@ -326,7 +326,7 @@ def associate(\n     angle_diff_cost = angle_diff_cost.T\n     angle_diff_cost = angle_diff_cost * scores\n \n-    if min(iou_matrix.shape) > 0:\n+    if min(iou_matrix.shape):\n         a = (iou_matrix > iou_threshold).astype(np.int32)\n         if a.sum(1).max() == 1 and a.sum(0).max() == 1:\n             matched_indices = np.stack(np.where(a), axis=1)\n@@ -343,10 +343,10 @@ def associate(\n \n             final_cost = -(iou_matrix + angle_diff_cost + emb_cost)\n             matched_indices = linear_assignment(final_cost)\n-    else:\n-        matched_indices = np.empty(shape=(0, 2))\n+            if matched_indices.size == 0:\n+                matched_indices = np.empty(shape=(0, 2))\n \n-    if matched_indices.size == 0:\n+    else:\n         matched_indices = np.empty(shape=(0, 2))\n \n     unmatched_detections = []\n",add,Add note about data volume to enable_metrics_collection
39a7933bf667ee7e96fc58cac9438ae7ecfad575,move kfs to folder and fix imports,boxmot/motion/kalman_filters/adapters/__init__.py,,"from .botsort_kf_adapter import BotSortKalmanFilterAdapter\nfrom .bytetrack_kf_adapter import ByteTrackKalmanFilterAdapter\nfrom .ocsort_kf_adapter import OCSortKalmanFilterAdapter\nfrom .strongsort_kf_adapter import StrongSortKalmanFilterAdapter\n\n__all__ = (""""BotSortKalmanFilterAdapter"""",\n           """"ByteTrackKalmanFilterAdapter"""",\n           """"OCSortKalmanFilterAdapter"""",\n           """"StrongSortKalmanFilterAdapter"""")\n","@@ -0,0 +1,9 @@\n+from .botsort_kf_adapter import BotSortKalmanFilterAdapter\n+from .bytetrack_kf_adapter import ByteTrackKalmanFilterAdapter\n+from .ocsort_kf_adapter import OCSortKalmanFilterAdapter\n+from .strongsort_kf_adapter import StrongSortKalmanFilterAdapter\n+\n+__all__ = (""""BotSortKalmanFilterAdapter"""",\n+           """"ByteTrackKalmanFilterAdapter"""",\n+           """"OCSortKalmanFilterAdapter"""",\n+           """"StrongSortKalmanFilterAdapter"""")\n",add,Add note about data volume to enable_metrics_collection
39a7933bf667ee7e96fc58cac9438ae7ecfad575,move kfs to folder and fix imports,boxmot/motion/kalman_filters/adapters/botsort_kf_adapter.py,,"import numpy as np\n\nfrom ..kalman_filter import KalmanFilter\nfrom ..kalman_filter import multi_predict as kf_multi_predict\n\n\nclass BotSortKalmanFilterAdapter(KalmanFilter):\n    ndim = 4\n\n    def __init__(self, dt=1):\n        super().__init__(dim_x=2 * self.ndim, dim_z=self.ndim)\n\n        # Set transition matrix\n        for i in range(self.ndim):\n            self.F[i, self.ndim + i] = dt\n\n        # Set observation matrix\n        self.H = np.eye(self.ndim, 2 * self.ndim)\n\n        # Motion and observation uncertainty are chosen relative to the current\n        # state estimate. These weights control the amount of uncertainty in\n        # the model. This is a bit hacky.\n        self._std_weight_position = 1.0 / 20\n        self._std_weight_velocity = 1.0 / 160\n\n    def initiate(self, measurement):\n        """"""""""""Create track from unassociated measurement.\n\n        Parameters\n        ----------\n        measurement : ndarray\n            Bounding box coordinates (x","@@ -0,0 +1,168 @@\n+import numpy as np\n+\n+from ..kalman_filter import KalmanFilter\n+from ..kalman_filter import multi_predict as kf_multi_predict\n+\n+\n+class BotSortKalmanFilterAdapter(KalmanFilter):\n+    ndim = 4\n+\n+    def __init__(self, dt=1):\n+        super().__init__(dim_x=2 * self.ndim, dim_z=self.ndim)\n+\n+        # Set transition matrix\n+        for i in range(self.ndim):\n+            self.F[i, self.ndim + i] = dt\n+\n+        # Set observation matrix\n+        self.H = np.eye(self.ndim, 2 * self.ndim)\n+\n+        # Motion and observation uncertainty are chosen relative to the current\n+        # state estimate. These weights control the amount of uncertainty in\n+        # the model. This is a bit hacky.\n+        self._std_weight_position = 1.0 / 20\n+        self._std_weight_velocity = 1.0 / 160\n+\n+    def initiate(self, measurement):\n+        """"""""""""Create track from unassociated measurement.\n+\n+        Parameters\n+        ----------\n+        measurement : ndarray\n+            Bounding box coordinates (x, y, w, h) with center position (x, y),\n+            width w, and height h.\n+\n+        Returns\n+        -------\n+        (ndarray, ndarray)\n+            Returns the mean vector (8 dimensional) and covariance matrix (8x8\n+            dimensional) of the new track. Unobserved velocities are initialized\n+            to 0 mean.\n+\n+        """"""""""""\n+        mean_pos = measurement\n+        mean_vel = np.zeros_like(mean_pos)\n+        self.x = (np.r_[mean_pos, mean_vel]).T\n+\n+        std = [\n+            2 * self._std_weight_position * measurement[2],\n+            2 * self._std_weight_position * measurement[3],\n+            2 * self._std_weight_position * measurement[2],\n+            2 * self._std_weight_position * measurement[3],\n+            10 * self._std_weight_velocity * measurement[2],\n+            10 * self._std_weight_velocity * measurement[3],\n+            10 * self._std_weight_velocity * measurement[2],\n+        ",add,Added STORM - 236 to Changelog
39a7933bf667ee7e96fc58cac9438ae7ecfad575,move kfs to folder and fix imports,boxmot/motion/kalman_filters/adapters/bytetrack_kf_adapter.py,,"import numpy as np\n\nfrom ..kalman_filter import KalmanFilter\nfrom ..kalman_filter import multi_predict as kf_multi_predict\n\n\nclass ByteTrackKalmanFilterAdapter(KalmanFilter):\n    ndim = 4\n\n    def __init__(self, dt=1):\n        super().__init__(dim_x=2 * self.ndim, dim_z=self.ndim)\n\n        # Set transition matrix\n        for i in range(self.ndim):\n            self.F[i, self.ndim + i] = dt\n\n        # Set observation matrix\n        self.H = np.eye(self.ndim, 2 * self.ndim)\n\n        # Motion and observation uncertainty are chosen relative to the current\n        # state estimate. These weights control the amount of uncertainty in\n        # the model. This is a bit hacky.\n        self._std_weight_position = 1.0 / 20\n        self._std_weight_velocity = 1.0 / 160\n\n    def initiate(self, measurement):\n        """"""""""""Create track from unassociated measurement.\n\n        Parameters\n        ----------\n        measurement : ndarray\n            Bounding box coordinates ","@@ -0,0 +1,168 @@\n+import numpy as np\n+\n+from ..kalman_filter import KalmanFilter\n+from ..kalman_filter import multi_predict as kf_multi_predict\n+\n+\n+class ByteTrackKalmanFilterAdapter(KalmanFilter):\n+    ndim = 4\n+\n+    def __init__(self, dt=1):\n+        super().__init__(dim_x=2 * self.ndim, dim_z=self.ndim)\n+\n+        # Set transition matrix\n+        for i in range(self.ndim):\n+            self.F[i, self.ndim + i] = dt\n+\n+        # Set observation matrix\n+        self.H = np.eye(self.ndim, 2 * self.ndim)\n+\n+        # Motion and observation uncertainty are chosen relative to the current\n+        # state estimate. These weights control the amount of uncertainty in\n+        # the model. This is a bit hacky.\n+        self._std_weight_position = 1.0 / 20\n+        self._std_weight_velocity = 1.0 / 160\n+\n+    def initiate(self, measurement):\n+        """"""""""""Create track from unassociated measurement.\n+\n+        Parameters\n+        ----------\n+        measurement : ndarray\n+            Bounding box coordinates (x, y, w, h) with center position (x, y),\n+            width w, and height h.\n+\n+        Returns\n+        -------\n+        (ndarray, ndarray)\n+            Returns the mean vector (8 dimensional) and covariance matrix (8x8\n+            dimensional) of the new track. Unobserved velocities are initialized\n+            to 0 mean.\n+\n+        """"""""""""\n+        mean_pos = measurement\n+        mean_vel = np.zeros_like(mean_pos)\n+        self.x = (np.r_[mean_pos, mean_vel]).T\n+\n+        std = [\n+            2 * self._std_weight_position * measurement[3],\n+            2 * self._std_weight_position * measurement[3],\n+            1e-2,\n+            2 * self._std_weight_position * measurement[3],\n+            10 * self._std_weight_velocity * measurement[3],\n+            10 * self._std_weight_velocity * measurement[3],\n+            1e-5,\n+            10 * self._std_weight_velocity * measurement[3],\n+        ]\n+        self.P =",add,Added STORM - 236 to Changelog
39a7933bf667ee7e96fc58cac9438ae7ecfad575,move kfs to folder and fix imports,boxmot/motion/kalman_filters/adapters/ocsort_kf_adapter.py,,"from ..kalman_filter import KalmanFilter\n\n\nclass OCSortKalmanFilterAdapter(KalmanFilter):\n    def __init__(self, dim_x, dim_z):\n        super().__init__(dim_x=dim_x, dim_z=dim_z)\n","@@ -0,0 +1,6 @@\n+from ..kalman_filter import KalmanFilter\n+\n+\n+class OCSortKalmanFilterAdapter(KalmanFilter):\n+    def __init__(self, dim_x, dim_z):\n+        super().__init__(dim_x=dim_x, dim_z=dim_z)\n",add,Add note about data volume to enable_metrics_collection
39a7933bf667ee7e96fc58cac9438ae7ecfad575,move kfs to folder and fix imports,boxmot/motion/kalman_filters/adapters/strongsort_kf_adapter.py,,"import numpy as np\nfrom filterpy.common import reshape_z\n\nfrom ..kalman_filter import KalmanFilter\n\n\nclass StrongSortKalmanFilterAdapter(KalmanFilter):\n    ndim = 4\n\n    def __init__(self, dt=1):\n        super().__init__(dim_x=2 * self.ndim, dim_z=self.ndim)\n\n        # Set transition matrix\n        for i in range(self.ndim):\n            self.F[i, self.ndim + i] = dt\n\n        # Set observation matrix\n        self.H = np.eye(self.ndim, 2 * self.ndim)\n\n        # Motion and observation uncertainty are chosen relative to the current\n        # state estimate. These weights control the amount of uncertainty in\n        # the model. This is a bit hacky.\n        self._std_weight_position = 1.0 / 20\n        self._std_weight_velocity = 1.0 / 160\n\n    def initiate(self, measurement):\n        """"""""""""Create track from unassociated measurement.\n\n        Parameters\n        ----------\n        measurement : ndarray\n            Bounding box coordinates (x, y, w, h) with cente","@@ -0,0 +1,169 @@\n+import numpy as np\n+from filterpy.common import reshape_z\n+\n+from ..kalman_filter import KalmanFilter\n+\n+\n+class StrongSortKalmanFilterAdapter(KalmanFilter):\n+    ndim = 4\n+\n+    def __init__(self, dt=1):\n+        super().__init__(dim_x=2 * self.ndim, dim_z=self.ndim)\n+\n+        # Set transition matrix\n+        for i in range(self.ndim):\n+            self.F[i, self.ndim + i] = dt\n+\n+        # Set observation matrix\n+        self.H = np.eye(self.ndim, 2 * self.ndim)\n+\n+        # Motion and observation uncertainty are chosen relative to the current\n+        # state estimate. These weights control the amount of uncertainty in\n+        # the model. This is a bit hacky.\n+        self._std_weight_position = 1.0 / 20\n+        self._std_weight_velocity = 1.0 / 160\n+\n+    def initiate(self, measurement):\n+        """"""""""""Create track from unassociated measurement.\n+\n+        Parameters\n+        ----------\n+        measurement : ndarray\n+            Bounding box coordinates (x, y, w, h) with center position (x, y),\n+            width w, and height h.\n+\n+        Returns\n+        -------\n+        (ndarray, ndarray)\n+            Returns the mean vector (8 dimensional) and covariance matrix (8x8\n+            dimensional) of the new track. Unobserved velocities are initialized\n+            to 0 mean.\n+\n+        """"""""""""\n+        mean_pos = measurement\n+        mean_vel = np.zeros_like(mean_pos)\n+        self.x = np.r_[mean_pos, mean_vel]\n+\n+        std = [\n+            2 * self._std_weight_position * measurement[0],  # the center point x\n+            2 * self._std_weight_position * measurement[1],  # the center point y\n+            1 * measurement[2],  # the ratio of width/height\n+            2 * self._std_weight_position * measurement[3],  # the height\n+            10 * self._std_weight_velocity * measurement[0],\n+            10 * self._std_weight_velocity * measurement[1],\n+            0.1 * measurement[2],\n+ ",add,Added STORM - 236 to Changelog
39a7933bf667ee7e96fc58cac9438ae7ecfad575,move kfs to folder and fix imports,boxmot/motion/kalman_filters/kalman_filter.py,,"# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","@@ -0,0 +1,1678 @@\n+# -*- coding: utf-8 -*-\n+# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n+# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n+\n+""""""""""""\n+This module implements the linear Kalman filter in both an object\n+oriented and procedural form. The KalmanFilter class implements\n+the filter by storing the various matrices in instance variables,\n+minimizing the amount of bookkeeping you have to do.\n+All Kalman filters operate with a predict->update cycle. The\n+predict step, implemented with the method or function predict(),\n+uses the state transition matrix F to predict the state in the next\n+time period (epoch). The state is stored as a gaussian (x, P), where\n+x is the state (column) vector, and P is its covariance. Covariance\n+matrix Q specifies the process covariance. In Bayesian terms, this\n+prediction is called the *prior*, which you can think of colloquially\n+as the estimate prior to incorporating the measurement.\n+The update step, implemented with the method or function `update()`,\n+incorporates the measurement z with covariance R, into the state\n+estimate (x, P). The class stores the system uncertainty in S,\n+the innovation (residual between prediction and measurement in\n+measurement space) in y, and the Kalman gain in k. The procedural\n+form returns these variables to you. In Bayesian terms this computes\n+the *posterior* - the estimate after the information from the\n+measurement is incorporated.\n+Whether you use the OO form or procedural form is up to you. If\n+matrices such as H, R, and F are changing each epoch, you'll probably\n+opt to use the procedural form. If they are unchanging, the OO\n+form is perhaps easier to use since you won't need to keep track\n+of these matrices. This is especially useful if you are implementing\n+banks of filters or comparing various KF designs for performance;\n+a trivial coding bug could lead to using the wrong sets of matrices.\n+This mo",add,Added example for MAP type in documentation
39a7933bf667ee7e96fc58cac9438ae7ecfad575,move kfs to folder and fix imports,boxmot/trackers/botsort/bot_sort.py,"from collections import deque\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.adapters import BotSortKalmanFilterAdapter\nfrom boxmot.motion.cmc.sof import SparseOptFlow\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = BotSortKalmanFilterAdapter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        self.update_cls(cls, score)\n\n        self.score = score","from collections import deque\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SparseOptFlow\nfrom boxmot.motion.kalman_filters.adapters import BotSortKalmanFilterAdapter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = BotSortKalmanFilterAdapter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        self.update_cls(cls, score)\n\n        sel","@@ -4,8 +4,8 @@ import numpy as np\n import torch\n \n from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n-from boxmot.motion.adapters import BotSortKalmanFilterAdapter\n from boxmot.motion.cmc.sof import SparseOptFlow\n+from boxmot.motion.kalman_filters.adapters import BotSortKalmanFilterAdapter\n from boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\n from boxmot.utils.matching import (embedding_distance, fuse_score,\n                                    iou_distance, linear_assignment)\n",fix,Added Type name for DFI ( # 254 )
39a7933bf667ee7e96fc58cac9438ae7ecfad575,move kfs to folder and fix imports,boxmot/trackers/bytetrack/byte_tracker.py,"import numpy as np\n\nfrom ...motion.adapters import ByteTrackKalmanFilterAdapter\nfrom ...utils.matching import fuse_score, iou_distance, linear_assignment\nfrom ...utils.ops import xywh2xyxy, xyxy2xywh\nfrom .basetrack import BaseTrack, TrackState\n\n\nclass STrack(BaseTrack):\n    shared_kalman = ByteTrackKalmanFilterAdapter()\n\n    def __init__(self, tlwh, score, cls):\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n        self.cls = cls\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(\n            mean_state, self.covariance\n        )\n\n    @staticmethod\n    def multi_predict(stracks):\n        if len(stracks","import numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import ByteTrackKalmanFilterAdapter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = ByteTrackKalmanFilterAdapter()\n\n    def __init__(self, tlwh, score, cls):\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n        self.cls = cls\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(\n            mean_state, self.covariance\n        )\n\n    @staticmethod\n   ","@@ -1,9 +1,9 @@\n import numpy as np\n \n-from ...motion.adapters import ByteTrackKalmanFilterAdapter\n-from ...utils.matching import fuse_score, iou_distance, linear_assignment\n-from ...utils.ops import xywh2xyxy, xyxy2xywh\n-from .basetrack import BaseTrack, TrackState\n+from boxmot.motion.kalman_filters.adapters import ByteTrackKalmanFilterAdapter\n+from boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\n+from boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\n+from boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n \n \n class STrack(BaseTrack):\n",add,Added note about dates .
39a7933bf667ee7e96fc58cac9438ae7ecfad575,move kfs to folder and fix imports,boxmot/trackers/deepocsort/deep_ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.association import (associate, associate_kitti, ciou_batch,\n                                      ct_dist, diou_batch, giou_batch,\n                                      iou_batch, linear_assignment)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2]","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.association import (associate, associate_kitti, ciou_batch,\n                                      ct_dist, diou_batch, giou_batch,\n                                      iou_batch, linear_assignment)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the for","@@ -6,8 +6,8 @@ import numpy as np\n import torch\n \n from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n-from boxmot.motion.adapters import OCSortKalmanFilterAdapter\n from boxmot.motion.cmc import get_cmc_method\n+from boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\n from boxmot.utils import PerClassDecorator\n from boxmot.utils.association import (associate, associate_kitti, ciou_batch,\n                                       ct_dist, diou_batch, giou_batch,\n",add,Added the user group to the contributors
39a7933bf667ee7e96fc58cac9438ae7ecfad575,move kfs to folder and fix imports,boxmot/trackers/ocsort/ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\n\nfrom boxmot.motion.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils.association import (associate, ciou_batch, ct_dist,\n                                      diou_batch, giou_batch, iou_batch,\n                                      linear_assignment)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - b","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils.association import (associate, ciou_batch, ct_dist,\n                                      diou_batch, giou_batch, iou_batch,\n                                      linear_assignment)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    ","@@ -3,7 +3,7 @@\n """"""""""""\n import numpy as np\n \n-from boxmot.motion.adapters import OCSortKalmanFilterAdapter\n+from boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\n from boxmot.utils.association import (associate, ciou_batch, ct_dist,\n                                       diou_batch, giou_batch, iou_batch,\n                                       linear_assignment)\n",add,Added the UNSTARTED state to the YouTube PlayerState enum
0a81844d2fdabe758097ede5fe8497b01a8a017a,fix imports,boxmot/trackers/deepocsort/deep_ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.association import (associate, associate_kitti, ciou_batch,\n                                      ct_dist, diou_batch, giou_batch,\n                                      iou_batch, linear_assignment)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the for","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.association import (associate, associate_kitti, ct_dist,\n                                      linear_assignment)\nfrom boxmot.utils.iou import ciou_batch, diou_batch, giou_batch, iou_batch\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x","@@ -9,9 +9,9 @@ from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n from boxmot.motion.cmc import get_cmc_method\n from boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\n from boxmot.utils import PerClassDecorator\n-from boxmot.utils.association import (associate, associate_kitti, ciou_batch,\n-                                      ct_dist, diou_batch, giou_batch,\n-                                      iou_batch, linear_assignment)\n+from boxmot.utils.association import (associate, associate_kitti, ct_dist,\n+                                      linear_assignment)\n+from boxmot.utils.iou import ciou_batch, diou_batch, giou_batch, iou_batch\n \n \n def k_previous_obs(observations, cur_age, k):\n",add,Added note about dates .
0a81844d2fdabe758097ede5fe8497b01a8a017a,fix imports,boxmot/trackers/ocsort/ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils.association import (associate, ciou_batch, ct_dist,\n                                      diou_batch, giou_batch, iou_batch,\n                                      linear_assignment)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    ","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils.association import associate, ct_dist, linear_assignment\nfrom boxmot.utils.iou import (ciou_batch, diou_batch, get_asso_func,\n                              giou_batch, iou_batch)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h =","@@ -4,9 +4,9 @@\n import numpy as np\n \n from boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\n-from boxmot.utils.association import (associate, ciou_batch, ct_dist,\n-                                      diou_batch, giou_batch, iou_batch,\n-                                      linear_assignment)\n+from boxmot.utils.association import associate, ct_dist, linear_assignment\n+from boxmot.utils.iou import (ciou_batch, diou_batch, get_asso_func,\n+                              giou_batch, iou_batch)\n \n \n def k_previous_obs(observations, cur_age, k):\n@@ -222,7 +222,7 @@ class OCSort(object):\n         self.frame_count = 0\n         self.det_thresh = det_thresh\n         self.delta_t = delta_t\n-        self.asso_func = ASSO_FUNCS[asso_func]\n+        self.asso_func = get_asso_func(asso_func)\n         self.inertia = inertia\n         self.use_byte = use_byte\n         KalmanBoxTracker.count = 0\n",add,Added STORM - 360 to CHANGELOG
0a81844d2fdabe758097ede5fe8497b01a8a017a,fix imports,boxmot/utils/association.py,"import numpy as np\n\nfrom boxmot.utils.iou import iou_batch\n\n\ndef ct_dist(bboxes1, bboxes2):\n    """"""""""""\n    Measure the center distance between two sets of bounding boxes,\n    this is a coarse implementation, we don't recommend using it only\n    for association, which can be unstable and sensitive to frame rate\n    and object speed.\n    """"""""""""\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    centerx1 = (bboxes1[..., 0] + bboxes1[..., 2]) / 2.0\n    centery1 = (bboxes1[..., 1] + bboxes1[..., 3]) / 2.0\n    centerx2 = (bboxes2[..., 0] + bboxes2[..., 2]) / 2.0\n    centery2 = (bboxes2[..., 1] + bboxes2[..., 3]) / 2.0\n\n    ct_dist2 = (centerx1 - centerx2) ** 2 + (centery1 - centery2) ** 2\n\n    ct_dist = np.sqrt(ct_dist2)\n\n    # The linear rescaling is a naive version and needs more study\n    ct_dist = ct_dist / ct_dist.max()\n    return ct_dist.max() - ct_dist  # resize to (0,1)\n\n\ndef speed_direction_batch(dets, tracks):\n    tr","import numpy as np\n\nfrom boxmot.utils.iou import iou_batch\n\n\ndef speed_direction_batch(dets, tracks):\n    tracks = tracks[..., np.newaxis]\n    CX1, CY1 = (dets[:, 0] + dets[:, 2]) / 2.0, (dets[:, 1] + dets[:, 3]) / 2.0\n    CX2, CY2 = (tracks[:, 0] + tracks[:, 2]) / 2.0, (tracks[:, 1] + tracks[:, 3]) / 2.0\n    dx = CX1 - CX2\n    dy = CY1 - CY2\n    norm = np.sqrt(dx**2 + dy**2) + 1e-6\n    dx = dx / norm\n    dy = dy / norm\n    return dy, dx  # size: num_track x num_det\n\n\ndef linear_assignment(cost_matrix):\n    try:\n        import lap\n        _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n        return np.array([[y[i], i] for i in x if i >= 0])  #\n    except ImportError:\n        from scipy.optimize import linear_sum_assignment\n        x, y = linear_sum_assignment(cost_matrix)\n        return np.array([list(zip(x, y))])\n\n\ndef associate_detections_to_trackers(detections, trackers, iou_threshold=0.3):\n    """"""""""""\n    Assigns detections to tracked object (both ","@@ -3,30 +3,6 @@ import numpy as np\n from boxmot.utils.iou import iou_batch\n \n \n-def ct_dist(bboxes1, bboxes2):\n-    """"""""""""\n-    Measure the center distance between two sets of bounding boxes,\n-    this is a coarse implementation, we don't recommend using it only\n-    for association, which can be unstable and sensitive to frame rate\n-    and object speed.\n-    """"""""""""\n-    bboxes2 = np.expand_dims(bboxes2, 0)\n-    bboxes1 = np.expand_dims(bboxes1, 1)\n-\n-    centerx1 = (bboxes1[..., 0] + bboxes1[..., 2]) / 2.0\n-    centery1 = (bboxes1[..., 1] + bboxes1[..., 3]) / 2.0\n-    centerx2 = (bboxes2[..., 0] + bboxes2[..., 2]) / 2.0\n-    centery2 = (bboxes2[..., 1] + bboxes2[..., 3]) / 2.0\n-\n-    ct_dist2 = (centerx1 - centerx2) ** 2 + (centery1 - centery2) ** 2\n-\n-    ct_dist = np.sqrt(ct_dist2)\n-\n-    # The linear rescaling is a naive version and needs more study\n-    ct_dist = ct_dist / ct_dist.max()\n-    return ct_dist.max() - ct_dist  # resize to (0,1)\n-\n-\n def speed_direction_batch(dets, tracks):\n     tracks = tracks[..., np.newaxis]\n     CX1, CY1 = (dets[:, 0] + dets[:, 2]) / 2.0, (dets[:, 1] + dets[:, 3]) / 2.0\n",add,Added STORM - 295 to Changelog
fef905274aa8fa974f37b1820eb8f1e99c029575,fix imports,boxmot/trackers/deepocsort/deep_ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.association import (associate, associate_kitti, ct_dist,\n                                      linear_assignment)\nfrom boxmot.utils.iou import ciou_batch, diou_batch, giou_batch, iou_batch\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.association import (associate, associate_kitti,\n                                      linear_assignment)\nfrom boxmot.utils.iou import get_asso_func\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x","@@ -9,9 +9,9 @@ from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n from boxmot.motion.cmc import get_cmc_method\n from boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\n from boxmot.utils import PerClassDecorator\n-from boxmot.utils.association import (associate, associate_kitti, ct_dist,\n+from boxmot.utils.association import (associate, associate_kitti,\n                                       linear_assignment)\n-from boxmot.utils.iou import ciou_batch, diou_batch, giou_batch, iou_batch\n+from boxmot.utils.iou import get_asso_func\n \n \n def k_previous_obs(observations, cur_age, k):\n@@ -299,21 +299,6 @@ class KalmanBoxTracker(object):\n         return self.kf.md_for_measurement(self.bbox_to_z_func(bbox))\n \n \n-""""""""""""\n-    We support multiple ways for association cost calculation, by default\n-    we use IoU. GIoU may have better performance in some situations. We note\n-    that we hardly normalize the cost by all methods to (0,1) which may not be\n-    the best practice.\n-""""""""""""\n-ASSO_FUNCS = {\n-    """"iou"""": iou_batch,\n-    """"giou"""": giou_batch,\n-    """"ciou"""": ciou_batch,\n-    """"diou"""": diou_batch,\n-    """"ct_dist"""": ct_dist,\n-}\n-\n-\n class DeepOCSort(object):\n     def __init__(\n         self,\n@@ -347,7 +332,7 @@ class DeepOCSort(object):\n         self.frame_count = 0\n         self.det_thresh = det_thresh\n         self.delta_t = delta_t\n-        self.asso_func = ASSO_FUNCS[asso_func]\n+        self.asso_func = get_asso_func(asso_func)\n         self.inertia = inertia\n         self.w_association_emb = w_association_emb\n         self.alpha_fixed_emb = alpha_fixed_emb\n",add,Added STORM - 264 to changelog
fef905274aa8fa974f37b1820eb8f1e99c029575,fix imports,boxmot/trackers/ocsort/ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils.association import associate, ct_dist, linear_assignment\nfrom boxmot.utils.iou import (ciou_batch, diou_batch, get_asso_func,\n                              giou_batch, iou_batch)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h =","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    s = w * h  ","@@ -4,9 +4,8 @@\n import numpy as np\n \n from boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\n-from boxmot.utils.association import associate, ct_dist, linear_assignment\n-from boxmot.utils.iou import (ciou_batch, diou_batch, get_asso_func,\n-                              giou_batch, iou_batch)\n+from boxmot.utils.association import associate, linear_assignment\n+from boxmot.utils.iou import get_asso_func\n \n \n def k_previous_obs(observations, cur_age, k):\n@@ -184,21 +183,6 @@ class KalmanBoxTracker(object):\n         return convert_x_to_bbox(self.kf.x)\n \n \n-""""""""""""\n-    We support multiple ways for association cost calculation, by default\n-    we use IoU. GIoU may have better performance in some situations. We note\n-    that we hardly normalize the cost by all methods to (0,1) which may not be\n-    the best practice.\n-""""""""""""\n-ASSO_FUNCS = {\n-    """"iou"""": iou_batch,\n-    """"giou"""": giou_batch,\n-    """"ciou"""": ciou_batch,\n-    """"diou"""": diou_batch,\n-    """"ct_dist"""": ct_dist,\n-}\n-\n-\n class OCSort(object):\n     def __init__(\n         self,\n",add,Added the UNSTARTED state to the YouTube PlayerState enum
21f3af4fb47a4ed8987053ed7e71bf17b97b18d2,fix import,boxmot/trackers/strongsort/sort/track.py,"# vim: expandtab:ts=4:sw=4\nfrom collections import deque\n\nimport numpy as np\n\nfrom ....motion.adapters import StrongSortKalmanFilterAdapter\n\n\nclass TrackState:\n    """"""""""""\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n\n    """"""""""""\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    """"""""""""\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initial state distrib","# vim: expandtab:ts=4:sw=4\nfrom collections import deque\n\nimport numpy as np\n\nfrom ....motion.kalman_filters.adapters import StrongSortKalmanFilterAdapter\n\n\nclass TrackState:\n    """"""""""""\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n\n    """"""""""""\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    """"""""""""\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initia","@@ -3,7 +3,7 @@ from collections import deque\n \n import numpy as np\n \n-from ....motion.adapters import StrongSortKalmanFilterAdapter\n+from ....motion.kalman_filters.adapters import StrongSortKalmanFilterAdapter\n \n \n class TrackState:\n",add,Added hypest who apparently actually wrote the C # port
8fb5b44481119f68921fcc5ab57718488814d50f,fix,examples/detectors/yolo_interface.py,"import numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass YoloInterface:\n\n    def inference(self, im):\n        raise NotImplementedError('Subclasses must implement this method.')\n\n    def postprocess(self, preds):\n        raise NotImplementedError('Subclasses must implement this method.')\n\n    def filter_results(self, i, predictor):\n        if predictor.tracker_outputs[i].size != 0:\n            # filter boxes masks and pose results by tracking results\n            sorted_confs = predictor.tracker_outputs[i][:, 5].argsort()[::-1]\n            predictor.tracker_outputs[i] = predictor.tracker_outputs[i][sorted_confs]\n            yolo_confs = predictor.results[i].boxes.conf.cpu().numpy()\n            tracker_confs = predictor.tracker_outputs[i][:, 5]\n            mask = np.in1d(yolo_confs, tracker_confs)\n\n            if predictor.results[i].masks is not None:\n                predictor.results[i].masks = predictor.results[i].masks[","import numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass YoloInterface:\n\n    def inference(self, im):\n        raise NotImplementedError('Subclasses must implement this method.')\n\n    def postprocess(self, preds):\n        raise NotImplementedError('Subclasses must implement this method.')\n\n    def filter_results(self, i, predictor):\n        if predictor.tracker_outputs[i].size != 0:\n            # filter boxes masks and pose results by tracking results\n            sorted_confs = predictor.tracker_outputs[i][:, 5].argsort()[::-1]\n            predictor.tracker_outputs[i] = predictor.tracker_outputs[i][sorted_confs]\n            yolo_confs = predictor.results[i].boxes.conf.cpu().numpy()\n            tracker_confs = predictor.tracker_outputs[i][:, 5]\n            mask = np.in1d(yolo_confs, tracker_confs)\n\n            if predictor.results[i].masks is not None:\n                predictor.results[i].masks = predictor.results[i].masks[","@@ -55,7 +55,8 @@ class YoloInterface:\n         preds[:, [0, 2]] = preds[:, [0, 2]] * self.w_r\n         preds[:, [1, 3]] = preds[:, [1, 3]] * self.h_r\n \n-        preds = torch.from_numpy(preds)\n+        if not isinstance(preds, (torch.Tensor)):\n+            preds = torch.from_numpy(preds)\n \n         preds[:, [0, 2]] = torch.clip(preds[:, [0, 2]], min=0, max=im_w)\n         preds[:, [1, 3]] = torch.clip(preds[:, [1, 3]], min=0, max=im_h)\n",fix,Don ' t define the dependency on real_commad twice
b06a5a7e8cab70e72b469e7f420cfc2a010568bd,fix,boxmot/postprocessing/gsi.py,"from pathlib import Path\n\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor as GPR\nfrom sklearn.gaussian_process.kernels import RBF\n\nfrom boxmot.utils import logger as LOGGER\n\n\ndef linear_interpolation(input_, interval):\n    input_ = input_[np.lexsort([input_[:, 0], input_[:, 1]])]\n    output_ = input_.copy()\n\n    id_pre, f_pre, row_pre = -1, -1, np.zeros((10,))\n    for row in input_:\n        f_curr, id_curr = row[:2].astype(int)\n        if id_curr == id_pre:\n            if f_pre + 1 < f_curr < f_pre + interval:\n                for i, f in enumerate(range(f_pre + 1, f_curr), start=1):\n                    step = (row - row_pre) / (f_curr - f_pre) * i\n                    row_new = row_pre + step\n                    output_ = np.append(output_, row_new[np.newaxis, :], axis=0)\n        else:\n            id_pre = id_curr\n        row_pre = row\n        f_pre = f_curr\n    output_ = output_[np.lexsort([output_[:, 0], output_[:, 1]])]\n    ","from pathlib import Path\n\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor as GPR\nfrom sklearn.gaussian_process.kernels import RBF\n\nfrom boxmot.utils import logger as LOGGER\n\n\ndef linear_interpolation(input_, interval):\n    input_ = input_[np.lexsort([input_[:, 0], input_[:, 1]])]\n    output_ = input_.copy()\n\n    id_pre, f_pre, row_pre = -1, -1, np.zeros((10,))\n    for row in input_:\n        f_curr, id_curr = row[:2].astype(int)\n        if id_curr == id_pre:\n            if f_pre + 1 < f_curr < f_pre + interval:\n                for i, f in enumerate(range(f_pre + 1, f_curr), start=1):\n                    step = (row - row_pre) / (f_curr - f_pre) * i\n                    row_new = row_pre + step\n                    output_ = np.append(output_, row_new[np.newaxis, :], axis=0)\n        else:\n            id_pre = id_curr\n        row_pre = row\n        f_pre = f_curr\n    output_ = output_[np.lexsort([output_[:, 0], output_[:, 1]])]\n    ","@@ -70,4 +70,9 @@ def gsi(mot_results_folder=Path('examples/runs/val/exp87/labels'), interval=20,\n             print('No tracking result in {p}. Skipping...')\n \n \n-gsi()\n+def main():\n+    gsi()\n+\n+\n+if __name__ == """"__main__"""":\n+    main()\n",add,Add note about data volume to enable_metrics_collection
8aab44d9f3cdba3882c97b1510aa68d0464bb761,comment out debug msgs,boxmot/appearance/backbones/clip/make_model.py,"import torch\nimport torch.nn as nn\n\nfrom .clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n\n_tokenizer = _Tokenizer()\nfrom timm.models.layers import trunc_normal_\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\n        nn.init.constant_(m.bias, 0.0)\n\n    elif classname.find('Conv') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n    elif classname.find('BatchNorm') != -1:\n        if m.affine:\n            nn.init.constant_(m.weight, 1.0)\n            nn.init.constant_(m.bias, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.normal_(m.weight, std=0.001)\n        if m.bias:\n            nn.init.constant_(m.bias, 0.0)\n\n\nclass build_transformer(nn.Module):\n","import torch\nimport torch.nn as nn\n\nfrom .clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n\n_tokenizer = _Tokenizer()\nfrom timm.models.layers import trunc_normal_\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\n        nn.init.constant_(m.bias, 0.0)\n\n    elif classname.find('Conv') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n    elif classname.find('BatchNorm') != -1:\n        if m.affine:\n            nn.init.constant_(m.weight, 1.0)\n            nn.init.constant_(m.bias, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.normal_(m.weight, std=0.001)\n        if m.bias:\n            nn.init.constant_(m.bias, 0.0)\n\n\nclass build_transformer(nn.Module):\n","@@ -124,18 +124,16 @@ class build_transformer(nn.Module):\n                 return torch.cat([img_feature, img_feature_proj], dim=1)\n \n     def load_param(self, trained_path):\n-        print(f'loading\n\n\n{trained_path}')\n         param_dict = torch.load('/home/mikel.brostrom/CLIP-ReID/Market1501_clipreid_ViT-B-16_60.pth')\n         for i in self.state_dict():\n-            print(i)\n             self.state_dict()[i.replace('module.', '')].copy_(param_dict[i])\n-        print('Loading pretrained model from {}'.format('/home/mikel.brostrom/yolo_tracking/clip_market1501.pt'))\n+        # print('Loading pretrained model from {}'.format('/home/mikel.brostrom/yolo_tracking/clip_market1501.pt'))\n \n     def load_param_finetune(self, model_path):\n         param_dict = torch.load(model_path)\n         for i in param_dict:\n             self.state_dict()[i].copy_(param_dict[i])\n-        print('Loading pretrained model for finetuning from {}'.format(model_path))\n+        # print('Loading pretrained model for finetuning from {}'.format(model_path))\n \n \n def make_model(cfg, num_class, camera_num, view_num):\n",add,Add note about data volume to enable_metrics_collection
8aab44d9f3cdba3882c97b1510aa68d0464bb761,comment out debug msgs,boxmot/appearance/reid_model_factory.py,"import sys\nimport time\nfrom collections import OrderedDict\n\nimport torch\n\nfrom boxmot.utils import logger as LOGGER\n\n__model_types = [\n    """"resnet50"""",\n    """"resnet101"""",\n    """"mlfn"""",\n    """"hacnn"""",\n    """"mobilenetv2_x1_0"""",\n    """"mobilenetv2_x1_4"""",\n    """"osnet_x1_0"""",\n    """"osnet_x0_75"""",\n    """"osnet_x0_5"""",\n    """"osnet_x0_25"""",\n    """"osnet_ibn_x1_0"""",\n    """"osnet_ain_x1_0"""",\n    """"lmbn_n"""",\n    """"clip"""",\n]\n\nlmbn_loc = 'https://github.com/mikel-brostrom/yolov8_tracking/releases/download/v9.0/'\n\n__trained_urls = {\n    # resnet50\n    """"resnet50_market1501.pt"""": """"https://drive.google.com/uc?id=1dUUZ4rHDWohmsQXCRe2C_HbYkzz94iBV"""",\n    """"resnet50_dukemtmcreid.pt"""": """"https://drive.google.com/uc?id=17ymnLglnc64NRvGOitY3BqMRS9UWd1wg"""",\n    """"resnet50_msmt17.pt"""": """"https://drive.google.com/uc?id=1ep7RypVDOthCRIAqDnn4_N-UhkkFHJsj"""",\n    """"resnet50_fc512_market1501.pt"""": """"https://drive.google.com/uc?id=1kv8l5laX_YCdIGVCetjlNdzKIA3NvsSt"""",\n    """"resnet50_fc","import sys\nimport time\nfrom collections import OrderedDict\n\nimport torch\n\nfrom boxmot.utils import logger as LOGGER\n\n__model_types = [\n    """"resnet50"""",\n    """"resnet101"""",\n    """"mlfn"""",\n    """"hacnn"""",\n    """"mobilenetv2_x1_0"""",\n    """"mobilenetv2_x1_4"""",\n    """"osnet_x1_0"""",\n    """"osnet_x0_75"""",\n    """"osnet_x0_5"""",\n    """"osnet_x0_25"""",\n    """"osnet_ibn_x1_0"""",\n    """"osnet_ain_x1_0"""",\n    """"lmbn_n"""",\n    """"clip"""",\n]\n\nlmbn_loc = 'https://github.com/mikel-brostrom/yolov8_tracking/releases/download/v9.0/'\n\n__trained_urls = {\n    # resnet50\n    """"resnet50_market1501.pt"""": """"https://drive.google.com/uc?id=1dUUZ4rHDWohmsQXCRe2C_HbYkzz94iBV"""",\n    """"resnet50_dukemtmcreid.pt"""": """"https://drive.google.com/uc?id=17ymnLglnc64NRvGOitY3BqMRS9UWd1wg"""",\n    """"resnet50_msmt17.pt"""": """"https://drive.google.com/uc?id=1ep7RypVDOthCRIAqDnn4_N-UhkkFHJsj"""",\n    """"resnet50_fc512_market1501.pt"""": """"https://drive.google.com/uc?id=1kv8l5laX_YCdIGVCetjlNdzKIA3NvsSt"""",\n    """"resnet50_fc","@@ -170,11 +170,14 @@ def load_pretrained_weights(model, weight_path):\n         def forward_override(self, x: torch.Tensor, cv_emb=None, old_forward=None):\n             _, image_features, image_features_proj = old_forward(x, cv_emb)\n             return torch.cat([image_features[:, 0], image_features_proj[:, 0]], dim=1)\n-        print('model.load_param(str(weight_path))', str(weight_path))\n+        # print('model.load_param(str(weight_path))', str(weight_path))\n         model.load_param(str(weight_path))\n         model = model.image_encoder\n         # old_forward = model.forward\n         # model.forward = lambda *args, **kwargs: forward_override(model, old_forward=old_forward, *args, **kwargs)\n+        LOGGER.success(\n+            f'Successfully loaded pretrained weights from """"{weight_path}""""'\n+        )\n     else:\n         new_state_dict = OrderedDict()\n         matched_layers, discarded_layers = [], []\n",add,Add the user about a failed delivery the region .
c86da7446b2babc0aa6de42874277c3cabeb3a44,hotfix,boxmot/postprocessing/__init__.py,,,,add,Added TypeSpec . xml
3dd4023f9e1d437f246a370960f5691b6c3cd02f,fix,boxmot/appearance/backbones/clip/clip/bpe_simple_vocab_16e6.txt,"""""bpe_simple_vocab_16e6.txt#version: 0.2\ni n\nt h\na n\nr e\na r\ne r\nth e</w>\nin g</w>\no u\no n\ns t\no r\ne n\no n</w>\na l\na t\ne r</w>\ni t\ni n</w>\nt o</w>\nr o\ni s</w>\nl e\ni c\na t</w>\nan d</w>\ne d</w>\no f</w>\nc h\no r</w>\ne s</w>\ni l\ne l\ns t</w>\na c\no m\na m\nl o\na n</w>\na y</w>\ns h\nr i\nl i\nt i\nf or</w>\nn e\nÃ° Å\nr a\nh a\nd e\no l\nv e</w>\ns i\nu r\na l</w>\ns e\n' s</w>\nu n\nd i\nb e\nl a\nw h\no o\nd ay</w>\ne n</w>\nm a\nn o\nl e</w>\nt o\nou r</w>\ni r\ng h\nw it\ni t</w>\ny o\na s\ns p\nth is</w>\nt s</w>\nat i\nyo u</w>\nwit h</w>\na d\ni s\na b\nl y</w>\nw e\nth e\nt e\na s</w>\na g\nv i\np p\ns u\nh o\nm y</w>\n. .\nb u\nc om\ns e</w>\ner s</w>\nm e\nm e</w>\nal l</w>\nc on\nm o\nk e</w>\ng e\nou t</w>\nen t</w>\nc o\nf e\nv er\na r</w>\nf ro\na u\np o\nc e</w>\ngh t</w>\nar e</w>\ns s</w>\nfro m</w>\nc h</w>\nt r\nou n\non e</w>\nb y</w>\nd o\nt h</w>\nw or\ner e</w>\nk e\np ro\nf or\nd s</w>\nb o\nt a\nw e</w>\ng o\nh e\nt er</w>\nin g\nd ",,"@@ -1,262145 +0,0 @@\n-""""bpe_simple_vocab_16e6.txt#version: 0.2\n-i n\n-t h\n-a n\n-r e\n-a r\n-e r\n-th e</w>\n-in g</w>\n-o u\n-o n\n-s t\n-o r\n-e n\n-o n</w>\n-a l\n-a t\n-e r</w>\n-i t\n-i n</w>\n-t o</w>\n-r o\n-i s</w>\n-l e\n-i c\n-a t</w>\n-an d</w>\n-e d</w>\n-o f</w>\n-c h\n-o r</w>\n-e s</w>\n-i l\n-e l\n-s t</w>\n-a c\n-o m\n-a m\n-l o\n-a n</w>\n-a y</w>\n-s h\n-r i\n-l i\n-t i\n-f or</w>\n-n e\n-Ã° Å\n-r a\n-h a\n-d e\n-o l\n-v e</w>\n-s i\n-u r\n-a l</w>\n-s e\n-' s</w>\n-u n\n-d i\n-b e\n-l a\n-w h\n-o o\n-d ay</w>\n-e n</w>\n-m a\n-n o\n-l e</w>\n-t o\n-ou r</w>\n-i r\n-g h\n-w it\n-i t</w>\n-y o\n-a s\n-s p\n-th is</w>\n-t s</w>\n-at i\n-yo u</w>\n-wit h</w>\n-a d\n-i s\n-a b\n-l y</w>\n-w e\n-th e\n-t e\n-a s</w>\n-a g\n-v i\n-p p\n-s u\n-h o\n-m y</w>\n-. .\n-b u\n-c om\n-s e</w>\n-er s</w>\n-m e\n-m e</w>\n-al l</w>\n-c on\n-m o\n-k e</w>\n-g e\n-ou t</w>\n-en t</w>\n-c o\n-f e\n-v er\n-a r</w>\n-f ro\n-a u\n-p o\n-c e</w>\n-gh t</w>\n-ar e</w>\n-s s</w>\n-fro m</w>\n-c h</w>\n-t r\n-ou n\n-on e</w>\n-b y</w>\n-d o\n-t h</w>\n-w or\n-er e</w>\n-k e\n-p ro\n-f or\n-d s</w>\n-b o\n-t a\n-w e</w>\n-g o\n-h e\n-t er</w>\n-in g\n-d e</w>\n-b e</w>\n-ati on</w>\n-m or\n-a y\n-e x\n-il l</w>\n-p e\n-k s</w>\n-s c\n-l u\n-f u\n-q u\n-v er</w>\n-Ã°Å Äº\n-j u\n-m u\n-at e</w>\n-an d\n-v e\n-k ing</w>\n-m ar\n-o p\n-h i\n-.. .</w>\n-p re\n-a d</w>\n-r u\n-th at</w>\n-j o\n-o f\n-c e\n-ne w</w>\n-a m</w>\n-a p\n-g re\n-s s\n-d u\n-no w</w>\n-y e\n-t ing</w>\n-y our</w>\n-it y</w>\n-n i\n-c i\n-p ar\n-g u\n-f i\n-a f\n-p er\n-t er\n-u p</w>\n-s o</w>\n-g i\n-on s</w>\n-g r\n-g e</w>\n-b r\n-p l\n-' t</w>\n-m i\n-in e</w>\n-we e\n-b i\n-u s</w>\n-sh o\n-ha ve</w>\n-to day</w>\n-a v\n-m an\n-en t\n-ac k</w>\n-ur e</w>\n-ou r\n-Ã¢ Ä¢\n-c u\n-l d</w>\n-lo o\n-i m\n-ic e</w>\n-s om\n-f in\n-re d</w>\n-re n\n-oo d</w>\n-w as</w>\n-ti on</w>\n-p i\n-i r</w>\n-th er</w>\n-t y</w>\n-p h\n-ar d</w>\n-e c\n-! !</w>\n-m on\n-mor e</w>\n-w ill</w>\n-t ra\n-c an</w>\n-c ol\n-",add,Added the shutdown command to the service provider
3dd4023f9e1d437f246a370960f5691b6c3cd02f,fix,boxmot/appearance/backbones/clip/clip/simple_tokenizer.py,"import gzip\nimport html\nimport os\nfrom functools import lru_cache\n\nimport ftfy\nimport regex as re\n\n\n@lru_cache()\ndef default_bpe():\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), """"bpe_simple_vocab_16e6.txt.gz"""")\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""""""""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""""""""\n    bs = list(range(ord(""""!""""), ord(""""~"""") + 1)) + list(range(ord(""""Â¡""""), ord(""""Â¬"""") + 1)) + list(range(ord(""""Â®""""), ","import gzip\nimport html\nfrom functools import lru_cache\n\nimport ftfy\nimport regex as re\n\nfrom boxmot.utils import ROOT\n\n\n@lru_cache()\ndef default_bpe():\n    return ROOT / """"boxmot/appearance/backbones/clip/clip/bpe_simple_vocab_16e6.txt""""\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""""""""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""""""""\n    bs = list(range(ord(""""!""""), ord(""""~"""") + 1)) + list(range(ord(""""Â¡""""), ord(""""Â¬"""") + 1)) + list(range(ord(""""","@@ -1,15 +1,16 @@\n import gzip\n import html\n-import os\n from functools import lru_cache\n \n import ftfy\n import regex as re\n \n+from boxmot.utils import ROOT\n+\n \n @lru_cache()\n def default_bpe():\n-    return os.path.join(os.path.dirname(os.path.abspath(__file__)), """"bpe_simple_vocab_16e6.txt.gz"""")\n+    return ROOT / """"boxmot/appearance/backbones/clip/clip/bpe_simple_vocab_16e6.txt""""\n \n \n @lru_cache()\n",fix,Add note about data volume to enable EGL
686d9b845e020500e4a650d069d5cb6658296b4a,"Revert ""Clip reid fix""",.github/workflows/ci.yml,"# name of the workflow, what it is doing (optional)\nname: CI CPU testing\n\n# events that trigger the workflow (required)\non:\n  push:\n    branches: [master, CIdebug]\n  pull_request:\n    # pull request where master is target\n    branches: [master]\n\nenv:\n  # Directory of PyPi package to be tested\n  PACKAGE_DIR: boxmot\n  # Minimum acceptable test coverage\n  # Increase as you add more tests to increase coverage\n  COVERAGE_FAIL_UNDER: 29\n\n# the workflow that gets triggerd\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]   # skip windows-latest for\n        python-version: ['3.8', '3.9', '3.10']\n        #model: ['yolov8n', 'yolo_nas_s', yolox_n]  # yolo models to test\n        #tracking-methods: ['deepocsort', 'ocsort', 'botsort', 'strongsort', 'bytetrack']  # tracking methods to  test\n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n\n      -","# name of the workflow, what it is doing (optional)\nname: CI CPU testing\n\n# events that trigger the workflow (required)\non:\n  push:\n    branches: [master, CIdebug]\n  pull_request:\n    # pull request where master is target\n    branches: [master]\n\nenv:\n  # Directory of PyPi package to be tested\n  PACKAGE_DIR: boxmot\n  # Minimum acceptable test coverage\n  # Increase as you add more tests to increase coverage\n  COVERAGE_FAIL_UNDER: 29\n\n# the workflow that gets triggerd\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]   # skip windows-latest for\n        python-version: ['3.8', '3.9', '3.10']\n        #model: ['yolov8n', 'yolo_nas_s', yolox_n]  # yolo models to test\n        #tracking-methods: ['deepocsort', 'ocsort', 'botsort', 'strongsort', 'bytetrack']  # tracking methods to  test\n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n\n      -","@@ -58,7 +58,7 @@ jobs:\n           IMG: ./assets/MOT17-mini/train/MOT17-05-FRCNN/img1/000001.jpg\n         run: |\n           # deepocsort fro all supported yolo models\n-          python examples/track.py --tracking-method deepocsort --source $IMG --imgsz 320 --tracking-model clip_market1501.pt\n+          python examples/track.py --tracking-method deepocsort --source $IMG --imgsz 320\n           python examples/track.py --yolo-model yolo_nas_s --tracking-method deepocsort --source $IMG --imgsz 320\n           python examples/track.py --yolo-model yolox_n --tracking-method deepocsort --source $IMG --imgsz 320\n \n",add,Add note about data volume to enable
686d9b845e020500e4a650d069d5cb6658296b4a,"Revert ""Clip reid fix""",boxmot/appearance/backbones/clip/clip/bpe_simple_vocab_16e6.txt.gz,," F[_ tÉ²,q$p(P\n^O >)}qUOHt0I@RT~Ô¾]-í¶~\G._xe/nsVÇ?pé§¢a.[?SWoo_b?b?}S_S_Z\\mm{b3^nk0Z'{?./7_c2G?7|o+~GÊ?ozrCmA.+^:gc}Ý7/={>l3<á°[7/}fz*7:ldSFrEO*.b?fKbmÅW=Yf	vv}H]mu0|QV|7i4OrE	{NuIG|[&O*ì½½/Î{Æ¿_zbqW>+>f8lÛ¸1~tm`\Y_^U|y+XO?s7ßµ%;.?[H/d_8x>W^lvG×´Fm5-j6%{wgkR*:j.sgXR=Um=G?l3Ò3x8qv*&x×³(ZX~q|X5\}V-?,0l3Ø=~PbcËÑ®Ã£à¦8-J7'W_	pU)zu;kuqHØ­:vmwÓvoY.+ev3JËVvO?(]s^_C;Oe9vI	må¢¶G-huë±ÊnmÞ±0lÃ§34	>?Ð¦itM;;}OWOPCn=lv$A.`y/OL[)-UY|cc`r8vX&7G;a`×.'?7}Ï»CaW*>;n#paeæ¹5kq=`o`}cYÊª(V7:tV>WsooÅ}\nLfsx>}i:v&Awk@|UDG_zi;- nNwc36Nv-[Ú²yFpá¥³3(8}V'*N00^UK{yN;2Zml.[|30vd`2=homJ/wnN~m,%Ì¸Ó?1NS\0_<.TP5*VÈºMy?yoRGr_p)|mÚ¢xhil7K'wab[K2=nnØ¹001hdÖvXL=VOvÚ#+\nO}Fd5ib^p%w|;u|:Ü~{1:O7!9}d7UpÅ´""""iw_++]@sq'qb6n,+hCq"""" pB5vtfv60|z2Ç5IP`~U|_i@ÔgoGyU%<	!B=L9GW%Ó²)p2=~%EÝJX3v,hKYn\V""""%Nyh[Aå¤DW	.Ø2JWÎ·2;&xwwpl8mU;ZR'wW8`KQiìµS~tr2_vJ{f3!r})+K2rv9:Í´}""""o(#!%g",Binary files /dev/null and b/boxmot/appearance/backbones/clip/clip/bpe_simple_vocab_16e6.txt.gz differ\n,add,Added Type name for DFI ( # 18480 )
686d9b845e020500e4a650d069d5cb6658296b4a,"Revert ""Clip reid fix""",boxmot/appearance/backbones/clip/clip/simple_tokenizer.py,"import gzip\nimport html\nfrom functools import lru_cache\n\nimport ftfy\nimport regex as re\n\nfrom boxmot.utils import ROOT\n\n\n@lru_cache()\ndef default_bpe():\n    return ROOT / """"boxmot/appearance/backbones/clip/clip/bpe_simple_vocab_16e6.txt""""\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""""""""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""""""""\n    bs = list(range(ord(""""!""""), ord(""""~"""") + 1)) + list(range(ord(""""Â¡""""), ord(""""Â¬"""") + 1)) + list(range(ord(""""","import gzip\nimport html\nimport os\nfrom functools import lru_cache\n\nimport ftfy\nimport regex as re\n\n\n@lru_cache()\ndef default_bpe():\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), """"bpe_simple_vocab_16e6.txt.gz"""")\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""""""""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""""""""\n    bs = list(range(ord(""""!""""), ord(""""~"""") + 1)) + list(range(ord(""""Â¡""""), ord(""""Â¬"""") + 1)) + list(range(ord(""""Â®""""), ","@@ -1,16 +1,15 @@\n import gzip\n import html\n+import os\n from functools import lru_cache\n \n import ftfy\n import regex as re\n \n-from boxmot.utils import ROOT\n-\n \n @lru_cache()\n def default_bpe():\n-    return ROOT / """"boxmot/appearance/backbones/clip/clip/bpe_simple_vocab_16e6.txt""""\n+    return os.path.join(os.path.dirname(os.path.abspath(__file__)), """"bpe_simple_vocab_16e6.txt.gz"""")\n \n \n @lru_cache()\n",fix,Add KHR_gl_texture_2D_image extension string .
18fb731aff927484d47b7c09880147916f9aaf86,fix path,boxmot/appearance/backbones/clip/clip/simple_tokenizer.py,"import gzip\nimport html\nfrom functools import lru_cache\n\nimport ftfy\nimport regex as re\n\nfrom boxmot.utils import ROOT\n\n\n@lru_cache()\ndef default_bpe():\n    return ROOT / """"appearance/backbones/clip/clip/bpe_simple_vocab_16e6.txt.gz""""\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""""""""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""""""""\n    bs = list(range(ord(""""!""""), ord(""""~"""") + 1)) + list(range(ord(""""Â¡""""), ord(""""Â¬"""") + 1)) + list(range(ord(""""Â®"""")","import gzip\nimport html\nfrom functools import lru_cache\n\nimport ftfy\nimport regex as re\n\nfrom boxmot.utils import BOXMOT\n\n\n@lru_cache()\ndef default_bpe():\n    return BOXMOT / """"appearance/backbones/clip/clip/bpe_simple_vocab_16e6.txt.gz""""\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""""""""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""""""""\n    bs = list(range(ord(""""!""""), ord(""""~"""") + 1)) + list(range(ord(""""Â¡""""), ord(""""Â¬"""") + 1)) + list(range(ord(""""","@@ -5,12 +5,12 @@ from functools import lru_cache\n import ftfy\n import regex as re\n \n-from boxmot.utils import ROOT\n+from boxmot.utils import BOXMOT\n \n \n @lru_cache()\n def default_bpe():\n-    return ROOT / """"appearance/backbones/clip/clip/bpe_simple_vocab_16e6.txt.gz""""\n+    return BOXMOT / """"appearance/backbones/clip/clip/bpe_simple_vocab_16e6.txt.gz""""\n \n \n @lru_cache()\n",add,Added TypeSpec . h for layers .
e3d200c7e8d72f67fafdad8d6ed9380d8aee38d0,fix gsi testÃ¤,tests/test_python.py,"# pytest tests/test_python.py\n\nfrom pathlib import Path\n\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom boxmot import (OCSORT, BoTSORT, BYTETracker, DeepOCSORT, StrongSORT,\n                    create_tracker, get_tracker_config)\nfrom boxmot.postprocessing import gaussian_smooth, linear_interpolation\nfrom boxmot.utils import WEIGHTS\n\n\ndef test_strongsort_instantiation():\n    StrongSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\ndef test_botsort_instantiation():\n    BoTSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\ndef test_deepocsort_instantiation():\n    DeepOCSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n        per_class=False\n    )\n\n\ndef test_ocsort_instantiation():\n    OCSORT(\n        per_class=False\n    )\n\n\ndef test_b","# pytest tests/test_python.py\n\nfrom pathlib import Path\n\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom boxmot import (OCSORT, BoTSORT, BYTETracker, DeepOCSORT, StrongSORT,\n                    create_tracker, get_tracker_config)\nfrom boxmot.postprocessing.gsi import gaussian_smooth, linear_interpolation\nfrom boxmot.utils import WEIGHTS\n\n\ndef test_strongsort_instantiation():\n    StrongSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\ndef test_botsort_instantiation():\n    BoTSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\ndef test_deepocsort_instantiation():\n    DeepOCSORT(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n        per_class=False\n    )\n\n\ndef test_ocsort_instantiation():\n    OCSORT(\n        per_class=False\n    )\n\n\ndef te","@@ -7,7 +7,7 @@ from numpy.testing import assert_allclose\n \n from boxmot import (OCSORT, BoTSORT, BYTETracker, DeepOCSORT, StrongSORT,\n                     create_tracker, get_tracker_config)\n-from boxmot.postprocessing import gaussian_smooth, linear_interpolation\n+from boxmot.postprocessing.gsi import gaussian_smooth, linear_interpolation\n from boxmot.utils import WEIGHTS\n \n \n",add,Add Rossen
dddf3df1af2d12c5321f9b3c6de9e91329e042c6,fix model paths,.github/workflows/ci.yml,"# name of the workflow, what it is doing (optional)\nname: CI CPU testing\n\n# events that trigger the workflow (required)\non:\n  push:\n    branches: [master, CIdebug]\n  pull_request:\n    # pull request where master is target\n    branches: [master]\n\nenv:\n  # Directory of PyPi package to be tested\n  PACKAGE_DIR: boxmot\n  # Minimum acceptable test coverage\n  # Increase as you add more tests to increase coverage\n  COVERAGE_FAIL_UNDER: 29\n\n# the workflow that gets triggerd\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]   # skip windows-latest for\n        python-version: ['3.8', '3.9', '3.10']\n        #model: ['yolov8n', 'yolo_nas_s', yolox_n]  # yolo models to test\n        #tracking-methods: ['deepocsort', 'ocsort', 'botsort', 'strongsort', 'bytetrack']  # tracking methods to  test\n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n\n      -","# name of the workflow, what it is doing (optional)\nname: CI CPU testing\n\n# events that trigger the workflow (required)\non:\n  push:\n    branches: [master, CIdebug]\n  pull_request:\n    # pull request where master is target\n    branches: [master]\n\nenv:\n  # Directory of PyPi package to be tested\n  PACKAGE_DIR: boxmot\n  # Minimum acceptable test coverage\n  # Increase as you add more tests to increase coverage\n  COVERAGE_FAIL_UNDER: 29\n\n# the workflow that gets triggerd\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]   # skip windows-latest for\n        python-version: ['3.8', '3.9', '3.10']\n        #model: ['yolov8n', 'yolo_nas_s', yolox_n]  # yolo models to test\n        #tracking-methods: ['deepocsort', 'ocsort', 'botsort', 'strongsort', 'bytetrack']  # tracking methods to  test\n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n\n      -","@@ -58,7 +58,7 @@ jobs:\n           IMG: ./assets/MOT17-mini/train/MOT17-05-FRCNN/img1/000001.jpg\n         run: |\n           # deepocsort fro all supported yolo models\n-          python examples/track.py --tracking-method deepocsort --source $IMG --imgsz 320 --reid-model clip_market1501.pt\n+          python examples/track.py --tracking-method deepocsort --source $IMG --imgsz 320 --reid-model examples/weights/clip_market1501.pt\n           python examples/track.py --yolo-model yolo_nas_s --tracking-method deepocsort --source $IMG --imgsz 320\n           python examples/track.py --yolo-model yolox_n --tracking-method deepocsort --source $IMG --imgsz 320\n \n",add,Add note about data volume to enable
dddf3df1af2d12c5321f9b3c6de9e91329e042c6,fix model paths,boxmot/appearance/backbones/clip/make_model.py,"import torch\nimport torch.nn as nn\n\nfrom .clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n\n_tokenizer = _Tokenizer()\nfrom timm.models.layers import trunc_normal_\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\n        nn.init.constant_(m.bias, 0.0)\n\n    elif classname.find('Conv') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n    elif classname.find('BatchNorm') != -1:\n        if m.affine:\n            nn.init.constant_(m.weight, 1.0)\n            nn.init.constant_(m.bias, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.normal_(m.weight, std=0.001)\n        if m.bias:\n            nn.init.constant_(m.bias, 0.0)\n\n\nclass build_transformer(nn.Module):\n","import torch\nimport torch.nn as nn\n\nfrom .clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n\n_tokenizer = _Tokenizer()\nfrom timm.models.layers import trunc_normal_\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\n        nn.init.constant_(m.bias, 0.0)\n\n    elif classname.find('Conv') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n    elif classname.find('BatchNorm') != -1:\n        if m.affine:\n            nn.init.constant_(m.weight, 1.0)\n            nn.init.constant_(m.bias, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.normal_(m.weight, std=0.001)\n        if m.bias:\n            nn.init.constant_(m.bias, 0.0)\n\n\nclass build_transformer(nn.Module):\n","@@ -123,7 +123,7 @@ class build_transformer(nn.Module):\n                 return torch.cat([img_feature, img_feature_proj], dim=1)\n \n     def load_param(self, trained_path):\n-        param_dict = torch.load('/home/mikel.brostrom/CLIP-ReID/Market1501_clipreid_ViT-B-16_60.pth')\n+        param_dict = torch.load(trained_path)\n         for i in self.state_dict():\n             self.state_dict()[i.replace('module.', '')].copy_(param_dict[i])\n         # print('Loading pretrained model from {}'.format('/home/mikel.brostrom/yolo_tracking/clip_market1501.pt'))\n",fix,Add note about data volume to enable_metrics_collection
f6c9ef30c17b8f9d0ba0041631fce86330fee645,fix model paths,boxmot/appearance/backbones/clip/make_model.py,"import torch\nimport torch.nn as nn\n\nfrom .clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n\n_tokenizer = _Tokenizer()\nfrom timm.models.layers import trunc_normal_\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\n        nn.init.constant_(m.bias, 0.0)\n\n    elif classname.find('Conv') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n    elif classname.find('BatchNorm') != -1:\n        if m.affine:\n            nn.init.constant_(m.weight, 1.0)\n            nn.init.constant_(m.bias, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.normal_(m.weight, std=0.001)\n        if m.bias:\n            nn.init.constant_(m.bias, 0.0)\n\n\nclass build_transformer(nn.Module):\n","import torch\nimport torch.nn as nn\n\nfrom .clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n\n_tokenizer = _Tokenizer()\nfrom timm.models.layers import trunc_normal_\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\n        nn.init.constant_(m.bias, 0.0)\n\n    elif classname.find('Conv') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n    elif classname.find('BatchNorm') != -1:\n        if m.affine:\n            nn.init.constant_(m.weight, 1.0)\n            nn.init.constant_(m.bias, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.normal_(m.weight, std=0.001)\n        if m.bias:\n            nn.init.constant_(m.bias, 0.0)\n\n\nclass build_transformer(nn.Module):\n","@@ -123,7 +123,8 @@ class build_transformer(nn.Module):\n                 return torch.cat([img_feature, img_feature_proj], dim=1)\n \n     def load_param(self, trained_path):\n-        param_dict = torch.load(trained_path)\n+        print(trained_path)\n+        param_dict = torch.load(trained_path, map_location=torch.device(""""cpu""""))\n         for i in self.state_dict():\n             self.state_dict()[i.replace('module.', '')].copy_(param_dict[i])\n         # print('Loading pretrained model from {}'.format('/home/mikel.brostrom/yolo_tracking/clip_market1501.pt'))\n",fix,Add note about data volume to enable_metrics_collection
d8e951d30b14ae4988145095030c5aaf2e2f42e7,delete debug print,boxmot/appearance/backbones/clip/make_model.py,"import torch\nimport torch.nn as nn\n\nfrom .clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n\n_tokenizer = _Tokenizer()\nfrom timm.models.layers import trunc_normal_\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\n        nn.init.constant_(m.bias, 0.0)\n\n    elif classname.find('Conv') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n    elif classname.find('BatchNorm') != -1:\n        if m.affine:\n            nn.init.constant_(m.weight, 1.0)\n            nn.init.constant_(m.bias, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.normal_(m.weight, std=0.001)\n        if m.bias:\n            nn.init.constant_(m.bias, 0.0)\n\n\nclass build_transformer(nn.Module):\n","import torch\nimport torch.nn as nn\n\nfrom .clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n\n_tokenizer = _Tokenizer()\nfrom timm.models.layers import trunc_normal_\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\n        nn.init.constant_(m.bias, 0.0)\n\n    elif classname.find('Conv') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n    elif classname.find('BatchNorm') != -1:\n        if m.affine:\n            nn.init.constant_(m.weight, 1.0)\n            nn.init.constant_(m.bias, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.normal_(m.weight, std=0.001)\n        if m.bias:\n            nn.init.constant_(m.bias, 0.0)\n\n\nclass build_transformer(nn.Module):\n","@@ -123,7 +123,6 @@ class build_transformer(nn.Module):\n                 return torch.cat([img_feature, img_feature_proj], dim=1)\n \n     def load_param(self, trained_path):\n-        print(trained_path)\n         param_dict = torch.load(trained_path, map_location=torch.device(""""cpu""""))\n         for i in self.state_dict():\n             self.state_dict()[i.replace('module.', '')].copy_(param_dict[i])\n",add,Add note about data volume to enable_metrics_collection
ab9f16a782f12fe1594c6d320ab5c81681064d7e,fix https://github.com/mikel-brostrom/yolo_tracking/issues/1044\#issuecomment-1659640316,examples/detectors/yolo_interface.py,"import numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass YoloInterface:\n\n    def inference(self, im):\n        raise NotImplementedError('Subclasses must implement this method.')\n\n    def postprocess(self, preds):\n        raise NotImplementedError('Subclasses must implement this method.')\n\n    def filter_results(self, i, predictor):\n        if predictor.tracker_outputs[i].size != 0:\n            # filter boxes masks and pose results by tracking results\n            sorted_confs = predictor.tracker_outputs[i][:, 5].argsort()[::-1]\n            predictor.tracker_outputs[i] = predictor.tracker_outputs[i][sorted_confs]\n            yolo_confs = predictor.results[i].boxes.conf.cpu().numpy()\n            tracker_confs = predictor.tracker_outputs[i][:, 5]\n            mask = np.in1d(yolo_confs, tracker_confs)\n\n            if predictor.results[i].masks is not None:\n                predictor.results[i].masks = predictor.results[i].masks[","import numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass YoloInterface:\n\n    def inference(self, im):\n        raise NotImplementedError('Subclasses must implement this method.')\n\n    def postprocess(self, preds):\n        raise NotImplementedError('Subclasses must implement this method.')\n\n    def filter_results(self, i, predictor):\n        if predictor.tracker_outputs[i].size != 0:\n            # filter boxes masks and pose results by tracking results\n            sorted_confs = predictor.tracker_outputs[i][:, 5].argsort()[::-1]\n            predictor.tracker_outputs[i] = predictor.tracker_outputs[i][sorted_confs]\n            yolo_confs = predictor.results[i].boxes.conf.cpu().numpy()\n            tracker_confs = predictor.tracker_outputs[i][:, 5]\n            mask = np.in1d(yolo_confs, tracker_confs)\n\n            if predictor.results[i].masks is not None:\n                predictor.results[i].masks = predictor.results[i].masks[","@@ -58,8 +58,8 @@ class YoloInterface:\n         if not isinstance(preds, (torch.Tensor)):\n             preds = torch.from_numpy(preds)\n \n-        preds[:, [0, 2]] = torch.clip(preds[:, [0, 2]], min=0, max=im_w)\n-        preds[:, [1, 3]] = torch.clip(preds[:, [1, 3]], min=0, max=im_h)\n+        preds[:, [0, 2]] = torch.clip(preds[:, [0, 2]], min=0)  # max=im_w\n+        preds[:, [1, 3]] = torch.clip(preds[:, [1, 3]], min=0)  # max=im_h\n \n         return preds\n \n",fix,Add note about data volume to enable_metrics_collection
39a1481bb654537256bca0bb490e9daab296c35a,add error mesage for when inference mode cannot be infered from yolo model name,examples/detectors/__init__.py,"from boxmot.utils.checks import TestRequirements\n\ntr = TestRequirements()\n\n\ndef get_yolo_inferer(yolo_model):\n\n    if 'yolox' in str(yolo_model):\n        try:\n            import yolox  # for linear_assignment\n            assert yolox.__version__\n        except (ImportError, AssertionError, AttributeError):\n            tr.check_packages(('yolox==0.3.0',), cmds='--no-dependencies')\n            tr.check_packages(('tabulate',))  # needed dependency\n            tr.check_packages(('thop',))  # needed dependency\n        from .yolox import YoloXStrategy\n        return YoloXStrategy\n    elif 'yolov8' in str(yolo_model):\n        # ultralytics already installed when running track.py\n        from .yolov8 import Yolov8Strategy\n        return Yolov8Strategy\n    elif 'yolo_nas' in str(yolo_model):\n        try:\n            import super_gradients  # for linear_assignment\n            assert super_gradients.__version__\n        except (ImportError, AssertionError, AttributeError):","from boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\ntr = TestRequirements()\n\n\ndef get_yolo_inferer(yolo_model):\n\n    if 'yolox' in str(yolo_model):\n        try:\n            import yolox  # for linear_assignment\n            assert yolox.__version__\n        except (ImportError, AssertionError, AttributeError):\n            tr.check_packages(('yolox==0.3.0',), cmds='--no-dependencies')\n            tr.check_packages(('tabulate',))  # needed dependency\n            tr.check_packages(('thop',))  # needed dependency\n        from .yolox import YoloXStrategy\n        return YoloXStrategy\n    elif 'yolov8' in str(yolo_model):\n        # ultralytics already installed when running track.py\n        from .yolov8 import Yolov8Strategy\n        return Yolov8Strategy\n    elif 'yolo_nas' in str(yolo_model):\n        try:\n            import super_gradients  # for linear_assignment\n            assert super_gradients.__version__\n        except (Im","@@ -1,3 +1,4 @@\n+from boxmot.utils import logger as LOGGER\n from boxmot.utils.checks import TestRequirements\n \n tr = TestRequirements()\n@@ -27,3 +28,7 @@ def get_yolo_inferer(yolo_model):\n             tr.check_packages(('super-gradients==3.1.1',))  # install\n         from .yolonas import YoloNASStrategy\n         return YoloNASStrategy\n+    else:\n+        LOGGER.error('Failed to infer inference mode from yolo model name')\n+        LOGGER.error('Your model name has to contain either yolox, yolo_nas or yolov8')\n+        exit()\n",add,Added Type name for DFI ( # 3 )
4813714352bba3183933abf116253a619a2f0457,delete debug prints,boxmot/trackers/deepocsort/deep_ocsort.py,"""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.association import (associate, associate_kitti,\n                                      linear_assignment)\nfrom boxmot.utils.iou import get_asso_func\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x","""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the sc","@@ -9,8 +9,7 @@ from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n from boxmot.motion.cmc import get_cmc_method\n from boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\n from boxmot.utils import PerClassDecorator\n-from boxmot.utils.association import (associate, associate_kitti,\n-                                      linear_assignment)\n+from boxmot.utils.association import associate, linear_assignment\n from boxmot.utils.iou import get_asso_func\n \n \n@@ -358,7 +357,6 @@ class DeepOCSort(object):\n         Returns the a similar array, where the last column is the object ID.\n         NOTE: The number of objects returned may differ from the number of detections provided.\n         """"""""""""\n-\n         assert isinstance(dets, np.ndarray), f""""Unsupported 'dets' input type '{type(dets)}', valid format is np.ndarray""""\n         assert isinstance(img, np.ndarray), f""""Unsupported 'img' input type '{type(img)}', valid format is np.ndarray""""\n         assert len(dets.shape) == 2, """"Unsupported 'dets' dimensions, valid number of dimensions is two""""\n@@ -533,128 +531,3 @@ class DeepOCSort(object):\n             features = np.array([])\n \n         return features\n-\n-    def update_public(self, dets, cates, scores):\n-        self.frame_count += 1\n-\n-        det_scores = np.ones((dets.shape[0], 1))\n-        dets = np.concatenate((dets, det_scores), axis=1)\n-\n-        remain_inds = scores > self.det_thresh\n-\n-        cates = cates[remain_inds]\n-        dets = dets[remain_inds]\n-\n-        trks = np.zeros((len(self.trackers), 5))\n-        to_del = []\n-        ret = []\n-        for t, trk in enumerate(trks):\n-            pos = self.trackers[t].predict()[0]\n-            cat = self.trackers[t].cate\n-            trk[:] = [pos[0], pos[1], pos[2], pos[3], cat]\n-            if np.any(np.isnan(pos)):\n-                to_del.append(t)\n-        trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n-        for t in revers",add,Added the user group to the contributors
4813714352bba3183933abf116253a619a2f0457,delete debug prints,examples/detectors/yolo_interface.py,"import numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Boxes, Results\n\n\nclass YoloInterface:\n\n    def inference(self, im):\n        raise NotImplementedError('Subclasses must implement this method.')\n\n    def postprocess(self, preds):\n        raise NotImplementedError('Subclasses must implement this method.')\n\n    def filter_results(self, i, predictor):\n        if predictor.tracker_outputs[i].size != 0:\n            # filter boxes masks and pose results by tracking results\n            sorted_confs = predictor.tracker_outputs[i][:, 5].argsort()[::-1]\n            predictor.tracker_outputs[i] = predictor.tracker_outputs[i][sorted_confs]\n            yolo_confs = predictor.results[i].boxes.conf.cpu().numpy()\n            tracker_confs = predictor.tracker_outputs[i][:, 5]\n            mask = np.in1d(yolo_confs, tracker_confs)\n\n            if predictor.results[i].masks is not None:\n                predictor.results[i].masks = predictor.results[i].masks[","import numpy as np\nimport torch\nfrom ultralytics.yolo.engine.results import Results\n\n\nclass YoloInterface:\n\n    def inference(self, im):\n        raise NotImplementedError('Subclasses must implement this method.')\n\n    def postprocess(self, preds):\n        raise NotImplementedError('Subclasses must implement this method.')\n\n    def filter_results(self, i, predictor):\n        if predictor.tracker_outputs[i].size != 0:\n            # filter boxes masks and pose results by tracking results\n            sorted_confs = predictor.tracker_outputs[i][:, 5].argsort()[::-1]\n            predictor.tracker_outputs[i] = predictor.tracker_outputs[i][sorted_confs]\n            yolo_confs = predictor.results[i].boxes.conf.cpu().numpy()\n            tracker_confs = predictor.tracker_outputs[i][:, 5]\n            mask = np.in1d(yolo_confs, tracker_confs)\n\n            if predictor.results[i].masks is not None:\n                predictor.results[i].masks = predictor.results[i].masks[mask]\n","@@ -1,6 +1,6 @@\n import numpy as np\n import torch\n-from ultralytics.yolo.engine.results import Boxes, Results\n+from ultralytics.yolo.engine.results import Results\n \n \n class YoloInterface:\n@@ -29,22 +29,13 @@ class YoloInterface:\n         else:\n             pass\n \n-    def overwrite_results(self, i, im0_shape, predictor):\n-        # overwrite bbox results with tracker predictions\n-        if predictor.tracker_outputs[i].size != 0:\n-            predictor.results[i].boxes = Boxes(\n-                # xyxy, (track_id), conf, cls\n-                boxes=torch.from_numpy(predictor.tracker_outputs[i]).to(predictor.device),\n-                orig_shape=im0_shape,  # (height, width)\n-            )\n-\n-    def get_scaling_factors(self, im, im0s):\n+    def get_scaling_factors(self, im, im0):\n \n         # im to im0 factor for predictions\n-        im0_w = im0s[0].shape[1]\n-        im0_h = im0s[0].shape[0]\n-        im_w = im[0].shape[2]\n-        im_h = im[0].shape[1]\n+        im0_w = im0.shape[1]\n+        im0_h = im0.shape[0]\n+        im_w = im.shape[2]\n+        im_h = im.shape[1]\n         w_r = im0_w / im_w\n         h_r = im0_h / im_h\n \n@@ -63,11 +54,10 @@ class YoloInterface:\n \n         return preds\n \n-    def preds_to_yolov8_results(self, path, preds, im, im0s, predictor):\n-        predictor.results[0] = Results(\n+    def preds_to_yolov8_results(self, path, preds, im, im0s, names):\n+        return Results(\n             path=path,\n             boxes=preds,\n             orig_img=im0s[0],\n-            names=predictor.model.names\n+            names=names\n         )\n-        return predictor.results\n",add,added foreman procfile for WS server
4813714352bba3183933abf116253a619a2f0457,delete debug prints,examples/detectors/yolox.py,"import gdown\nimport torch\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import WEIGHTS\n\nfrom .yolo_interface import YoloInterface\n\nYOLOX_ZOO = {\n    'yolox_n': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass YoloXStrategy(YoloInterface):\n    def __init__(self, model, device, args):\n\n        self.args = args\n        self.has_run = False\n\n        model = str(model)\n        if model == 'yolox_n':\n            exp = get_exp(None, 'yolox_nano')\n        else:\n            exp = get_exp(None, model)\n        exp.num_classes = ","import gdown\nimport torch\nfrom ultralytics.models.yolo.detect.predict import DetectionPredictor\nfrom ultralytics.utils import ops\nfrom ultralytics.yolo.engine.results import Results\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import WEIGHTS\n\nfrom .yolo_interface import YoloInterface\n\nYOLOX_ZOO = {\n    'yolox_n': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass YoloXStrategy(DetectionPredictor, YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = {\n        0: 'person', 1: 'bicy","@@ -1,5 +1,8 @@\n import gdown\n import torch\n+from ultralytics.models.yolo.detect.predict import DetectionPredictor\n+from ultralytics.utils import ops\n+from ultralytics.yolo.engine.results import Results\n from yolox.exp import get_exp\n from yolox.utils import postprocess\n from yolox.utils.model_utils import fuse_model\n@@ -17,11 +20,36 @@ YOLOX_ZOO = {\n }\n \n \n-class YoloXStrategy(YoloInterface):\n+class YoloXStrategy(DetectionPredictor, YoloInterface):\n+    pt = False\n+    stride = 32\n+    fp16 = False\n+    triton = False\n+    names = {\n+        0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus',\n+        6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n+        11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat',\n+        16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant',\n+        21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella',\n+        26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis',\n+        31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove',\n+        36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass',\n+        41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl',\n+        46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli',\n+        51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake',\n+        56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table',\n+        61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote',\n+        66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster',\n+        71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase',\n+        76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'\n+    }\n+\n     def __init__(self, model, device, args):\n \n         self.args = args\n         self.has_run ",add,Addedency for
4813714352bba3183933abf116253a619a2f0457,delete debug prints,examples/track.py,"# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n\nimport argparse\nfrom pathlib import Path\n\nimport cv2\nimport torch\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics==8.0.124',))  # install\n\nfrom detectors import get_yolo_inferer\nfrom ultralytics.yolo.data.utils import VID_FORMATS\nfrom ultralytics.yolo.engine.model import TASK_MAP, YOLO\nfrom ultralytics.yolo.utils import IterableSimpleNamespace, colorstr, ops\nfrom ultralytics.yolo.utils.checks import check_imgsz\nfrom ultralytics.yolo.utils.files import increment_path\nfrom ultralytics.yolo.utils.plotting import save_one_box\nfrom utils import write_MOT_results\n\nfrom boxmot.utils import EXAMPLES\n\n\ndef on_predict_start(predictor):\n    predictor","import argparse\nfrom functools import partial\nfrom pathlib import Path\n\nimport torch\nfrom ultralytics import YOLO\nfrom ultralytics.yolo.data.utils import VID_FORMATS\nfrom ultralytics.yolo.utils.plotting import save_one_box\n\nfrom boxmot import DeepOCSORT\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS\nfrom examples.detectors import get_yolo_inferer\nfrom examples.utils import write_mot_results\n\n\ndef on_predict_start(predictor, persist=False):\n    """"""""""""\n    Initialize trackers for object tracking during prediction.\n\n    Args:\n        predictor (object): The predictor object to initialize trackers for.\n        persist (bool, optional): Whether to persist the trackers if they already exist. Defaults to False.\n    """"""""""""\n    predictor.args.tracking_config = \\n        ROOT /\\n        'boxmot' /\\n        'configs' /\\n        'deepocsort.yaml'\n    trackers = []\n    for i in range(predictor.dataset.bs):\n        tracker = DeepOCSORT(\n            model_weights=Path","@@ -1,231 +1,115 @@\n-# https://github.com/ultralytics/ultralytics/issues/1429#issuecomment-1519239409\n-\n import argparse\n+from functools import partial\n from pathlib import Path\n \n-import cv2\n import torch\n-\n-from boxmot.tracker_zoo import create_tracker\n-from boxmot.utils import ROOT, WEIGHTS\n-from boxmot.utils import logger as LOGGER\n-from boxmot.utils.checks import TestRequirements\n-from boxmot.utils.torch_utils import select_device\n-\n-__tr = TestRequirements()\n-__tr.check_packages(('ultralytics==8.0.124',))  # install\n-\n-from detectors import get_yolo_inferer\n+from ultralytics import YOLO\n from ultralytics.yolo.data.utils import VID_FORMATS\n-from ultralytics.yolo.engine.model import TASK_MAP, YOLO\n-from ultralytics.yolo.utils import IterableSimpleNamespace, colorstr, ops\n-from ultralytics.yolo.utils.checks import check_imgsz\n-from ultralytics.yolo.utils.files import increment_path\n from ultralytics.yolo.utils.plotting import save_one_box\n-from utils import write_MOT_results\n \n-from boxmot.utils import EXAMPLES\n+from boxmot import DeepOCSORT\n+from boxmot.utils import EXAMPLES, ROOT, WEIGHTS\n+from examples.detectors import get_yolo_inferer\n+from examples.utils import write_mot_results\n+\n \n+def on_predict_start(predictor, persist=False):\n+    """"""""""""\n+    Initialize trackers for object tracking during prediction.\n \n-def on_predict_start(predictor):\n-    predictor.trackers = []\n-    predictor.tracker_outputs = [None] * predictor.dataset.bs\n+    Args:\n+        predictor (object): The predictor object to initialize trackers for.\n+        persist (bool, optional): Whether to persist the trackers if they already exist. Defaults to False.\n+    """"""""""""\n     predictor.args.tracking_config = \\n         ROOT /\\n         'boxmot' /\\n         'configs' /\\n-        (opt.tracking_method + '.yaml')\n+        'deepocsort.yaml'\n+    trackers = []\n     for i in range(predictor.dataset.bs):\n-        tracker = create_tracker(\n-     ",add,Fix format
8661f7bf116deb5c0d4fb28460174383215e8639,fix evaluation,examples/track.py,"import argparse\nfrom functools import partial\nfrom pathlib import Path\n\nimport torch\nfrom ultralytics import YOLO\nfrom ultralytics.yolo.data.utils import VID_FORMATS\nfrom ultralytics.yolo.utils.plotting import save_one_box\n\nfrom boxmot import DeepOCSORT\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS\nfrom examples.detectors import get_yolo_inferer\nfrom examples.utils import write_mot_results\n\n\ndef on_predict_start(predictor, persist=False):\n    """"""""""""\n    Initialize trackers for object tracking during prediction.\n\n    Args:\n        predictor (object): The predictor object to initialize trackers for.\n        persist (bool, optional): Whether to persist the trackers if they already exist. Defaults to False.\n    """"""""""""\n    predictor.args.tracking_config = \\n        ROOT /\\n        'boxmot' /\\n        'configs' /\\n        'deepocsort.yaml'\n    trackers = []\n    for i in range(predictor.dataset.bs):\n        tracker = DeepOCSORT(\n            model_weights=Path","import argparse\nfrom functools import partial\nfrom pathlib import Path\n\nimport torch\nfrom ultralytics import YOLO\nfrom ultralytics.yolo.data.utils import VID_FORMATS\nfrom ultralytics.yolo.utils.plotting import save_one_box\n\nfrom boxmot import DeepOCSORT\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS\nfrom examples.detectors import get_yolo_inferer\nfrom examples.utils import write_mot_results\n\n\ndef on_predict_start(predictor, persist=False):\n    """"""""""""\n    Initialize trackers for object tracking during prediction.\n\n    Args:\n        predictor (object): The predictor object to initialize trackers for.\n        persist (bool, optional): Whether to persist the trackers if they already exist. Defaults to False.\n    """"""""""""\n    predictor.args.tracking_config = \\n        ROOT /\\n        'boxmot' /\\n        'configs' /\\n        'deepocsort.yaml'\n    trackers = []\n    for i in range(predictor.dataset.bs):\n        tracker = DeepOCSORT(\n            model_weights=Path","@@ -35,7 +35,9 @@ def on_predict_start(predictor, persist=False):\n             per_class=False\n         )\n         trackers.append(tracker)\n+\n     predictor.trackers = trackers\n+    predictor.save_dir = predictor.get_save_dir()\n \n \n @torch.no_grad()\n@@ -44,6 +46,7 @@ def run(args):\n     yolo = YOLO(\n         'yolov8n.pt',\n     )\n+    print(yolo.__dict__.keys())\n \n     results = yolo.track(\n         source=args.source,\n@@ -61,7 +64,6 @@ def run(args):\n     yolo.add_callback('on_predict_start', partial(on_predict_start, persist=True))\n \n     if 'yolov8' not in str(args.yolo_model):\n-\n         # replace yolov8 model\n         m = get_yolo_inferer(args.yolo_model)\n         model = m(\n@@ -71,13 +73,20 @@ def run(args):\n         )\n         yolo.predictor.model = model\n \n+    yolo.predictor.args.project = args.project\n+    yolo.predictor.args.name = args.name\n+    yolo.predictor.args.exist_ok = args.exist_ok\n+    yolo.predictor.args.classes = args.classes\n+\n     for frame_idx, r in enumerate(results):\n         if len(r.boxes.data) != 0:\n \n             if yolo.predictor.source_type.webcam or args.source.endswith(VID_FORMATS):\n-                yolo.predictor.mot_txt_path = yolo.predictor.save_dir / (args.source + '.txt')\n+                p = yolo.predictor.save_dir / 'mot' / (args.source + '.txt')\n+                yolo.predictor.mot_txt_path = p\n             elif 'MOT16' or 'MOT17' or 'MOT20' in args.source:\n-                yolo.predictor.mot_txt_path = yolo.predictor.save_dir / (Path(args.source).parent.name + '.txt')\n+                p = yolo.predictor.save_dir / 'mot' / (Path(args.source).parent.name + '.txt')\n+                yolo.predictor.mot_txt_path = p\n \n             if args.save_mot:\n                 write_mot_results(\n",add,Added missing property name for playWhenInactive .
8661f7bf116deb5c0d4fb28460174383215e8639,fix evaluation,examples/val.py,"#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport argparse\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport zipfile\nfrom pathlib import Path\n\nimport git\nfrom git import Repo\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom ultralytics.yolo.utils.files import increment_path\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS\nfrom boxmot.utils import logger as LOGGER\n\n\nclass Evaluator:\n    """"""""""""Evaluate","#  Yolov5_StrongSORT_OSNet, GPL-3.0 license\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport argparse\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport zipfile\nfrom pathlib import Path\n\nimport git\nfrom git import Repo\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom ultralytics.yolo.utils.checks import check_requirements, print_args\nfrom ultralytics.yolo.utils.files import increment_path\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS\nfrom boxmot.utils import logger as LOGGER\n\n\nclass Evaluator:\n    """"""""""""Evaluate","@@ -218,7 +218,7 @@ class Evaluator:\n         if opt.gsi:\n             # apply gaussian-smoothed interpolation\n             from boxmot.postprocessing.gsi import gsi\n-            gsi(mot_results_folder=save_dir / 'labels')\n+            gsi(mot_results_folder=save_dir / 'mot')\n \n         # run the evaluation on the generated txts\n         d = [seq_path.parent.name for seq_path in seq_paths]\n@@ -228,7 +228,7 @@ class Evaluator:\n                 """"--GT_FOLDER"""", gt_folder,\n                 """"--BENCHMARK"""", """""""",\n                 """"--TRACKERS_FOLDER"""", save_dir,   # project/name\n-                """"--TRACKERS_TO_EVAL"""", """"labels"""",  # project/name/labels\n+                """"--TRACKERS_TO_EVAL"""", """"mot"""",  # project/name/mot\n                 """"--SPLIT_TO_EVAL"""", """"train"""",\n                 """"--METRICS"""", """"HOTA"""", """"CLEAR"""", """"Identity"""",\n                 """"--USE_PARALLEL"""", """"True"""",\n@@ -335,7 +335,7 @@ def parse_opt():\n     parser.add_argument('--split', type=str, default='train',\n                         help='existing project/name ok, do not increment')\n     parser.add_argument('--eval-existing', action='store_true',\n-                        help='evaluate existing results under project/name/labels')\n+                        help='evaluate existing results under project/name/mot')\n     parser.add_argument('--conf', type=float, default=0.45,\n                         help='confidence threshold')\n     parser.add_argument('--imgsz', '--img-size', nargs='+', type=int, default=[1280],\n",add,Added note about data volume to enable_metrics_collection
af327d48aa6bd84f60cf1059f09499014e4e3eff,fix install,.github/workflows/ci.yml,"# name of the workflow, what it is doing (optional)\nname: CI CPU testing\n\n# events that trigger the workflow (required)\non:\n  push:\n    branches: [master, CIdebug]\n  pull_request:\n    # pull request where master is target\n    branches: [master]\n\nenv:\n  # Directory of PyPi package to be tested\n  PACKAGE_DIR: boxmot\n  # Minimum acceptable test coverage\n  # Increase as you add more tests to increase coverage\n  COVERAGE_FAIL_UNDER: 29\n\n# the workflow that gets triggerd\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]   # skip windows-latest for\n        python-version: ['3.8', '3.9', '3.10']\n        #model: ['yolov8n', 'yolo_nas_s', yolox_n]  # yolo models to test\n        #tracking-methods: ['deepocsort', 'ocsort', 'botsort', 'strongsort', 'bytetrack']  # tracking methods to  test\n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n\n      -","# name of the workflow, what it is doing (optional)\nname: CI CPU testing\n\n# events that trigger the workflow (required)\non:\n  push:\n    branches: [master, CIdebug]\n  pull_request:\n    # pull request where master is target\n    branches: [master]\n\nenv:\n  # Directory of PyPi package to be tested\n  PACKAGE_DIR: boxmot\n  # Minimum acceptable test coverage\n  # Increase as you add more tests to increase coverage\n  COVERAGE_FAIL_UNDER: 29\n\n# the workflow that gets triggerd\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]   # skip windows-latest for\n        python-version: ['3.8', '3.9', '3.10']\n        #model: ['yolov8n', 'yolo_nas_s', yolox_n]  # yolo models to test\n        #tracking-methods: ['deepocsort', 'ocsort', 'botsort', 'strongsort', 'bytetrack']  # tracking methods to  test\n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n\n      -","@@ -42,7 +42,6 @@ jobs:\n         run: |\n           python -m pip install --upgrade pip setuptools wheel\n           pip install -e . pytest pytest-cov --extra-index-url https://download.pytorch.org/whl/cpu\n-          pip install -e .[yolo] --no-deps\n           python --version\n           pip --version\n           pip list\n",add,Added Type name for Specification pattern
af327d48aa6bd84f60cf1059f09499014e4e3eff,fix install,examples/track.py,"import argparse\nfrom functools import partial\nfrom pathlib import Path\n\nimport torch\nfrom ultralytics import YOLO\nfrom ultralytics.yolo.data.utils import VID_FORMATS\nfrom ultralytics.yolo.utils.plotting import save_one_box\n\nfrom boxmot import DeepOCSORT\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS\nfrom examples.detectors import get_yolo_inferer\nfrom examples.utils import write_mot_results\n\n\ndef on_predict_start(predictor, persist=False):\n    """"""""""""\n    Initialize trackers for object tracking during prediction.\n\n    Args:\n        predictor (object): The predictor object to initialize trackers for.\n        persist (bool, optional): Whether to persist the trackers if they already exist. Defaults to False.\n    """"""""""""\n    predictor.args.tracking_config = \\n        ROOT /\\n        'boxmot' /\\n        'configs' /\\n        'deepocsort.yaml'\n    trackers = []\n    for i in range(predictor.dataset.bs):\n        tracker = DeepOCSORT(\n            model_weights=Path","import argparse\nfrom functools import partial\nfrom pathlib import Path\n\nimport torch\n\nfrom boxmot import DeepOCSORT\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom examples.detectors import get_yolo_inferer\n\n__tr = TestRequirements()\n__tr.check_packages(('git+https://github.com/mikel-brostrom/ultralytics.git',))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.yolo.data.utils import VID_FORMATS\nfrom ultralytics.yolo.utils.plotting import save_one_box\n\nfrom examples.utils import write_mot_results\n\n\ndef on_predict_start(predictor, persist=False):\n    """"""""""""\n    Initialize trackers for object tracking during prediction.\n\n    Args:\n        predictor (object): The predictor object to initialize trackers for.\n        persist (bool, optional): Whether to persist the trackers if they already exist. Defaults to False.\n    """"""""""""\n    predictor.args.tracking_config = \\n        ROOT /\\n        'boxmot' /","@@ -3,13 +3,19 @@ from functools import partial\n from pathlib import Path\n \n import torch\n-from ultralytics import YOLO\n-from ultralytics.yolo.data.utils import VID_FORMATS\n-from ultralytics.yolo.utils.plotting import save_one_box\n \n from boxmot import DeepOCSORT\n from boxmot.utils import EXAMPLES, ROOT, WEIGHTS\n+from boxmot.utils.checks import TestRequirements\n from examples.detectors import get_yolo_inferer\n+\n+__tr = TestRequirements()\n+__tr.check_packages(('git+https://github.com/mikel-brostrom/ultralytics.git',))  # install\n+\n+from ultralytics import YOLO\n+from ultralytics.yolo.data.utils import VID_FORMATS\n+from ultralytics.yolo.utils.plotting import save_one_box\n+\n from examples.utils import write_mot_results\n \n \n",add,added meteor
c7a8718be7e20ef2f37b775101f48bdb65947aa7,check if warp matrix is none,boxmot/motion/cmc/sof.py,"import time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\nfrom boxmot.utils import BOXMOT\nfrom boxmot.utils import logger as LOGGER\n\n\nclass SparseOptFlow(CMCInterface):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        align=False,\n        grayscale=True,\n        draw_optical_flow=False\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [","import time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.cmc_interface import CMCInterface\nfrom boxmot.utils import BOXMOT\nfrom boxmot.utils import logger as LOGGER\n\n\nclass SparseOptFlow(CMCInterface):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.1,\n        align=False,\n        grayscale=True,\n        draw_optical_flow=False\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: float or [","@@ -146,6 +146,9 @@ class SparseOptFlow(CMCInterface):\n         except Exception as e:\n             LOGGER.warning(f'Affine matrix could not be generated: {e}')\n             return H\n+        finally:\n+            if H is None:\n+                return np.eye(2, 3)\n \n         if self.draw_optical_flow:\n             self.warped_img = cv2.warpAffine(self.prev_img, H, (w, h), flags=cv2.INTER_LINEAR)\n",add,Added TypeSpec . h for PGSimple .
e7861ffeb39dce6e01c2721fe57ee10b027ea1da,fix import,examples/detectors/yolox.py,"import gdown\nimport torch\nfrom ultralytics.models.yolo.detect.predict import DetectionPredictor\nfrom ultralytics.utils import ops\nfrom ultralytics.yolo.results import Results\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import WEIGHTS\n\nfrom .yolo_interface import YoloInterface\n\nYOLOX_ZOO = {\n    'yolox_n': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass YoloXStrategy(DetectionPredictor, YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = {\n        0: 'person', 1: 'bicycle', 2","import gdown\nimport torch\nfrom ultralytics.models.yolo.detect.predict import DetectionPredictor\nfrom ultralytics.utils import ops\nfrom ultralytics.engine.results import Results\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import WEIGHTS\n\nfrom .yolo_interface import YoloInterface\n\nYOLOX_ZOO = {\n    'yolox_n': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass YoloXStrategy(DetectionPredictor, YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = {\n        0: 'person', 1: 'bicycle',","@@ -2,7 +2,7 @@ import gdown\n import torch\n from ultralytics.models.yolo.detect.predict import DetectionPredictor\n from ultralytics.utils import ops\n-from ultralytics.yolo.results import Results\n+from ultralytics.engine.results import Results\n from yolox.exp import get_exp\n from yolox.utils import postprocess\n from yolox.utils.model_utils import fuse_model\n",add,Add Rossen
32e03eb8680459c90cef86bb623705e18e6250be,default config,boxmot/configs/strongsort.yaml,ecc: true\nema_alpha: 0.8962157769329083\nmax_age: 40\nmax_dist: 0.1594374041012136\nmax_iou_dist: 0.5431835667667874\nmax_unmatched_preds: 0\nmc_lambda: 0.995\nn_init: 3\nnn_budget: 100\nconf_thres: 0.5122620708221085\n,ecc: true\nema_alpha: 0.9\nmax_age: 30\nmax_dist: 0.2\nmax_iou_dist: 0.7\nmax_unmatched_preds: 0\nmc_lambda: 0.995\nn_init: 3\nnn_budget: 100\nconf_thres: 0.5122620708221085\n,"@@ -1,8 +1,8 @@\n ecc: true\n-ema_alpha: 0.8962157769329083\n-max_age: 40\n-max_dist: 0.1594374041012136\n-max_iou_dist: 0.5431835667667874\n+ema_alpha: 0.9\n+max_age: 30\n+max_dist: 0.2\n+max_iou_dist: 0.7\n max_unmatched_preds: 0\n mc_lambda: 0.995\n n_init: 3\n",add,Added STORM - 1270 to Changelog
32e03eb8680459c90cef86bb623705e18e6250be,default config,boxmot/trackers/strongsort/strong_sort.py,"import cv2\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.trackers.strongsort.sort.detection import Detection\nfrom boxmot.trackers.strongsort.sort.tracker import Tracker\nfrom boxmot.utils.matching import NearestNeighborDistanceMetric\nfrom boxmot.utils.ops import xyxy2xywh\n\n\nclass StrongSORT(object):\n    def __init__(\n        self,\n        model_weights,\n        device,\n        fp16,\n        max_dist=0.2,\n        max_iou_dist=0.7,\n        max_age=70,\n        max_unmatched_preds=7,\n        n_init=3,\n        nn_budget=100,\n        mc_lambda=0.995,\n        ema_alpha=0.9,\n    ):\n        self.model = ReIDDetectMultiBackend(\n            weights=model_weights, device=device, fp16=fp16\n        )\n\n        self.max_dist = max_dist\n        metric = NearestNeighborDistanceMetric(""""cosine"""", self.max_dist, nn_budget)\n        self.tracker = Tracker(\n            metric,\n            max_iou_dist=max_i","import cv2\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.trackers.strongsort.sort.detection import Detection\nfrom boxmot.trackers.strongsort.sort.tracker import Tracker\nfrom boxmot.utils.matching import NearestNeighborDistanceMetric\nfrom boxmot.utils.ops import xyxy2xywh\n\n\nclass StrongSORT(object):\n    def __init__(\n        self,\n        model_weights,\n        device,\n        fp16,\n        max_dist=0.2,\n        max_iou_dist=0.7,\n        max_age=30,\n        max_unmatched_preds=7,\n        n_init=3,\n        nn_budget=100,\n        mc_lambda=0.995,\n        ema_alpha=0.9,\n    ):\n        self.model = ReIDDetectMultiBackend(\n            weights=model_weights, device=device, fp16=fp16\n        )\n\n        self.max_dist = max_dist\n        metric = NearestNeighborDistanceMetric(""""cosine"""", self.max_dist, nn_budget)\n        self.tracker = Tracker(\n            metric,\n            max_iou_dist=max_i","@@ -17,7 +17,7 @@ class StrongSORT(object):\n         fp16,\n         max_dist=0.2,\n         max_iou_dist=0.7,\n-        max_age=70,\n+        max_age=30,\n         max_unmatched_preds=7,\n         n_init=3,\n         nn_budget=100,\n",add,Added net . kano . joustsim . oscar . os
50a925314f570cf1a0ef772e17861708f05a623f,fix install,boxmot/appearance/reid_export.py,"import argparse\nimport os\nimport platform\nimport subprocess\nimport time\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\n\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1e6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n    else:\n        return 0.0\n\n\ndef export_torchscript(model, im, file, optimize):\n    try:\n        L","import argparse\nimport os\nimport platform\nimport subprocess\nimport time\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\n\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1e6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n    else:\n        return 0.0\n\n\ndef export_torchscript(model, im, file, optimize):\n    try:\n        L","@@ -132,7 +132,8 @@ def export_openvino(file, half):\n def export_tflite(file):\n     try:\n         __tr.check_packages(\n-            (""""onnx2tf"""", """"tensorflow"""", """"onnx_graphsurgeon"""", """"sng4onnx"""")\n+            (""""onnx2tf"""", """"tensorflow"""", """"onnx_graphsurgeon"""", """"sng4onnx""""),\n+            cmds='--extra-index-url https://pypi.ngc.nvidia.com'\n         )  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n         import onnx2tf\n \n",add,Add debug flag to enable causes exceptions
c8374172440ac9b4cae88280b6e9676ab78caa5c,fix,boxmot/appearance/reid_multibackend.py,"from collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\n__tr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Pat","from collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\n__tr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Pat","@@ -102,7 +102,12 @@ class ReIDDetectMultiBackend(nn.Module):\n         elif self.onnx:  # ONNX Runtime\n             LOGGER.info(f""""Loading {w} for ONNX Runtime inference..."""")\n             cuda = torch.cuda.is_available() and device.type != """"cpu""""\n-            __tr.check_packages([""""onnx"""", """"onnxruntime-gpu"""" if cuda else """"onnxruntime""""])\n+            __tr.check_packages(\n+                (\n+                    """"onnx"""",\n+                    """"onnxruntime-gpu"""" if cuda else """"onnxruntime""""\n+                ),\n+            )\n             import onnxruntime\n \n             providers = (\n",add,Add note about data volume to enable
27f0d21abcfba4a16a7eb563743ae39453f909a3,fix,boxmot/appearance/reid_multibackend.py,"from collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\n__tr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Pat","from collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\n__tr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Pat","@@ -105,7 +105,7 @@ class ReIDDetectMultiBackend(nn.Module):\n             __tr.check_packages(\n                 (\n                     """"onnx"""",\n-                    """"onnxruntime-gpu"""" if cuda else """"onnxruntime""""\n+                    """"onnxruntime-gpu"""" if cuda else """"onnxruntime"""",\n                 ),\n             )\n             import onnxruntime\n",add,Add note about data volume to enable
8e6d5acdc237ecd3bf6f112a2502b487df245382,fix,boxmot/appearance/reid_multibackend.py,"from collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\n__tr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Pat","from collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\n__tr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Pat","@@ -102,12 +102,7 @@ class ReIDDetectMultiBackend(nn.Module):\n         elif self.onnx:  # ONNX Runtime\n             LOGGER.info(f""""Loading {w} for ONNX Runtime inference..."""")\n             cuda = torch.cuda.is_available() and device.type != """"cpu""""\n-            __tr.check_packages(\n-                (\n-                    """"onnx"""",\n-                    """"onnxruntime-gpu"""" if cuda else """"onnxruntime"""",\n-                ),\n-            )\n+            __tr.check_packages((""""onnx"""", """"onnxruntime-gpu"""" if cuda else """"onnxruntime"""", ))\n             import onnxruntime\n \n             providers = (\n",add,Add note about data volume to enable
d603678081e58fb2d8fa0767361ee3f92c9ff7bf,fix bugs,boxmot/trackers/bytetrack/byte_tracker.py,"import numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import ByteTrackKalmanFilterAdapter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = ByteTrackKalmanFilterAdapter()\n\n    def __init__(self, tlwh, score, cls):\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n        self.cls = cls\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(\n            mean_state, self.covariance\n        )\n\n    @staticmethod\n   ","import numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import ByteTrackKalmanFilterAdapter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = ByteTrackKalmanFilterAdapter()\n\n    def __init__(self, tlwh, score, cls):\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n        self.cls = cls\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(\n            mean_state, self.covariance\n        )\n\n    @staticmethod\n   ","@@ -161,7 +161,7 @@ class BYTETracker(object):\n \n         self.track_thresh = track_thresh\n         self.match_thresh = match_thresh\n-        self.det_thresh = track_thresh + 0.1\n+        self.det_thresh = track_thresh\n         self.buffer_size = int(frame_rate / 30.0 * track_buffer)\n         self.max_time_lost = self.buffer_size\n         self.kalman_filter = ByteTrackKalmanFilterAdapter()\n",fix,Add warning about data volume to enable_metrics_collection
65ee1c0a205b294aabbc6c668636f2c6661f44be,fix,boxmot/appearance/reid_multibackend.py,"from collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\n__tr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Pat","from collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\ntr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(","@@ -16,7 +16,7 @@ from boxmot.appearance.reid_model_factory import (get_model_name,\n from boxmot.utils import logger as LOGGER\n from boxmot.utils.checks import TestRequirements\n \n-__tr = TestRequirements()\n+tr = TestRequirements()\n \n \n def check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n@@ -48,9 +48,7 @@ class ReIDDetectMultiBackend(nn.Module):\n             self.xml,\n             self.engine,\n             self.tflite,\n-        ) = self.model_type(\n-            w\n-        )  # get backend\n+        ) = self.model_type(w)  # get backend\n         self.fp16 = fp16\n         self.fp16 &= self.pt or self.jit or self.engine  # FP16\n \n@@ -102,7 +100,7 @@ class ReIDDetectMultiBackend(nn.Module):\n         elif self.onnx:  # ONNX Runtime\n             LOGGER.info(f""""Loading {w} for ONNX Runtime inference..."""")\n             cuda = torch.cuda.is_available() and device.type != """"cpu""""\n-            __tr.check_packages((""""onnx"""", """"onnxruntime-gpu"""" if cuda else """"onnxruntime"""", ))\n+            tr.check_packages((""""onnx"""", """"onnxruntime-gpu"""" if cuda else """"onnxruntime"""", ))\n             import onnxruntime\n \n             providers = (\n@@ -113,7 +111,7 @@ class ReIDDetectMultiBackend(nn.Module):\n             self.session = onnxruntime.InferenceSession(str(w), providers=providers)\n         elif self.engine:  # TensorRT\n             LOGGER.info(f""""Loading {w} for TensorRT inference..."""")\n-            __tr.check_packages((""""nvidia-tensorrt"""",))\n+            tr.check_packages((""""nvidia-tensorrt"""",))\n             import tensorrt as trt  # https://developer.nvidia.com/nvidia-tensorrt-download\n \n             if device.type == """"cpu"""":\n",add,Add note about data volume to enable_metrics_collection
7b9436ec9f1c7dd8d8edb5941936a310dae7d416,temporary fix,boxmot/appearance/reid_export.py,"import argparse\nimport os\nimport platform\nimport subprocess\nimport time\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\n\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1e6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n    else:\n        return 0.0\n\n\ndef export_torchscript(model, im, file, optimize):\n    try:\n        L","import argparse\nimport os\nimport platform\nimport subprocess\nimport time\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\n\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1e6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n    else:\n        return 0.0\n\n\ndef export_torchscript(model, im, file, optimize):\n    try:\n        L","@@ -139,7 +139,7 @@ def export_tflite(file):\n \n         LOGGER.info(f""""\nStarting {file} export with onnx2tf {onnx2tf.__version__}"""")\n         f = str(file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n-        cmd = f""""onnx2tf -i {file} -o {f} -nuo --non_verbose""""\n+        cmd = f""""onnx2tf -i {file} -o {f} -nuo --non_verbose -b 10""""\n         print(cmd.split())\n         subprocess.check_output(cmd.split())  # export\n         LOGGER.info(f""""Export success, results saved in {f} ({file_size(f):.1f} MB)"""")\n",add,Don ' t define the flag on real_commad twice
5e2f252bc5fcb98efe6deab84eee55891c6b8f9f,fix,boxmot/appearance/reid_export.py,"import argparse\nimport os\nimport platform\nimport subprocess\nimport time\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\n\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1e6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n    else:\n        return 0.0\n\n\ndef export_torchscript(model, im, file, optimize):\n    try:\n        L","import argparse\nimport os\nimport platform\nimport subprocess\nimport time\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\n\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1e6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n    else:\n        return 0.0\n\n\ndef export_torchscript(model, im, file, optimize):\n    try:\n        L","@@ -139,7 +139,7 @@ def export_tflite(file):\n \n         LOGGER.info(f""""\nStarting {file} export with onnx2tf {onnx2tf.__version__}"""")\n         f = str(file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n-        cmd = f""""onnx2tf -i {file} -o {f} -nuo --non_verbose -b 10""""\n+        cmd = f""""onnx2tf -i {file} -o {f} -nuo --non_verbose -b 4""""\n         print(cmd.split())\n         subprocess.check_output(cmd.split())  # export\n         LOGGER.info(f""""Export success, results saved in {f} ({file_size(f):.1f} MB)"""")\n",add,Don ' t define the dependency on real_commad twice
23ec159ad81318779b939af011e3eb979ad16520,fix,boxmot/appearance/reid_export.py,"import argparse\nimport os\nimport platform\nimport subprocess\nimport time\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\n\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1e6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n    else:\n        return 0.0\n\n\ndef export_torchscript(model, im, file, optimize):\n    try:\n        L","import argparse\nimport os\nimport platform\nimport subprocess\nimport time\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\n\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1e6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n    else:\n        return 0.0\n\n\ndef export_torchscript(model, im, file, optimize):\n    try:\n        L","@@ -58,7 +58,7 @@ def export_onnx(model, im, file, opset, dynamic, fp16, simplify):\n         LOGGER.info(f""""\nStarting export with onnx {onnx.__version__}..."""")\n \n         if dynamic:\n-            # input --> shape(N,3,640,640), output --> shape(N, feature-size)\n+            # input --> shape(N, 3, h, w), output --> shape(N, feat_size)\n             dynamic = {""""images"""": {0: """"batch""""}, """"output"""": {0: """"batch""""}}\n \n         torch.onnx.export(\n@@ -139,7 +139,7 @@ def export_tflite(file):\n \n         LOGGER.info(f""""\nStarting {file} export with onnx2tf {onnx2tf.__version__}"""")\n         f = str(file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n-        cmd = f""""onnx2tf -i {file} -o {f} -nuo --non_verbose -b 4""""\n+        cmd = f""""onnx2tf -i {file} -o {f} -nuo --non_verbose -b 1""""\n         print(cmd.split())\n         subprocess.check_output(cmd.split())  # export\n         LOGGER.info(f""""Export success, results saved in {f} ({file_size(f):.1f} MB)"""")\n",add,Don ' t define the dependency on real_commad twice
111d7eb3c42e0375b09b1b7fcd0133ef5912e717,fix dynamic model bug,boxmot/appearance/reid_export.py,"import argparse\nimport os\nimport platform\nimport subprocess\nimport time\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\n\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1e6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n    else:\n        return 0.0\n\n\ndef export_torchscript(model, im, file, optimize):\n    try:\n        L","import argparse\nimport os\nimport platform\nimport subprocess\nimport time\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\n\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1e6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n    else:\n        return 0.0\n\n\ndef export_torchscript(model, im, file, optimize):\n    try:\n        L","@@ -62,8 +62,8 @@ def export_onnx(model, im, file, opset, dynamic, fp16, simplify):\n             dynamic = {""""images"""": {0: """"batch""""}, """"output"""": {0: """"batch""""}}\n \n         torch.onnx.export(\n-            model.half() if fp16 else model.cpu(),\n-            im.half() if fp16 else im.cpu(),\n+            model.cpu() if dynamic else model,  # --dynamic only compatible with cpu\n+            im.cpu() if dynamic else im,\n             f,\n             verbose=False,\n             opset_version=opset,\n@@ -228,7 +228,7 @@ def export_engine(model, im, file, half, dynamic, simplify, workspace=4, verbose\n \n if __name__ == """"__main__"""":\n     parser = argparse.ArgumentParser(description=""""ReID export"""")\n-    parser.add_argument(""""--batch-size"""", type=int, default=1, help=""""batch size"""")\n+    parser.add_argument(""""--batch-size"""", type=int, default=None, help=""""batch size"""")\n     parser.add_argument(\n         """"--imgsz"""",\n         """"--img"""",\n",add,Add note about data volume to enable
111d7eb3c42e0375b09b1b7fcd0133ef5912e717,fix dynamic model bug,boxmot/appearance/reid_multibackend.py,"from collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\ntr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(","from collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\ntr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(","@@ -187,18 +187,8 @@ class ReIDDetectMultiBackend(nn.Module):\n             self.interpreter.allocate_tensors()\n             # Get input and output tensors.\n             self.input_details = self.interpreter.get_input_details()\n+            print(self.input_details)\n             self.output_details = self.interpreter.get_output_details()\n-\n-            # Test model on random input data.\n-            input_data = np.array(\n-                np.random.random_sample((1, 256, 128, 3)), dtype=np.float32\n-            )\n-            self.interpreter.set_tensor(self.input_details[0][""""index""""], input_data)\n-\n-            self.interpreter.invoke()\n-\n-            # The function `get_tensor()` returns a copy of the tensor data.\n-            # output_data = self.interpreter.get_tensor(self.output_details[0][""""index""""])\n         else:\n             LOGGER.error(""""This model framework is not supported yet!"""")\n             exit()\n@@ -250,6 +240,7 @@ class ReIDDetectMultiBackend(nn.Module):\n                 {self.session.get_inputs()[0].name: im_batch},\n             )[0]\n         elif self.tflite:\n+            print(im_batch.shape)\n             im_batch = im_batch.cpu().numpy()\n             details = self.input_details[0]\n             self.interpreter.set_tensor(details['index'], im_batch)\n",add,Added Type name for DFI ( # 3 )
111d7eb3c42e0375b09b1b7fcd0133ef5912e717,fix dynamic model bug,tests/test_exports.py,"import torch\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_export import (export_onnx, export_openvino,\n                                           export_tflite, export_torchscript)\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\n\nPT_WEIGHTS = WEIGHTS / 'osnet_x0_25_msmt17.pt'\nONNX_WEIGHTS = WEIGHTS / 'osnet_x0_25_msmt17.onnx'\n\n\nim = torch.zeros(1, 3, 256, 128)\n\nmodel = build_model(\n    get_model_name(PT_WEIGHTS),\n    num_classes=1,\n    pretrained=False,\n    use_gpu=False,\n).to('cpu')\n\nload_pretrained_weights(model, PT_WEIGHTS)\nmodel.eval()\n\n\ndef test_export_torchscript():\n    f = export_torchscript(\n        model,\n        im,\n        PT_WEIGHTS,\n        True,\n    )\n    assert f is not None\n\n\ndef test_export_onnx():\n    f = export_onnx(\n        model=model,\n        im=im,\n        file=PT_","import torch\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_export import (export_onnx, export_openvino,\n                                           export_tflite, export_torchscript)\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\n\nPT_WEIGHTS = WEIGHTS / 'osnet_x0_25_msmt17.pt'\nONNX_WEIGHTS = WEIGHTS / 'osnet_x0_25_msmt17.onnx'\n\n\nim = torch.zeros(None, 3, 256, 128)  # make models dynamic\n\nmodel = build_model(\n    get_model_name(PT_WEIGHTS),\n    num_classes=1,\n    pretrained=False,\n    use_gpu=False,\n).to('cpu')\n\nload_pretrained_weights(model, PT_WEIGHTS)\nmodel.eval()\n\n\ndef test_export_torchscript():\n    f = export_torchscript(\n        model,\n        im,\n        PT_WEIGHTS,\n        True,\n    )\n    assert f is not None\n\n\ndef test_export_onnx():\n    f = export_onnx(\n        model=model,\n      ","@@ -11,7 +11,7 @@ PT_WEIGHTS = WEIGHTS / 'osnet_x0_25_msmt17.pt'\n ONNX_WEIGHTS = WEIGHTS / 'osnet_x0_25_msmt17.onnx'\n \n \n-im = torch.zeros(1, 3, 256, 128)\n+im = torch.zeros(None, 3, 256, 128)  # make models dynamic\n \n model = build_model(\n     get_model_name(PT_WEIGHTS),\n",fix,Added Type name for activating
96db3f3404266664cd0d7350ef58dd230b87a441,fix tflite cmd,boxmot/appearance/reid_export.py,"import argparse\nimport os\nimport platform\nimport subprocess\nimport time\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\n\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1e6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n    else:\n        return 0.0\n\n\ndef export_torchscript(model, im, file, optimize):\n    try:\n        L","import argparse\nimport os\nimport platform\nimport subprocess\nimport time\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\n\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1e6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n    else:\n        return 0.0\n\n\ndef export_torchscript(model, im, file, optimize):\n    try:\n        L","@@ -139,7 +139,7 @@ def export_tflite(file):\n \n         LOGGER.info(f""""\nStarting {file} export with onnx2tf {onnx2tf.__version__}"""")\n         f = str(file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n-        cmd = f""""onnx2tf -i {file} -o {f} -nuo --non_verbose -b 1""""\n+        cmd = f""""onnx2tf -i {file} -o {f} -osd -coion --non_verbose""""\n         print(cmd.split())\n         subprocess.check_output(cmd.split())  # export\n         LOGGER.info(f""""Export success, results saved in {f} ({file_size(f):.1f} MB)"""")\n@@ -228,7 +228,7 @@ def export_engine(model, im, file, half, dynamic, simplify, workspace=4, verbose\n \n if __name__ == """"__main__"""":\n     parser = argparse.ArgumentParser(description=""""ReID export"""")\n-    parser.add_argument(""""--batch-size"""", type=int, default=None, help=""""batch size"""")\n+    parser.add_argument(""""--batch-size"""", type=int, default=1, help=""""batch size"""")\n     parser.add_argument(\n         """"--imgsz"""",\n         """"--img"""",\n@@ -306,7 +306,7 @@ if __name__ == """"__main__"""":\n     if """"lmbn"""" in str(args.weights):\n         args.imgsz = (384, 128)\n \n-    im = torch.zeros(args.batch_size, 3, args.imgsz[0], args.imgsz[1]).to(\n+    im = torch.empty(args.batch_size, 3, args.imgsz[0], args.imgsz[1]).to(\n         args.device\n     )  # image size(1,3,640,480) BCHW iDetection\n     for _ in range(2):\n",add,Don ' t define the dependency on real_commad twice
5aff0ee7251a1e40df52198bf0198f2298599c9e,fix tflite dynamic batch inference,boxmot/appearance/reid_multibackend.py,"from collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\ntr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(","from collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\ntr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(","@@ -167,9 +167,6 @@ class ReIDDetectMultiBackend(nn.Module):\n             network = ie.read_model(model=w, weights=Path(w).with_suffix("""".bin""""))\n             if network.get_parameters()[0].get_layout().empty:\n                 network.get_parameters()[0].set_layout(Layout(""""NCWH""""))\n-            # batch_dim = get_batch(network)\n-            # if batch_dim.is_static:\n-            #     batch_size = batch_dim.get_length()\n             self.executable_network = ie.compile_model(\n                 network, device_name=""""CPU""""\n             )  # device_name=""""MYRIAD"""" for Intel NCS2\n@@ -177,17 +174,13 @@ class ReIDDetectMultiBackend(nn.Module):\n \n         elif self.tflite:\n             LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n-\n             import tensorflow as tf\n             interpreter = tf.lite.Interpreter(model_path=str(w))\n-            print(interpreter.get_signature_list())\n-            self.tf_lite_model = interpreter.get_signature_runner()\n-            inputs = {\n-                'images': np.ones([5, 256, 128, 3], dtype=np.float32),\n-            }\n-            tf_lite_output = self.tf_lite_model(**inputs)\n-            print(f""""[TFLite] Model Predictions shape: {tf_lite_output['output'].shape}"""")\n-            print(""""[TFLite] Model Predictions:"""")\n+            try:\n+                self.tf_lite_model = interpreter.get_signature_runner()\n+            except Exception as e:\n+                LOGGER.error(f'{e}. If SignatureDef error. Export you model with the official onn2tf docker')\n+                exit()\n         else:\n             LOGGER.error(""""This model framework is not supported yet!"""")\n             exit()\n@@ -239,10 +232,12 @@ class ReIDDetectMultiBackend(nn.Module):\n                 {self.session.get_inputs()[0].name: im_batch},\n             )[0]\n         elif self.tflite:\n-            print(im_batch.shape)\n             im_batch = im_batch.cpu().numpy()\n-            features = self.tf_lite_model(i",add,Add PullApprove config
1ac61a06b4757268fac76641e9b1bfe5466be2e7,fix save passing to subprocess,examples/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport argparse\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport zipfile\nfrom pathlib import Path\n\nfrom boxmot.utils.checks import TestRequirements\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\nimport git\nfrom git import Repo\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom ultralytics.utils.checks import check_requirements, print_args\","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nEvaluate on the benchmark of your choice. MOT16, 17 and 20 are donwloaded and unpackaged automatically when selected.\nMimic the structure of either of these datasets to evaluate on your custom one\n\nUsage:\n\n    $ python3 val.py --tracking-method strongsort --benchmark MOT16\n                     --tracking-method ocsort     --benchmark MOT17\n                     --tracking-method ocsort     --benchmark <your-custom-dataset>\n""""""""""""\n\nimport argparse\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport zipfile\nfrom pathlib import Path\n\nfrom boxmot.utils.checks import TestRequirements\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\nimport git\nfrom git import Repo\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom ultralytics.utils.checks import check_requirements, print_args\","@@ -203,12 +203,13 @@ class Evaluator:\n                         """"--imgsz"""", str(self.opt.imgsz[0]),\n                         """"--classes"""", *self.opt.classes,\n                         """"--name"""", save_dir.name,\n+                        """"--save"""" if self.opt.save else """"""""\n                         """"--save-mot"""",\n                         """"--project"""", self.opt.project,\n                         """"--device"""", str(tracking_subprocess_device),\n                         """"--source"""", seq_path,\n                         """"--exist-ok"""",\n-                    ] + [""""--save"""" if self.opt.save else None],\n+                    ],\n                 )\n                 processes.append(p)\n                 # Wait for the subprocess to complete and capture output\n",add,Add note about data volume to enable_metrics_collection
f9e25d9143c5ecfd4ad5da2be6646a6c65028425,fix evolve,examples/evolve.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport argparse\n\nimport yaml\nfrom ultralytics.utils.checks import check_requirements, print_args\nfrom val import Evaluator\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, logger\n\n\nclass Objective(Evaluator):\n    """"""""""""Objective function to evolve best set of hyperparams for\n\n    This object is passed to an objective function and provides interfaces to overwrite\n    a tracker's config yaml file and the call to th","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport argparse\n\nimport yaml\nfrom ultralytics.utils.checks import check_requirements, print_args\nfrom val import Evaluator\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, logger\n\n\nclass Objective(Evaluator):\n    """"""""""""Objective function to evolve best set of hyperparams for\n\n    This object is passed to an objective function and provides interfaces to overwrite\n    a tracker's config yaml file and the call to th","@@ -291,6 +291,8 @@ def parse_opt():\n                         help='nr of trials for evolution')\n     parser.add_argument('--resume', action='store_true',\n                         help='resume hparam search')\n+    parser.add_argument('--save', action='store_true',\n+                        help='save video tracking results')\n     parser.add_argument('--processes-per-device', type=int, default=2,\n                         help='how many subprocesses can be invoked per GPU (to manage memory consumption)')\n     parser.add_argument('--objectives', type=str, default='HOTA,MOTA,IDF1',\n",add,Add note about data volume to enable_metrics_collection
2e5bc899dbae4b18f51d40b0d791f52d8f89af8c,fix botsort with new det feature extraction,boxmot/trackers/botsort/bot_sort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SparseOptFlow\nfrom boxmot.motion.kalman_filters.adapters import BotSortKalmanFilterAdapter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = BotSortKalmanFilterAdapter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        s","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SparseOptFlow\nfrom boxmot.motion.kalman_filters.adapters import BotSortKalmanFilterAdapter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = BotSortKalmanFilterAdapter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        s","@@ -324,14 +324,14 @@ class BoTSORT(object):\n         self.height, self.width = img.shape[:2]\n \n         """"""""""""Extract embeddings """"""""""""\n-        features_keep = self.model.get_features(dets, img)\n+        features_keep = self.model.get_features(xyxys[remain_inds], img)\n         dets[:, :4], img\n \n         if len(dets) > 0:\n             """"""""""""Detections""""""""""""\n \n             detections = [\n-                STrack(xyxy, s, c, f.cpu().numpy())\n+                STrack(xyxy, s, c, f)\n                 for (xyxy, s, c, f) in zip(\n                     dets, scores_keep, classes_keep, features_keep\n                 )\n",add,Added the UNSTARTED state to the YouTube PlayerState enum
4c317ca7b3e2910588ef64f2dea82e17676dfc84,"FIX: resize in cv2 is w, h; in torchvision h, w",boxmot/appearance/reid_multibackend.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport cv2\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\ntr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport cv2\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\ntr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else","@@ -54,7 +54,6 @@ class ReIDDetectMultiBackend(nn.Module):\n         self.fp16 = fp16\n         self.fp16 &= self.pt or self.jit or self.engine  # FP16\n         self.device = device\n-        self.image_size = (256, 128)\n         self.nhwc = self.tflite  # activate bhwc --> bcwh\n \n         model_name = get_model_name(w)\n@@ -192,13 +191,13 @@ class ReIDDetectMultiBackend(nn.Module):\n         for box in xyxys:\n             x1, y1, x2, y2 = box.astype('int')\n             crop = img[y1:y2, x1:x2]\n-\n             # resize\n             crop = cv2.resize(\n                 crop,\n-                self.image_size,\n+                (128, 256),  # from (x, y) to (128, 256) | (w, h)\n                 interpolation=cv2.INTER_LINEAR,\n             )\n+\n             # (cv2) BGR 2 (PIL) RGB. The ReID models have been trained with this channel order\n             crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n \n@@ -213,7 +212,7 @@ class ReIDDetectMultiBackend(nn.Module):\n             crops.append(crop)\n \n         crops = torch.stack(crops, dim=0)\n-        crops = torch.permute(crops, (0, 3, 1, 2))  # (b, h, w, c) --> (b, c, h, w)\n+        crops = torch.permute(crops, (0, 3, 1, 2))\n         crops = crops.to(dtype=torch.half if self.fp16 else torch.float, device=self.device)\n \n         return crops\n",add,Add note about data volume to enable_metrics_collection
6afb31b73704c56944bd3e713001a3300a35476e,delete debug print,boxmot/appearance/backbones/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import\n\nfrom boxmot.appearance.backbones.clip.make_model import make_model\nfrom boxmot.appearance.backbones.hacnn import HACNN\nfrom boxmot.appearance.backbones.lmbn.lmbn_n import LMBN_n\nfrom boxmot.appearance.backbones.mlfn import mlfn\nfrom boxmot.appearance.backbones.mobilenetv2 import (mobilenetv2_x1_0,\n                                                     mobilenetv2_x1_4)\nfrom boxmot.appearance.backbones.osnet import (osnet_ibn_x1_0, osnet_x0_5,\n                                               osnet_x0_25, osnet_x0_75,\n                                               osnet_x1_0)\nfrom boxmot.appearance.backbones.osnet_ain import (osnet_ain_x0_5,\n                                                   osnet_ain_x0_25,\n                                                   osnet_ain_x0_75,\n                                                   osnet_ain_x1_0)\nfrom boxmot.appearance.backbones.resnet imp","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import\n\nfrom boxmot.appearance.backbones.clip.make_model import make_model\nfrom boxmot.appearance.backbones.hacnn import HACNN\nfrom boxmot.appearance.backbones.lmbn.lmbn_n import LMBN_n\nfrom boxmot.appearance.backbones.mlfn import mlfn\nfrom boxmot.appearance.backbones.mobilenetv2 import (mobilenetv2_x1_0,\n                                                     mobilenetv2_x1_4)\nfrom boxmot.appearance.backbones.osnet import (osnet_ibn_x1_0, osnet_x0_5,\n                                               osnet_x0_25, osnet_x0_75,\n                                               osnet_x1_0)\nfrom boxmot.appearance.backbones.osnet_ain import (osnet_ain_x0_5,\n                                                   osnet_ain_x0_25,\n                                                   osnet_ain_x0_75,\n                                                   osnet_ain_x1_0)\nfrom boxmot.appearance.backbones.resnet imp","@@ -55,7 +55,6 @@ def show_avai_models():\n \n def get_nr_classes(weigths):\n     num_classes = [value for key, value in NR_CLASSES_DICT.items() if key in str(weigths.name)][0]\n-    print('num_classes\n\n', num_classes)\n     return num_classes\n \n \n",add,Added KHR_gl_texture_2D_image extension string
baac1387cfc0c72a138768606879360b15398385,fix list out of ranfge,boxmot/appearance/backbones/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import\n\nfrom boxmot.appearance.backbones.clip.make_model import make_model\nfrom boxmot.appearance.backbones.hacnn import HACNN\nfrom boxmot.appearance.backbones.lmbn.lmbn_n import LMBN_n\nfrom boxmot.appearance.backbones.mlfn import mlfn\nfrom boxmot.appearance.backbones.mobilenetv2 import (mobilenetv2_x1_0,\n                                                     mobilenetv2_x1_4)\nfrom boxmot.appearance.backbones.osnet import (osnet_ibn_x1_0, osnet_x0_5,\n                                               osnet_x0_25, osnet_x0_75,\n                                               osnet_x1_0)\nfrom boxmot.appearance.backbones.osnet_ain import (osnet_ain_x0_5,\n                                                   osnet_ain_x0_25,\n                                                   osnet_ain_x0_75,\n                                                   osnet_ain_x1_0)\nfrom boxmot.appearance.backbones.resnet imp","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import\n\nfrom boxmot.appearance.backbones.clip.make_model import make_model\nfrom boxmot.appearance.backbones.hacnn import HACNN\nfrom boxmot.appearance.backbones.lmbn.lmbn_n import LMBN_n\nfrom boxmot.appearance.backbones.mlfn import mlfn\nfrom boxmot.appearance.backbones.mobilenetv2 import (mobilenetv2_x1_0,\n                                                     mobilenetv2_x1_4)\nfrom boxmot.appearance.backbones.osnet import (osnet_ibn_x1_0, osnet_x0_5,\n                                               osnet_x0_25, osnet_x0_75,\n                                               osnet_x1_0)\nfrom boxmot.appearance.backbones.osnet_ain import (osnet_ain_x0_5,\n                                                   osnet_ain_x0_25,\n                                                   osnet_ain_x0_75,\n                                                   osnet_ain_x1_0)\nfrom boxmot.appearance.backbones.resnet imp","@@ -54,7 +54,11 @@ def show_avai_models():\n \n \n def get_nr_classes(weigths):\n-    num_classes = [value for key, value in NR_CLASSES_DICT.items() if key in str(weigths.name)][0]\n+    num_classes = [value for key, value in NR_CLASSES_DICT.items() if key in str(weigths.name)]\n+    if len(num_classes) == 0:\n+        num_classes = 1\n+    else:\n+        num_classes = num_classes[0]\n     return num_classes\n \n \n",add,Added limits . h to groups
7537b086369f1bb6aac05f0f56710f404ad29dee,fix loading,examples/detectors/yolox.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect.predict import DetectionPredictor\nfrom ultralytics.utils import ops\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import WEIGHTS\n\nfrom .yolo_interface import YoloInterface\n\nYOLOX_ZOO = {\n    'yolox_n': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass YoloXStrategy(DetectionPredictor, YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = F","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom pathlib import Path\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect.predict import DetectionPredictor\nfrom ultralytics.utils import ops\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import WEIGHTS\n\nfrom .yolo_interface import YoloInterface\n\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass YoloXStrategy(DetectionPredictor, YoloInterface):\n    pt = False\n    st","@@ -1,5 +1,7 @@\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n+from pathlib import Path\n+\n import gdown\n import torch\n from ultralytics.engine.results import Results\n@@ -14,11 +16,11 @@ from boxmot.utils import WEIGHTS\n from .yolo_interface import YoloInterface\n \n YOLOX_ZOO = {\n-    'yolox_n': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n-    'yolox_s': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n-    'yolox_m': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n-    'yolox_l': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n-    'yolox_x': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n+    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n+    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n+    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n+    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n+    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n }\n \n \n@@ -49,28 +51,30 @@ class YoloXStrategy(DetectionPredictor, YoloInterface):\n     def __init__(self, model, device, args):\n \n         self.args = args\n-        self.has_run = False\n         self.pt = False\n         self.stride = 32  # max stride in YOLOX\n \n-        model = str(model)\n-        if model == 'yolox_n':\n+        # one of: 'yolox_n', 'yolox_s', 'yolox_m', 'yolox_l', 'yolox_x'\n+        model_type = str([Path(key).with_suffix('') for key in YOLOX_ZOO.keys() if key in str(model)][0])\n+        if model_type == 'yolox_n':\n             exp = get_exp(None, 'yolox_nano')\n         else:\n-            exp = get_exp(None, model)\n-        exp.num_classes = 1  # bytetrack yolox models\n+            exp = get_exp(None, model_type)\n+        # needed for bytetrack yolox people models, update with your cu",add,Added TypeSpec .
7537b086369f1bb6aac05f0f56710f404ad29dee,fix loading,examples/track.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom functools import partial\nfrom pathlib import Path\n\nimport torch\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom examples.detectors import get_yolo_inferer\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\nfrom ultralytics.utils.plotting import save_one_box\n\nfrom examples.utils import write_mot_results\n\n\ndef on_predict_start(predictor, persist=False):\n    """"""""""""\n    Initialize trackers for object tracking during prediction.\n\n    Args:\n        predictor (object): The predictor object to initialize trackers for.\n        persist (bool, optional): Whether to persist the trackers if they already exist. Defaults to ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom functools import partial\nfrom pathlib import Path\n\nimport torch\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom examples.detectors import get_yolo_inferer\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\nfrom ultralytics.utils.plotting import save_one_box\n\nfrom examples.utils import write_mot_results\n\n\ndef on_predict_start(predictor, persist=False):\n    """"""""""""\n    Initialize trackers for object tracking during prediction.\n\n    Args:\n        predictor (object): The predictor object to initialize trackers for.\n        persist (bool, optional): Whether to persist the trackers if they already exist. Defaults to ","@@ -61,7 +61,7 @@ def on_predict_start(predictor, persist=False):\n def run(args):\n \n     yolo = YOLO(\n-        'yolov8n.pt',\n+        args.yolo_model if 'yolov8' in str(args.yolo_model) else 'yolov8n.pt',\n     )\n \n     results = yolo.track(\n@@ -78,7 +78,8 @@ def run(args):\n         exist_ok=args.exist_ok,\n         project=args.project,\n         name=args.name,\n-        classes=args.classes\n+        classes=args.classes,\n+        imgsz=args.imgsz\n     )\n \n     yolo.add_callback('on_predict_start', partial(on_predict_start, persist=True))\n@@ -134,8 +135,10 @@ def run(args):\n \n def parse_opt():\n     parser = argparse.ArgumentParser()\n-    parser.add_argument('--yolo-model', type=Path, default='yolov8n', help='model.pt path(s)')\n-    parser.add_argument('--reid-model', type=Path, default=WEIGHTS / 'osnet_x0_25_msmt17.pt')\n+    parser.add_argument('--yolo-model', type=Path, default=WEIGHTS / 'yolov8n',\n+                        help='yolo model path')\n+    parser.add_argument('--reid-model', type=Path, default=WEIGHTS / 'osnet_x0_25_msmt17.pt',\n+                        help='reid model path')\n     parser.add_argument('--tracking-method', type=str, default='deepocsort',\n                         help='deepocsort, botsort, strongsort, ocsort, bytetrack')\n     parser.add_argument('--source', type=str, default='0',\n@@ -152,7 +155,7 @@ def parse_opt():\n                         help='display tracking video results')\n     parser.add_argument('--save', action='store_true',\n                         help='save video tracking results')\n-    # # class 0 is person, 1 is bycicle, 2 is car... 79 is oven\n+    # class 0 is person, 1 is bycicle, 2 is car... 79 is oven\n     parser.add_argument('--classes', nargs='+', type=int,\n                         help='filter by class: --classes 0, or --classes 0 2 3')\n     parser.add_argument('--project', default=ROOT / 'runs' / 'track',\n",add,Add forced permission to for typedefs
bd1c036e5c35349f0129436b8a0f0aa5afa11a71,fix num class setting for yolox crowdhuman models,examples/detectors/yolox.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom pathlib import Path\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect.predict import DetectionPredictor\nfrom ultralytics.utils import ops\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import WEIGHTS\n\nfrom .yolo_interface import YoloInterface\n\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass YoloXStrategy(DetectionPredictor, YoloInterface):\n    pt = False\n    st","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom pathlib import Path\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect.predict import DetectionPredictor\nfrom ultralytics.utils import ops\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom examples.detectors.yolo_interface import YoloInterface\n\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass YoloXStrategy(DetectionPredictor, YoloInterface):\n    pt = False\n    stride = 32\n    fp1","@@ -11,9 +11,7 @@ from yolox.exp import get_exp\n from yolox.utils import postprocess\n from yolox.utils.model_utils import fuse_model\n \n-from boxmot.utils import WEIGHTS\n-\n-from .yolo_interface import YoloInterface\n+from examples.detectors.yolo_interface import YoloInterface\n \n YOLOX_ZOO = {\n     'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n@@ -54,34 +52,42 @@ class YoloXStrategy(DetectionPredictor, YoloInterface):\n         self.pt = False\n         self.stride = 32  # max stride in YOLOX\n \n-        # one of: 'yolox_n', 'yolox_s', 'yolox_m', 'yolox_l', 'yolox_x'\n-        model_type = str([Path(key).with_suffix('') for key in YOLOX_ZOO.keys() if key in str(model)][0])\n+        # model_type one of: 'yolox_n', 'yolox_s', 'yolox_m', 'yolox_l', 'yolox_x'\n+        for key in YOLOX_ZOO.keys():\n+            if Path(key).stem in str(model.name):\n+                model_type = str(Path(key).with_suffix(''))\n+                break\n+\n         if model_type == 'yolox_n':\n             exp = get_exp(None, 'yolox_nano')\n         else:\n             exp = get_exp(None, model_type)\n-        # needed for bytetrack yolox people models, update with your custom model needs\n-        exp.num_classes = 1\n-\n-        self.model = exp.get_model()\n-        self.model.eval()\n-\n+        # download crowdhuman bytetrack models\n         if not model.exists() and model.stem == model_type:\n             gdown.download(\n-                url=YOLOX_ZOO[model_type],\n-                output=str(WEIGHTS / model.name),\n+                url=YOLOX_ZOO[model_type + '.pt'],\n+                output=str(model),\n                 quiet=False\n             )\n+            # needed for bytetrack yolox people models\n+            # update with your custom model needs\n+            exp.num_classes = 1\n+        elif model.stem == model_type:\n+            exp.num_classes = 1\n \n         ckpt = torch.load(\n             str(model),\n             map",add,Add note about data volume to enable_metrics_collection
9849eda5aaa15dc9a215d0bb105efb8c2cad8d1b,fix custom yolo_nas model loading,examples/detectors/yolonas.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nfrom super_gradients.training import models\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\n\nfrom examples.detectors.yolo_interface import YoloInterface\n\n\nclass YoloNASStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = {\n        0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus',\n        6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n        11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat',\n        16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant',\n        21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella',\n        26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis',\n        31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove',\n     ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\n\nfrom boxmot.utils import logger as LOGGER\nfrom examples.detectors.yolo_interface import YoloInterface\n\n\nclass YoloNASStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = {\n        0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus',\n        6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n        11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat',\n        16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant',\n        21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella',\n        26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis',\n     ","@@ -2,10 +2,12 @@\n \n import numpy as np\n import torch\n+from super_gradients.common.object_names import Models\n from super_gradients.training import models\n from ultralytics.engine.results import Results\n from ultralytics.utils import ops\n \n+from boxmot.utils import logger as LOGGER\n from examples.detectors.yolo_interface import YoloInterface\n \n \n@@ -36,10 +38,22 @@ class YoloNASStrategy(YoloInterface):\n     def __init__(self, model, device, args):\n         self.args = args\n \n-        self.model = models.get(\n-            str(model),\n-            pretrained_weights=""""coco""""\n-        ).to(device)\n+        avail_models = [x.lower() for x in list(Models.__dict__.keys())]\n+        model_type = self.get_model_from_weigths(avail_models, model)\n+\n+        LOGGER.info(f'Loading {model_type} with {str(model)}')\n+        if not model.exists() and model.stem == model_type:\n+            LOGGER.info('Downloading pretrained weights...')\n+            self.model = models.get(\n+                model_type,\n+                pretrained_weights=""""coco""""\n+            ).to(device)\n+        else:\n+            self.model = models.get(\n+                model_type,\n+                num_classes=-1,  # set your num classes\n+                checkpoint_path=str(model)\n+            ).to(device)\n \n         self.device = device\n \n",add,Add note about data volume to enable_metrics_collection
9849eda5aaa15dc9a215d0bb105efb8c2cad8d1b,fix custom yolo_nas model loading,examples/detectors/yolox.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom pathlib import Path\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect.predict import DetectionPredictor\nfrom ultralytics.utils import ops\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom examples.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass YoloXStrategy(DetectionPredictor, YoloInterf","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom examples.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass YoloXStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False","@@ -1,16 +1,14 @@\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n-from pathlib import Path\n-\n import gdown\n import torch\n from ultralytics.engine.results import Results\n-from ultralytics.models.yolo.detect.predict import DetectionPredictor\n from ultralytics.utils import ops\n from yolox.exp import get_exp\n from yolox.utils import postprocess\n from yolox.utils.model_utils import fuse_model\n \n+from boxmot.utils import logger as LOGGER\n from examples.detectors.yolo_interface import YoloInterface\n \n # default model weigths for these model names\n@@ -23,7 +21,7 @@ YOLOX_ZOO = {\n }\n \n \n-class YoloXStrategy(DetectionPredictor, YoloInterface):\n+class YoloXStrategy(YoloInterface):\n     pt = False\n     stride = 32\n     fp16 = False\n@@ -54,17 +52,18 @@ class YoloXStrategy(DetectionPredictor, YoloInterface):\n         self.stride = 32  # max stride in YOLOX\n \n         # model_type one of: 'yolox_n', 'yolox_s', 'yolox_m', 'yolox_l', 'yolox_x'\n-        for key in YOLOX_ZOO.keys():\n-            if Path(key).stem in str(model.name):\n-                model_type = str(Path(key).with_suffix(''))\n-                break\n+        model_type = self.get_model_from_weigths(YOLOX_ZOO.keys(), model)\n \n         if model_type == 'yolox_n':\n             exp = get_exp(None, 'yolox_nano')\n         else:\n             exp = get_exp(None, model_type)\n+\n+        LOGGER.info(f'Loading {model_type} with {str(model)}')\n+\n         # download crowdhuman bytetrack models\n         if not model.exists() and model.stem == model_type:\n+            LOGGER.info('Downloading pretrained weights...')\n             gdown.download(\n                 url=YOLOX_ZOO[model_type + '.pt'],\n                 output=str(model),\n",add,Don ' t define the right side title
d93b2373820100db23d8ff8a9413532246ac206b,fix det ind,boxmot/trackers/botsort/bot_sort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SparseOptFlow\nfrom boxmot.motion.kalman_filters.adapters import BotSortKalmanFilterAdapter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = BotSortKalmanFilterAdapter()\n\n    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.cls = -1\n        self.cls_hist = []  # (cls id, freq)\n        s","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SparseOptFlow\nfrom boxmot.motion.kalman_filters.adapters import BotSortKalmanFilterAdapter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = BotSortKalmanFilterAdapter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self._tlwh = det[0:4]\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        self.cls_hist = []  # (c","@@ -16,18 +16,18 @@ from boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n class STrack(BaseTrack):\n     shared_kalman = BotSortKalmanFilterAdapter()\n \n-    def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n+    def __init__(self, det, feat=None, feat_history=50):\n         # wait activate\n-        self._tlwh = np.asarray(tlwh, dtype=np.float32)\n+        self._tlwh = det[0:4]\n+        self.score = det[4]\n+        self.cls = det[5]\n+        self.det_ind = det[6]\n         self.kalman_filter = None\n         self.mean, self.covariance = None, None\n         self.is_activated = False\n-\n-        self.cls = -1\n         self.cls_hist = []  # (cls id, freq)\n-        self.update_cls(cls, score)\n+        self.update_cls(self.cls, self.score)\n \n-        self.score = score\n         self.tracklet_len = 0\n \n         self.smooth_feat = None\n@@ -113,7 +113,7 @@ class STrack(BaseTrack):\n     def activate(self, kalman_filter, frame_id):\n         """"""""""""Start a new tracklet""""""""""""\n         self.kalman_filter = kalman_filter\n-        self.track_id = self.next_id()\n+        self.id = self.next_id()\n \n         self.mean, self.covariance = self.kalman_filter.initiate(\n             self.tlwh_to_xywh(self._tlwh)\n@@ -137,8 +137,10 @@ class STrack(BaseTrack):\n         self.is_activated = True\n         self.frame_id = frame_id\n         if new_id:\n-            self.track_id = self.next_id()\n+            self.id = self.next_id()\n         self.score = new_track.score\n+        self.cls = new_track.cls\n+        self.det_ind = new_track.det_ind\n \n         self.update_cls(new_track.cls, new_track.score)\n \n@@ -166,6 +168,8 @@ class STrack(BaseTrack):\n         self.is_activated = True\n \n         self.score = new_track.score\n+        self.cls = new_track.cls\n+        self.det_ind = new_track.det_ind\n         self.update_cls(new_track.cls, new_track.score)\n \n     @property\n@@ -232,7 +236,7 @@ class STrack(BaseTrack):\n         return ret\n \n ",add,Added STORM - 360 to CHANGELOG
34b63a25914f4ecf41a95bcfa5ac7d0e899d9557,fix det_ind output,boxmot/trackers/strongsort/sort/detection.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\n\nclass Detection(object):\n    """"""""""""\n    This class represents a bounding box detection in a single image.\n\n    Parameters\n    ----------\n    tlwh : array_like\n        Bounding box in format `(x, y, w, h)`.\n    confidence : float\n        Detector confidence score.\n    feature : array_like\n        A feature vector that describes the object contained in this image.\n\n    Attributes\n    ----------\n    tlwh : ndarray\n        Bounding box in format `(top left x, top left y, width, height)`.\n    confidence : ndarray\n        Detector confidence score.\n    feature : ndarray | NoneType\n        A feature vector that describes the object contained in this image.\n\n    """"""""""""\n\n    def __init__(self, tlwh, confidence, feature):\n        self.tlwh = np.asarray(tlwh, dtype=np.float32)\n        self.confidence = float(confidence)\n        self.feature = feature\n\n    def to_xyah(self):\n        """"""""""""C","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nclass Detection(object):\n    """"""""""""\n    This class represents a bounding box detection in a single image.\n\n    Parameters\n    ----------\n    tlwh : array_like\n        Bounding box in format `(x, y, w, h)`.\n    confidence : float\n        Detector confidence score.\n    feature : array_like\n        A feature vector that describes the object contained in this image.\n\n    Attributes\n    ----------\n    tlwh : ndarray\n        Bounding box in format `(top left x, top left y, width, height)`.\n    confidence : ndarray\n        Detector confidence score.\n    feature : ndarray | NoneType\n        A feature vector that describes the object contained in this image.\n\n    """"""""""""\n\n    def __init__(self, tlwh, conf, cls, det_ind, feat):\n        self.tlwh = tlwh\n        self.conf = conf\n        self.cls = cls\n        self.det_ind = det_ind\n        self.feat = feat\n\n    def to_xyah(self):\n        """"""""""""Convert bounding bo","@@ -1,8 +1,5 @@\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n-import numpy as np\n-\n-\n class Detection(object):\n     """"""""""""\n     This class represents a bounding box detection in a single image.\n@@ -27,10 +24,12 @@ class Detection(object):\n \n     """"""""""""\n \n-    def __init__(self, tlwh, confidence, feature):\n-        self.tlwh = np.asarray(tlwh, dtype=np.float32)\n-        self.confidence = float(confidence)\n-        self.feature = feature\n+    def __init__(self, tlwh, conf, cls, det_ind, feat):\n+        self.tlwh = tlwh\n+        self.conf = conf\n+        self.cls = cls\n+        self.det_ind = det_ind\n+        self.feat = feat\n \n     def to_xyah(self):\n         """"""""""""Convert bounding box to format `(center x, center y, aspect ratio,\n",add,Added STORM - 370 to Changelog
34b63a25914f4ecf41a95bcfa5ac7d0e899d9557,fix det_ind output,boxmot/trackers/strongsort/sort/track.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import StrongSortKalmanFilterAdapter\n\n\nclass TrackState:\n    """"""""""""\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n\n    """"""""""""\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    """"""""""""\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        C","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import StrongSortKalmanFilterAdapter\n\n\nclass TrackState:\n    """"""""""""\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n\n    """"""""""""\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    """"""""""""\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initial s","@@ -1,7 +1,5 @@\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n-from collections import deque\n-\n import numpy as np\n \n from boxmot.motion.kalman_filters.adapters import StrongSortKalmanFilterAdapter\n@@ -72,16 +70,16 @@ class Track:\n     def __init__(\n         self,\n         detection,\n-        track_id,\n-        class_id,\n-        conf,\n+        id,\n         n_init,\n         max_age,\n         ema_alpha,\n-        feature=None,\n     ):\n-        self.track_id = track_id\n-        self.class_id = int(class_id)\n+        self.id = id\n+        self.bbox = detection.to_xyah()\n+        self.conf = detection.conf\n+        self.cls = detection.cls\n+        self.det_ind = detection.det_ind\n         self.hits = 1\n         self.age = 1\n         self.time_since_update = 0\n@@ -89,19 +87,15 @@ class Track:\n \n         self.state = TrackState.Tentative\n         self.features = []\n-        if feature is not None:\n-            feature /= np.linalg.norm(feature)\n-            self.features.append(feature)\n+        if detection.feat is not None:\n+            detection.feat /= np.linalg.norm(detection.feat)\n+            self.features.append(detection.feat)\n \n-        self.conf = conf\n         self._n_init = n_init\n         self._max_age = max_age\n \n         self.kf = StrongSortKalmanFilterAdapter()\n-        self.mean, self.covariance = self.kf.initiate(detection)\n-\n-        # Initializing trajectory queue\n-        self.q = deque(maxlen=25)\n+        self.mean, self.covariance = self.kf.initiate(self.bbox)\n \n     def to_tlwh(self):\n         """"""""""""Get current position in bounding box format `(top left x, top left y,\n@@ -155,7 +149,7 @@ class Track:\n         self.age += 1\n         self.time_since_update += 1\n \n-    def update(self, detection, class_id, conf):\n+    def update(self, detection):\n         """"""""""""Perform Kalman filter measurement update step and update the feature\n         cache.\n         Parameters\n@@ -163,13 +15",add,Added the UNSTARTED state to the YouTube PlayerState enum
34b63a25914f4ecf41a95bcfa5ac7d0e899d9557,fix det_ind output,boxmot/trackers/strongsort/sort/tracker.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import\n\nimport numpy as np\n\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.strongsort.sort import iou_matching, linear_assignment\nfrom boxmot.trackers.strongsort.sort.track import Track\nfrom boxmot.utils.matching import chi2inv95\n\n\nclass Tracker:\n    """"""""""""\n    This is the multi-target tracker.\n    Parameters\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        A distance metric for measurement-to-track association.\n    max_age : int\n        Maximum number of missed misses before a track is deleted.\n    n_init : int\n        Number of consecutive detections before the track is confirmed. The\n        track state is set to `Deleted` if a miss occurs within the first\n        `n_init` frames.\n    Attributes\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        The distance metric used for measurement to track ass","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import\n\nimport numpy as np\n\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.strongsort.sort import iou_matching, linear_assignment\nfrom boxmot.trackers.strongsort.sort.track import Track\nfrom boxmot.utils.matching import chi2inv95\n\n\nclass Tracker:\n    """"""""""""\n    This is the multi-target tracker.\n    Parameters\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        A distance metric for measurement-to-track association.\n    max_age : int\n        Maximum number of missed misses before a track is deleted.\n    n_init : int\n        Number of consecutive detections before the track is confirmed. The\n        track state is set to `Deleted` if a miss occurs within the first\n        `n_init` frames.\n    Attributes\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        The distance metric used for measurement to track ass","@@ -72,7 +72,7 @@ class Tracker:\n             track.increment_age()\n             track.mark_missed()\n \n-    def update(self, detections, classes, confidences):\n+    def update(self, detections):\n         """"""""""""Perform measurement update and track management.\n \n         Parameters\n@@ -86,37 +86,29 @@ class Tracker:\n \n         # Update track set.\n         for track_idx, detection_idx in matches:\n-            self.tracks[track_idx].update(\n-                detections[detection_idx],\n-                classes[detection_idx],\n-                confidences[detection_idx],\n-            )\n+            self.tracks[track_idx].update(detections[detection_idx])\n         for track_idx in unmatched_tracks:\n             self.tracks[track_idx].mark_missed()\n         for detection_idx in unmatched_detections:\n-            self._initiate_track(\n-                detections[detection_idx],\n-                classes[detection_idx].item(),\n-                confidences[detection_idx].item(),\n-            )\n+            self._initiate_track(detections[detection_idx])\n         self.tracks = [t for t in self.tracks if not t.is_deleted()]\n \n         # Update distance metric.\n-        active_targets = [t.track_id for t in self.tracks if t.is_confirmed()]\n+        active_targets = [t.id for t in self.tracks if t.is_confirmed()]\n         features, targets = [], []\n         for track in self.tracks:\n             if not track.is_confirmed():\n                 continue\n             features += track.features\n-            targets += [track.track_id for _ in track.features]\n+            targets += [track.id for _ in track.features]\n         self.metric.partial_fit(\n             np.asarray(features), np.asarray(targets), active_targets\n         )\n \n     def _match(self, detections):\n         def gated_metric(tracks, dets, track_indices, detection_indices):\n-            features = np.array([dets[i].feature for i in detection_indices])\n-            targets = np",add,Added Type name for DFI ( # 3 )
34b63a25914f4ecf41a95bcfa5ac7d0e899d9557,fix det_ind output,boxmot/trackers/strongsort/strong_sort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.strongsort.sort.detection import Detection\nfrom boxmot.trackers.strongsort.sort.tracker import Tracker\nfrom boxmot.utils.matching import NearestNeighborDistanceMetric\nfrom boxmot.utils.ops import xyxy2tlwh\n\n\nclass StrongSORT(object):\n    def __init__(\n        self,\n        model_weights,\n        device,\n        fp16,\n        max_dist=0.2,\n        max_iou_dist=0.7,\n        max_age=30,\n        n_init=1,\n        nn_budget=100,\n        mc_lambda=0.995,\n        ema_alpha=0.9,\n    ):\n        self.model = ReIDDetectMultiBackend(\n            weights=model_weights,\n            device=device,\n            fp16=fp16\n        )\n        self.tracker = Tracker(\n            metric=NearestNeighborDistanceMetric(""""cosine"""", max_dist, nn_budget),\n            max_iou_","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.strongsort.sort.detection import Detection\nfrom boxmot.trackers.strongsort.sort.tracker import Tracker\nfrom boxmot.utils.matching import NearestNeighborDistanceMetric\nfrom boxmot.utils.ops import xyxy2tlwh\n\n\nclass StrongSORT(object):\n    def __init__(\n        self,\n        model_weights,\n        device,\n        fp16,\n        max_dist=0.2,\n        max_iou_dist=0.7,\n        max_age=30,\n        n_init=1,\n        nn_budget=100,\n        mc_lambda=0.995,\n        ema_alpha=0.9,\n    ):\n        self.model = ReIDDetectMultiBackend(\n            weights=model_weights,\n            device=device,\n            fp16=fp16\n        )\n        self.tracker = Tracker(\n            metric=NearestNeighborDistanceMetric(""""cosine"""", max_dist, nn_budget),\n            max_iou_","@@ -53,9 +53,11 @@ class StrongSORT(object):\n             dets.shape[1] == 6\n         ), """"Unsupported 'dets' 2nd dimension lenght, valid lenghts is 6""""\n \n+        dets = np.hstack([dets, np.arange(len(dets)).reshape(-1, 1)])\n         xyxy = dets[:, 0:4]\n         confs = dets[:, 4]\n         clss = dets[:, 5]\n+        det_ind = dets[:, 6]\n \n         if len(self.tracker.tracks) >= 1:\n             warp_matrix = self.cmc.apply(img, xyxy)\n@@ -67,27 +69,31 @@ class StrongSORT(object):\n \n         tlwh = xyxy2tlwh(xyxy)\n         detections = [\n-            Detection(a, b, c) for a, b, c in zip(tlwh, confs, features)\n+            Detection(box, conf, cls, det_ind, feat) for\n+            box, conf, cls, det_ind, feat in\n+            zip(tlwh, confs, clss, det_ind, features)\n         ]\n \n         # update tracker\n         self.tracker.predict()\n-        self.tracker.update(detections, clss, confs)\n+        self.tracker.update(detections)\n \n         # output bbox identities\n         outputs = []\n         for track in self.tracker.tracks:\n-            if not track.is_confirmed() or track.time_since_update > 1:\n+            if not track.is_confirmed() or track.time_since_update >= 1:\n                 continue\n \n             x1, y1, x2, y2 = track.to_tlbr()\n \n-            track_id = track.track_id\n-            class_id = track.class_id\n+            id = track.id\n             conf = track.conf\n+            cls = track.cls\n+            det_ind = track.det_ind\n \n             outputs.append(\n-                np.array([x1, y1, x2, y2, track_id, conf, class_id], dtype=np.float64)\n+                np.concatenate(([x1, y1, x2, y2], [id], [conf], [cls], [det_ind])).reshape(1, -1)\n             )\n-        outputs = np.asarray(outputs)\n-        return outputs\n+        if len(outputs) > 0:\n+            return np.concatenate(outputs)\n+        return np.array([])\n",add,Added example for MAP type in documentation
14964807dea585fd841ec05925926f668fa72704,fix det ind passing to tracker,boxmot/trackers/ocsort/ocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0","@@ -307,7 +307,7 @@ class OCSort(object):\n                     if iou_left[m[0], m[1]] < self.iou_threshold:\n                         continue\n                     self.trackers[trk_ind].update(\n-                        dets_second[det_ind, :5], dets_second[det_ind, 5]\n+                        dets_second[det_ind, :5], dets_second[det_ind, 5], dets_second[det_ind, 6]\n                     )\n                     to_remove_trk_indices.append(trk_ind)\n                 unmatched_trks = np.setdiff1d(\n@@ -332,7 +332,7 @@ class OCSort(object):\n                     det_ind, trk_ind = unmatched_dets[m[0]], unmatched_trks[m[1]]\n                     if iou_left[m[0], m[1]] < self.iou_threshold:\n                         continue\n-                    self.trackers[trk_ind].update(dets[det_ind, :5], dets[det_ind, 5])\n+                    self.trackers[trk_ind].update(dets[det_ind, :5], dets[det_ind, 5], dets[det_ind, 6])\n                     to_remove_det_indices.append(det_ind)\n                     to_remove_trk_indices.append(trk_ind)\n                 unmatched_dets = np.setdiff1d(\n",fix,Add note about data volume to enable_metrics_collection
f29cdf01b4ac43ecabd46337a3ba5af51265d2f3,fix bytetrack,boxmot/trackers/bytetrack/byte_tracker.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import ByteTrackKalmanFilterAdapter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = ByteTrackKalmanFilterAdapter()\n\n    def __init__(self, det):\n        # wait activate\n        self._tlwh = det[0:4]\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        self.tracklet_len = 0\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(\n            mean_state, sel","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import ByteTrackKalmanFilterAdapter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = ByteTrackKalmanFilterAdapter()\n\n    def __init__(self, det):\n        # wait activate\n        self.xywh = det[0:4]  # (xc, yc, w, h)\n        self.xyah = tlwh2xyah(det[0:4])\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        self.tracklet_len = 0\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.","@@ -5,7 +5,7 @@ import numpy as np\n from boxmot.motion.kalman_filters.adapters import ByteTrackKalmanFilterAdapter\n from boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\n from boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\n-from boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n+from boxmot.utils.ops import tlwh2xyah, xywh2xyxy, xyxy2xywh\n \n \n class STrack(BaseTrack):\n@@ -13,7 +13,8 @@ class STrack(BaseTrack):\n \n     def __init__(self, det):\n         # wait activate\n-        self._tlwh = det[0:4]\n+        self.xywh = det[0:4]  # (xc, yc, w, h)\n+        self.xyah = tlwh2xyah(det[0:4])\n         self.score = det[4]\n         self.cls = det[5]\n         self.det_ind = det[6]\n@@ -49,9 +50,7 @@ class STrack(BaseTrack):\n         """"""""""""Start a new tracklet""""""""""""\n         self.kalman_filter = kalman_filter\n         self.track_id = self.next_id()\n-        self.mean, self.covariance = self.kalman_filter.initiate(\n-            self.tlwh_to_xyah(self._tlwh)\n-        )\n+        self.mean, self.covariance = self.kalman_filter.initiate(self.xyah)\n \n         self.tracklet_len = 0\n         self.state = TrackState.Tracked\n@@ -63,7 +62,7 @@ class STrack(BaseTrack):\n \n     def re_activate(self, new_track, frame_id, new_id=False):\n         self.mean, self.covariance = self.kalman_filter.update(\n-            self.mean, self.covariance, self.tlwh_to_xyah(new_track.tlwh)\n+            self.mean, self.covariance, new_track.xyah\n         )\n         self.tracklet_len = 0\n         self.state = TrackState.Tracked\n@@ -87,9 +86,8 @@ class STrack(BaseTrack):\n         self.tracklet_len += 1\n         # self.cls = cls\n \n-        new_tlwh = new_track.tlwh\n         self.mean, self.covariance = self.kalman_filter.update(\n-            self.mean, self.covariance, self.tlwh_to_xyah(new_tlwh)\n+            self.mean, self.covariance, new_track.xyah\n         )\n         self.state = TrackState.Tracked\n         self.is_activated =",add,Added note about data volume to enable_metrics_collection
f29cdf01b4ac43ecabd46337a3ba5af51265d2f3,fix bytetrack,boxmot/utils/matching.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport lap\nimport numpy as np\nimport scipy\nimport torch\nfrom scipy.spatial.distance import cdist\n\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919,\n}\n\n\ndef merge_matches(m1, m2, shape):\n    O, P, Q = shape\n    m1 = np.asarray(m1)\n    m2 = np.asarray(m2)\n\n    M1 = scipy.sparse.coo_matrix((np.ones(len(m1)), (m1[:, 0], m1[:, 1])), shape=(O, P))\n    M2 = scipy.sparse.coo_matrix((np.ones(len(m2)), (m2[:, 0], m2[:, 1])), shape=(P, Q))\n\n    mask = M1 * M2\n    match = mask.nonzero()\n    match = list(zip(match[0], match[1]))\n    unmatched_O = tuple(set(range(O)) - set([i for i, j in match]))\n","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport lap\nimport numpy as np\nimport scipy\nimport torch\nfrom scipy.spatial.distance import cdist\n\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919,\n}\n\n\ndef merge_matches(m1, m2, shape):\n    O, P, Q = shape\n    m1 = np.asarray(m1)\n    m2 = np.asarray(m2)\n\n    M1 = scipy.sparse.coo_matrix((np.ones(len(m1)), (m1[:, 0], m1[:, 1])), shape=(O, P))\n    M2 = scipy.sparse.coo_matrix((np.ones(len(m2)), (m2[:, 0], m2[:, 1])), shape=(P, Q))\n\n    mask = M1 * M2\n    match = mask.nonzero()\n    match = list(zip(match[0], match[1]))\n    unmatched_O = tuple(set(range(O)) - set([i for i, j in match]))\n","@@ -105,8 +105,8 @@ def iou_distance(atracks, btracks):\n         atlbrs = atracks\n         btlbrs = btracks\n     else:\n-        atlbrs = [track.tlbr for track in atracks]\n-        btlbrs = [track.tlbr for track in btracks]\n+        atlbrs = [track.xyxy for track in atracks]\n+        btlbrs = [track.xyxy for track in btracks]\n     _ious = ious(atlbrs, btlbrs)\n     cost_matrix = 1 - _ious\n \n",add,Add note about data volume to enable_metrics_collection
f29cdf01b4ac43ecabd46337a3ba5af51265d2f3,fix bytetrack,boxmot/utils/ops.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\n\n\ndef xyxy2xywh(x):\n    """"""""""""\n    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format.\n\n    Args:\n        x (np.ndarray) or (torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n    Returns:\n       y (np.ndarray) or (torch.Tensor): The bounding box coordinates in (x, y, width, height) format.\n    """"""""""""\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n    y[..., 2] = x[..., 2] - x[..., 0]  # width\n    y[..., 3] = x[..., 3] - x[..., 1]  # height\n    return y\n\n\ndef xywh2xyxy(x):\n    """"""""""""\n    Convert bounding box coordinates from (x_c, y_c, width, height) format to\n    (x1, y1, x2, y2) format where (x1, y1) is the top-left corner and (x2, y2)\n    is the bottom-right corner.\n\n    Arg","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\n\n\ndef xyxy2xywh(x):\n    """"""""""""\n    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format.\n\n    Args:\n        x (np.ndarray) or (torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n    Returns:\n       y (np.ndarray) or (torch.Tensor): The bounding box coordinates in (x, y, width, height) format.\n    """"""""""""\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n    y[..., 2] = x[..., 2] - x[..., 0]  # width\n    y[..., 3] = x[..., 3] - x[..., 1]  # height\n    return y\n\n\ndef xywh2xyxy(x):\n    """"""""""""\n    Convert bounding box coordinates from (x_c, y_c, width, height) format to\n    (x1, y1, x2, y2) format where (x1, y1) is the top-left corner and (x2, y2)\n    is the bottom-right corner.\n\n    Arg","@@ -95,3 +95,58 @@ def tlwh2xyah(x):\n     y[..., 2] = x[..., 2] / x[..., 3]\n     y[..., 3] = x[..., 3]\n     return y\n+\n+\n+def xyah2tlbr(x):\n+    """"""""""""\n+    Convert bounding box coordinates from (xc, yc ,a ,h), where the aspect ratio is `width / height`\n+    to (t, l, w, h)`\n+    """"""""""""\n+    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n+    y[..., 2] = x[..., 2] * x[..., 3]  # (xc, yc, w, h)\n+    y[..., 0] = x[..., 0] - (x[..., 2] / 2)  # xc --> t\n+    y[..., 1] = x[..., 1] - (x[..., 3] / 2)  # yc --> l\n+    y[..., 2] = x[..., 0] + x[..., 2]\n+    y[..., 3] = x[..., 1] + x[..., 3]\n+    return y\n+\n+\n+def tlwh2tlbr(x):\n+    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n+    y[..., 0] = x[..., 0]\n+    y[..., 1] = x[..., 1]\n+    y[..., 2] = x[..., 0] + x[..., 2]\n+    y[..., 3] = x[..., 1] + x[..., 3]\n+    return y\n+\n+\n+def tlbr2tlwh(x):\n+    """"""""""""\n+    """"""""""""\n+    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n+    y[..., 0] = x[..., 0]\n+    y[..., 1] = x[..., 1]\n+    y[..., 2] = x[..., 2] - x[..., 0]\n+    y[..., 3] = x[..., 3] + x[..., 1]\n+    return y\n+\n+\n+def tlbr2xyah(x):\n+    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n+    y[..., 0] = x[..., 0] + (x[..., 2] / 2)  # t --> xc\n+    y[..., 1] = x[..., 1] + (x[..., 3] / 2)  # l --> yc\n+    y[..., 2] = x[..., 2] / x[..., 3]  # w --> a\n+    y[..., 3] = x[..., 3] - x[..., 1]\n+    return y\n+\n+\n+def xyah2tlwh(x):\n+    """"""""""""\n+    Convert bounding box coordinates from (xc, yc ,a ,h), where the aspect ratio is `width / height`\n+    to (t, l, w, h)`\n+    """"""""""""\n+    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n+    y[..., 2] = x[..., 2] * x[..., 3]  # (xc, yc, w, h)\n+    y[..., 0] = x[..., 0] - (x[..., 2] / 2)  # xc --> t\n+    y[..., 1] = x[..., 1] - (x[..., 3] / 2)  # yc --> l\n+    return y\n",add,Added STORM - 236 to Changelog
7f3eaf7c63b6b5c56126042cff7ddb7c92027d86,fix bytetrack,boxmot/trackers/bytetrack/byte_tracker.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import ByteTrackKalmanFilterAdapter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = ByteTrackKalmanFilterAdapter()\n\n    def __init__(self, det):\n        # wait activate\n        self.xywh = det[0:4]  # (xc, yc, w, h)\n        self.xyah = tlwh2xyah(det[0:4])\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        self.tracklet_len = 0\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import ByteTrackKalmanFilterAdapter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = ByteTrackKalmanFilterAdapter()\n\n    def __init__(self, det):\n        # wait activate\n        self.xywh = det[0:4]  # (xc, yc, w, h)\n        self.xyah = tlwh2xyah(det[0:4])\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        self.tracklet_len = 0\n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.","@@ -102,11 +102,11 @@ class STrack(BaseTrack):\n         `(top left, bottom right)`.\n         """"""""""""\n         if self.mean is None:\n-            ret = self.xywh.copy()\n+            ret = self.xywh.copy()  # (xc, yc, w, h)\n         else:\n-            ret = self.mean[:4].copy()  # (xc, yc, a, h)\n-            ret[2] *= ret[3]  # (xc, yc, a, h) --> (xc, yc, w, h)\n-            ret[:2] -= ret[2:] / 2  # (xc, yc, w, h) --> (t, l, w, h)\n+            ret = self.mean[:4].copy()\n+            ret[2] *= ret[3]\n+            ret[:2] -= ret[2:] / 2  # (xc, yc, w, h)\n         ret = xywh2xyxy(ret)\n         return ret\n \n",add,Added hypest who apparently actually wrote the C # port
6329bb88d219aefa194e32764bdac731a1c8e823,fix botsort,boxmot/trackers/botsort/bot_sort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SparseOptFlow\nfrom boxmot.motion.kalman_filters.adapters import BotSortKalmanFilterAdapter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = BotSortKalmanFilterAdapter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self._tlwh = det[0:4]\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        self.cls_hist = []  # (c","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SparseOptFlow\nfrom boxmot.motion.kalman_filters.adapters import BotSortKalmanFilterAdapter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = BotSortKalmanFilterAdapter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self._tlwh = det[0:4]\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        self.cls_hist = []  # (c","@@ -192,25 +192,6 @@ class STrack(BaseTrack):\n         ret[2:] += ret[:2]\n         return ret\n \n-    @property\n-    def xywh(self):\n-        """"""""""""Convert bounding box to format `(min x, min y, max x, max y)`, i.e.,\n-        `(top left, bottom right)`.\n-        """"""""""""\n-        ret = self.tlwh.copy()\n-        ret[:2] += ret[2:] / 2.0\n-        return ret\n-\n-    @staticmethod\n-    def tlwh_to_xyah(tlwh):\n-        """"""""""""Convert bounding box to format `(center x, center y, aspect ratio,\n-        height)`, where the aspect ratio is `width / height`.\n-        """"""""""""\n-        ret = np.asarray(tlwh).copy()\n-        ret[:2] += ret[2:] / 2\n-        ret[2] /= ret[3]\n-        return ret\n-\n     @staticmethod\n     def tlwh_to_xywh(tlwh):\n         """"""""""""Convert bounding box to format `(center x, center y, width,\n@@ -220,24 +201,6 @@ class STrack(BaseTrack):\n         ret[:2] += ret[2:] / 2\n         return ret\n \n-    def to_xywh(self):\n-        return self.tlwh_to_xywh(self.tlwh)\n-\n-    @staticmethod\n-    def tlbr_to_tlwh(tlbr):\n-        ret = np.asarray(tlbr).copy()\n-        ret[2:] -= ret[:2]\n-        return ret\n-\n-    @staticmethod\n-    def tlwh_to_tlbr(tlwh):\n-        ret = np.asarray(tlwh).copy()\n-        ret[2:] += ret[:2]\n-        return ret\n-\n-    def __repr__(self):\n-        return """"OT_{}_({}-{})"""".format(self.id, self.start_frame, self.end_frame)\n-\n \n class BoTSORT(object):\n     def __init__(\n",add,Added example for MAP type in onCreate ( )
31e0754844a31afd03f736a7a7368592f8277d0f,fix wrong shapes,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"examples""""\nWEIGHTS = ROOT / """"examples"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        self.update = method\n\n    def __get__(self, instance, owner):\n        def wrapper(*args, **kwargs):\n            modified_args = list(args)\n            dets = modified_args[0]\n            im = modified_args[1]\n\n            # input one class of detections at a time in order to not mix them up\n            if instance.per_class is True and dets.size != 0:\n                dets_dict = {\n                    class_id: np.array([det for det in dets if det[5] == ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"examples""""\nWEIGHTS = ROOT / """"examples"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        self.update = method\n\n    def __get__(self, instance, owner):\n        def wrapper(*args, **kwargs):\n            modified_args = list(args)\n            dets = modified_args[0]\n            im = modified_args[1]\n\n            # input one class of detections at a time in order to not mix them up\n            if instance.per_class is True and dets.size != 0:\n                dets_dict = {\n                    class_id: np.array([det for det in dets if det[5] == ","@@ -42,7 +42,7 @@ class PerClassDecorator:\n                 # get tracks that are both active and in the current detections\n                 relevant_classes = active_classes.union(detected_classes)\n \n-                mc_dets = np.empty(shape=(0, 7))\n+                mc_dets = np.empty(shape=(0, 8))\n                 for class_id in relevant_classes:\n                     modified_args[0] = np.array(\n                         dets_dict.get(int(class_id), np.empty((0, 6)))\n@@ -56,7 +56,7 @@ class PerClassDecorator:\n                 logger.debug(f""""Per class updates output: {mc_dets.shape}"""")\n             else:\n                 mc_dets = self.update(instance, dets, im)\n-\n+            print('mc_dets.shape', mc_dets.shape)\n             return mc_dets\n \n         return wrapper\n",fix,Add note about data volume to enable
2502d415632bf462f87b545d42b057e1b9697224,fix wrong shapes,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"examples""""\nWEIGHTS = ROOT / """"examples"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        self.update = method\n\n    def __get__(self, instance, owner):\n        def wrapper(*args, **kwargs):\n            modified_args = list(args)\n            dets = modified_args[0]\n            im = modified_args[1]\n\n            # input one class of detections at a time in order to not mix them up\n            if instance.per_class is True and dets.size != 0:\n                dets_dict = {\n                    class_id: np.array([det for det in dets if det[5] == ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"examples""""\nWEIGHTS = ROOT / """"examples"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        self.update = method\n\n    def __get__(self, instance, owner):\n        def wrapper(*args, **kwargs):\n            modified_args = list(args)\n            dets = modified_args[0]\n            im = modified_args[1]\n\n            # input one class of detections at a time in order to not mix them up\n            if instance.per_class is True and dets.size != 0:\n                dets_dict = {\n                    class_id: np.array([det for det in dets if det[5] == ","@@ -56,7 +56,6 @@ class PerClassDecorator:\n                 logger.debug(f""""Per class updates output: {mc_dets.shape}"""")\n             else:\n                 mc_dets = self.update(instance, dets, im)\n-            print('mc_dets.shape', mc_dets.shape)\n             return mc_dets\n \n         return wrapper\n",update,Add known type of dir ( )
ee2797c119dd275690e10acab0d610a32f7d3015,fix,boxmot/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n__version__ = '10.0.37'\n\nfrom boxmot.postprocessing.gsi import gsi\nfrom boxmot.tracker_zoo import create_tracker, get_tracker_config\nfrom boxmot.trackers.botsort.bot_sort import BoTSORT\nfrom boxmot.trackers.bytetrack.byte_tracker import BYTETracker\nfrom boxmot.trackers.deepocsort.deep_ocsort import DeepOCSort as DeepOCSORT\nfrom boxmot.trackers.ocsort.ocsort import OCSort as OCSORT\nfrom boxmot.trackers.strongsort.strong_sort import StrongSORT\n\nTRACKERS = ['bytetrack', 'botsort', 'strongsort', 'ocsort', 'deepocsort']\n\n__all__ = (""""__version__"""",\n           """"StrongSORT"""", """"OCSORT"""", """"BYTETracker"""", """"BoTSORT"""", """"DeepOCSORT"""",\n           """"create_tracker"""", """"get_tracker_config"""", """"gsi"""")\n","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n__version__ = '10.0.37'\n\nfrom boxmot.postprocessing.gsi import gsi\nfrom boxmot.tracker_zoo import create_tracker, get_tracker_config\nfrom boxmot.trackers.botsort.bot_sort import BoTSORT\nfrom boxmot.trackers.bytetrack.byte_tracker import BYTETracker\nfrom boxmot.trackers.deepocsort.deep_ocsort import DeepOCSort as DeepOCSORT\nfrom boxmot.trackers.hybridsort.hybridsort import HybridSORT\nfrom boxmot.trackers.ocsort.ocsort import OCSort as OCSORT\nfrom boxmot.trackers.strongsort.strong_sort import StrongSORT\n\nTRACKERS = ['bytetrack', 'botsort', 'strongsort', 'ocsort', 'deepocsort', 'hybridsort']\n\n__all__ = (""""__version__"""",\n           """"StrongSORT"""", """"OCSORT"""", """"BYTETracker"""", """"BoTSORT"""", """"DeepOCSORT"""", """"HybridSORT"""",\n           """"create_tracker"""", """"get_tracker_config"""", """"gsi"""")\n","@@ -7,11 +7,12 @@ from boxmot.tracker_zoo import create_tracker, get_tracker_config\n from boxmot.trackers.botsort.bot_sort import BoTSORT\n from boxmot.trackers.bytetrack.byte_tracker import BYTETracker\n from boxmot.trackers.deepocsort.deep_ocsort import DeepOCSort as DeepOCSORT\n+from boxmot.trackers.hybridsort.hybridsort import HybridSORT\n from boxmot.trackers.ocsort.ocsort import OCSort as OCSORT\n from boxmot.trackers.strongsort.strong_sort import StrongSORT\n \n-TRACKERS = ['bytetrack', 'botsort', 'strongsort', 'ocsort', 'deepocsort']\n+TRACKERS = ['bytetrack', 'botsort', 'strongsort', 'ocsort', 'deepocsort', 'hybridsort']\n \n __all__ = (""""__version__"""",\n-           """"StrongSORT"""", """"OCSORT"""", """"BYTETracker"""", """"BoTSORT"""", """"DeepOCSORT"""",\n+           """"StrongSORT"""", """"OCSORT"""", """"BYTETracker"""", """"BoTSORT"""", """"DeepOCSORT"""", """"HybridSORT"""",\n            """"create_tracker"""", """"get_tracker_config"""", """"gsi"""")\n",add,Added note about dates .
ee2797c119dd275690e10acab0d610a32f7d3015,fix,boxmot/configs/hybridsort.yaml,,asso_func: giou\nconf_thres: 0.5122620708221085\ndelta_t: 3\ndet_thresh: 0\ninertia: 0.2\niou_thresh: 0.3\nmax_age: 30\nmin_hits: 1\nuse_byte: false\n,"@@ -0,0 +1,9 @@\n+asso_func: giou\n+conf_thres: 0.5122620708221085\n+delta_t: 3\n+det_thresh: 0\n+inertia: 0.2\n+iou_thresh: 0.3\n+max_age: 30\n+min_hits: 1\n+use_byte: false\n",add,Added STORM - 2056 to Changelog
ee2797c119dd275690e10acab0d610a32f7d3015,fix,boxmot/motion/kalman_filters/adapters/hybridsort_kf_adapter.py,,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P).","@@ -0,0 +1,1555 @@\n+# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n+\n+""""""""""""\n+This module implements the linear Kalman filter in both an object\n+oriented and procedural form. The KalmanFilter class implements\n+the filter by storing the various matrices in instance variables,\n+minimizing the amount of bookkeeping you have to do.\n+All Kalman filters operate with a predict->update cycle. The\n+predict step, implemented with the method or function predict(),\n+uses the state transition matrix F to predict the state in the next\n+time period (epoch). The state is stored as a gaussian (x, P), where\n+x is the state (column) vector, and P is its covariance. Covariance\n+matrix Q specifies the process covariance. In Bayesian terms, this\n+prediction is called the *prior*, which you can think of colloquially\n+as the estimate prior to incorporating the measurement.\n+The update step, implemented with the method or function `update()`,\n+incorporates the measurement z with covariance R, into the state\n+estimate (x, P). The class stores the system uncertainty in S,\n+the innovation (residual between prediction and measurement in\n+measurement space) in y, and the Kalman gain in k. The procedural\n+form returns these variables to you. In Bayesian terms this computes\n+the *posterior* - the estimate after the information from the\n+measurement is incorporated.\n+Whether you use the OO form or procedural form is up to you. If\n+matrices such as H, R, and F are changing each epoch, you'll probably\n+opt to use the procedural form. If they are unchanging, the OO\n+form is perhaps easier to use since you won't need to keep track\n+of these matrices. This is especially useful if you are implementing\n+banks of filters or comparing various KF designs for performance;\n+a trivial coding bug could lead to using the wrong sets of matrices.\n+This module also offers an implementation of the RTS smoother, and\n+other helper functions, such as log likelihood computations.\n+Th",add,Added example for MAP type in documentation
ee2797c119dd275690e10acab0d610a32f7d3015,fix,boxmot/tracker_zoo.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom types import SimpleNamespace\n\nimport yaml\n\nfrom boxmot.utils import BOXMOT\n\n\ndef get_tracker_config(tracker_type):\n    tracking_config = \\n        BOXMOT /\\n        'configs' /\\n        (tracker_type + '.yaml')\n    return tracking_config\n\n\ndef create_tracker(tracker_type, tracker_config, reid_weights, device, half, per_class):\n\n    with open(tracker_config, """"r"""") as f:\n        cfg = yaml.load(f.read(), Loader=yaml.FullLoader)\n    cfg = SimpleNamespace(**cfg)  # easier dict acces by dot, instead of ['']\n\n    if tracker_type == 'strongsort':\n        from boxmot.trackers.strongsort.strong_sort import StrongSORT\n        strongsort = StrongSORT(\n            reid_weights,\n            device,\n            half,\n            max_dist=cfg.max_dist,\n            max_iou_dist=cfg.max_iou_dist,\n            max_age=cfg.max_age,\n            n_init=cfg.n_init,\n            nn_budget=cfg.nn_budget,\n            mc_","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom types import SimpleNamespace\n\nimport yaml\n\nfrom boxmot.utils import BOXMOT\n\n\ndef get_tracker_config(tracker_type):\n    tracking_config = \\n        BOXMOT /\\n        'configs' /\\n        (tracker_type + '.yaml')\n    return tracking_config\n\n\ndef create_tracker(tracker_type, tracker_config, reid_weights, device, half, per_class):\n\n    with open(tracker_config, """"r"""") as f:\n        cfg = yaml.load(f.read(), Loader=yaml.FullLoader)\n    cfg = SimpleNamespace(**cfg)  # easier dict acces by dot, instead of ['']\n\n    if tracker_type == 'strongsort':\n        from boxmot.trackers.strongsort.strong_sort import StrongSORT\n        strongsort = StrongSORT(\n            reid_weights,\n            device,\n            half,\n            max_dist=cfg.max_dist,\n            max_iou_dist=cfg.max_iou_dist,\n            max_age=cfg.max_age,\n            n_init=cfg.n_init,\n            nn_budget=cfg.nn_budget,\n            mc_","@@ -97,6 +97,22 @@ def create_tracker(tracker_type, tracker_config, reid_weights, device, half, per\n             inertia=cfg.inertia,\n         )\n         return deepocsort\n+    elif tracker_type == 'hybridsort':\n+        from boxmot.trackers.hybridsort.hybridsort import HybridSORT\n+\n+        hybridsort = HybridSORT(\n+            reid_weights,\n+            device,\n+            half,\n+            det_thresh=cfg.det_thresh,\n+            max_age=cfg.max_age,\n+            min_hits=cfg.min_hits,\n+            iou_threshold=cfg.iou_thresh,\n+            delta_t=cfg.delta_t,\n+            asso_func=cfg.asso_func,\n+            inertia=cfg.inertia,\n+        )\n+        return hybridsort\n     else:\n         print('No such tracker')\n         exit()\n",add,Added the necessary Harfbuzz scripts for rendering Japanese text correctly .
ee2797c119dd275690e10acab0d610a32f7d3015,fix,boxmot/trackers/hybridsort/hybridsort.py,,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.utils.association import (associate_4_points_with_score,\n                                      associate_4_points_with_score_with_reid,\n                                      associate_kitti,\n                                      cal_score_dif_batch_two_score,\n                                      embedding_distance, linear_assignment)\nfrom boxmot.utils.iou import get_asso_func\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            re","@@ -0,0 +1,669 @@\n+# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n+\n+""""""""""""\n+    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n+""""""""""""\n+\n+from collections import deque  # [hgx0418] deque for reid feature\n+\n+import numpy as np\n+\n+from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n+from boxmot.motion.cmc import get_cmc_method\n+from boxmot.utils.association import (associate_4_points_with_score,\n+                                      associate_4_points_with_score_with_reid,\n+                                      associate_kitti,\n+                                      cal_score_dif_batch_two_score,\n+                                      embedding_distance, linear_assignment)\n+from boxmot.utils.iou import get_asso_func\n+\n+np.random.seed(0)\n+\n+\n+def k_previous_obs(observations, cur_age, k):\n+    if len(observations) == 0:\n+        return [-1, -1, -1, -1, -1]\n+    for i in range(k):\n+        dt = k - i\n+        if cur_age - dt in observations:\n+            return observations[cur_age - dt]\n+    max_age = max(observations.keys())\n+    return observations[max_age]\n+\n+\n+def convert_bbox_to_z(bbox):\n+    """"""""""""\n+    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n+      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n+      the aspect ratio\n+    """"""""""""\n+    w = bbox[2] - bbox[0]\n+    h = bbox[3] - bbox[1]\n+    x = bbox[0] + w / 2.\n+    y = bbox[1] + h / 2.\n+    s = w * h  # scale is just area\n+    r = w / float(h + 1e-6)\n+    score = bbox[4]\n+    if score:\n+        return np.array([x, y, s, score, r]).reshape((5, 1))\n+    else:\n+        return np.array([x, y, s, r]).reshape((4, 1))\n+\n+\n+def convert_x_to_bbox(x, score=None):\n+    """"""""""""\n+    Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\n+      [x1,y1,x2,y2] where x1,y1 is the top left and x2,y2 is the bottom right\n+    """"""""""""\n+    w =",add,Added missing property for playWhenInactive .
ee2797c119dd275690e10acab0d610a32f7d3015,fix,boxmot/utils/association.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.utils.iou import iou_batch\n\n\ndef speed_direction_batch(dets, tracks):\n    tracks = tracks[..., np.newaxis]\n    CX1, CY1 = (dets[:, 0] + dets[:, 2]) / 2.0, (dets[:, 1] + dets[:, 3]) / 2.0\n    CX2, CY2 = (tracks[:, 0] + tracks[:, 2]) / 2.0, (tracks[:, 1] + tracks[:, 3]) / 2.0\n    dx = CX1 - CX2\n    dy = CY1 - CY2\n    norm = np.sqrt(dx**2 + dy**2) + 1e-6\n    dx = dx / norm\n    dy = dy / norm\n    return dy, dx  # size: num_track x num_det\n\n\ndef linear_assignment(cost_matrix):\n    try:\n        import lap\n        _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n        return np.array([[y[i], i] for i in x if i >= 0])  #\n    except ImportError:\n        from scipy.optimize import linear_sum_assignment\n        x, y = linear_sum_assignment(cost_matrix)\n        return np.array([list(zip(x, y))])\n\n\ndef associate_detections_to_trackers(detections, trackers, iou_threshold=0.3):\n    ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.utils.iou import iou_batch\n\n\ndef speed_direction_batch(dets, tracks):\n    tracks = tracks[..., np.newaxis]\n    CX1, CY1 = (dets[:, 0] + dets[:, 2]) / 2.0, (dets[:, 1] + dets[:, 3]) / 2.0\n    CX2, CY2 = (tracks[:, 0] + tracks[:, 2]) / 2.0, (tracks[:, 1] + tracks[:, 3]) / 2.0\n    dx = CX1 - CX2\n    dy = CY1 - CY2\n    norm = np.sqrt(dx**2 + dy**2) + 1e-6\n    dx = dx / norm\n    dy = dy / norm\n    return dy, dx  # size: num_track x num_det\n\n\ndef linear_assignment(cost_matrix):\n    try:\n        import lap\n        _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n        return np.array([[y[i], i] for i in x if i >= 0])  #\n    except ImportError:\n        from scipy.optimize import linear_sum_assignment\n        x, y = linear_sum_assignment(cost_matrix)\n        return np.array([list(zip(x, y))])\n\n\ndef associate_detections_to_trackers(detections, trackers, iou_threshold=0.3):\n    ","@@ -277,3 +277,231 @@ def associate_kitti(\n         matches = np.concatenate(matches, axis=0)\n \n     return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n+\n+\n+def associate_4_points_with_score(\n+    detections, trackers, iou_threshold,\n+    lt, rt, lb, rb, previous_obs, vdc_weight,\n+    iou_type=None, TCM_first_step_weight=0\n+):\n+    if (len(trackers) == 0):\n+        return np.empty((0, 2), dtype=int), np.arange(len(detections)), np.empty((0, 5), dtype=int)\n+\n+    Y1, X1 = speed_direction_batch_lt(detections, previous_obs)\n+    Y2, X2 = speed_direction_batch_rt(detections, previous_obs)\n+    Y3, X3 = speed_direction_batch_lb(detections, previous_obs)\n+    Y4, X4 = speed_direction_batch_rb(detections, previous_obs)\n+    cost_lt = cost_vel(Y1, X1, trackers, lt, detections, previous_obs, vdc_weight)\n+    cost_rt = cost_vel(Y2, X2, trackers, rt, detections, previous_obs, vdc_weight)\n+    cost_lb = cost_vel(Y3, X3, trackers, lb, detections, previous_obs, vdc_weight)\n+    cost_rb = cost_vel(Y4, X4, trackers, rb, detections, previous_obs, vdc_weight)\n+    iou_matrix = iou_type(detections, trackers)\n+    score_dif = cal_score_dif_batch(detections, trackers)\n+\n+    angle_diff_cost = cost_lt + cost_rt + cost_lb + cost_rb\n+\n+    # TCM\n+    angle_diff_cost -= score_dif * TCM_first_step_weight\n+\n+    if min(iou_matrix.shape) > 0:\n+        a = (iou_matrix > iou_threshold).astype(np.int32)\n+        if a.sum(1).max() == 1 and a.sum(0).max() == 1:\n+            matched_indices = np.stack(np.where(a), axis=1)\n+        else:\n+            matched_indices = linear_assignment(-(iou_matrix + angle_diff_cost))\n+    else:\n+        matched_indices = np.empty(shape=(0, 2))\n+\n+    unmatched_detections = []\n+    for d, det in enumerate(detections):\n+        if (d not in matched_indices[:, 0]):\n+            unmatched_detections.append(d)\n+    unmatched_trackers = []\n+    for t, trk in enumerate(trackers):\n+        if (t not in m",add,Added STORM - 236 to Changelog
11eb3c7a4fc815a78fafcab3fd238dbac53649e7,fix,boxmot/utils/association.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.utils.iou import iou_batch\n\n\ndef speed_direction_batch(dets, tracks):\n    tracks = tracks[..., np.newaxis]\n    CX1, CY1 = (dets[:, 0] + dets[:, 2]) / 2.0, (dets[:, 1] + dets[:, 3]) / 2.0\n    CX2, CY2 = (tracks[:, 0] + tracks[:, 2]) / 2.0, (tracks[:, 1] + tracks[:, 3]) / 2.0\n    dx = CX1 - CX2\n    dy = CY1 - CY2\n    norm = np.sqrt(dx**2 + dy**2) + 1e-6\n    dx = dx / norm\n    dy = dy / norm\n    return dy, dx  # size: num_track x num_det\n\n\ndef linear_assignment(cost_matrix):\n    try:\n        import lap\n        _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n        return np.array([[y[i], i] for i in x if i >= 0])  #\n    except ImportError:\n        from scipy.optimize import linear_sum_assignment\n        x, y = linear_sum_assignment(cost_matrix)\n        return np.array([list(zip(x, y))])\n\n\ndef associate_detections_to_trackers(detections, trackers, iou_threshold=0.3):\n    ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.utils.iou import iou_batch\n\n\ndef speed_direction_batch(dets, tracks):\n    tracks = tracks[..., np.newaxis]\n    CX1, CY1 = (dets[:, 0] + dets[:, 2]) / 2.0, (dets[:, 1] + dets[:, 3]) / 2.0\n    CX2, CY2 = (tracks[:, 0] + tracks[:, 2]) / 2.0, (tracks[:, 1] + tracks[:, 3]) / 2.0\n    dx = CX1 - CX2\n    dy = CY1 - CY2\n    norm = np.sqrt(dx**2 + dy**2) + 1e-6\n    dx = dx / norm\n    dy = dy / norm\n    return dy, dx  # size: num_track x num_det\n\n\ndef linear_assignment(cost_matrix):\n    try:\n        import lap\n        _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n        return np.array([[y[i], i] for i in x if i >= 0])  #\n    except ImportError:\n        from scipy.optimize import linear_sum_assignment\n        x, y = linear_sum_assignment(cost_matrix)\n        return np.array([list(zip(x, y))])\n\n\ndef associate_detections_to_trackers(detections, trackers, iou_threshold=0.3):\n    ","@@ -505,3 +505,16 @@ def cal_score_dif_batch(bboxes1, bboxes2):\n     score1 = bboxes1[..., 4]\n \n     return (abs(score2 - score1))\n+\n+\n+def cal_score_dif_batch_two_score(bboxes1, bboxes2):\n+    """"""""""""\n+    From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n+    """"""""""""\n+    bboxes2 = np.expand_dims(bboxes2, 0)\n+    bboxes1 = np.expand_dims(bboxes1, 1)\n+\n+    score2 = bboxes2[..., 5]\n+    score1 = bboxes1[..., 4]\n+\n+    return (abs(score2 - score1))\n",add,Added STORM - 146 to Changelog
a6ee01baf140de6cd0576d3eb97b3dd6709a1515,fix,boxmot/trackers/hybridsort/hybridsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.utils.association import (associate_4_points_with_score,\n                                      associate_4_points_with_score_with_reid,\n                                      cal_score_dif_batch_two_score,\n                                      embedding_distance, linear_assignment)\nfrom boxmot.utils.iou import get_asso_func\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(obser","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.utils.association import (associate_4_points_with_score,\n                                      associate_4_points_with_score_with_reid,\n                                      cal_score_dif_batch_two_score,\n                                      embedding_distance, linear_assignment)\nfrom boxmot.utils.iou import get_asso_func\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(obser","@@ -112,6 +112,8 @@ class KalmanBoxTracker(object):\n     def __init__(\n         self,\n         bbox,\n+        cls,\n+        det_ind,\n         temp_feat,\n         delta_t=3,\n         orig=False,\n@@ -165,8 +167,8 @@ class KalmanBoxTracker(object):\n         self.hit_streak = 0\n         self.age = 0\n         self.conf = bbox[4]\n-        # self.cls = bbox[5]\n-        # self.det_ind = bbox[6]\n+        self.cls = cls\n+        self.det_ind = det_ind\n         self.adapfs = False\n         """"""""""""\n         NOTE: [-1,-1,-1,-1,-1] is a compromising placeholder for non-observation status, the same for the return of\n@@ -229,7 +231,7 @@ class KalmanBoxTracker(object):\n         # cx, cy = x1_ + w / 2, y1_ + h / 2\n         self.kf.x[:5] = convert_bbox_to_z([x1_, y1_, x2_, y2_, s])\n \n-    def update(self, bbox, id_feature, update_feature=True):\n+    def update(self, bbox, cls, det_ind, id_feature, update_feature=True):\n         """"""""""""\n         Updates the state vector with observed bbox.\n         """"""""""""\n@@ -238,10 +240,9 @@ class KalmanBoxTracker(object):\n         velocity_lb = None\n         velocity_rb = None\n         if bbox is not None:\n-            print('kf update', bbox.shape)\n-            self.conf = bbox[4]\n-            # self.cls = bbox[5]\n-            # self.det_ind = bbox[6]\n+            self.conf = bbox[-1]\n+            self.cls = cls\n+            self.det_ind = det_ind\n             if self.last_observation.sum() >= 0:  # no previous observation\n                 previous_box = None\n                 for i in range(self.delta_t):\n@@ -393,6 +394,7 @@ class HybridSORT(object):\n         # scale = min(img_size[0] / float(img_h), img_size[1] / float(img_w))\n         # bboxes /= scale\n         dets_embs = self.model.get_features(bboxes, im)\n+        dets0 = np.concatenate((dets, np.expand_dims(scores, axis=-1)), axis=1)\n         dets = np.concatenate((bboxes, np.expand_dims(scores, axis=-1)), axis=1)\n         inds_low = scores > self",add,Added debugging information
41d91ec9153e156f05f58ebe8fcf470db33539df,fix hybridsort,boxmot/trackers/hybridsort/association.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\n\ndef intersection_batch(bboxes1, bboxes2):\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n    yy2 = np.minimum(bboxes1[..., 3], bboxes2[..., 3])\n    w = np.maximum(0., xx2 - xx1)\n    h = np.maximum(0., yy2 - yy1)\n    intersections = w * h\n    return intersections\n\n\ndef box_area(bbox):\n    area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n    return area\n\n\ndef iou_batch(bboxes1, bboxes2):\n    """"""""""""\n    From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n    """"""""""""\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.mi","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\n\ndef intersection_batch(bboxes1, bboxes2):\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n    yy2 = np.minimum(bboxes1[..., 3], bboxes2[..., 3])\n    w = np.maximum(0., xx2 - xx1)\n    h = np.maximum(0., yy2 - yy1)\n    intersections = w * h\n    return intersections\n\n\ndef box_area(bbox):\n    area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n    return area\n\n\ndef iou_batch(bboxes1, bboxes2):\n    """"""""""""\n    From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n    """"""""""""\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.mi","@@ -595,11 +595,14 @@ def associate_4_points_with_score(\n     return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n \n \n-def associate_4_points_with_score_with_reid(detections, trackers, iou_threshold, lt, rt, lb, rb, previous_obs,\n-                                            vdc_weight, iou_type=None, args=None, emb_cost=None, weights=(1.0, 0),\n-                                            thresh=0.8, long_emb_dists=None, with_longterm_reid=False,\n-                                            longterm_reid_weight=0.0, with_longterm_reid_correction=False,\n-                                            longterm_reid_correction_thresh=0.0, dataset=""""dancetrack""""):\n+def associate_4_points_with_score_with_reid(\n+    detections, trackers, iou_threshold, lt, rt, lb, rb, previous_obs, vdc_weight,\n+    iou_type=None, emb_cost=None, weights=(1.0, 0), thresh=0.8,\n+    long_emb_dists=None, with_longterm_reid=False,\n+    longterm_reid_weight=0.0, with_longterm_reid_correction=False,\n+    longterm_reid_correction_thresh=0.0, dataset=""""dancetrack""""\n+):\n+\n     if (len(trackers) == 0):\n         return np.empty((0, 2), dtype=int), np.arange(len(detections)), np.empty((0, 5), dtype=int)\n \n@@ -617,7 +620,7 @@ def associate_4_points_with_score_with_reid(detections, trackers, iou_threshold,\n     angle_diff_cost = cost_lt + cost_rt + cost_lb + cost_rb\n \n     # TCM\n-    angle_diff_cost -= score_dif * args.TCM_first_step_weight\n+    angle_diff_cost -= score_dif * 1.0\n \n     if min(iou_matrix.shape) > 0:\n         if emb_cost is None:\n@@ -629,12 +632,13 @@ def associate_4_points_with_score_with_reid(detections, trackers, iou_threshold,\n         else:\n             if not with_longterm_reid:\n                 matched_indices = linear_assignment(\n-                    weights[0] * (-(iou_matrix + angle_diff_cost)) + weights[1] * emb_cost\n-                )  # , thresh=thresh\n+                    weights[0] * (-(iou_matrix + angle_diff_cost)) +",add,Added STORM - 1270 to Changelog
41d91ec9153e156f05f58ebe8fcf470db33539df,fix hybridsort,boxmot/trackers/hybridsort/hybridsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.utils.association import (associate_4_points_with_score,\n                                      associate_4_points_with_score_with_reid,\n                                      cal_score_dif_batch_two_score,\n                                      embedding_distance, linear_assignment)\nfrom boxmot.utils.iou import get_asso_func\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(obser","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.hybridsort.association import (\n    associate_4_points_with_score, associate_4_points_with_score_with_reid,\n    cal_score_dif_batch_two_score, embedding_distance, linear_assignment)\nfrom boxmot.utils.iou import get_asso_func\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""","@@ -10,10 +10,9 @@ import numpy as np\n \n from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n from boxmot.motion.cmc import get_cmc_method\n-from boxmot.utils.association import (associate_4_points_with_score,\n-                                      associate_4_points_with_score_with_reid,\n-                                      cal_score_dif_batch_two_score,\n-                                      embedding_distance, linear_assignment)\n+from boxmot.trackers.hybridsort.association import (\n+    associate_4_points_with_score, associate_4_points_with_score_with_reid,\n+    cal_score_dif_batch_two_score, embedding_distance, linear_assignment)\n from boxmot.utils.iou import get_asso_func\n \n np.random.seed(0)\n@@ -385,8 +384,6 @@ class HybridSORT(object):\n         #         self.camera_update(self.trackers, warp_matrix)\n \n         self.frame_count += 1\n-        # post_process detections\n-        # if output_results.shape[1] == 5:\n         scores = dets[:, 4]\n         bboxes = dets[:, :4]\n \n@@ -447,7 +444,7 @@ class HybridSORT(object):\n                 assert emb_dists.shape == long_emb_dists.shape\n                 matched, unmatched_dets, unmatched_trks = associate_4_points_with_score_with_reid(\n                     dets, trks, self.iou_threshold, velocities_lt, velocities_rt, velocities_lb, velocities_rb,\n-                    k_observations, self.inertia, self.asso_func, self.args, emb_cost=emb_dists,\n+                    k_observations, self.inertia, self.TCM_first_step_weight, self.asso_func, emb_cost=emb_dists,\n                     weights=(1.0, self.args.EG_weight_high_score), thresh=self.args.high_score_matching_thresh,\n                     long_emb_dists=long_emb_dists, with_longterm_reid=self.args.with_longterm_reid,\n                     longterm_reid_weight=self.args.longterm_reid_weight,\n@@ -571,6 +568,5 @@ class HybridSORT(object):\n             if (trk.time_since_update > self.max_age):\n                 self.tracke",add,Added the user group to the contributor list
196111d72f336c64e48f98ce86bd86e1f29f7d06,fix hybridsort,boxmot/utils/association.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.utils.iou import iou_batch\n\n\ndef speed_direction_batch(dets, tracks):\n    tracks = tracks[..., np.newaxis]\n    CX1, CY1 = (dets[:, 0] + dets[:, 2]) / 2.0, (dets[:, 1] + dets[:, 3]) / 2.0\n    CX2, CY2 = (tracks[:, 0] + tracks[:, 2]) / 2.0, (tracks[:, 1] + tracks[:, 3]) / 2.0\n    dx = CX1 - CX2\n    dy = CY1 - CY2\n    norm = np.sqrt(dx**2 + dy**2) + 1e-6\n    dx = dx / norm\n    dy = dy / norm\n    return dy, dx  # size: num_track x num_det\n\n\ndef linear_assignment(cost_matrix):\n    try:\n        import lap\n        _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n        return np.array([[y[i], i] for i in x if i >= 0])  #\n    except ImportError:\n        from scipy.optimize import linear_sum_assignment\n        x, y = linear_sum_assignment(cost_matrix)\n        return np.array([list(zip(x, y))])\n\n\ndef associate_detections_to_trackers(detections, trackers, iou_threshold=0.3):\n    ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.utils.iou import iou_batch\n\n\ndef speed_direction_batch(dets, tracks):\n    tracks = tracks[..., np.newaxis]\n    CX1, CY1 = (dets[:, 0] + dets[:, 2]) / 2.0, (dets[:, 1] + dets[:, 3]) / 2.0\n    CX2, CY2 = (tracks[:, 0] + tracks[:, 2]) / 2.0, (tracks[:, 1] + tracks[:, 3]) / 2.0\n    dx = CX1 - CX2\n    dy = CY1 - CY2\n    norm = np.sqrt(dx**2 + dy**2) + 1e-6\n    dx = dx / norm\n    dy = dy / norm\n    return dy, dx  # size: num_track x num_det\n\n\ndef linear_assignment(cost_matrix):\n    try:\n        import lap\n        _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n        return np.array([[y[i], i] for i in x if i >= 0])  #\n    except ImportError:\n        from scipy.optimize import linear_sum_assignment\n        x, y = linear_sum_assignment(cost_matrix)\n        return np.array([list(zip(x, y))])\n\n\ndef associate_detections_to_trackers(detections, trackers, iou_threshold=0.3):\n    ","@@ -277,244 +277,3 @@ def associate_kitti(\n         matches = np.concatenate(matches, axis=0)\n \n     return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n-\n-\n-def associate_4_points_with_score(\n-    detections, trackers, iou_threshold,\n-    lt, rt, lb, rb, previous_obs, vdc_weight,\n-    iou_type=None, TCM_first_step_weight=0\n-):\n-    if (len(trackers) == 0):\n-        return np.empty((0, 2), dtype=int), np.arange(len(detections)), np.empty((0, 5), dtype=int)\n-\n-    Y1, X1 = speed_direction_batch_lt(detections, previous_obs)\n-    Y2, X2 = speed_direction_batch_rt(detections, previous_obs)\n-    Y3, X3 = speed_direction_batch_lb(detections, previous_obs)\n-    Y4, X4 = speed_direction_batch_rb(detections, previous_obs)\n-    cost_lt = cost_vel(Y1, X1, trackers, lt, detections, previous_obs, vdc_weight)\n-    cost_rt = cost_vel(Y2, X2, trackers, rt, detections, previous_obs, vdc_weight)\n-    cost_lb = cost_vel(Y3, X3, trackers, lb, detections, previous_obs, vdc_weight)\n-    cost_rb = cost_vel(Y4, X4, trackers, rb, detections, previous_obs, vdc_weight)\n-    iou_matrix = iou_type(detections, trackers)\n-    score_dif = cal_score_dif_batch(detections, trackers)\n-\n-    angle_diff_cost = cost_lt + cost_rt + cost_lb + cost_rb\n-\n-    # TCM\n-    angle_diff_cost -= score_dif * TCM_first_step_weight\n-\n-    if min(iou_matrix.shape) > 0:\n-        a = (iou_matrix > iou_threshold).astype(np.int32)\n-        if a.sum(1).max() == 1 and a.sum(0).max() == 1:\n-            matched_indices = np.stack(np.where(a), axis=1)\n-        else:\n-            matched_indices = linear_assignment(-(iou_matrix + angle_diff_cost))\n-    else:\n-        matched_indices = np.empty(shape=(0, 2))\n-\n-    unmatched_detections = []\n-    for d, det in enumerate(detections):\n-        if (d not in matched_indices[:, 0]):\n-            unmatched_detections.append(d)\n-    unmatched_trackers = []\n-    for t, trk in enumerate(trackers):\n-        if (t not in m",add,Added STORM - 236 to Changelog
5a1732f6406031a49554ce2f3f740eb4fa5e4990,fix per class decorator,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"examples""""\nWEIGHTS = ROOT / """"examples"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        self.update = method\n\n    def __get__(self, instance, owner):\n        def wrapper(*args, **kwargs):\n            modified_args = list(args)\n            dets = modified_args[0]\n            im = modified_args[1]\n\n            # input one class of detections at a time in order to not mix them up\n            if instance.per_class is True and dets.size != 0:\n                dets_dict = {\n                    class_id: np.array([det for det in dets if det[5] == ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"examples""""\nWEIGHTS = ROOT / """"examples"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        self.update = method\n\n    def __get__(self, instance, owner):\n        def wrapper(*args, **kwargs):\n            modified_args = list(args)\n            dets = modified_args[0]\n            im = modified_args[1]\n\n            # input one class of detections at a time in order to not mix them up\n            if instance.per_class[0] is True and dets.size != 0:\n                dets_dict = {\n                    class_id: np.array([det for det in dets if det[5] ","@@ -30,7 +30,7 @@ class PerClassDecorator:\n             im = modified_args[1]\n \n             # input one class of detections at a time in order to not mix them up\n-            if instance.per_class is True and dets.size != 0:\n+            if instance.per_class[0] is True and dets.size != 0:\n                 dets_dict = {\n                     class_id: np.array([det for det in dets if det[5] == class_id])\n                     for class_id in set(det[5] for det in dets)\n",add,Added example for MAP type in documentation
39c870f8ae44f0af9b718d67ae8c925965ec74ce,fix per class,boxmot/trackers/hybridsort/hybridsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.hybridsort.association import (\n    associate_4_points_with_score, associate_4_points_with_score_with_reid,\n    cal_score_dif_batch_two_score, embedding_distance, linear_assignment)\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.iou import get_asso_func\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.hybridsort.association import (\n    associate_4_points_with_score, associate_4_points_with_score_with_reid,\n    cal_score_dif_batch_two_score, embedding_distance, linear_assignment)\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.iou import get_asso_func\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age","@@ -340,7 +340,7 @@ class HybridSORT(object):\n         self.min_hits = min_hits\n         self.iou_threshold = iou_threshold\n         self.trackers = []\n-        self.per_class = True,\n+        self.per_class = True\n         self.frame_count = 0\n         self.det_thresh = det_thresh\n         self.delta_t = delta_t\n",add,Added Type name for DFI ( # 57480 )
39c870f8ae44f0af9b718d67ae8c925965ec74ce,fix per class,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"examples""""\nWEIGHTS = ROOT / """"examples"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        self.update = method\n\n    def __get__(self, instance, owner):\n        def wrapper(*args, **kwargs):\n            modified_args = list(args)\n            dets = modified_args[0]\n            im = modified_args[1]\n\n            # input one class of detections at a time in order to not mix them up\n            if instance.per_class[0] is True and dets.size != 0:\n                dets_dict = {\n                    class_id: np.array([det for det in dets if det[5] ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"examples""""\nWEIGHTS = ROOT / """"examples"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        self.update = method\n\n    def __get__(self, instance, owner):\n        def wrapper(*args, **kwargs):\n            modified_args = list(args)\n            dets = modified_args[0]\n            im = modified_args[1]\n\n            # input one class of detections at a time in order to not mix them up\n            if instance.per_class is True and dets.size != 0:\n                dets_dict = {\n                    class_id: np.array([det for det in dets if det[5] == ","@@ -30,7 +30,7 @@ class PerClassDecorator:\n             im = modified_args[1]\n \n             # input one class of detections at a time in order to not mix them up\n-            if instance.per_class[0] is True and dets.size != 0:\n+            if instance.per_class is True and dets.size != 0:\n                 dets_dict = {\n                     class_id: np.array([det for det in dets if det[5] == class_id])\n                     for class_id in set(det[5] for det in dets)\n",add,Added example for MAP type in documentation
9e5bf5d7bca53a33c4458099f2bbc6fb58602c20,fix https://github.com/mikel-brostrom/yolo_tracking/issues/1083,boxmot/configs/botsort.yaml,"# Trial number:      232\n# HOTA, MOTA, IDF1:  [45.31]\nappearance_thresh: 0.4818211117541298\ncmc_method: sparseOptFlow\nconf_thres: 0.3501265956918775\nframe_rate: 30\nlambda_: 0.9896143462366406\nmatch_thresh: 0.22734550911325851\nnew_track_thresh: 0.21144301345190655\nproximity_thresh: 0.5945380911899254\ntrack_buffer: 60\ntrack_high_thresh: 0.33824964456239337\n","# Trial number:      232\n# HOTA, MOTA, IDF1:  [45.31]\nappearance_thresh: 0.4818211117541298\ncmc_method: sparseOptFlow\nconf_thres: 0.3501265956918775\nframe_rate: 30\nmatch_thresh: 0.22734550911325851\nnew_track_thresh: 0.21144301345190655\nproximity_thresh: 0.5945380911899254\ntrack_buffer: 60\ntrack_high_thresh: 0.33824964456239337\ntrack_low_thresh: 0.1\n","@@ -4,9 +4,9 @@ appearance_thresh: 0.4818211117541298\n cmc_method: sparseOptFlow\n conf_thres: 0.3501265956918775\n frame_rate: 30\n-lambda_: 0.9896143462366406\n match_thresh: 0.22734550911325851\n new_track_thresh: 0.21144301345190655\n proximity_thresh: 0.5945380911899254\n track_buffer: 60\n track_high_thresh: 0.33824964456239337\n+track_low_thresh: 0.1\n",add,Add note about data volume to enable_metrics_collection
9e5bf5d7bca53a33c4458099f2bbc6fb58602c20,fix https://github.com/mikel-brostrom/yolo_tracking/issues/1083,boxmot/tracker_zoo.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom types import SimpleNamespace\n\nimport yaml\n\nfrom boxmot.utils import BOXMOT\n\n\ndef get_tracker_config(tracker_type):\n    tracking_config = \\n        BOXMOT /\\n        'configs' /\\n        (tracker_type + '.yaml')\n    return tracking_config\n\n\ndef create_tracker(tracker_type, tracker_config, reid_weights, device, half, per_class):\n\n    with open(tracker_config, """"r"""") as f:\n        cfg = yaml.load(f.read(), Loader=yaml.FullLoader)\n    cfg = SimpleNamespace(**cfg)  # easier dict acces by dot, instead of ['']\n\n    if tracker_type == 'strongsort':\n        from boxmot.trackers.strongsort.strong_sort import StrongSORT\n        strongsort = StrongSORT(\n            reid_weights,\n            device,\n            half,\n            max_dist=cfg.max_dist,\n            max_iou_dist=cfg.max_iou_dist,\n            max_age=cfg.max_age,\n            n_init=cfg.n_init,\n            nn_budget=cfg.nn_budget,\n            mc_","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom types import SimpleNamespace\n\nimport yaml\n\nfrom boxmot.utils import BOXMOT\n\n\ndef get_tracker_config(tracker_type):\n    tracking_config = \\n        BOXMOT /\\n        'configs' /\\n        (tracker_type + '.yaml')\n    return tracking_config\n\n\ndef create_tracker(tracker_type, tracker_config, reid_weights, device, half, per_class):\n\n    with open(tracker_config, """"r"""") as f:\n        cfg = yaml.load(f.read(), Loader=yaml.FullLoader)\n    cfg = SimpleNamespace(**cfg)  # easier dict acces by dot, instead of ['']\n\n    if tracker_type == 'strongsort':\n        from boxmot.trackers.strongsort.strong_sort import StrongSORT\n        strongsort = StrongSORT(\n            reid_weights,\n            device,\n            half,\n            max_dist=cfg.max_dist,\n            max_iou_dist=cfg.max_iou_dist,\n            max_age=cfg.max_age,\n            n_init=cfg.n_init,\n            nn_budget=cfg.nn_budget,\n            mc_","@@ -70,6 +70,7 @@ def create_tracker(tracker_type, tracker_config, reid_weights, device, half, per\n             device,\n             half,\n             track_high_thresh=cfg.track_high_thresh,\n+            track_low_thresh=cfg.track_low_thresh,\n             new_track_thresh=cfg.new_track_thresh,\n             track_buffer=cfg.track_buffer,\n             match_thresh=cfg.match_thresh,\n@@ -77,7 +78,6 @@ def create_tracker(tracker_type, tracker_config, reid_weights, device, half, per\n             appearance_thresh=cfg.appearance_thresh,\n             cmc_method=cfg.cmc_method,\n             frame_rate=cfg.frame_rate,\n-            lambda_=cfg.lambda_\n         )\n         return botsort\n     elif tracker_type == 'deepocsort':\n",add,Add note about data volume to enable_metrics_collection
9e5bf5d7bca53a33c4458099f2bbc6fb58602c20,fix https://github.com/mikel-brostrom/yolo_tracking/issues/1083,boxmot/trackers/botsort/bot_sort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SparseOptFlow\nfrom boxmot.motion.kalman_filters.adapters import BotSortKalmanFilterAdapter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = BotSortKalmanFilterAdapter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_ac","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SparseOptFlow\nfrom boxmot.motion.kalman_filters.adapters import BotSortKalmanFilterAdapter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = BotSortKalmanFilterAdapter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_ac","@@ -187,7 +187,8 @@ class BoTSORT(object):\n         model_weights,\n         device,\n         fp16,\n-        track_high_thresh: float = 0.45,\n+        track_high_thresh: float = 0.5,\n+        track_low_thresh: float = 0.1,\n         new_track_thresh: float = 0.6,\n         track_buffer: int = 30,\n         match_thresh: float = 0.8,\n@@ -195,7 +196,6 @@ class BoTSORT(object):\n         appearance_thresh: float = 0.25,\n         cmc_method: str = """"sparseOptFlow"""",\n         frame_rate=30,\n-        lambda_=0.985,\n     ):\n         self.tracked_stracks = []  # type: list[STrack]\n         self.lost_stracks = []  # type: list[STrack]\n@@ -204,9 +204,10 @@ class BoTSORT(object):\n \n         self.frame_id = 0\n \n-        self.lambda_ = lambda_\n         self.track_high_thresh = track_high_thresh\n+        self.track_low_thresh = track_low_thresh\n         self.new_track_thresh = new_track_thresh\n+        self.match_thresh = match_thresh\n \n         self.buffer_size = int(frame_rate / 30.0 * track_buffer)\n         self.max_time_lost = self.buffer_size\n@@ -215,7 +216,6 @@ class BoTSORT(object):\n         # ReID module\n         self.proximity_thresh = proximity_thresh\n         self.appearance_thresh = appearance_thresh\n-        self.match_thresh = match_thresh\n \n         self.model = ReIDDetectMultiBackend(\n             weights=model_weights, device=device, fp16=fp16\n@@ -237,33 +237,31 @@ class BoTSORT(object):\n             dets.shape[1] == 6\n         ), """"Unsupported 'dets' 2nd dimension lenght, valid lenghts is 6""""\n \n-        dets = np.hstack([dets, np.arange(len(dets)).reshape(-1, 1)])\n         self.frame_id += 1\n         activated_starcks = []\n         refind_stracks = []\n         lost_stracks = []\n         removed_stracks = []\n \n-        xyxys = dets[:, 0:4]\n-        confs = dets[:, 4]\n-\n-        remain_inds = confs > self.track_high_thresh\n-        inds_low = confs > 0.1\n-        inds_high = confs < self.track_high_thresh\n+        ",add,Added auto permission to def
88de710dc19a5087ed632b2cc748c7476948ec52,bug: fix import hybridsort,boxmot/trackers/hybridsort/__init__.py,,# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n,"@@ -0,0 +1 @@\n+# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n",add,Added STORM - 370 to Changelog
528a5ed922384a6ba9d7cae447dd30a206c31e0b,delete debug print,boxmot/trackers/deepocsort/deep_ocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y","@@ -457,7 +457,6 @@ class DeepOCSort(object):\n             left_trks = last_boxes[unmatched_trks]\n             left_trks_embs = trk_embs[unmatched_trks]\n \n-            print('left_dets.shape, left_trks.shape', left_dets.shape, left_trks.shape)\n             iou_left = self.asso_func(left_dets, left_trks)\n             # TODO: is better without this\n             emb_cost_left = left_dets_embs @ left_trks_embs.T\n",add,Add group has a name
550f753e436258604ca7564bbbd81976c1f959b2,fix indexing bug,boxmot/trackers/deepocsort/deep_ocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale","@@ -9,7 +9,6 @@ import numpy as np\n from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n from boxmot.motion.cmc import get_cmc_method\n from boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\n-from boxmot.utils import PerClassDecorator\n from boxmot.utils.association import associate, linear_assignment\n from boxmot.utils.iou import get_asso_func\n \n@@ -355,7 +354,6 @@ class DeepOCSort(object):\n         self.aw_off = aw_off\n         self.new_kf_off = new_kf_off\n \n-    @PerClassDecorator\n     def update(self, dets, img):\n         """"""""""""\n         Params:\n@@ -432,7 +430,7 @@ class DeepOCSort(object):\n         else:\n             stage1_emb_cost = dets_embs @ trk_embs.T\n         matched, unmatched_dets, unmatched_trks = associate(\n-            dets,\n+            dets[:, 0:5],\n             trks,\n             self.iou_threshold,\n             velocities,\n",add,Added STORM - 236 to Changelog
550f753e436258604ca7564bbbd81976c1f959b2,fix indexing bug,boxmot/trackers/ocsort/ocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.adapters import OCSortKalmanFilterAdapter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0","@@ -279,7 +279,7 @@ class OCSort(object):\n             First round of association\n         """"""""""""\n         matched, unmatched_dets, unmatched_trks = associate(\n-            dets, trks, self.iou_threshold, velocities, k_observations, self.inertia\n+            dets[:, 0:5], trks, self.iou_threshold, velocities, k_observations, self.inertia\n         )\n         for m in matched:\n             self.trackers[m[1]].update(dets[m[0], :5], dets[m[0], 5], dets[m[0], 6])\n",add,Added STORM - 1270 to Changelog
e479a6bde0275a826903e9b1e47a76422b16c8ea,fix clip vehiclereid model link,boxmot/appearance/reid_model_factory.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nimport time\nfrom collections import OrderedDict\n\nimport torch\n\nfrom boxmot.utils import logger as LOGGER\n\n__model_types = [\n    """"resnet50"""",\n    """"resnet101"""",\n    """"mlfn"""",\n    """"hacnn"""",\n    """"mobilenetv2_x1_0"""",\n    """"mobilenetv2_x1_4"""",\n    """"osnet_x1_0"""",\n    """"osnet_x0_75"""",\n    """"osnet_x0_5"""",\n    """"osnet_x0_25"""",\n    """"osnet_ibn_x1_0"""",\n    """"osnet_ain_x1_0"""",\n    """"lmbn_n"""",\n    """"clip"""",\n]\n\nlmbn_loc = 'https://github.com/mikel-brostrom/yolov8_tracking/releases/download/v9.0/'\n\n__trained_urls = {\n    # resnet50\n    """"resnet50_market1501.pt"""": """"https://drive.google.com/uc?id=1dUUZ4rHDWohmsQXCRe2C_HbYkzz94iBV"""",\n    """"resnet50_dukemtmcreid.pt"""": """"https://drive.google.com/uc?id=17ymnLglnc64NRvGOitY3BqMRS9UWd1wg"""",\n    """"resnet50_msmt17.pt"""": """"https://drive.google.com/uc?id=1ep7RypVDOthCRIAqDnn4_N-UhkkFHJsj"""",\n    """"resnet50_fc512_market1501.pt"""": """"https://drive.google.com/uc?id=","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nimport time\nfrom collections import OrderedDict\n\nimport torch\n\nfrom boxmot.utils import logger as LOGGER\n\n__model_types = [\n    """"resnet50"""",\n    """"resnet101"""",\n    """"mlfn"""",\n    """"hacnn"""",\n    """"mobilenetv2_x1_0"""",\n    """"mobilenetv2_x1_4"""",\n    """"osnet_x1_0"""",\n    """"osnet_x0_75"""",\n    """"osnet_x0_5"""",\n    """"osnet_x0_25"""",\n    """"osnet_ibn_x1_0"""",\n    """"osnet_ain_x1_0"""",\n    """"lmbn_n"""",\n    """"clip"""",\n]\n\nlmbn_loc = 'https://github.com/mikel-brostrom/yolov8_tracking/releases/download/v9.0/'\n\n__trained_urls = {\n    # resnet50\n    """"resnet50_market1501.pt"""": """"https://drive.google.com/uc?id=1dUUZ4rHDWohmsQXCRe2C_HbYkzz94iBV"""",\n    """"resnet50_dukemtmcreid.pt"""": """"https://drive.google.com/uc?id=17ymnLglnc64NRvGOitY3BqMRS9UWd1wg"""",\n    """"resnet50_msmt17.pt"""": """"https://drive.google.com/uc?id=1ep7RypVDOthCRIAqDnn4_N-UhkkFHJsj"""",\n    """"resnet50_fc512_market1501.pt"""": """"https://drive.google.com/uc?id=","@@ -74,7 +74,7 @@ __trained_urls = {\n     """"clip_market1501.pt"""": """"https://drive.google.com/uc?id=1GnyAVeNOg3Yug1KBBWMKKbT2x43O5Ch7"""",\n     """"clip_duke.pt"""": """"https://drive.google.com/uc?id=1ldjSkj-7pXAWmx8on5x0EftlCaolU4dY"""",\n     """"clip_veri.pt"""": """"https://drive.google.com/uc?id=1RyfHdOBI2pan_wIGSim5-l6cM4S2WN8e"""",\n-    """"clip_vehicleid.pt"""": """"https://drive.google.com/uc?id=1RyfHdOBI2pan_wIGSim5-l6cM4S2WN8e""""\n+    """"clip_vehicleid.pt"""": """"https://drive.google.com/uc?id=168BLegHHxNqatW5wx1YyL2REaThWoof5""""\n }\n \n \n",add,Added TypeSpec .
ee79fe076f1f57d185e1390374bf01b801463ba1,fix second matching index for bot_sort,boxmot/trackers/botsort/bot_sort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SparseOptFlow\nfrom boxmot.motion.kalman_filters.adapters import BotSortKalmanFilterAdapter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = BotSortKalmanFilterAdapter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_ac","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SparseOptFlow\nfrom boxmot.motion.kalman_filters.adapters import BotSortKalmanFilterAdapter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = BotSortKalmanFilterAdapter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_ac","@@ -249,7 +249,7 @@ class BoTSORT(object):\n         confs = dets[:, 4]\n \n         # find second round association detections\n-        second_mask = np.logical_or(confs > self.track_low_thresh, confs < self.track_high_thresh)\n+        second_mask = np.logical_and(confs > self.track_low_thresh, confs < self.track_high_thresh)\n         dets_second = dets[second_mask]\n \n         # find first round association detections\n",fix,Added example for MAP type in documentation
d378e6e41e01af01a600c2fbe04efe64e7b7df1b,fix overflow during frame cropping,boxmot/appearance/reid_multibackend.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport cv2\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom boxmot.appearance.backbones import build_model, get_nr_classes\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\ntr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (li","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport cv2\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom boxmot.appearance.backbones import build_model, get_nr_classes\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\ntr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (li","@@ -186,10 +186,14 @@ class ReIDDetectMultiBackend(nn.Module):\n         return types\n \n     def preprocess(self, xyxys, img):\n-        crops = []\n+        h, w = img.shape[:2]\n         # dets are of different sizes so batch preprocessing is not possible\n         for box in xyxys:\n             x1, y1, x2, y2 = box.astype('int')\n+            x1 = max(0, x1)\n+            y1 = max(0, y1)\n+            x2 = min(w - 1, x2)\n+            y2 = min(h - 1, y2)\n             crop = img[y1:y2, x1:x2]\n             # resize\n             crop = cv2.resize(\n",add,Add note about data volume to enable
5ee526ccd526d81fe4bf9911762d29242783b048,fix crop overflow issue,boxmot/appearance/reid_multibackend.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport cv2\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom boxmot.appearance.backbones import build_model, get_nr_classes\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\ntr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (li","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport cv2\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom boxmot.appearance.backbones import build_model, get_nr_classes\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\ntr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (li","@@ -186,6 +186,7 @@ class ReIDDetectMultiBackend(nn.Module):\n         return types\n \n     def preprocess(self, xyxys, img):\n+        crops = []\n         h, w = img.shape[:2]\n         # dets are of different sizes so batch preprocessing is not possible\n         for box in xyxys:\n",add,Add note about data volume to enable
8117fbb5a09ad4e3978eb6c94bf0fc60648cf71f,"HybridSort: Fix assignment to trk in update, so that it alligns with the types returned by predict",boxmot/trackers/hybridsort/hybridsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.hybridsort.association import (\n    associate_4_points_with_score, associate_4_points_with_score_with_reid,\n    cal_score_dif_batch_two_score, embedding_distance, linear_assignment)\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.iou import get_asso_func\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.hybridsort.association import (\n    associate_4_points_with_score, associate_4_points_with_score_with_reid,\n    cal_score_dif_batch_two_score, embedding_distance, linear_assignment)\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.iou import get_asso_func\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age","@@ -408,10 +408,7 @@ class HybridSORT(object):\n         ret = []\n         for t, trk in enumerate(trks):\n             pos, kalman_score, simple_score = self.trackers[t].predict()\n-            try:\n-                trk[:6] = [pos[0][0], pos[0][1], pos[0][2], pos[0][3], kalman_score, simple_score[0]]\n-            except Exception:\n-                trk[:6] = [pos[0][0], pos[0][1], pos[0][2], pos[0][3], kalman_score, simple_score]\n+            trk[:6] = [pos[0][0], pos[0][1], pos[0][2], pos[0][3], kalman_score[0], simple_score]\n             if np.any(np.isnan(pos)):\n                 to_del.append(t)\n         trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n",fix,Added Type name for DFI ( # 3 )
5e8acfb1a1e193bab47daf0da196b0d79fc5b478,fix asso function identification,boxmot/configs/ocsort.yaml,"# Trial number:      137\n# HOTA, MOTA, IDF1:  [55.567]\nasso_func: giou\nconf_thres: 0.5122620708221085\ndelta_t: 3\ndet_thresh: 0\ninertia: 0.2\niou_thresh: 0.3\nmax_age: 30\nmin_hits: 1\nuse_byte: false\n","# Trial number:      137\n# HOTA, MOTA, IDF1:  [55.567]\nasso_func: centroid\nconf_thres: 0.5122620708221085\ndelta_t: 3\ndet_thresh: 0\ninertia: 0.2\niou_thresh: 0.3\nmax_age: 30\nmin_hits: 1\nuse_byte: false\n","@@ -1,6 +1,6 @@\n # Trial number:      137\n # HOTA, MOTA, IDF1:  [55.567]\n-asso_func: giou\n+asso_func: centroid\n conf_thres: 0.5122620708221085\n delta_t: 3\n det_thresh: 0\n",add,Added STORM - 205 to Changelog
5e8acfb1a1e193bab47daf0da196b0d79fc5b478,fix asso function identification,boxmot/utils/iou.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\n\ndef iou_batch(bboxes1, bboxes2) -> np.ndarray:\n    """"""""""""\n    From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n    """"""""""""\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n    yy2 = np.minimum(bboxes1[..., 3], bboxes2[..., 3])\n    w = np.maximum(0.0, xx2 - xx1)\n    h = np.maximum(0.0, yy2 - yy1)\n    wh = w * h\n    o = wh / (\n        (bboxes1[..., 2] - bboxes1[..., 0]) * (bboxes1[..., 3] - bboxes1[..., 1]) +\n        (bboxes2[..., 2] - bboxes2[..., 0]) * (bboxes2[..., 3] - bboxes2[..., 1]) -\n        wh\n    )\n    return o\n\n\ndef giou_batch(bboxes1, bboxes2) -> np.ndarray:\n    """"""""""""\n    :param bbox_p: predict of bbox(N,4)(x1,y1,x2,y2)\n    :param bbox_g: groundtruth of bbox(","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\n\ndef iou_batch(bboxes1, bboxes2) -> np.ndarray:\n    """"""""""""\n    From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n    """"""""""""\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n    yy2 = np.minimum(bboxes1[..., 3], bboxes2[..., 3])\n    w = np.maximum(0.0, xx2 - xx1)\n    h = np.maximum(0.0, yy2 - yy1)\n    wh = w * h\n    o = wh / (\n        (bboxes1[..., 2] - bboxes1[..., 0]) * (bboxes1[..., 3] - bboxes1[..., 1]) +\n        (bboxes2[..., 2] - bboxes2[..., 0]) * (bboxes2[..., 3] - bboxes2[..., 1]) -\n        wh\n    )\n    return o\n\n\ndef giou_batch(bboxes1, bboxes2) -> np.ndarray:\n    """"""""""""\n    :param bbox_p: predict of bbox(N,4)(x1,y1,x2,y2)\n    :param bbox_g: groundtruth of bbox(","@@ -200,10 +200,10 @@ def run_asso_func(func, *args):\n     if func not in [iou_batch, giou_batch, diou_batch, ciou_batch, centroid_batch]:\n         raise ValueError(""""Invalid function specified. Must be either '(g,d,c, )iou_batch' or 'centroid_batch'."""")\n \n-    if func is iou_batch or giou_batch or diou_batch or ciou_batch:\n+    if func is (iou_batch or giou_batch or diou_batch or ciou_batch):\n         if len(args) != 4 or not all(isinstance(arg, (list, np.ndarray)) for arg in args[0:2]):\n             raise ValueError(""""Invalid arguments for iou_batch. Expected two bounding boxes."""")\n-        return func(*args[0:2])\n+        return func(*args)\n     elif func is centroid_batch:\n         if len(args) != 4 or not all(isinstance(arg, (list, np.ndarray)) for arg in args[:2]) or not all(isinstance(arg, (int)) for arg in args[2:]):\n             raise ValueError(""""Invalid arguments for centroid_batch. Expected two bounding boxes and two size parameters."""")\n",add,Add note about data volume to enable_metrics_collection
f48804519a55e81d2065fcf10d697c6ceb230eff,with reid as default,boxmot/trackers/botsort/bot_sort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SparseOptFlow\nfrom boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        s","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SparseOptFlow\nfrom boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        s","@@ -197,7 +197,7 @@ class BoTSORT(object):\n         cmc_method: str = """"sparseOptFlow"""",\n         frame_rate=30,\n         fuse_first_associate: bool = False,\n-        with_reid: bool = False,\n+        with_reid: bool = True,\n     ):\n         self.tracked_stracks = []  # type: list[STrack]\n         self.lost_stracks = []  # type: list[STrack]\n",add,Added the user group to the contributors
ef9d971ccde687e73712402675394644d03d5264,debug prints,examples/track_w_pregenerated_dets_n_embs.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom functools import partial\n\nimport torch\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\n\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom examples.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom ultralytics.data.loaders import LoadImages\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\n\n@torch.no_grad()\ndef run(args):\n\n    tracking_config = \\n        ROOT /\\n        'boxmot' /\\n        'configs' /\\n        (args.tracking_method + '.yaml')\n\n    tracker = create_tracker(\n        args.tracking_method,\n        tracking_config,\n        args.reid_model,\n        'cpu',\n        args.half,\n        args.per_class\n    )\n\n    dat","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom functools import partial\n\nimport torch\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\n\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom examples.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom ultralytics.data.loaders import LoadImages\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\n\n@torch.no_grad()\ndef run(args):\n\n    tracking_config = \\n        ROOT /\\n        'boxmot' /\\n        'configs' /\\n        (args.tracking_method + '.yaml')\n\n    tracker = create_tracker(\n        args.tracking_method,\n        tracking_config,\n        args.reid_model,\n        'cpu',\n        args.half,\n        args.per_class\n    )\n\n    dat","@@ -54,6 +54,9 @@ def run(args):\n         dets = frame_dets_n_embs[:, :6]\n         embs = frame_dets_n_embs[:, 6:]\n \n+        print(dets.shape)\n+        print(embs.shape)\n+\n         tracks = tracker.update(dets, embs)\n \n \n",update,Added net . kano . joustsim . oscar . os
c42f128a37a3fd0230714882ecc0263eb73259f9,fix paths,experimentation/utils.py,,"import os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\n\nfrom boxmot.utils import EXAMPLES, ROOT\n\ndef download_mot_eval_tools(val_tools_path):\n    """"""""""""\n    Download the official evaluation tools for MOT metrics from the GitHub repository.\n    \n    Parameters:\n        val_tools_path (Path): Path to the destination folder where the evaluation tools will be downloaded.\n    \n    Returns:\n        None. Clones the evaluation tools repository and updates deprecated numpy types.\n    """"""""""""\n    val_tools_url = """"https://github.com/JonathonLuiten/TrackEval""""\n\n    try:\n        # Clone the repository\n        Repo.clone_from(val_tools_url, val_tools_path)\n        LOGGER.info('Official MOT evaluation repo downloaded successfully.')\n    except exc.GitError as err:\n        LOGGER.info(f'Evaluation repo already downloaded or an error","@@ -0,0 +1,175 @@\n+import os\n+import sys\n+import git\n+import requests\n+import zipfile\n+import subprocess\n+from git import Repo, exc\n+from pathlib import Path\n+from boxmot.utils import logger as LOGGER\n+from tqdm import tqdm\n+\n+from boxmot.utils import EXAMPLES, ROOT\n+\n+def download_mot_eval_tools(val_tools_path):\n+    """"""""""""\n+    Download the official evaluation tools for MOT metrics from the GitHub repository.\n+    \n+    Parameters:\n+        val_tools_path (Path): Path to the destination folder where the evaluation tools will be downloaded.\n+    \n+    Returns:\n+        None. Clones the evaluation tools repository and updates deprecated numpy types.\n+    """"""""""""\n+    val_tools_url = """"https://github.com/JonathonLuiten/TrackEval""""\n+\n+    try:\n+        # Clone the repository\n+        Repo.clone_from(val_tools_url, val_tools_path)\n+        LOGGER.info('Official MOT evaluation repo downloaded successfully.')\n+    except exc.GitError as err:\n+        LOGGER.info(f'Evaluation repo already downloaded or an error occurred: {err}')\n+\n+    # Fix deprecated np.float, np.int & np.bool by replacing them with native Python types\n+    deprecated_types = {'np.float': 'float', 'np.int': 'int', 'np.bool': 'bool'}\n+    for old_type, new_type in deprecated_types.items():\n+        # check if there are any occurrences of the old_type\n+        grep_cmd = [""""grep"""", """"-rl"""", old_type, str(val_tools_path)]\n+        grep_result = subprocess.run(grep_cmd, stdout=subprocess.PIPE, text=True)\n+\n+        # if occurrences are found, use sed to replace them\n+        if grep_result.stdout:\n+            cmd = f""""grep -rl {old_type} {val_tools_path} | xargs sed -i 's/{old_type}/{new_type}/g'""""\n+            try:\n+                subprocess.run(cmd, shell=True, check=True)\n+                LOGGER.info(f'Replaced all occurrences of {old_type} with {new_type}.')\n+            except subprocess.CalledProcessError as e:\n+                LOGGER.error(f'Error occur",update,Add note about data volume to enable_metrics_collection
c42f128a37a3fd0230714882ecc0263eb73259f9,fix paths,experimentation/val_results.py,"import sys\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS\nfrom pathlib import Path\n\n\nmot_seqs_path = ROOT / 'assets' / 'MOT17-mini' / 'train'\ngt_folder = ROOT / 'assets' / 'MOT17-mini' / 'train'\nseq_paths = [p / 'img1' for p in Path(mot_seqs_path).iterdir() if Path(p).is_dir()]\nd = [seq_path.parent.name for seq_path in seq_paths]\np = subprocess.Popen(\n    args=[\n        sys.executable, """"/home/mikel.brostrom/yolo_tracking/examples/val_utils/scripts/run_mot_challenge.py"""",\n        """"--GT_FOLDER"""", gt_folder,\n        """"--BENCHMARK"""", """""""",\n        """"--TRACKERS_FOLDER"""", """"/home/mikel.brostrom/yolo_tracking/runs/track/exp"""",   # project/name\n        """"--TRACKERS_TO_EVAL"""", """"mot"""",  # project/name/mot\n        """"--SPLIT_TO_EVAL"""", """"train"""",\n        """"--METRICS"""", """"HOTA"""", """"CLEAR"""", """"Identity"""",\n        """"--USE_PARALLEL"""", """"True"""",\n        """"--TRACKER_SUB_FOLDER"""", """""""",\n        """"--NUM_PARALLEL_CORES"""", """"4"""",\n        """"--SKIP_SPLIT_FOL"""", """"T","import sys\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, EXPERIMENTATION\nfrom pathlib import Path\nfrom experimentation.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\n\n\n\ndef run_trackeval(\n    args,\n    seq_paths,\n    save_dir,\n    MOT_results_folder,\n    gt_folder,\n    metrics = [""""HOTA"""", """"CLEAR"""", """"Identity""""]\n):\n    """"""""""""\n    Executes a Python script to evaluate MOT challenge tracking results using specified metrics.\n    \n    Parameters:\n        script_path (str): The path to the evaluation script to run.\n        trackers_folder (str): The folder where tracker results are stored.\n        metrics (list): A list of metrics to use for evaluation. Defaults to [""""HOTA"""", """"CLEAR"""", """"Identity""""].\n        num_parallel_cores (int): The number of parallel cores to use for evaluation. Defaults to 4.\n    \n    Outputs:\n        Prints the standard output and st","@@ -1,33 +1,143 @@\n import sys\n+import argparse\n import subprocess\n-from boxmot.utils import EXAMPLES, ROOT, WEIGHTS\n+from boxmot.utils import EXAMPLES, ROOT, WEIGHTS, EXPERIMENTATION\n from pathlib import Path\n+from experimentation.utils import (\n+    download_mot_eval_tools,\n+    download_mot_dataset,\n+    unzip_mot_dataset,\n+    eval_setup\n+)\n+\n \n \n-mot_seqs_path = ROOT / 'assets' / 'MOT17-mini' / 'train'\n-gt_folder = ROOT / 'assets' / 'MOT17-mini' / 'train'\n-seq_paths = [p / 'img1' for p in Path(mot_seqs_path).iterdir() if Path(p).is_dir()]\n-d = [seq_path.parent.name for seq_path in seq_paths]\n-p = subprocess.Popen(\n-    args=[\n-        sys.executable, """"/home/mikel.brostrom/yolo_tracking/examples/val_utils/scripts/run_mot_challenge.py"""",\n-        """"--GT_FOLDER"""", gt_folder,\n+def run_trackeval(\n+    args,\n+    seq_paths,\n+    save_dir,\n+    MOT_results_folder,\n+    gt_folder,\n+    metrics = [""""HOTA"""", """"CLEAR"""", """"Identity""""]\n+):\n+    """"""""""""\n+    Executes a Python script to evaluate MOT challenge tracking results using specified metrics.\n+    \n+    Parameters:\n+        script_path (str): The path to the evaluation script to run.\n+        trackers_folder (str): The folder where tracker results are stored.\n+        metrics (list): A list of metrics to use for evaluation. Defaults to [""""HOTA"""", """"CLEAR"""", """"Identity""""].\n+        num_parallel_cores (int): The number of parallel cores to use for evaluation. Defaults to 4.\n+    \n+    Outputs:\n+        Prints the standard output and standard error from the evaluation script.\n+    """"""""""""\n+    # Define paths\n+    mot_seqs_path = ROOT / 'assets' / 'MOT17-mini' / 'train'\n+    gt_folder = mot_seqs_path\n+    d = [seq_path.parent.name for seq_path in seq_paths]\n+    # Prepare arguments for subprocess call\n+    args = [\n+        sys.executable, EXPERIMENTATION / 'val_utils' / 'scripts' / 'run_mot_challenge.py',\n+        """"--GT_FOLDER"""", str(gt_folder),\n         """"--BENCHMARK""""",add,Fix build on FreeBSD ( ) .
f9b7841954a4e5fed0144c781960e2adc0aa0e2c,fix paths,experimentation/utils.py,"import os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\n\nfrom boxmot.utils import EXAMPLES, ROOT\n\ndef download_mot_eval_tools(val_tools_path):\n    """"""""""""\n    Download the official evaluation tools for MOT metrics from the GitHub repository.\n    \n    Parameters:\n        val_tools_path (Path): Path to the destination folder where the evaluation tools will be downloaded.\n    \n    Returns:\n        None. Clones the evaluation tools repository and updates deprecated numpy types.\n    """"""""""""\n    val_tools_url = """"https://github.com/JonathonLuiten/TrackEval""""\n\n    try:\n        # Clone the repository\n        Repo.clone_from(val_tools_url, val_tools_path)\n        LOGGER.info('Official MOT evaluation repo downloaded successfully.')\n    except exc.GitError as err:\n        LOGGER.info(f'Evaluation repo already downloaded or an error","import os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\n\nfrom boxmot.utils import EXAMPLES, ROOT\n\ndef download_mot_eval_tools(val_tools_path):\n    """"""""""""\n    Download the official evaluation tools for MOT metrics from the GitHub repository.\n    \n    Parameters:\n        val_tools_path (Path): Path to the destination folder where the evaluation tools will be downloaded.\n    \n    Returns:\n        None. Clones the evaluation tools repository and updates deprecated numpy types.\n    """"""""""""\n    val_tools_url = """"https://github.com/JonathonLuiten/TrackEval""""\n\n    try:\n        # Clone the repository\n        Repo.clone_from(val_tools_url, val_tools_path)\n        LOGGER.info('Official MOT evaluation repo downloaded successfully.')\n    except exc.GitError as err:\n        LOGGER.info(f'Evaluation repo already downloaded or an error","@@ -65,55 +65,63 @@ def download_mot_dataset(val_tools_path, benchmark):\n \n     if not zip_dst.exists():\n         try:\n-            response = requests.get(url, stream=True)\n-            response.raise_for_status()  # Check for HTTP request errors\n-            total_size_in_bytes = int(response.headers.get('content-length', 0))\n-\n-            with open(zip_dst, 'wb') as file, tqdm(\n-                desc=zip_dst.name,\n-                total=total_size_in_bytes,\n-                unit='iB',\n-                unit_scale=True,\n-                unit_divisor=1024,\n-            ) as bar:\n-                for data in response.iter_content(chunk_size=1024):\n-                    size = file.write(data)\n-                    bar.update(size)\n-\n-            LOGGER.info(f'{benchmark}.zip downloaded successfully.')\n-            return zip_dst\n+            response = requests.head(url, allow_redirects=True)\n+            # Consider any status code less than 400 (e.g., 200, 302) as indicating that the resource exists\n+            if response.status_code < 400:\n+                response = requests.get(url, stream=True)\n+                response.raise_for_status()  # Check for HTTP request errors\n+                total_size_in_bytes = int(response.headers.get('content-length', 0))\n+\n+                with open(zip_dst, 'wb') as file, tqdm(\n+                    desc=zip_dst.name,\n+                    total=total_size_in_bytes,\n+                    unit='iB',\n+                    unit_scale=True,\n+                    unit_divisor=1024,\n+                ) as bar:\n+                    for data in response.iter_content(chunk_size=1024):\n+                        size = file.write(data)\n+                        bar.update(size)\n+                LOGGER.info(f'{benchmark}.zip downloaded successfully.')\n+            else:\n+                LOGGER.warning(f'{benchmark} is not downloadeable from {url}')\n+                zip_dst = None\n         except requests.",add,Added STORM - 1273 to Changelog
f9b7841954a4e5fed0144c781960e2adc0aa0e2c,fix paths,experimentation/val_results.py,"import sys\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, EXPERIMENTATION\nfrom pathlib import Path\nfrom experimentation.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\n\n\n\ndef run_trackeval(\n    args,\n    seq_paths,\n    save_dir,\n    MOT_results_folder,\n    gt_folder,\n    metrics = [""""HOTA"""", """"CLEAR"""", """"Identity""""]\n):\n    """"""""""""\n    Executes a Python script to evaluate MOT challenge tracking results using specified metrics.\n    \n    Parameters:\n        script_path (str): The path to the evaluation script to run.\n        trackers_folder (str): The folder where tracker results are stored.\n        metrics (list): A list of metrics to use for evaluation. Defaults to [""""HOTA"""", """"CLEAR"""", """"Identity""""].\n        num_parallel_cores (int): The number of parallel cores to use for evaluation. Defaults to 4.\n    \n    Outputs:\n        Prints the standard output and st","import sys\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, EXPERIMENTATION\nfrom pathlib import Path\nfrom experimentation.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\n\n\n\ndef run_trackeval(\n    args,\n    seq_paths,\n    save_dir,\n    MOT_results_folder,\n    gt_folder,\n    metrics = [""""HOTA"""", """"CLEAR"""", """"Identity""""]\n):\n    """"""""""""\n    Executes a Python script to evaluate MOT challenge tracking results using specified metrics.\n    \n    Parameters:\n        script_path (str): The path to the evaluation script to run.\n        trackers_folder (str): The folder where tracker results are stored.\n        metrics (list): A list of metrics to use for evaluation. Defaults to [""""HOTA"""", """"CLEAR"""", """"Identity""""].\n        num_parallel_cores (int): The number of parallel cores to use for evaluation. Defaults to 4.\n    \n    Outputs:\n        Prints the standard output and st","@@ -33,8 +33,6 @@ def run_trackeval(\n         Prints the standard output and standard error from the evaluation script.\n     """"""""""""\n     # Define paths\n-    mot_seqs_path = ROOT / 'assets' / 'MOT17-mini' / 'train'\n-    gt_folder = mot_seqs_path\n     d = [seq_path.parent.name for seq_path in seq_paths]\n     # Prepare arguments for subprocess call\n     args = [\n@@ -135,9 +133,8 @@ if __name__ == """"__main__"""":\n \n     val_tools_path = EXPERIMENTATION / 'val_utils'\n     download_mot_eval_tools(val_tools_path)\n-    zip_dst = download_mot_dataset(val_tools_path, opt.benchmark)\n-    if zip_dst:  # Proceed with unzipping only if the download was successful\n-        unzip_mot_dataset(zip_dst, val_tools_path, opt.benchmark)\n+    zip_path = download_mot_dataset(val_tools_path, opt.benchmark)\n+    unzip_mot_dataset(zip_path, val_tools_path, opt.benchmark)\n     seq_paths, save_dir, MOT_results_folder, gt_folder = eval_setup(opt, val_tools_path)\n     run_trackeval(opt, seq_paths, save_dir, MOT_results_folder, gt_folder)\n \n",add,Added hypest who apparently actually wrote the C # port
183e3522640489b848330bb38338de0b459984d9,fix reid dst,experimentation/generate_dets_and_embs.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\n\nimport torch\n\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom examples.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\n\n@torch.no_grad()\ndef run(args):\n\n    WEIGHTS.mkdir(parents=True, exist_ok=True)\n\n    yolo = YOLO(\n        args.yolo_model if 'yolov8' in str(args.yolo_model) else 'yolov8n.pt',\n    )\n\n    results = yolo(\n        source=args.source,\n        conf=args.conf,\n        iou=args.iou,\n        agnostic_nms=args.agnostic_nms,\n        stream=True,\n        device=args.device,\n        verbose=False,\n        exist_ok=args.exis","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\n\nimport torch\n\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom examples.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\n\n@torch.no_grad()\ndef run(args):\n\n    WEIGHTS.mkdir(parents=True, exist_ok=True)\n\n    yolo = YOLO(\n        args.yolo_model if 'yolov8' in str(args.yolo_model) else 'yolov8n.pt',\n    )\n\n    results = yolo(\n        source=args.source,\n        conf=args.conf,\n        iou=args.iou,\n        agnostic_nms=args.agnostic_nms,\n        stream=True,\n        device=args.device,\n        verbose=False,\n        exist_ok=args.exis","@@ -110,6 +110,8 @@ def parse_opt():\n     parser = argparse.ArgumentParser()\n     parser.add_argument('--yolo-model', nargs='+', type=Path, default=WEIGHTS / 'yolov8n',\n                         help='yolo model path')\n+    parser.add_argument('--reid-model', nargs='+', type=Path, default=WEIGHTS / 'osnet_x0_25_msmt17.pt',\n+                        help='reid model path')\n     parser.add_argument('--source', type=str, default='0',\n                         help='file/dir/URL/glob, 0 for webcam')\n     parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640],\n@@ -120,8 +122,6 @@ def parse_opt():\n                         help='intersection over union (IoU) threshold for NMS')\n     parser.add_argument('--device', default='',\n                         help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n-    parser.add_argument('--reid-model', nargs='+', type=Path, default=WEIGHTS / 'osnet_x0_25_msmt17.pt',\n-                        help='reid model path')\n     parser.add_argument('--tracking-method', type=str, default='deepocsort',\n                         help='deepocsort, botsort, strongsort, ocsort, bytetrack')\n     # class 0 is person, 1 is bycicle, 2 is car... 79 is oven\n",add,Added STORM - 2056 to Changelog
76088971e6555bdd111025ac264e0c7ca5ba4a7f,discarded layers warning now debug,boxmot/appearance/reid_model_factory.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nimport time\nfrom collections import OrderedDict\n\nimport torch\n\nfrom boxmot.utils import logger as LOGGER\n\n__model_types = [\n    """"resnet50"""",\n    """"resnet101"""",\n    """"mlfn"""",\n    """"hacnn"""",\n    """"mobilenetv2_x1_0"""",\n    """"mobilenetv2_x1_4"""",\n    """"osnet_x1_0"""",\n    """"osnet_x0_75"""",\n    """"osnet_x0_5"""",\n    """"osnet_x0_25"""",\n    """"osnet_ibn_x1_0"""",\n    """"osnet_ain_x1_0"""",\n    """"lmbn_n"""",\n    """"clip"""",\n]\n\nlmbn_loc = 'https://github.com/mikel-brostrom/yolov8_tracking/releases/download/v9.0/'\n\n__trained_urls = {\n    # resnet50\n    """"resnet50_market1501.pt"""": """"https://drive.google.com/uc?id=1dUUZ4rHDWohmsQXCRe2C_HbYkzz94iBV"""",\n    """"resnet50_dukemtmcreid.pt"""": """"https://drive.google.com/uc?id=17ymnLglnc64NRvGOitY3BqMRS9UWd1wg"""",\n    """"resnet50_msmt17.pt"""": """"https://drive.google.com/uc?id=1ep7RypVDOthCRIAqDnn4_N-UhkkFHJsj"""",\n    """"resnet50_fc512_market1501.pt"""": """"https://drive.google.com/uc?id=","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nimport time\nfrom collections import OrderedDict\n\nimport torch\n\nfrom boxmot.utils import logger as LOGGER\n\n__model_types = [\n    """"resnet50"""",\n    """"resnet101"""",\n    """"mlfn"""",\n    """"hacnn"""",\n    """"mobilenetv2_x1_0"""",\n    """"mobilenetv2_x1_4"""",\n    """"osnet_x1_0"""",\n    """"osnet_x0_75"""",\n    """"osnet_x0_5"""",\n    """"osnet_x0_25"""",\n    """"osnet_ibn_x1_0"""",\n    """"osnet_ain_x1_0"""",\n    """"lmbn_n"""",\n    """"clip"""",\n]\n\nlmbn_loc = 'https://github.com/mikel-brostrom/yolov8_tracking/releases/download/v9.0/'\n\n__trained_urls = {\n    # resnet50\n    """"resnet50_market1501.pt"""": """"https://drive.google.com/uc?id=1dUUZ4rHDWohmsQXCRe2C_HbYkzz94iBV"""",\n    """"resnet50_dukemtmcreid.pt"""": """"https://drive.google.com/uc?id=17ymnLglnc64NRvGOitY3BqMRS9UWd1wg"""",\n    """"resnet50_msmt17.pt"""": """"https://drive.google.com/uc?id=1ep7RypVDOthCRIAqDnn4_N-UhkkFHJsj"""",\n    """"resnet50_fc512_market1501.pt"""": """"https://drive.google.com/uc?id=","@@ -198,7 +198,7 @@ def load_pretrained_weights(model, weight_path):\n         model.load_state_dict(model_dict)\n \n         if len(matched_layers) == 0:\n-            LOGGER.warning(\n+            LOGGER.debug(\n                 f'The pretrained weights """"{weight_path}"""" cannot be loaded, '\n                 """"please check the key names manually """"\n                 """"(** ignored and continue **)""""\n@@ -208,7 +208,7 @@ def load_pretrained_weights(model, weight_path):\n                 f'Successfully loaded pretrained weights from """"{weight_path}""""'\n             )\n             if len(discarded_layers) > 0:\n-                LOGGER.warning(\n+                LOGGER.debug(\n                     """"The following layers are discarded """"\n                     f""""due to unmatched keys or layer size: {*discarded_layers,}""""\n                 )\n",add,Added STORM - 1270 to Changelog
00407752c55e40e6c7ba0646ddc2d33edd7a3507,fix paths,tests/test_exports.py,"import torch\nfrom pathlib import Path\n\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_export import (export_onnx, export_openvino,\n                                           export_tflite, export_torchscript)\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\n\nPT_WEIGHTS = Path('osnet_x0_25_msmt17.pt')\nONNX_WEIGHTS = Path('osnet_x0_25_msmt17.onnx')\n\n\nim = torch.zeros(1, 3, 256, 128)\n\nmodel = build_model(\n    get_model_name(PT_WEIGHTS),\n    num_classes=1,\n    pretrained=False,\n    use_gpu=False,\n).to('cpu')\n\nload_pretrained_weights(model, PT_WEIGHTS)\nmodel.eval()\n\n\ndef test_export_torchscript():\n    f = export_torchscript(\n        model,\n        im,\n        PT_WEIGHTS,\n        True,\n    )\n    assert f is not None\n\n\ndef test_export_onnx():\n    f = export_onnx(\n        model=model,\n        im=i","import torch\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_export import (export_onnx, export_openvino,\n                                           export_tflite, export_torchscript)\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\n\nPT_WEIGHTS = WEIGHTS / 'osnet_x0_25_msmt17.pt'\nONNX_WEIGHTS = WEIGHTS / 'osnet_x0_25_msmt17.onnx'\n\n\nim = torch.zeros(1, 3, 256, 128)\n\nmodel = build_model(\n    get_model_name(PT_WEIGHTS),\n    num_classes=1,\n    pretrained=False,\n    use_gpu=False,\n).to('cpu')\n\nload_pretrained_weights(model, PT_WEIGHTS)\nmodel.eval()\n\n\ndef test_export_torchscript():\n    f = export_torchscript(\n        model,\n        im,\n        PT_WEIGHTS,\n        True,\n    )\n    assert f is not None\n\n\ndef test_export_onnx():\n    f = export_onnx(\n        model=model,\n        im=im,\n        file=PT_","@@ -1,6 +1,4 @@\n import torch\n-from pathlib import Path\n-\n \n from boxmot.appearance.backbones import build_model\n from boxmot.appearance.reid_export import (export_onnx, export_openvino,\n@@ -9,8 +7,8 @@ from boxmot.appearance.reid_model_factory import (get_model_name,\n                                                   load_pretrained_weights)\n from boxmot.utils import WEIGHTS\n \n-PT_WEIGHTS = Path('osnet_x0_25_msmt17.pt')\n-ONNX_WEIGHTS = Path('osnet_x0_25_msmt17.onnx')\n+PT_WEIGHTS = WEIGHTS / 'osnet_x0_25_msmt17.pt'\n+ONNX_WEIGHTS = WEIGHTS / 'osnet_x0_25_msmt17.onnx'\n \n \n im = torch.zeros(1, 3, 256, 128)\n",add,Add _binomial_double_trees by default for R API
1fb3b52859dcfc3cfd74c5b5971f3f6240354a4e,fix paths,tests/test_exports.py,"import torch\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_export import (export_onnx, export_openvino,\n                                           export_tflite, export_torchscript)\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\n\nPT_WEIGHTS = WEIGHTS / 'osnet_x0_25_msmt17.pt'\nONNX_WEIGHTS = WEIGHTS / 'osnet_x0_25_msmt17.onnx'\n\n\nim = torch.zeros(1, 3, 256, 128)\n\nmodel = build_model(\n    get_model_name(PT_WEIGHTS),\n    num_classes=1,\n    pretrained=False,\n    use_gpu=False,\n).to('cpu')\n\nload_pretrained_weights(model, PT_WEIGHTS)\nmodel.eval()\n\n\ndef test_export_torchscript():\n    f = export_torchscript(\n        model,\n        im,\n        PT_WEIGHTS,\n        True,\n    )\n    assert f is not None\n\n\ndef test_export_onnx():\n    f = export_onnx(\n        model=model,\n        im=im,\n        file=PT_","import torch\nfrom pathlib import Path\n\nfrom boxmot.appearance.backbones import build_model\nfrom boxmot.appearance.reid_export import (export_onnx, export_openvino,\n                                           export_tflite, export_torchscript)\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\n\nPT_WEIGHTS = Path('osnet_x0_25_msmt17.pt')\nONNX_WEIGHTS = Path('osnet_x0_25_msmt17.onnx')\n\n\nim = torch.zeros(1, 3, 256, 128)\n\nmodel = build_model(\n    get_model_name(PT_WEIGHTS),\n    num_classes=1,\n    pretrained=False,\n    use_gpu=False,\n).to('cpu')\n\nload_pretrained_weights(model, PT_WEIGHTS)\nmodel.eval()\n\n\ndef test_export_torchscript():\n    f = export_torchscript(\n        model,\n        im,\n        PT_WEIGHTS,\n        True,\n    )\n    assert f is not None\n\n\ndef test_export_onnx():\n    f = export_onnx(\n        model=model,\n        im=im,","@@ -1,4 +1,5 @@\n import torch\n+from pathlib import Path\n \n from boxmot.appearance.backbones import build_model\n from boxmot.appearance.reid_export import (export_onnx, export_openvino,\n@@ -7,8 +8,8 @@ from boxmot.appearance.reid_model_factory import (get_model_name,\n                                                   load_pretrained_weights)\n from boxmot.utils import WEIGHTS\n \n-PT_WEIGHTS = WEIGHTS / 'osnet_x0_25_msmt17.pt'\n-ONNX_WEIGHTS = WEIGHTS / 'osnet_x0_25_msmt17.onnx'\n+PT_WEIGHTS = Path('osnet_x0_25_msmt17.pt')\n+ONNX_WEIGHTS = Path('osnet_x0_25_msmt17.onnx')\n \n \n im = torch.zeros(1, 3, 256, 128)\n",add,Add note about data volume to enable_metrics_collection
45ace05e79a63ba53bee3177ef79d89768a2fad2,fix imports,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"examples""""\nEXPERIMENTATION = ROOT / """"experimentation""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"examples"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        self.update = method\n\n    def __get__(self, instance, owner):\n        def wrapper(*args, **kwargs):\n            modified_args = list(args)\n            dets = modified_args[0]\n            im = modified_args[1]\n\n            # input one class of detections at a time in order to not mix them up\n            if instance.per_class is True and dets.size != 0:\n       ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nEXPERIMENTATION = ROOT / """"experimentation""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        self.update = method\n\n    def __get__(self, instance, owner):\n        def wrapper(*args, **kwargs):\n            modified_args = list(args)\n            dets = modified_args[0]\n            im = modified_args[1]\n\n            # input one class of detections at a time in order to not mix them up\n            if instance.per_class is True and dets.size != 0:\n       ","@@ -8,10 +8,10 @@ import numpy as np\n FILE = Path(__file__).resolve()\n ROOT = FILE.parents[2]  # root directory\n BOXMOT = ROOT / """"boxmot""""\n-EXAMPLES = ROOT / """"examples""""\n+EXAMPLES = ROOT / """"tracking""""\n EXPERIMENTATION = ROOT / """"experimentation""""\n TRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\n-WEIGHTS = ROOT / """"examples"""" / """"weights""""\n+WEIGHTS = ROOT / """"tracking"""" / """"weights""""\n REQUIREMENTS = ROOT / """"requirements.txt""""\n \n # global logger\n",add,Add group setup mapping for Pixelmr
45ace05e79a63ba53bee3177ef79d89768a2fad2,fix imports,experimentation/generate_dets_and_embs.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\n\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom examples.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\n\n@torch.no_grad()\ndef run(args):\n\n    WEIGHTS.mkdir(parents=True, exist_ok=True)\n\n    yolo = YOLO(\n        args.yolo_model if 'yolov8' in str(args.yolo_model) else 'yolov8n.pt',\n    )\n\n    results = yolo(\n        source=args.source,\n        conf=args.conf,\n        iou=args.iou,\n        agnostic_nms=args.agnostic_nms,\n        stream=True,\n        device=args.device,\n        verbose=False,\n   ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\n\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom tracking.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\n\n@torch.no_grad()\ndef run(args):\n\n    WEIGHTS.mkdir(parents=True, exist_ok=True)\n\n    yolo = YOLO(\n        args.yolo_model if 'yolov8' in str(args.yolo_model) else 'yolov8n.pt',\n    )\n\n    results = yolo(\n        source=args.source,\n        conf=args.conf,\n        iou=args.iou,\n        agnostic_nms=args.agnostic_nms,\n        stream=True,\n        device=args.device,\n        verbose=False,\n   ","@@ -9,7 +9,7 @@ import torch\n \n from boxmot.utils import ROOT, WEIGHTS\n from boxmot.utils.checks import TestRequirements\n-from examples.detectors import get_yolo_inferer\n+from tracking.detectors import get_yolo_inferer\n from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n \n __tr = TestRequirements()\n",add,Add note about data volume to enable
45ace05e79a63ba53bee3177ef79d89768a2fad2,fix imports,experimentation/track_w_dets_n_embs.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom functools import partial\nimport json\nimport torch\n\nfrom tqdm import tqdm\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\n\nfrom ultralytics.utils.files import increment_path \nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\nfrom boxmot.utils.checks import TestRequirements\nfrom examples.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.utils import logger as LOGGER\n\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom examples.utils import write_np_mot_results\n\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\n\ndef track(args):\n\n    tracker = create_tracker(\n        args.tracking_","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom functools import partial\nimport json\nimport torch\n\nfrom tqdm import tqdm\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\n\nfrom ultralytics.utils.files import increment_path \nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\nfrom boxmot.utils.checks import TestRequirements\nfrom tracking.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.utils import logger as LOGGER\n\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom experimentation.utils import convert_to_mot_format, write_mot_results\n\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\n\ndef track(args):\n\n    tracker = create_track","@@ -15,7 +15,7 @@ from boxmot.tracker_zoo import create_tracker\n from ultralytics.utils.files import increment_path \n from boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\n from boxmot.utils.checks import TestRequirements\n-from examples.detectors import get_yolo_inferer\n+from tracking.detectors import get_yolo_inferer\n from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n from boxmot.utils import logger as LOGGER\n \n@@ -23,7 +23,7 @@ from ultralytics.data.loaders import LoadImages\n from ultralytics import YOLO\n from ultralytics.data.utils import VID_FORMATS\n \n-from examples.utils import write_np_mot_results\n+from experimentation.utils import convert_to_mot_format, write_mot_results\n \n \n __tr = TestRequirements()\n@@ -58,7 +58,7 @@ def track(args):\n \n     dataset = LoadImages(args.source)\n     \n-    p = args.exp_folder_path / (Path(args.source).parent.name + '.txt')\n+    txt_path = args.exp_folder_path / (Path(args.source).parent.name + '.txt')\n     for frame_idx, d in enumerate(tqdm(dataset)):\n \n         im = d[1][0]\n@@ -71,16 +71,8 @@ def track(args):\n         embs = frame_dets_n_embs[:, 7:]\n         tracks = tracker.update(dets, im, embs)\n \n-        if tracks.ndim == 1:\n-            # The array is 1D; add a new axis to make it 2D\n-            # For example, convert it to a column vector\n-            print(tracks)\n-\n-        write_np_mot_results(\n-            p,\n-            tracks,\n-            frame_idx + 1,\n-        )\n+        mot_results = convert_to_mot_format(tracks, frame_idx + 1)\n+        write_mot_results(txt_path, mot_results)\n \n \n def parse_opt():\n",add,added meteor
a9c550f29632c306507d07979af81cd70092d9e2,fix paths,.github/workflows/ci.yml,"# name of the workflow, what it is doing (optional)\nname: BoxMOT CI\n\n# events that trigger the workflow (required)\non:\n  push:\n    # pushes to the following branches\n    branches:\n      - master\n  pull_request:\n    # pull request where master is target\n    branches:\n      - master\n\n\nenv:\n  # Directory of PyPi package to be tested\n  PACKAGE_DIR: boxmot\n  # Minimum acceptable test coverage\n  # Increase as you add more tests to increase coverage\n  COVERAGE_FAIL_UNDER: 29\n\n\njobs:\n  test-tracking-methods:\n    runs-on: ${{ matrix.os }}\n    outputs:\n      status: ${{ job.status }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]   # skip windows-latest for\n        python-version: ['3.8', '3.11']\n        # leads to too many workflow which ends up queued\n        # tracking-method: [hybridsort, botsort, ocsort, bytetrack] \n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      - us","# name of the workflow, what it is doing (optional)\nname: BoxMOT CI\n\n# events that trigger the workflow (required)\non:\n  push:\n    # pushes to the following branches\n    branches:\n      - master\n  pull_request:\n    # pull request where master is target\n    branches:\n      - master\n\n\nenv:\n  # Directory of PyPi package to be tested\n  PACKAGE_DIR: boxmot\n  # Minimum acceptable test coverage\n  # Increase as you add more tests to increase coverage\n  COVERAGE_FAIL_UNDER: 29\n\n\njobs:\n  test-tracking-methods:\n    runs-on: ${{ matrix.os }}\n    outputs:\n      status: ${{ job.status }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]   # skip windows-latest for\n        python-version: ['3.8', '3.11']\n        # leads to too many workflow which ends up queued\n        # tracking-method: [hybridsort, botsort, ocsort, bytetrack] \n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      - us","@@ -52,14 +52,14 @@ jobs:\n       - name: Generate detections and embeddings\n         run: |\n           python tracking/generate_dets_and_embs.py --source ./assets/MOT17-mini/train --yolo-model yolov8n.pt --reid-model osnet_x0_25_msmt17.pt --imgsz 320\n-          python tracking/track_w_pregenerated.py --dets yolov8n --embs osnet_x0_25_msmt17\n+          python tracking/generate_mot_results.py --dets yolov8n --embs osnet_x0_25_msmt17\n \n       - name: Run tracking method\n         run: |\n-          python tracking/track_w_pregenerated.py --tracking-method hybridsort --dets yolov8n --embs osnet_x0_25_msmt17 --imgsz 320\n-          python tracking/track_w_pregenerated.py --tracking-method botsort    --dets yolov8n --embs osnet_x0_25_msmt17 --imgsz 320\n-          python tracking/track_w_pregenerated.py --tracking-method ocsort     --dets yolov8n --embs osnet_x0_25_msmt17 --imgsz 320\n-          python tracking/track_w_pregenerated.py --tracking-method bytetrack  --dets yolov8n --embs osnet_x0_25_msmt17 --imgsz 320\n+          python tracking/generate_mot_results.py --tracking-method hybridsort --dets yolov8n --embs osnet_x0_25_msmt17 --imgsz 320\n+          python tracking/generate_mot_results.py --tracking-method botsort    --dets yolov8n --embs osnet_x0_25_msmt17 --imgsz 320\n+          python tracking/generate_mot_results.py --tracking-method ocsort     --dets yolov8n --embs osnet_x0_25_msmt17 --imgsz 320\n+          python tracking/generate_mot_results.py --tracking-method bytetrack  --dets yolov8n --embs osnet_x0_25_msmt17 --imgsz 320\n \n   test-evolution:\n     runs-on: ${{ matrix.os }}\n@@ -90,7 +90,7 @@ jobs:\n       - name: Generate detections and embeddings\n         run: |\n           python tracking/generate_dets_and_embs.py --source ./assets/MOT17-mini/train --yolo-model yolov8n.pt --reid-model osnet_x0_25_msmt17.pt --imgsz 320\n-          python tracking/track_w_pregenerated.py --dets yolov8n --reid osnet_x0_25_msmt17\n+          python tracking/gen",add,Add note about data volume to enable_metrics_collection
a9c550f29632c306507d07979af81cd70092d9e2,fix paths,tracking/generate_mot_results.py,,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom functools import partial\nimport json\nimport torch\n\nfrom tqdm import tqdm\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\n\nfrom ultralytics.utils.files import increment_path \nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\nfrom boxmot.utils.checks import TestRequirements\nfrom tracking.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.utils import logger as LOGGER\n\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.utils import convert_to_mot_format, write_mot_results\n\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\n\ndef generate_mot_results(args):\n\n    tracker = crea","@@ -0,0 +1,160 @@\n+# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n+\n+import argparse\n+from pathlib import Path\n+import numpy as np\n+from functools import partial\n+import json\n+import torch\n+\n+from tqdm import tqdm\n+\n+from boxmot import TRACKERS\n+from boxmot.tracker_zoo import create_tracker\n+\n+from ultralytics.utils.files import increment_path \n+from boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\n+from boxmot.utils.checks import TestRequirements\n+from tracking.detectors import get_yolo_inferer\n+from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n+from boxmot.utils import logger as LOGGER\n+\n+from ultralytics.data.loaders import LoadImages\n+from ultralytics import YOLO\n+from ultralytics.data.utils import VID_FORMATS\n+\n+from tracking.utils import convert_to_mot_format, write_mot_results\n+\n+\n+__tr = TestRequirements()\n+__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n+\n+\n+def generate_mot_results(args):\n+\n+    tracker = create_tracker(\n+        args.tracking_method,\n+        TRACKER_CONFIGS / (args.tracking_method + '.yaml'),\n+        args.reid_model.with_suffix('.pt'),\n+        'cpu',\n+        False,\n+        False\n+    )\n+\n+    with open(args.dets_file_path, 'r') as file:\n+        args.source = file.readline().strip().replace(""""# """", """""""")  # .strip() removes leading/trailing whitespace and newline characters\n+\n+    LOGGER.info(f""""\nStarting tracking on:\n\t{args.source}\nwith preloaded dets\n\t({args.dets_file_path.relative_to(ROOT)})\nand embs\n\t({args.embs_file_path.relative_to(ROOT)})\nusing\n\t{args.tracking_method}"""")\n+\n+    dets = np.loadtxt(args.dets_file_path, skiprows=1)  # skiprows=1 skips the header row\n+    embs = np.loadtxt(args.embs_file_path)  # skiprows=1 skips the header row\n+\n+    dets_n_embs = np.concatenate(\n+        [\n+            dets,\n+            embs\n+        ], axis=1\n+    )\n+\n+    dataset = LoadIma",add,Fix build
4a5f0c7408efcaebc05a35008a5670eee947447e,fix paths,tracking/evolve.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport argparse\n\nimport yaml\nfrom ultralytics.utils.checks import check_requirements, print_args\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, logger\nfrom tracking.val import run_trackeval\nfrom tracking.track_w_pregenerated import run_track\n\n\nclass Objective():\n    """"""""""""Objective function to evolve best set of hyperparams for\n\n    This object is passed to an objective function and provides interfaces to overw","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport argparse\n\nimport yaml\nfrom ultralytics.utils.checks import check_requirements, print_args\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, logger\nfrom tracking.val import run_trackeval\nfrom tracking.generate_mot_results import run_generate_mot_results\n\n\nclass Objective():\n    """"""""""""Objective function to evolve best set of hyperparams for\n\n    This object is passed to an objective function and provides inte","@@ -19,7 +19,7 @@ from ultralytics.utils.checks import check_requirements, print_args\n \n from boxmot.utils import EXAMPLES, ROOT, WEIGHTS, logger\n from tracking.val import run_trackeval\n-from tracking.track_w_pregenerated import run_track\n+from tracking.generate_mot_results import run_generate_mot_results\n \n \n class Objective():\n@@ -218,7 +218,6 @@ class Objective():\n         # generate new set of params\n         self.get_new_config(trial)\n         # run trial, get HOTA, MOTA, IDF1 COMBINED results\n-        print('run_track(self.opt)')\n         run_track(self.opt)\n         results = run_trackeval(self.opt)\n         # extract objective results of current trial\n",add,Add note about data volume to enable_metrics_collection
4a5f0c7408efcaebc05a35008a5670eee947447e,fix paths,tracking/generate_mot_results.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom functools import partial\nimport json\nimport torch\n\nfrom tqdm import tqdm\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\n\nfrom ultralytics.utils.files import increment_path \nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\nfrom boxmot.utils.checks import TestRequirements\nfrom tracking.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.utils import logger as LOGGER\n\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.utils import convert_to_mot_format, write_mot_results\n\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\n\ndef generate_mot_results(args):\n\n    tracker = crea","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom functools import partial\nimport json\nimport torch\n\nfrom tqdm import tqdm\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\n\nfrom ultralytics.utils.files import increment_path \nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\nfrom boxmot.utils.checks import TestRequirements\nfrom tracking.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.utils import logger as LOGGER\n\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.utils import convert_to_mot_format, write_mot_results\n\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\n\ndef generate_mot_results(args):\n\n    tracker = crea","@@ -157,4 +157,4 @@ def run_generate_mot_results(opt):\n \n \n if __name__ == """"__main__"""":\n-    run_track(None)\n+    run_generate_mot_results(None)\n",fix,Add group has a name
1f21943470e20058e27eaf5b37020ba2f69d4c9f,avoid saving txt when tracking results are none,tracking/utils.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nfrom ultralytics.utils import ops\nfrom ultralytics.engine.results import Results\nfrom typing import Union\nfrom pathlib import Path\nimport os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\nfrom boxmot.utils import EXAMPLES, ROOT\n\n\ndef download_mot_eval_tools(val_tools_path):\n    """"""""""""\n    Download the official evaluation tools for MOT metrics from the GitHub repository.\n    \n    Parameters:\n        val_tools_path (Path): Path to the destination folder where the evaluation tools will be downloaded.\n    \n    Returns:\n        None. Clones the evaluation tools repository and updates deprecated numpy types.\n    """"""""""""\n    val_tools_url = """"https://github.com/JonathonLuiten/TrackEval""""\n\n    try:\n        # Clone the repository\n        Repo.clone_from(val_tools_url","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nfrom ultralytics.utils import ops\nfrom ultralytics.engine.results import Results\nfrom typing import Union\nfrom pathlib import Path\nimport os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\nfrom boxmot.utils import EXAMPLES, ROOT\n\n\ndef download_mot_eval_tools(val_tools_path):\n    """"""""""""\n    Download the official evaluation tools for MOT metrics from the GitHub repository.\n    \n    Parameters:\n        val_tools_path (Path): Path to the destination folder where the evaluation tools will be downloaded.\n    \n    Returns:\n        None. Clones the evaluation tools repository and updates deprecated numpy types.\n    """"""""""""\n    val_tools_url = """"https://github.com/JonathonLuiten/TrackEval""""\n\n    try:\n        # Clone the repository\n        Repo.clone_from(val_tools_url","@@ -218,34 +218,36 @@ def convert_to_mot_format(results: Union[Results, np.ndarray], frame_idx: int) -\n     - Union[torch.Tensor, np.ndarray]: A tensor or array containing the MOT formatted results for the frame.\n     """"""""""""\n \n-    if isinstance(results, np.ndarray):\n-        # convert numpy array results to MOT format\n-        tlwh = ops.xyxy2ltwh(results[:, 0:4])\n-        frame_idx_column = np.full((results.shape[0], 1), frame_idx + 1)\n-        mot_results = np.column_stack((\n-            frame_idx_column,\n-            results[:, 4],  # track id\n-            tlwh,  # top,left,bottom, right\n-            results[:, 5],  # confidence\n-            results[:, 6],  # class\n-        ))\n-        return mot_results\n-    else:\n-        # convert ultralytics results to MOT format\n-        num_detections = len(results.boxes)\n-        frame_indices = torch.full((num_detections, 1), frame_idx + 1, dtype=torch.float32)\n-        dont_care_values = torch.full((num_detections, 1), -1, dtype=torch.float32)\n-        \n-        mot_results = torch.cat([\n-            frame_indices,\n-            results.boxes.id.unsqueeze(1).float(),\n-            ops.xyxy2ltwh(results.boxes.xyxy).float(),  # Convert bbox format\n-            results.boxes.conf.unsqueeze(1),\n-            results.boxes.cls.unsqueeze(1).float(),\n-            dont_care_values\n-        ], dim=1)\n-\n-        return mot_results.numpy()\n+    # do not try to safe if array is empty\n+    if results.size != 0:\n+        if isinstance(results, np.ndarray):\n+            # convert numpy array results to MOT format\n+            tlwh = ops.xyxy2ltwh(results[:, 0:4])\n+            frame_idx_column = np.full((results.shape[0], 1), frame_idx + 1)\n+            mot_results = np.column_stack((\n+                frame_idx_column,\n+                results[:, 4],  # track id\n+                tlwh,  # top,left,bottom, right\n+                results[:, 5],  # confidence\n+                results[:, 6],  # class\",add,Added STORM - 1404 to Changelog
846fa4bedb6ac856014a0b74024013a6330dd05a,fix imports,tracking/detectors/yolonas.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\n\nfrom boxmot.utils import logger as LOGGER\nfrom examples.detectors.yolo_interface import YoloInterface\n\n\nclass YoloNASStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = {\n        0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus',\n        6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n        11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat',\n        16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant',\n        21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella',\n        26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis',\n     ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n\nclass YoloNASStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = {\n        0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus',\n        6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n        11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat',\n        16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant',\n        21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella',\n        26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis',\n     ","@@ -8,7 +8,7 @@ from ultralytics.engine.results import Results\n from ultralytics.utils import ops\n \n from boxmot.utils import logger as LOGGER\n-from examples.detectors.yolo_interface import YoloInterface\n+from tracking.detectors.yolo_interface import YoloInterface\n \n \n class YoloNASStrategy(YoloInterface):\n",add,added meteor
846fa4bedb6ac856014a0b74024013a6330dd05a,fix imports,tracking/detectors/yolox.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom examples.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass YoloXStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n\n\nclass YoloXStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False","@@ -9,7 +9,7 @@ from yolox.utils import postprocess\n from yolox.utils.model_utils import fuse_model\n \n from boxmot.utils import logger as LOGGER\n-from examples.detectors.yolo_interface import YoloInterface\n+from tracking.detectors.yolo_interface import YoloInterface\n \n # default model weigths for these model names\n YOLOX_ZOO = {\n",fix,Add note about data volume to enable_metrics_collection
6925d4fdbbf4e9651662c57fa397ed6cc13544ba,fix paths,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nEXPERIMENTATION = ROOT / """"experimentation""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        self.update = method\n\n    def __get__(self, instance, owner):\n        def wrapper(*args, **kwargs):\n            modified_args = list(args)\n            dets = modified_args[0]\n            im = modified_args[1]\n\n            # input one class of detections at a time in order to not mix them up\n            if instance.per_class is True and dets.size != 0:\n       ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        self.update = method\n\n    def __get__(self, instance, owner):\n        def wrapper(*args, **kwargs):\n            modified_args = list(args)\n            dets = modified_args[0]\n            im = modified_args[1]\n\n            # input one class of detections at a time in order to not mix them up\n            if instance.per_class is True and dets.size != 0:\n                dets_dict = {\n","@@ -7,9 +7,9 @@ import numpy as np\n \n FILE = Path(__file__).resolve()\n ROOT = FILE.parents[2]  # root directory\n+DATA = ROOT / 'data'\n BOXMOT = ROOT / """"boxmot""""\n EXAMPLES = ROOT / """"tracking""""\n-EXPERIMENTATION = ROOT / """"experimentation""""\n TRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\n WEIGHTS = ROOT / """"tracking"""" / """"weights""""\n REQUIREMENTS = ROOT / """"requirements.txt""""\n",add,Added net . android . js
6925d4fdbbf4e9651662c57fa397ed6cc13544ba,fix paths,tracking/val.py,"import re\nimport sys\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, EXPERIMENTATION\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        # pack everything in dict\n        combined_results = {key: value for key, value in zip(['HOTA', 'MOTA', 'IDF1'], combined_results)","import re\nimport sys\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        # pack everything in dict\n        combined_results = {key: value for key, value in zip(['HOTA', 'MOTA', 'IDF1'], combined_results)}\n        ","@@ -2,7 +2,7 @@ import re\n import sys\n import argparse\n import subprocess\n-from boxmot.utils import EXAMPLES, ROOT, WEIGHTS, EXPERIMENTATION\n+from boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\n from pathlib import Path\n from tracking.utils import (\n     download_mot_eval_tools,\n@@ -56,7 +56,7 @@ def trackeval(\n     d = [seq_path.parent.name for seq_path in seq_paths]\n     # Prepare arguments for subprocess call\n     args = [\n-        sys.executable, EXPERIMENTATION / 'val_utils' / 'scripts' / 'run_mot_challenge.py',\n+        sys.executable, EXAMPLES / 'val_utils' / 'scripts' / 'run_mot_challenge.py',\n         """"--GT_FOLDER"""", str(gt_folder),\n         """"--BENCHMARK"""", """""""",\n         """"--TRACKERS_FOLDER"""", args.exp_folder_path,\n@@ -112,7 +112,7 @@ def parse_opt():\n     # class 0 is person, 1 is bycicle, 2 is car... 79 is oven\n     parser.add_argument('--classes', nargs='+', type=int, default=0,\n                         help='filter by class: --classes 0, or --classes 0 2 3')\n-    parser.add_argument('--project', default=ROOT / 'runs' / 'mot',\n+    parser.add_argument('--project', default=ROOT / 'runs' / 'mot', type=Path,\n                         help='save results to project/name')\n     parser.add_argument('--name', default='yolov8n_osnet_x0_25_msmt17',\n                         help='save results to project/name')\n@@ -163,7 +163,7 @@ def run_trackeval(opt):\n         opt = opt\n         opt.exist_ok = False\n \n-    val_tools_path = EXPERIMENTATION / 'val_utils'\n+    val_tools_path = EXAMPLES / 'val_utils'\n     download_mot_eval_tools(val_tools_path)\n     zip_path = download_mot_dataset(val_tools_path, opt.benchmark)\n     unzip_mot_dataset(zip_path, val_tools_path, opt.benchmark)\n",add,Added STORM - 236 to Changelog
2074a57c8bc48f2f0f96b086785055d8560dc55e,fix imports,boxmot/trackers/deepocsort/deep_ocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.deepocsort_kf import KalmanFilter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.deepocsort_kf import KalmanFilter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,","@@ -13,6 +13,7 @@ from boxmot.utils.association import associate, linear_assignment\n from boxmot.utils.iou import get_asso_func\n from boxmot.trackers.basetracker import BaseTracker\n \n+\n def k_previous_obs(observations, cur_age, k):\n     if len(observations) == 0:\n         return [-1, -1, -1, -1, -1]\n",add,Added STORM - 1270 to Changelog
2074a57c8bc48f2f0f96b086785055d8560dc55e,fix imports,boxmot/trackers/hybridsort/hybridsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.hybridsort.association import (\n    associate_4_points_with_score, associate_4_points_with_score_with_reid,\n    cal_score_dif_batch_two_score, embedding_distance, linear_assignment)\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.iou import get_asso_func\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.hybridsort.association import (\n    associate_4_points_with_score, associate_4_points_with_score_with_reid,\n    cal_score_dif_batch_two_score, embedding_distance, linear_assignment)\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = ma","@@ -15,6 +15,8 @@ from boxmot.trackers.hybridsort.association import (\n     cal_score_dif_batch_two_score, embedding_distance, linear_assignment)\n from boxmot.utils import PerClassDecorator\n from boxmot.utils.iou import get_asso_func\n+from boxmot.trackers.basetracker import BaseTracker\n+\n \n np.random.seed(0)\n \n@@ -326,7 +328,7 @@ class KalmanBoxTracker(object):\n         return convert_x_to_bbox(self.kf.x)\n \n \n-class HybridSORT(object):\n+class HybridSORT(BaseTracker):\n     def __init__(self, reid_weights, device, half, det_thresh, max_age=30, min_hits=3,\n                  iou_threshold=0.3, delta_t=3, asso_func=""""iou"""", inertia=0.2, longterm_reid_weight=0, TCM_first_step_weight=0, use_byte=False):\n         """"""""""""\n@@ -335,7 +337,6 @@ class HybridSORT(object):\n         self.max_age = max_age\n         self.min_hits = min_hits\n         self.iou_threshold = iou_threshold\n-        self.trackers = []\n         self.per_class = True\n         self.frame_count = 0\n         self.det_thresh = det_thresh\n@@ -385,7 +386,7 @@ class HybridSORT(object):\n         if self.ECC:\n             warp_matrix = self.cmc.apply(im, dets)\n             if warp_matrix is not None:\n-                self.camera_update(self.trackers, warp_matrix)\n+                self.camera_update(self.active_tracks, warp_matrix)\n \n         self.frame_count += 1\n         scores = dets[:, 4]\n@@ -403,39 +404,39 @@ class HybridSORT(object):\n         id_feature_keep = dets_embs[remain_inds]  # ID feature of 1st stage matching\n         id_feature_second = dets_embs[inds_second]  # ID feature of 2nd stage matching\n \n-        trks = np.zeros((len(self.trackers), 8))\n+        trks = np.zeros((len(self.active_tracks), 8))\n         to_del = []\n         ret = []\n         for t, trk in enumerate(trks):\n-            pos, kalman_score, simple_score = self.trackers[t].predict()\n+            pos, kalman_score, simple_score = self.active_tracks[t].predict()\n             trk[:6] = [pos[0][0",add,Added the UNSTARTED state to the YouTube PlayerState enum
2074a57c8bc48f2f0f96b086785055d8560dc55e,fix imports,boxmot/trackers/ocsort/ocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.ocsort_kf import KalmanFilter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.utils.iou import run_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.ocsort_kf import KalmanFilter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.utils.iou import run_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ra","@@ -12,7 +12,6 @@ from boxmot.utils.iou import run_asso_func\n from boxmot.trackers.basetracker import BaseTracker\n \n \n-\n def k_previous_obs(observations, cur_age, k):\n     if len(observations) == 0:\n         return [-1, -1, -1, -1, -1]\n",fix,Added STORM - 1270 to Changelog
8e2ecb29ca0d21acbe476a12b9cfb2060c751e62,fix imports,boxmot/trackers/botsort/bot_sort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\nfrom boxmot.trackers.botsort.basetracker import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetrack import BaseTracker\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetrack import BaseTracker\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n   ","@@ -7,7 +7,7 @@ import numpy as np\n from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n from boxmot.motion.cmc.sof import SOF\n from boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\n-from boxmot.trackers.botsort.basetracker import BaseTrack, TrackState\n+from boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\n from boxmot.utils.matching import (embedding_distance, fuse_score,\n                                    iou_distance, linear_assignment)\n from boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n",add,Added note about dates .
8e2ecb29ca0d21acbe476a12b9cfb2060c751e62,fix imports,boxmot/trackers/bytetrack/byte_tracker.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.bytetrack_kf import KalmanFilter\nfrom boxmot.trackers.bytetrack.basetracker import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n        self.xyah = tlwh2xyah(self.tlwh)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        self.tracklet_len = 0\n\n    def predict(self):\n        mean_state = self.mean.copy()\n      ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.bytetrack_kf import KalmanFilter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetrack import BaseTracker\n\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n        self.xyah = tlwh2xyah(self.tlwh)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        self.tracklet_len = 0\n\n    def predict(se","@@ -3,9 +3,11 @@\n import numpy as np\n \n from boxmot.motion.kalman_filters.bytetrack_kf import KalmanFilter\n-from boxmot.trackers.bytetrack.basetracker import BaseTrack, TrackState\n+from boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\n from boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\n from boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\n+from boxmot.trackers.basetrack import BaseTracker\n+\n \n \n class STrack(BaseTrack):\n",add,Add warning about data volume to enable_metrics_collection
6b8d2ceed98c59c5549f05ec440bc679079624f6,fix imports,boxmot/trackers/botsort/bot_sort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetrack import BaseTracker\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n   ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n ","@@ -11,7 +11,7 @@ from boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\n from boxmot.utils.matching import (embedding_distance, fuse_score,\n                                    iou_distance, linear_assignment)\n from boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n-from boxmot.trackers.basetrack import BaseTracker\n+from boxmot.trackers.basetracker import BaseTracker\n \n \n class STrack(BaseTrack):\n",fix,Added note about dates .
6b8d2ceed98c59c5549f05ec440bc679079624f6,fix imports,boxmot/trackers/bytetrack/byte_tracker.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.bytetrack_kf import KalmanFilter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetrack import BaseTracker\n\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n        self.xyah = tlwh2xyah(self.tlwh)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        self.tracklet_len = 0\n\n    def predict(se","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.bytetrack_kf import KalmanFilter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n        self.xyah = tlwh2xyah(self.tlwh)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        self.tracklet_len = 0\n\n    def predict(","@@ -6,7 +6,7 @@ from boxmot.motion.kalman_filters.bytetrack_kf import KalmanFilter\n from boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\n from boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\n from boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\n-from boxmot.trackers.basetrack import BaseTracker\n+from boxmot.trackers.basetracker import BaseTracker\n \n \n \n",add,Added note about dates .
d94e853f3898bd7c2a96738a5d84eaa95f6902cb,fix tracks list name,boxmot/trackers/deepocsort/deep_ocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.deepocsort_kf import KalmanFilter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.deepocsort_kf import KalmanFilter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,","@@ -391,7 +391,7 @@ class DeepOCSort(BaseTracker):\n         # CMC\n         if not self.cmc_off:\n             transform = self.cmc.apply(img, dets[:, :4])\n-            for trk in self.trackers:\n+            for trk in self.active_tracks:\n                 trk.apply_affine_correction(transform)\n \n         trust = (dets[:, 4] - self.det_thresh) / (1 - self.det_thresh)\n@@ -400,17 +400,17 @@ class DeepOCSort(BaseTracker):\n         dets_alpha = af + (1 - af) * (1 - trust)\n \n         # get predicted locations from existing trackers.\n-        trks = np.zeros((len(self.trackers), 5))\n+        trks = np.zeros((len(self.active_tracks), 5))\n         trk_embs = []\n         to_del = []\n         ret = []\n         for t, trk in enumerate(trks):\n-            pos = self.trackers[t].predict()[0]\n+            pos = self.active_tracks[t].predict()[0]\n             trk[:] = [pos[0], pos[1], pos[2], pos[3], 0]\n             if np.any(np.isnan(pos)):\n                 to_del.append(t)\n             else:\n-                trk_embs.append(self.trackers[t].get_emb())\n+                trk_embs.append(self.active_tracks[t].get_emb())\n         trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n \n         if len(trk_embs) > 0:\n@@ -419,11 +419,11 @@ class DeepOCSort(BaseTracker):\n             trk_embs = np.array(trk_embs)\n \n         for t in reversed(to_del):\n-            self.trackers.pop(t)\n+            self.active_tracks.pop(t)\n \n-        velocities = np.array([trk.velocity if trk.velocity is not None else np.array((0, 0)) for trk in self.trackers])\n-        last_boxes = np.array([trk.last_observation for trk in self.trackers])\n-        k_observations = np.array([k_previous_obs(trk.observations, trk.age, self.delta_t) for trk in self.trackers])\n+        velocities = np.array([trk.velocity if trk.velocity is not None else np.array((0, 0)) for trk in self.active_tracks])\n+        last_boxes = np.array([trk.last_observation for trk in self.active_tracks])\n+",add,Added hypest who apparently actually wrote the C # port
ebf121127987b7192c2fbefe0c128ea950f1090a,fix update call,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nEXPERIMENTATION = ROOT / """"experimentation""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update_method = method\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def wrapper(*args, **kwargs):\n            # Unpack arguments for clarity\n            modified_args = list(args)\n            dets = modifi","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nEXPERIMENTATION = ROOT / """"experimentation""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update_method = method\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def wrapper(*args, **kwargs):\n            # Unpack arguments for clarity\n            modified_args = list(args)\n            dets = modifi","@@ -56,14 +56,14 @@ class PerClassDecorator:\n                     logger.debug(f""""Processing class {int(class_id)}: {current_class_detections.shape}"""")\n                     \n                     # Update detections using the decorated method\n-                    updated_dets = self.update_method(instance, current_class_detections, image)\n+                    updated_dets = self.update(instance, current_class_detections, image)\n                     if updated_dets.size != 0:\n                         modified_detections = np.append(modified_detections, updated_dets, axis=0)\n \n                 logger.debug(f""""Per-class update result: {modified_detections.shape}"""")\n             else:\n                 # Process all detections at once if per_class is False or detections are empty\n-                modified_detections = self.update_method(instance, detections, image)\n+                modified_detections = self.update(instance, detections, image)\n \n             return modified_detections\n \n",add,Add group has a name
5029d2aa4bdec48d2a761105fe3128202d9007d4,fix update call,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nEXPERIMENTATION = ROOT / """"experimentation""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update_method = method\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def wrapper(*args, **kwargs):\n            # Unpack arguments for clarity\n            modified_args = list(args)\n            dets = modifi","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nEXPERIMENTATION = ROOT / """"experimentation""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update_method = method\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def wrapper(*args, **kwargs):\n            # Unpack arguments for clarity\n            modified_args = list(args)\n            dets = modifi","@@ -34,11 +34,11 @@ class PerClassDecorator:\n             dets = modified_args[0]\n             im = modified_args[1]\n             \n-            if instance.per_class is True and detections.size != 0:\n+            if instance.per_class is True and dets.size != 0:\n                 # Organize detections by class ID for per-class processing\n                 detections_by_class = {\n-                    class_id: np.array([det for det in detections if det[5] == class_id])\n-                    for class_id in set(det[5] for det in detections)\n+                    class_id: np.array([det for det in dets if det[5] == class_id])\n+                    for class_id in set(det[5] for det in dets)\n                 }\n \n                 # Detect classes in the current frame and active trackers\n@@ -56,14 +56,14 @@ class PerClassDecorator:\n                     logger.debug(f""""Processing class {int(class_id)}: {current_class_detections.shape}"""")\n                     \n                     # Update detections using the decorated method\n-                    updated_dets = self.update(instance, current_class_detections, image)\n+                    updated_dets = self.update(instance, current_class_detections, im)\n                     if updated_dets.size != 0:\n                         modified_detections = np.append(modified_detections, updated_dets, axis=0)\n \n                 logger.debug(f""""Per-class update result: {modified_detections.shape}"""")\n             else:\n                 # Process all detections at once if per_class is False or detections are empty\n-                modified_detections = self.update(instance, detections, image)\n+                modified_detections = self.update(instance, dets, im)\n \n             return modified_detections\n \n",add,Add Rossen
f9ba25f4f8cbb65f977ffe7802b918dd9f294c57,fix duplicated test names,tests/unit/test_trackers.py,"import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, get_tracker_config, create_tracker,\n)\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_tracker_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_tracker_instantiation(Tracker):\n    Tracker()\n\n\n@pytest.mark.parametrize(""""tracker_type"""", ALL_TRACKERS)\ndef test_tracker_output_size(tracker_type):\n    tracker_conf = get_tracker_config(tracker","import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, get_tracker_config, create_tracker,\n)\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tracker()\n\n\n@pytest.mark.parametrize(""""tracker_type"""", AL","@@ -13,10 +13,11 @@ from boxmot import (\n MOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\n MOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\n ALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\n+PER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n \n \n @pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\n-def test_tracker_instantiation(Tracker):\n+def test_motion_n_appearance_trackers_instantiation(Tracker):\n     Tracker(\n         model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n         device='cpu',\n@@ -25,7 +26,7 @@ def test_tracker_instantiation(Tracker):\n \n \n @pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\n-def test_tracker_instantiation(Tracker):\n+def test_motion_only_trackers_instantiation(Tracker):\n     Tracker()\n \n \n@@ -47,3 +48,26 @@ def test_tracker_output_size(tracker_type):\n \n     output = tracker.update(det, rgb)\n     assert output.shape == (2, 8)  # two inputs should give two outputs\n+\n+\n+@pytest.mark.parametrize(""""tracker_type"""", PER_CLASS_TRACKERS)\n+def test_per_class_tracker_output_size(tracker_type):\n+    tracker_type = 'ocsort'\n+    tracker_conf = get_tracker_config(tracker_type)\n+    tracker = create_tracker(\n+        tracker_type=tracker_type,\n+        tracker_config=tracker_conf,\n+        reid_weights=WEIGHTS / 'mobilenetv2_x1_4_dukemtmcreid.pt',\n+        device='cpu',\n+        half=False,\n+        per_class=True\n+    )\n+\n+    rgb = np.random.randint(255, size=(640, 640, 3), dtype=np.uint8)\n+    det = np.array([[144, 212, 578, 480, 0.82, 0],\n+                    [425, 281, 576, 472, 0.56, 65]])\n+\n+    output = tracker.update(det, rgb)\n+    output = tracker.update(det, rgb)\n+    print(output)\n+    assert output.shape == (2, 8)  # two inputs should give two outputs\n",add,Added string type to outerHTML .
26bdcf8e7476281adc541665cee31ccb49f60df7,fix trajectory plotting for botsort,boxmot/trackers/basetracker.py,"import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\n\n\nclass BaseTracker(object):\n    def __init__(self, det_thresh=0.3, max_age=30, min_hits=3, iou_threshold=0.3):\n        self.det_thresh = det_thresh\n        self.max_age = max_age\n        self.min_hits = min_hits\n        self.iou_threshold = iou_threshold\n\n        self.frame_count = 0\n        self.active_tracks = []  # This might be handled differently in derived classes\n\n    def update(self, dets, img, embs=None):\n        raise NotImplementedError(""""The update method needs to be implemented by the subclass."""")\n\n    def id_to_color(self, id, saturation=0.75, value=0.95):\n        # Hash the ID to get a consistent unique value\n        hash_object = hashlib.sha256(str(id).encode())\n        hash_digest = hash_object.hexdigest()\n        \n        # Convert the first few characters of the hash to an integer\n        # and map it to a value between 0 and 1 for the hue\n        hue = int(hash_digest[:8], ","import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\n\n\nclass BaseTracker(object):\n    def __init__(self, det_thresh=0.3, max_age=30, min_hits=3, iou_threshold=0.3):\n        self.det_thresh = det_thresh\n        self.max_age = max_age\n        self.min_hits = min_hits\n        self.iou_threshold = iou_threshold\n\n        self.frame_count = 0\n        self.active_tracks = []  # This might be handled differently in derived classes\n\n    def update(self, dets, img, embs=None):\n        raise NotImplementedError(""""The update method needs to be implemented by the subclass."""")\n\n    def id_to_color(self, id, saturation=0.75, value=0.95):\n        # Hash the ID to get a consistent unique value\n        hash_object = hashlib.sha256(str(id).encode())\n        hash_digest = hash_object.hexdigest()\n        \n        # Convert the first few characters of the hash to an integer\n        # and map it to a value between 0 and 1 for the hue\n        hue = int(hash_digest[:8], ","@@ -44,9 +44,10 @@ class BaseTracker(object):\n \n         thickness = 2\n         fontscale = 0.5\n-\n         for a in self.active_tracks:\n+            \n             if a.history_observations:\n+                \n                 o = a.history_observations[-1]\n                 img = cv.rectangle(\n                     img,\n@@ -57,22 +58,23 @@ class BaseTracker(object):\n                 )\n                 img = cv.putText(\n                     img,\n-                    f'id: {a.id}, conf: {a.conf:.2f}, c: {a.cls}',\n+                    f'id: {int(a.id)}, conf: {a.conf:.2f}, c: {int(a.cls)}',\n                     (int(o[0]), int(o[1]) - 10),\n                     cv.FONT_HERSHEY_SIMPLEX,\n                     fontscale,\n                     self.id_to_color(a.id),\n                     thickness\n                 )\n-            for e, o in enumerate(a.history_observations):\n-                thickness = int(np.sqrt(float (e + 1)) * 1.2)\n-                cv.circle(\n-                    img,\n-                    (int((o[0] + o[2]) / 2),\n-                    int((o[1] + o[3]) / 2)), \n-                    2,\n-                    color=self.id_to_color(int(a.id)),\n-                    thickness=thickness\n-                )\n+            if len(a.history_observations) > 3:\n+                for e, o in enumerate(a.history_observations):\n+                    trajectory_thickness = int(np.sqrt(float (e + 1)) * 1.2)\n+                    cv.circle(\n+                        img,\n+                        (int((o[0] + o[2]) / 2),\n+                        int((o[1] + o[3]) / 2)), \n+                        2,\n+                        color=self.id_to_color(int(a.id)),\n+                        thickness=trajectory_thickness\n+                    )\n         return img\n \n",add,Added missing keywords . xml entry
26bdcf8e7476281adc541665cee31ccb49f60df7,fix trajectory plotting for botsort,boxmot/trackers/botsort/bot_sort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n      ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n       ","@@ -21,14 +21,15 @@ class STrack(BaseTrack):\n     def __init__(self, det, feat=None, feat_history=50):\n         # wait activate\n         self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n-        self.score = det[4]\n+        self.conf = det[4]\n         self.cls = det[5]\n         self.det_ind = det[6]\n         self.kalman_filter = None\n         self.mean, self.covariance = None, None\n         self.is_activated = False\n         self.cls_hist = []  # (cls id, freq)\n-        self.update_cls(self.cls, self.score)\n+        self.update_cls(self.cls, self.conf)\n+        self.history_observations = deque([], maxlen=50)\n \n         self.tracklet_len = 0\n \n@@ -49,23 +50,23 @@ class STrack(BaseTrack):\n         self.features.append(feat)\n         self.smooth_feat /= np.linalg.norm(self.smooth_feat)\n \n-    def update_cls(self, cls, score):\n+    def update_cls(self, cls, conf):\n         if len(self.cls_hist) > 0:\n             max_freq = 0\n             found = False\n             for c in self.cls_hist:\n                 if cls == c[0]:\n-                    c[1] += score\n+                    c[1] += conf\n                     found = True\n \n                 if c[1] > max_freq:\n                     max_freq = c[1]\n                     self.cls = c[0]\n             if not found:\n-                self.cls_hist.append([cls, score])\n+                self.cls_hist.append([cls, conf])\n                 self.cls = cls\n         else:\n-            self.cls_hist.append([cls, score])\n+            self.cls_hist.append([cls, conf])\n             self.cls = cls\n \n     def predict(self):\n@@ -138,11 +139,11 @@ class STrack(BaseTrack):\n         self.frame_id = frame_id\n         if new_id:\n             self.id = self.next_id()\n-        self.score = new_track.score\n+        self.conf = new_track.conf\n         self.cls = new_track.cls\n         self.det_ind = new_track.det_ind\n \n-        self.update_cls(new_track.cls, new_track.score)\",add,Added the UNSTARTED state to the YouTube PlayerState enum
7fcf7fede28ab9c0b57b4613cac0fc805b437af7,fix bytetrack plotting,boxmot/trackers/basetracker.py,"import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\n\n\nclass BaseTracker(object):\n    def __init__(self, det_thresh=0.3, max_age=30, min_hits=3, iou_threshold=0.3):\n        self.det_thresh = det_thresh\n        self.max_age = max_age\n        self.min_hits = min_hits\n        self.iou_threshold = iou_threshold\n\n        self.frame_count = 0\n        self.active_tracks = []  # This might be handled differently in derived classes\n\n    def update(self, dets, img, embs=None):\n        raise NotImplementedError(""""The update method needs to be implemented by the subclass."""")\n\n    def id_to_color(self, id, saturation=0.75, value=0.95):\n        # Hash the ID to get a consistent unique value\n        hash_object = hashlib.sha256(str(id).encode())\n        hash_digest = hash_object.hexdigest()\n        \n        # Convert the first few characters of the hash to an integer\n        # and map it to a value between 0 and 1 for the hue\n        hue = int(hash_digest[:8], ","import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\n\n\nclass BaseTracker(object):\n    def __init__(self, det_thresh=0.3, max_age=30, min_hits=3, iou_threshold=0.3):\n        self.det_thresh = det_thresh\n        self.max_age = max_age\n        self.min_hits = min_hits\n        self.iou_threshold = iou_threshold\n\n        self.frame_count = 0\n        self.active_tracks = []  # This might be handled differently in derived classes\n\n    def update(self, dets, img, embs=None):\n        raise NotImplementedError(""""The update method needs to be implemented by the subclass."""")\n\n    def id_to_color(self, id, saturation=0.75, value=0.95):\n        # Hash the ID to get a consistent unique value\n        hash_object = hashlib.sha256(str(id).encode())\n        hash_digest = hash_object.hexdigest()\n        \n        # Convert the first few characters of the hash to an integer\n        # and map it to a value between 0 and 1 for the hue\n        hue = int(hash_digest[:8], ","@@ -44,6 +44,7 @@ class BaseTracker(object):\n \n         thickness = 2\n         fontscale = 0.5\n+\n         for a in self.active_tracks:\n             \n             if a.history_observations:\n",add,Add note about data volume to enable_metrics_collection
7fcf7fede28ab9c0b57b4613cac0fc805b437af7,fix bytetrack plotting,boxmot/trackers/botsort/bot_sort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n       ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, feat=None, feat_history=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        s","@@ -1,8 +1,7 @@\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n-from collections import deque\n-\n import numpy as np\n+from collections import deque\n \n from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n from boxmot.motion.cmc.sof import SOF\n",add,Added hypest who apparently actually wrote the C # port
7fcf7fede28ab9c0b57b4613cac0fc805b437af7,fix bytetrack plotting,boxmot/trackers/bytetrack/basetrack.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import OrderedDict\n\nimport numpy as np\n\n\nclass TrackState(object):\n    New = 0\n    Tracked = 1\n    Lost = 2\n    Removed = 3\n\n\nclass BaseTrack(object):\n    _count = 0\n\n    track_id = 0\n    is_activated = False\n    state = TrackState.New\n\n    history = OrderedDict()\n    features = []\n    curr_feature = None\n    score = 0\n    start_frame = 0\n    frame_id = 0\n    time_since_update = 0\n\n    # multi-camera\n    location = (np.inf, np.inf)\n\n    @property\n    def end_frame(self):\n        return self.frame_id\n\n    @staticmethod\n    def next_id():\n        BaseTrack._count += 1\n        return BaseTrack._count\n\n    def activate(self, *args):\n        raise NotImplementedError\n\n    def predict(self):\n        raise NotImplementedError\n\n    def update(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def mark_lost(self):\n        self.state = TrackState.Lost\n\n    def ma","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import OrderedDict\n\nimport numpy as np\n\n\nclass TrackState(object):\n    New = 0\n    Tracked = 1\n    Lost = 2\n    Removed = 3\n\n\nclass BaseTrack(object):\n    _count = 0\n\n    track_id = 0\n    is_activated = False\n    state = TrackState.New\n\n    history = OrderedDict()\n    features = []\n    curr_feature = None\n    conf = 0\n    start_frame = 0\n    frame_id = 0\n    time_since_update = 0\n\n    # multi-camera\n    location = (np.inf, np.inf)\n\n    @property\n    def end_frame(self):\n        return self.frame_id\n\n    @staticmethod\n    def next_id():\n        BaseTrack._count += 1\n        return BaseTrack._count\n\n    def activate(self, *args):\n        raise NotImplementedError\n\n    def predict(self):\n        raise NotImplementedError\n\n    def update(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def mark_lost(self):\n        self.state = TrackState.Lost\n\n    def mar","@@ -22,7 +22,7 @@ class BaseTrack(object):\n     history = OrderedDict()\n     features = []\n     curr_feature = None\n-    score = 0\n+    conf = 0\n     start_frame = 0\n     frame_id = 0\n     time_since_update = 0\n",add,Added STORM - 1270 to Changelog
7fcf7fede28ab9c0b57b4613cac0fc805b437af7,fix bytetrack plotting,boxmot/trackers/bytetrack/byte_tracker.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.bytetrack_kf import KalmanFilter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n        self.xyah = tlwh2xyah(self.tlwh)\n        self.score = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n       ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.motion.kalman_filters.bytetrack_kf import KalmanFilter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n        self.xyah = tlwh2xyah(self.tlwh)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self","@@ -1,6 +1,7 @@\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n import numpy as np\n+from collections import deque\n \n from boxmot.motion.kalman_filters.bytetrack_kf import KalmanFilter\n from boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\n@@ -18,13 +19,14 @@ class STrack(BaseTrack):\n         self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n         self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n         self.xyah = tlwh2xyah(self.tlwh)\n-        self.score = det[4]\n+        self.conf = det[4]\n         self.cls = det[5]\n         self.det_ind = det[6]\n         self.kalman_filter = None\n         self.mean, self.covariance = None, None\n         self.is_activated = False\n         self.tracklet_len = 0\n+        self.history_observations = deque([], maxlen=50)\n \n     def predict(self):\n         mean_state = self.mean.copy()\n@@ -52,7 +54,7 @@ class STrack(BaseTrack):\n     def activate(self, kalman_filter, frame_id):\n         """"""""""""Start a new tracklet""""""""""""\n         self.kalman_filter = kalman_filter\n-        self.track_id = self.next_id()\n+        self.id = self.next_id()\n         self.mean, self.covariance = self.kalman_filter.initiate(self.xyah)\n \n         self.tracklet_len = 0\n@@ -72,8 +74,8 @@ class STrack(BaseTrack):\n         self.is_activated = True\n         self.frame_id = frame_id\n         if new_id:\n-            self.track_id = self.next_id()\n-        self.score = new_track.score\n+            self.id = self.next_id()\n+        self.conf = new_track.conf\n         self.cls = new_track.cls\n         self.det_ind = new_track.det_ind\n \n@@ -87,7 +89,7 @@ class STrack(BaseTrack):\n         """"""""""""\n         self.frame_id = frame_id\n         self.tracklet_len += 1\n-        # self.cls = cls\n+        self.history_observations.append(self.xyxy)\n \n         self.mean, self.covariance = self.kalman_filter.update(\n             self.mean, self.covariance, new_track.xya",add,Added the user guide
25448604ac1c31762f09b877e9e8f17bf7c1401f,fix bytetrack plotting,boxmot/utils/matching.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport lap\nimport numpy as np\nimport scipy\nimport torch\nfrom scipy.spatial.distance import cdist\nfrom boxmot.utils.iou import iou_batch\n\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919,\n}\n\n\ndef merge_matches(m1, m2, shape):\n    O, P, Q = shape\n    m1 = np.asarray(m1)\n    m2 = np.asarray(m2)\n\n    M1 = scipy.sparse.coo_matrix((np.ones(len(m1)), (m1[:, 0], m1[:, 1])), shape=(O, P))\n    M2 = scipy.sparse.coo_matrix((np.ones(len(m2)), (m2[:, 0], m2[:, 1])), shape=(P, Q))\n\n    mask = M1 * M2\n    match = mask.nonzero()\n    match = list(zip(match[0], match[1]))\n    unmatched_O = tuple(set(r","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport lap\nimport numpy as np\nimport scipy\nimport torch\nfrom scipy.spatial.distance import cdist\nfrom boxmot.utils.iou import iou_batch\n\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919,\n}\n\n\ndef merge_matches(m1, m2, shape):\n    O, P, Q = shape\n    m1 = np.asarray(m1)\n    m2 = np.asarray(m2)\n\n    M1 = scipy.sparse.coo_matrix((np.ones(len(m1)), (m1[:, 0], m1[:, 1])), shape=(O, P))\n    M2 = scipy.sparse.coo_matrix((np.ones(len(m2)), (m2[:, 0], m2[:, 1])), shape=(P, Q))\n\n    mask = M1 * M2\n    match = mask.nonzero()\n    match = list(zip(match[0], match[1]))\n    unmatched_O = tuple(set(r","@@ -203,9 +203,9 @@ def fuse_iou(cost_matrix, tracks, detections):\n     iou_dist = iou_distance(tracks, detections)\n     iou_sim = 1 - iou_dist\n     fuse_sim = reid_sim * (1 + iou_sim) / 2\n-    det_scores = np.array([det.score for det in detections])\n-    det_scores = np.expand_dims(det_scores, axis=0).repeat(cost_matrix.shape[0], axis=0)\n-    # fuse_sim = fuse_sim * (1 + det_scores) / 2\n+    det_confs = np.array([det.conf for det in detections])\n+    det_confs = np.expand_dims(det_confs, axis=0).repeat(cost_matrix.shape[0], axis=0)\n+    # fuse_sim = fuse_sim * (1 + det_confs) / 2\n     fuse_cost = 1 - fuse_sim\n     return fuse_cost\n \n@@ -214,9 +214,9 @@ def fuse_score(cost_matrix, detections):\n     if cost_matrix.size == 0:\n         return cost_matrix\n     iou_sim = 1 - cost_matrix\n-    det_scores = np.array([det.score for det in detections])\n-    det_scores = np.expand_dims(det_scores, axis=0).repeat(cost_matrix.shape[0], axis=0)\n-    fuse_sim = iou_sim * det_scores\n+    det_confs = np.array([det.conf for det in detections])\n+    det_confs = np.expand_dims(det_confs, axis=0).repeat(cost_matrix.shape[0], axis=0)\n+    fuse_sim = iou_sim * det_confs\n     fuse_cost = 1 - fuse_sim\n     return fuse_cost\n \n",add,Add note about data volume to enable_metrics_collection
deb8e163cbc6b70a75f4dc25c998d5717af172b8,fix per class tracking,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""DEBUG"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def w","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""DEBUG"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def w","@@ -49,23 +49,23 @@ class PerClassDecorator:\n                 per_class_tracks = []\n \n                 for cls_id in range(self.nr_classes):\n-                    if cls_id in detections_by_class:\n-                        class_dets = detections_by_class.get(int(cls_id), np.empty((0, 6)))\n-                        logger.debug(f""""Processing class {int(cls_id)}: {class_dets.shape}"""")\n+                    class_dets = detections_by_class.get(int(cls_id), np.empty((0, 6)))\n+                    #logger.debug(f""""Processing class {int(cls_id)}: {class_dets.shape}"""")\n \n-                        instance.active_tracks = self.per_class_active_tracks[cls_id]\n-                        \n-                        # Update detections using the decorated method\n-                        tracks = self.update(instance, class_dets, im)\n-\n-                        # save active tracks\n-                        self.per_class_active_tracks[cls_id] = instance.active_tracks\n+                    instance.active_tracks = self.per_class_active_tracks[cls_id]\n+                    \n+                    # Update detections using the decorated method\n+                    tracks = self.update(instance, class_dets, im)\n \n-                        instance.per_class_active_tracks = self.per_class_active_tracks\n+                    # save active tracks\n+                    self.per_class_active_tracks[cls_id] = instance.active_tracks\n \n-                        if tracks.size > 0:\n-                            per_class_tracks.append(tracks)\n+                    if tracks.size > 0:\n+                        per_class_tracks.append(tracks)\n                 \n+                # when all active tracks lists have been updated\n+                instance.per_class_active_tracks = self.per_class_active_tracks\n+\n                 if per_class_tracks:\n                     # Convert the list of arrays to a single NumPy array\n                     per_class_tracks = np.vstack(per_class_tra",add,Add group has a name
28210f160f4c8c7f46ec322fa96697c1cb0a5ead,unpack fix,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def wr","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def wr","@@ -34,7 +34,9 @@ class PerClassDecorator:\n         # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n         def wrapper(*args, **kwargs):\n             # Unpack arguments for clarity\n-            dets, im = args\n+            args = list(args)\n+            dets = args[0]\n+            im = args[1]\n             \n             if instance.per_class is True:\n \n",add,Add note about data volume to enable_metrics_collection
2815caa24974a07a0818c1c9b6a3f6b358abba0c,import fix,boxmot/appearance/reid_auto_backend.py,"from boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.backends.onnx_backend import ONNXBackend\nfrom boxmot.appearance.backends.openvino_backend import OpenVinoBackend\nfrom boxmot.appearance.backends.pytorch_backend import PyTorchBackend\nfrom boxmot.appearance.backends.tensorrt_backend import TensorRTBackend\nfrom boxmot.appearance.backends.tflite_backend import TFLiteBackend\n\n\n\nclass ReidAutoBackend():\n    def __init__(self, weights=""""osnet_x0_25_msmt17.pt"""", device=torch.device(""""cpu""""), half=False):\n        super().__init__()\n        w = weights[0] if isinstance(weights, list) else weights\n        (\n            self.pt,\n            self.jit,\n            self.onnx,\n            self.xml,\n            self.engine,\n            self.tflite,\n        ) = self.model_type(w)  # get backend\n\n        self.weights = weights\n        self.device = device\n        self.half = half\n\n    def get_backend(self, weights, device, half):\n        # Logic to determine whic","import torch\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.backends.onnx_backend import ONNXBackend\nfrom boxmot.appearance.backends.openvino_backend import OpenVinoBackend\nfrom boxmot.appearance.backends.pytorch_backend import PyTorchBackend\nfrom boxmot.appearance.backends.tensorrt_backend import TensorRTBackend\nfrom boxmot.appearance.backends.tflite_backend import TFLiteBackend\n\n\n\nclass ReidAutoBackend():\n    def __init__(self, weights=""""osnet_x0_25_msmt17.pt"""", device=torch.device(""""cpu""""), half=False):\n        super().__init__()\n        w = weights[0] if isinstance(weights, list) else weights\n        (\n            self.pt,\n            self.jit,\n            self.onnx,\n            self.xml,\n            self.engine,\n            self.tflite,\n        ) = self.model_type(w)  # get backend\n\n        self.weights = weights\n        self.device = device\n        self.half = half\n\n    def get_backend(self, weights, device, half):\n        # Logic t","@@ -1,3 +1,5 @@\n+import torch\n+\n from boxmot.utils import logger as LOGGER\n from boxmot.appearance.backends.onnx_backend import ONNXBackend\n from boxmot.appearance.backends.openvino_backend import OpenVinoBackend\n",add,Add note about data volume to enable_metrics_collection
ba1bf450e8289f88e2910a7ac8d13e05d6c90100,fix import,boxmot/appearance/reid_auto_backend.py,"import torch\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.backends.onnx_backend import ONNXBackend\nfrom boxmot.appearance.backends.openvino_backend import OpenVinoBackend\nfrom boxmot.appearance.backends.pytorch_backend import PyTorchBackend\nfrom boxmot.appearance.backends.tensorrt_backend import TensorRTBackend\nfrom boxmot.appearance.backends.tflite_backend import TFLiteBackend\n\n\n\nclass ReidAutoBackend():\n    def __init__(self, weights=""""osnet_x0_25_msmt17.pt"""", device=torch.device(""""cpu""""), half=False):\n        super().__init__()\n        w = weights[0] if isinstance(weights, list) else weights\n        (\n            self.pt,\n            self.jit,\n            self.onnx,\n            self.xml,\n            self.engine,\n            self.tflite,\n        ) = self.model_type(w)  # get backend\n\n        self.weights = weights\n        self.device = device\n        self.half = half\n\n    def get_backend(self, weights, device, half):\n        # Logic t","import torch\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.backends.onnx_backend import ONNXBackend\nfrom boxmot.appearance.backends.openvino_backend import OpenVinoBackend\nfrom boxmot.appearance.backends.pytorch_backend import PyTorchBackend\nfrom boxmot.appearance.backends.tensorrt_backend import TensorRTBackend\nfrom boxmot.appearance.backends.tflite_backend import TFLiteBackend\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\n\n\n\nclass ReidAutoBackend():\n    def __init__(self, weights=""""osnet_x0_25_msmt17.pt"""", device=torch.device(""""cpu""""), half=False):\n        super().__init__()\n        w = weights[0] if isinstance(weights, list) else weights\n        (\n            self.pt,\n            self.jit,\n            self.onnx,\n            self.xml,\n            self.engine,\n            self.tflite,\n        ) = BaseModelBackend.model_type(w)  # get backend\n\n        self.weights = weights\n        self.device = device\n        self.h","@@ -6,6 +6,7 @@ from boxmot.appearance.backends.openvino_backend import OpenVinoBackend\n from boxmot.appearance.backends.pytorch_backend import PyTorchBackend\n from boxmot.appearance.backends.tensorrt_backend import TensorRTBackend\n from boxmot.appearance.backends.tflite_backend import TFLiteBackend\n+from boxmot.appearance.backends.base_backend import BaseModelBackend\n \n \n \n@@ -20,7 +21,7 @@ class ReidAutoBackend():\n             self.xml,\n             self.engine,\n             self.tflite,\n-        ) = self.model_type(w)  # get backend\n+        ) = BaseModelBackend.model_type(w)  # get backend\n \n         self.weights = weights\n         self.device = device\n",add,Add note about data volume to enable_metrics_collection
234e811fea57a84718323bac3c5ea13f8c891a66,fix reid autobackend,boxmot/appearance/backends/base_backend.py,"import cv2\nimport torch\nimport gdown\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom boxmot.appearance.backbones import build_model, get_nr_classes\nfrom boxmot.appearance.reid_model_factory import (\n    get_model_name,\n    get_model_url\n)\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n        \n        self.download_model(self.weights)\n        self.model_name = get_model_name(self.weights)\n\n        self.model = build_model(\n            self.model_name,\n            num_classes=get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n        )\n        self.load_model(self.weights)\n\n        \n    @abstractmethod\n    def load_model(self):\n        raise NotImplementedE","import cv2\nimport torch\nimport gdown\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom boxmot.appearance.backbones import build_model, get_nr_classes\nfrom boxmot.appearance.reid_model_factory import (\n    get_model_name,\n    get_model_url\n)\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n        \n        self.download_model(self.weights)\n        self.model_name = get_model_name(self.weights)\n\n        self.model = build_model(\n            self.model_name,\n            num_classes=get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n        )\n        self.load_model(self.weights)\n\n        \n    @abstractmethod\n    def load_model(self):\n        raise NotImplementedE","@@ -97,29 +97,6 @@ class BaseModelBackend:\n     def to_numpy(self, x):\n         return x.cpu().numpy() if isinstance(x, torch.Tensor) else x\n \n-    def check_suffix(self, file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n-        # Check file(s) for acceptable suffix\n-        if file and suffix:\n-            if isinstance(suffix, str):\n-                suffix = [suffix]\n-            for f in file if isinstance(file, (list, tuple)) else [file]:\n-                s = Path(f).suffix.lower()  # file suffix\n-                if len(s):\n-                    try:\n-                        assert s in suffix\n-                    except AssertionError as err:\n-                        LOGGER.error(f""""{err}{f} acceptable suffix is {suffix}"""")\n-\n-    @staticmethod\n-    def model_type(p=""""path/to/model.pt""""):\n-        # Return model type from model path, i.e. path='path/to/model.onnx' -> type=onnx\n-        from boxmot.appearance import export_formats\n-\n-        sf = list(export_formats().Suffix)  # export suffixes\n-        self.check_suffix(p, sf)  # checks\n-        types = [s in Path(p).name for s in sf]\n-        return types\n-\n     def inference_preprocess(self, x):\n         if self.half and x.dtype != torch.float16:\n             x = x.half()\n",add,Added Type name for DFI ( # 210 )
234e811fea57a84718323bac3c5ea13f8c891a66,fix reid autobackend,boxmot/appearance/reid_auto_backend.py,"import torch\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.backends.onnx_backend import ONNXBackend\nfrom boxmot.appearance.backends.openvino_backend import OpenVinoBackend\nfrom boxmot.appearance.backends.pytorch_backend import PyTorchBackend\nfrom boxmot.appearance.backends.tensorrt_backend import TensorRTBackend\nfrom boxmot.appearance.backends.tflite_backend import TFLiteBackend\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\n\n\n\nclass ReidAutoBackend():\n    def __init__(self, weights=""""osnet_x0_25_msmt17.pt"""", device=torch.device(""""cpu""""), half=False):\n        super().__init__()\n        w = weights[0] if isinstance(weights, list) else weights\n        (\n            self.pt,\n            self.jit,\n            self.onnx,\n            self.xml,\n            self.engine,\n            self.tflite,\n        ) = BaseModelBackend.model_type(w)  # get backend\n\n        self.weights = weights\n        self.device = device\n        self.h","import torch\nfrom pathlib import Path\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.backends.onnx_backend import ONNXBackend\nfrom boxmot.appearance.backends.openvino_backend import OpenVinoBackend\nfrom boxmot.appearance.backends.pytorch_backend import PyTorchBackend\nfrom boxmot.appearance.backends.tensorrt_backend import TensorRTBackend\nfrom boxmot.appearance.backends.tflite_backend import TFLiteBackend\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\n\n\n\nclass ReidAutoBackend():\n    def __init__(self, weights=""""osnet_x0_25_msmt17.pt"""", device=torch.device(""""cpu""""), half=False):\n        super().__init__()\n        w = weights[0] if isinstance(weights, list) else weights\n        (\n            self.pt,\n            self.jit,\n            self.onnx,\n            self.xml,\n            self.engine,\n            self.tflite,\n        ) = self.model_type(w)  # get backend\n\n        self.weights = weights\n        self.device = device\n","@@ -1,4 +1,5 @@\n import torch\n+from pathlib import Path\n \n from boxmot.utils import logger as LOGGER\n from boxmot.appearance.backends.onnx_backend import ONNXBackend\n@@ -21,30 +22,54 @@ class ReidAutoBackend():\n             self.xml,\n             self.engine,\n             self.tflite,\n-        ) = BaseModelBackend.model_type(w)  # get backend\n+        ) = self.model_type(w)  # get backend\n \n         self.weights = weights\n         self.device = device\n         self.half = half\n \n-    def get_backend(self, weights, device, half):\n+    def get_backend(self):\n         # Logic to determine which backend to use\n         if self.pt:  # Condition for PyTorch\n-            return PyTorchBackend(weights, device, half)\n+            return PyTorchBackend(self.weights, self.device, self.half)\n         elif self.jit:  # Conditions for other backends\n-            return TorchscriptBackend(weights, device, half)\n+            return TorchscriptBackend(self.weights, self.device, self.half)\n         elif self.onnx:\n-            return ONNXBackend(weights, device, half)\n+            return ONNXBackend(self.weights, self.device, self.half)\n         elif self.engine:\n-            TensorRTBackend(weights, device, half)\n+            TensorRTBackend(self.weights, self.device, self.half)\n         elif self.xml:  # OpenVINO\n-            OpenVinoBackend(weights, device, half)\n+            OpenVinoBackend(self.weights, self.device, self.half)\n         elif self.tflite:\n-            TFLiteBackend(weights, device, half)\n+            TFLiteBackend(self.weights, self.device, self.half)\n         else:\n             LOGGER.error(""""This model framework is not supported yet!"""")\n             exit()\n \n     def forward(self, im_batch):\n         im_batch = self.backend.preprocess_input(im_batch)\n-        return self.backend.get_features(im_batch)\n\ No newline at end of file\n+        return self.backend.get_features(im_batch)\n+\n+    def check_suffix(self, file=",add,Add note about data volume to enable_metrics_collection
234e811fea57a84718323bac3c5ea13f8c891a66,fix reid autobackend,boxmot/appearance/reid_multibackend.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import OrderedDict, namedtuple\nfrom os.path import exists as file_exists\nfrom pathlib import Path\n\nimport cv2\nimport gdown\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom boxmot.appearance.backbones import build_model, get_nr_classes\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  get_model_url,\n                                                  load_pretrained_weights,\n                                                  show_downloadable_models)\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\n\ntr = TestRequirements()\n\n\ndef check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (li",,"@@ -1,311 +0,0 @@\n-# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n-\n-from collections import OrderedDict, namedtuple\n-from os.path import exists as file_exists\n-from pathlib import Path\n-\n-import cv2\n-import gdown\n-import numpy as np\n-import torch\n-import torch.nn as nn\n-\n-from boxmot.appearance.backbones import build_model, get_nr_classes\n-from boxmot.appearance.reid_model_factory import (get_model_name,\n-                                                  get_model_url,\n-                                                  load_pretrained_weights,\n-                                                  show_downloadable_models)\n-from boxmot.utils import logger as LOGGER\n-from boxmot.utils.checks import TestRequirements\n-\n-tr = TestRequirements()\n-\n-\n-def check_suffix(file=""""osnet_x0_25_msmt17.pt"""", suffix=("""".pt"""",), msg=""""""""):\n-    # Check file(s) for acceptable suffix\n-    if file and suffix:\n-        if isinstance(suffix, str):\n-            suffix = [suffix]\n-        for f in file if isinstance(file, (list, tuple)) else [file]:\n-            s = Path(f).suffix.lower()  # file suffix\n-            if len(s):\n-                try:\n-                    assert s in suffix\n-                except AssertionError as err:\n-                    LOGGER.error(f""""{err}{f} acceptable suffix is {suffix}"""")\n-\n-\n-class ReIDDetectMultiBackend(nn.Module):\n-    # ReID models MultiBackend class for python inference on various backends\n-    def __init__(\n-        self, weights=""""osnet_x0_25_msmt17.pt"""", device=torch.device(""""cpu""""), fp16=False\n-    ):\n-        super().__init__()\n-\n-        w = weights[0] if isinstance(weights, list) else weights\n-        (\n-            self.pt,\n-            self.jit,\n-            self.onnx,\n-            self.xml,\n-            self.engine,\n-            self.tflite,\n-        ) = self.model_type(w)  # get backend\n-        self.fp16 = fp16\n-        self.fp16 &= self.pt or self.jit or self.engine  # FP16\n-",add,Add note about data volume to enable
95f015e231792d342ff06935e66cbe22acddbbdc,fix reid auto backend imports,boxmot/appearance/reid_export.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport os\nimport platform\nimport subprocess\nimport time\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model, get_nr_classes\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n\n\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1e6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport os\nimport platform\nimport subprocess\nimport time\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backbones import build_model, get_nr_classes\nfrom boxmot.appearance.reid_model_factory import (get_model_name,\n                                                  load_pretrained_weights)\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\n__tr = TestRequirements()\n\n\ndef file_size(path):\n    # Return file/dir size (MB)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / 1e6\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob(""""*","@@ -18,8 +18,7 @@ from boxmot.utils import WEIGHTS\n from boxmot.utils import logger as LOGGER\n from boxmot.utils.checks import TestRequirements\n from boxmot.utils.torch_utils import select_device\n-from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n-\n+from boxmot.appearance.reid_auto_backend import ReidAutoBackend\n \n __tr = TestRequirements()\n \n@@ -292,11 +291,10 @@ if __name__ == """"__main__"""":\n             args.device.type != """"cpu""""\n         ), """"--half only compatible with GPU export, i.e. use --device 0""""\n \n-    ReIDDetectMultiBackend(\n-        weights=args.weights,\n-        device='cpu',\n-        fp16=False\n+    rab = ReidAutoBackend(\n+        weights=model_weights, device=device, half=fp16\n     )\n+    self.model = rab.get_backend()\n \n     model = build_model(\n         get_model_name(args.weights),\n",add,Added STORM - 1270 to Changelog
95f015e231792d342ff06935e66cbe22acddbbdc,fix reid auto backend imports,tests/unit/test_cuda.py,"import cv2\nimport torch\nimport pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import ROOT\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n\nREID_MODELS = [\n    Path('mobilenetv2_x1_0_market1501.pt'),\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_device(reid_model):\n\n    r = ReIDDetectMultiBackend(\n        weights=reid_model,\n        device='cuda:0' if torch.cuda.is_available() else 'cpu',\n        fp16=False  # not compatible with OSNet\n    )\n\n    if torch.cuda.is_available():\n        assert next(r.model.parameters()).is_cuda\n    else:\n        assert next(r.model.parameters()).device.type == 'cpu'\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_half(reid_model):\n\n    half = True if torch.cuda.is_available() else False\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    r = ReIDDetectMultiBackend(\n        weights=reid_model,\n        ","import cv2\nimport torch\nimport pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import ROOT\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nREID_MODELS = [\n    Path('mobilenetv2_x1_0_market1501.pt'),\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_device(reid_model):\n\n    rab = ReidAutoBackend(\n        weights=model_weights, device=device, half=fp16\n    )\n    self.model = rab.get_backend()\n\n    if torch.cuda.is_available():\n        assert next(r.model.parameters()).is_cuda\n    else:\n        assert next(r.model.parameters()).device.type == 'cpu'\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_half(reid_model):\n\n    half = True if torch.cuda.is_available() else False\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    r = ReIDDetectMultiBackend(\n        weights=reid_model,\n        device=device,\n        fp16=half\n    )\n\n    if device is 'c","@@ -5,7 +5,7 @@ import numpy as np\n from pathlib import Path\n from boxmot.utils import ROOT\n \n-from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n+from boxmot.appearance.reid_auto_backend import ReidAutoBackend\n \n REID_MODELS = [\n     Path('mobilenetv2_x1_0_market1501.pt'),\n@@ -15,11 +15,10 @@ REID_MODELS = [\n @pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\n def test_reidbackend_device(reid_model):\n \n-    r = ReIDDetectMultiBackend(\n-        weights=reid_model,\n-        device='cuda:0' if torch.cuda.is_available() else 'cpu',\n-        fp16=False  # not compatible with OSNet\n+    rab = ReidAutoBackend(\n+        weights=model_weights, device=device, half=fp16\n     )\n+    self.model = rab.get_backend()\n \n     if torch.cuda.is_available():\n         assert next(r.model.parameters()).is_cuda\n",add,Added net . kano . joustsim . oscar . os
95f015e231792d342ff06935e66cbe22acddbbdc,fix reid auto backend imports,tests/unit/test_reidbackend.py,"import cv2\nimport pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import ROOT\n\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n\nREID_MODELS = [\n    Path('osnet_x0_25_msmt17.pt'),\n    Path('osnet_x1_0_dukemtmcreid.pt')\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_output(reid_model):\n\n    r = ReIDDetectMultiBackend(\n        weights=reid_model,\n        device='cpu',\n        fp16=False\n    )\n\n    img = cv2.imread(str(ROOT / 'assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000001.jpg'))\n    dets = np.array([[144, 212, 578, 480, 0.82, 0],\n                    [425, 281, 576, 472, 0.56, 65]])\n\n    embs = r.get_features(dets[:, 0:4], img)\n    assert embs.shape[0] == 2   # two crops should give two embeddings\n    assert embs.shape[1] == 512 # osnet embeddings are of size 512","import cv2\nimport pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import ROOT\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nREID_MODELS = [\n    Path('osnet_x0_25_msmt17.pt'),\n    Path('osnet_x1_0_dukemtmcreid.pt')\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_output(reid_model):\n\n    rab = ReidAutoBackend(\n        weights=model_weights, device=device, half=fp16\n    )\n    b = rab.get_backend()\n\n    img = cv2.imread(str(ROOT / 'assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000001.jpg'))\n    dets = np.array([[144, 212, 578, 480, 0.82, 0],\n                    [425, 281, 576, 472, 0.56, 65]])\n\n    embs = b.get_features(dets[:, 0:4], img)\n    assert embs.shape[0] == 2   # two crops should give two embeddings\n    assert embs.shape[1] == 512 # osnet embeddings are of size 512","@@ -4,7 +4,7 @@ import numpy as np\n from pathlib import Path\n from boxmot.utils import ROOT\n \n-from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n+from boxmot.appearance.reid_auto_backend import ReidAutoBackend\n \n REID_MODELS = [\n     Path('osnet_x0_25_msmt17.pt'),\n@@ -15,16 +15,15 @@ REID_MODELS = [\n @pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\n def test_reidbackend_output(reid_model):\n \n-    r = ReIDDetectMultiBackend(\n-        weights=reid_model,\n-        device='cpu',\n-        fp16=False\n+    rab = ReidAutoBackend(\n+        weights=model_weights, device=device, half=fp16\n     )\n+    b = rab.get_backend()\n \n     img = cv2.imread(str(ROOT / 'assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000001.jpg'))\n     dets = np.array([[144, 212, 578, 480, 0.82, 0],\n                     [425, 281, 576, 472, 0.56, 65]])\n \n-    embs = r.get_features(dets[:, 0:4], img)\n+    embs = b.get_features(dets[:, 0:4], img)\n     assert embs.shape[0] == 2   # two crops should give two embeddings\n     assert embs.shape[1] == 512 # osnet embeddings are of size 512\n\ No newline at end of file\n",add,Added STORM - 1274 to Changelog
95f015e231792d342ff06935e66cbe22acddbbdc,fix reid auto backend imports,tracking/generate_dets_n_embs.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\n\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom tracking.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\n\n@torch.no_grad()\ndef run(args):\n\n    WEIGHTS.mkdir(parents=True, exist_ok=True)\n\n    yolo = YOLO(\n        args.yolo_model if 'yolov8' in str(args.yolo_model) else 'yolov8n.pt',\n    )\n\n    results = yolo(\n        source=args.source,\n        conf=args.conf,\n        iou=args.iou,\n        agnostic_nms=args.agnostic_nms,\n        stream=True,\n        device=args.device,\n        verbose=False,\n   ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\n\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom tracking.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\n\n@torch.no_grad()\ndef run(args):\n\n    WEIGHTS.mkdir(parents=True, exist_ok=True)\n\n    yolo = YOLO(\n        args.yolo_model if 'yolov8' in str(args.yolo_model) else 'yolov8n.pt',\n    )\n\n    results = yolo(\n        source=args.source,\n        conf=args.conf,\n        iou=args.iou,\n        agnostic_nms=args.agnostic_nms,\n        stream=True,\n        device=args.device,\n        verbose=False,\n        ex","@@ -10,7 +10,7 @@ import torch\n from boxmot.utils import ROOT, WEIGHTS\n from boxmot.utils.checks import TestRequirements\n from tracking.detectors import get_yolo_inferer\n-from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n+from boxmot.appearance.reid_auto_backend import ReidAutoBackend\n \n __tr = TestRequirements()\n __tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n@@ -56,13 +56,11 @@ def run(args):\n \n     reids = []\n     for r in opt.reid_model:\n-        reids.append(\n-            ReIDDetectMultiBackend(\n-                weights=r,\n-                device='cpu',\n-                fp16=False\n-            )\n+        rab = ReidAutoBackend(\n+            weights=model_weights, device=device, half=fp16\n         )\n+        model = rab.get_backend()\n+        reids.append(model)\n         embs_path = yolo.predictor.save_dir / 'embs' / r.stem / (Path(args.source).parent.name + '.txt')\n         embs_path.parent.mkdir(parents=True, exist_ok=True)\n         embs_path.touch(exist_ok=True)\n",fix,Added Type name for DFI ( # 3 )
95f015e231792d342ff06935e66cbe22acddbbdc,fix reid auto backend imports,tracking/generate_mot_results.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom functools import partial\nimport json\nimport torch\n\nfrom tqdm import tqdm\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\n\nfrom ultralytics.utils.files import increment_path \nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\nfrom boxmot.utils.checks import TestRequirements\nfrom tracking.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\nfrom boxmot.utils import logger as LOGGER\n\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.utils import convert_to_mot_format, write_mot_results\n\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\n\ndef generate_mot_results(args):\n\n    tracker = crea","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom functools import partial\nimport json\nimport torch\n\nfrom tqdm import tqdm\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\n\nfrom ultralytics.utils.files import increment_path \nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils import logger as LOGGER\n\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.utils import convert_to_mot_format, write_mot_results\n\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\n\ndef generate_mot_results(args):\n\n    tracker = create_tracker(\n        args.tracking_method,\n        TRACKER_CONFIGS / (args.tracking_method + '.yaml'),\n        args.rei","@@ -15,8 +15,6 @@ from boxmot.tracker_zoo import create_tracker\n from ultralytics.utils.files import increment_path \n from boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\n from boxmot.utils.checks import TestRequirements\n-from tracking.detectors import get_yolo_inferer\n-from boxmot.appearance.reid_multibackend import ReIDDetectMultiBackend\n from boxmot.utils import logger as LOGGER\n \n from ultralytics.data.loaders import LoadImages\n",add,Add TraceV2 deletion to script
9e8999bdc0f1def421696fe891adb4b7088dd8b3,fix auto backend,tracking/generate_dets_n_embs.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\n\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom tracking.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\n\n@torch.no_grad()\ndef run(args):\n\n    WEIGHTS.mkdir(parents=True, exist_ok=True)\n\n    yolo = YOLO(\n        args.yolo_model if 'yolov8' in str(args.yolo_model) else 'yolov8n.pt',\n    )\n\n    results = yolo(\n        source=args.source,\n        conf=args.conf,\n        iou=args.iou,\n        agnostic_nms=args.agnostic_nms,\n        stream=True,\n        device=args.device,\n        verbose=False,\n        ex","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\n\nfrom boxmot.utils import ROOT, WEIGHTS\nfrom boxmot.utils.checks import TestRequirements\nfrom tracking.detectors import get_yolo_inferer\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\n\n@torch.no_grad()\ndef run(args):\n\n    WEIGHTS.mkdir(parents=True, exist_ok=True)\n\n    yolo = YOLO(\n        args.yolo_model if 'yolov8' in str(args.yolo_model) else 'yolov8n.pt',\n    )\n\n    results = yolo(\n        source=args.source,\n        conf=args.conf,\n        iou=args.iou,\n        agnostic_nms=args.agnostic_nms,\n        stream=True,\n        device=args.device,\n        verbose=False,\n        ex","@@ -57,7 +57,7 @@ def run(args):\n     reids = []\n     for r in opt.reid_model:\n         rab = ReidAutoBackend(\n-            weights=model_weights, device=device, half=fp16\n+            weights=args.reid_model, device=yolo.predictor.device, half=args.half\n         )\n         model = rab.get_backend()\n         reids.append(model)\n",fix,Add note about data volume to enable_metrics_collection
3b0f44969e6eca655aba01c18cb64313764255ec,fix import,boxmot/appearance/reid_auto_backend.py,"import torch\nfrom pathlib import Path\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.backends.onnx_backend import ONNXBackend\nfrom boxmot.appearance.backends.openvino_backend import OpenVinoBackend\nfrom boxmot.appearance.backends.pytorch_backend import PyTorchBackend\nfrom boxmot.appearance.backends.tensorrt_backend import TensorRTBackend\nfrom boxmot.appearance.backends.tflite_backend import TFLiteBackend\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\n\n\n\nclass ReidAutoBackend():\n    def __init__(self, weights=""""osnet_x0_25_msmt17.pt"""", device=torch.device(""""cpu""""), half=False):\n        super().__init__()\n        w = weights[0] if isinstance(weights, list) else weights\n        (\n            self.pt,\n            self.jit,\n            self.onnx,\n            self.xml,\n            self.engine,\n            self.tflite,\n        ) = self.model_type(w)  # get backend\n\n        self.weights = weights\n        self.device = device\n","import torch\nfrom pathlib import Path\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.backends.onnx_backend import ONNXBackend\nfrom boxmot.appearance.backends.openvino_backend import OpenVinoBackend\nfrom boxmot.appearance.backends.pytorch_backend import PyTorchBackend\nfrom boxmot.appearance.backends.tensorrt_backend import TensorRTBackend\nfrom boxmot.appearance.backends.tflite_backend import TFLiteBackend\nfrom boxmot.appearance.backends.torchscript_backend import TorchscriptBackend\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\n\n\n\nclass ReidAutoBackend():\n    def __init__(self, weights=""""osnet_x0_25_msmt17.pt"""", device=torch.device(""""cpu""""), half=False):\n        super().__init__()\n        w = weights[0] if isinstance(weights, list) else weights\n        (\n            self.pt,\n            self.jit,\n            self.onnx,\n            self.xml,\n            self.engine,\n            self.tflite,\n        ) = self.model_type(w)  ","@@ -7,6 +7,7 @@ from boxmot.appearance.backends.openvino_backend import OpenVinoBackend\n from boxmot.appearance.backends.pytorch_backend import PyTorchBackend\n from boxmot.appearance.backends.tensorrt_backend import TensorRTBackend\n from boxmot.appearance.backends.tflite_backend import TFLiteBackend\n+from boxmot.appearance.backends.torchscript_backend import TorchscriptBackend\n from boxmot.appearance.backends.base_backend import BaseModelBackend\n \n \n",fix,Add note about data volume to enable_metrics_collection
d8f33daac2b4901f5c2091335b77b1fbc31fbbbd,fix reid arguments,tests/unit/test_reidbackend.py,"import cv2\nimport pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import ROOT\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nREID_MODELS = [\n    Path('osnet_x0_25_msmt17.pt'),\n    Path('osnet_x1_0_dukemtmcreid.pt')\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_output(reid_model):\n\n    rab = ReidAutoBackend(\n        weights=model_weights, device=device, half=fp16\n    )\n    b = rab.get_backend()\n\n    img = cv2.imread(str(ROOT / 'assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000001.jpg'))\n    dets = np.array([[144, 212, 578, 480, 0.82, 0],\n                    [425, 281, 576, 472, 0.56, 65]])\n\n    embs = b.get_features(dets[:, 0:4], img)\n    assert embs.shape[0] == 2   # two crops should give two embeddings\n    assert embs.shape[1] == 512 # osnet embeddings are of size 512","import cv2\nimport pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import ROOT\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nREID_MODELS = [\n    Path('osnet_x0_25_msmt17.pt'),\n    Path('osnet_x1_0_dukemtmcreid.pt')\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_output(reid_model):\n\n    rab = ReidAutoBackend(\n        weights=reid_model, device='cpu', half=False\n    )\n    b = rab.get_backend()\n\n    img = cv2.imread(str(ROOT / 'assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000001.jpg'))\n    dets = np.array([[144, 212, 578, 480, 0.82, 0],\n                    [425, 281, 576, 472, 0.56, 65]])\n\n    embs = b.get_features(dets[:, 0:4], img)\n    assert embs.shape[0] == 2   # two crops should give two embeddings\n    assert embs.shape[1] == 512 # osnet embeddings are of size 512","@@ -16,7 +16,7 @@ REID_MODELS = [\n def test_reidbackend_output(reid_model):\n \n     rab = ReidAutoBackend(\n-        weights=model_weights, device=device, half=fp16\n+        weights=reid_model, device='cpu', half=False\n     )\n     b = rab.get_backend()\n \n",fix,Add note about data volume to enable_metrics_collection
ae7478657839a2aa16948c5b8a6516c43f4d549a,fix reid arguments,tests/unit/test_cuda.py,"import cv2\nimport torch\nimport pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import ROOT\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nREID_MODELS = [\n    Path('mobilenetv2_x1_0_market1501.pt'),\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_device(reid_model):\n\n    rab = ReidAutoBackend(\n        weights=model_weights, device=device, half=fp16\n    )\n    self.model = rab.get_backend()\n\n    if torch.cuda.is_available():\n        assert next(r.model.parameters()).is_cuda\n    else:\n        assert next(r.model.parameters()).device.type == 'cpu'\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_half(reid_model):\n\n    half = True if torch.cuda.is_available() else False\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    r = ReIDDetectMultiBackend(\n        weights=reid_model,\n        device=device,\n        fp16=half\n    )\n\n    if device is 'c","import cv2\nimport torch\nimport pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import ROOT\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nREID_MODELS = [\n    Path('mobilenetv2_x1_0_market1501.pt'),\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_device(reid_model):\n\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\n    rab = ReidAutoBackend(\n        weights=reid_model, device=device, half=False\n    )\n    self.model = rab.get_backend()\n\n    if torch.cuda.is_available():\n        assert next(r.model.parameters()).is_cuda\n    else:\n        assert next(r.model.parameters()).device.type == 'cpu'\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_half(reid_model):\n\n    half = True if torch.cuda.is_available() else False\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    r = ReIDDetectMultiBackend(\n        weights=reid_model,\n        ","@@ -15,8 +15,10 @@ REID_MODELS = [\n @pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\n def test_reidbackend_device(reid_model):\n \n+    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n+\n     rab = ReidAutoBackend(\n-        weights=model_weights, device=device, half=fp16\n+        weights=reid_model, device=device, half=False\n     )\n     self.model = rab.get_backend()\n \n",fix,Add Table mapping for Pixelmator format ( Mac )
eed87c46a965751098b304875f854e393c7951cf,fix reid arguments,tests/unit/test_cuda.py,"import cv2\nimport torch\nimport pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import ROOT\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nREID_MODELS = [\n    Path('mobilenetv2_x1_0_market1501.pt'),\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_device(reid_model):\n\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\n    rab = ReidAutoBackend(\n        weights=reid_model, device=device, half=False\n    )\n    self.model = rab.get_backend()\n\n    if torch.cuda.is_available():\n        assert next(r.model.parameters()).is_cuda\n    else:\n        assert next(r.model.parameters()).device.type == 'cpu'\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_half(reid_model):\n\n    half = True if torch.cuda.is_available() else False\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    r = ReIDDetectMultiBackend(\n        weights=reid_model,\n        ","import cv2\nimport torch\nimport pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import ROOT\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nREID_MODELS = [\n    Path('mobilenetv2_x1_0_market1501.pt'),\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_device(reid_model):\n\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\n    rab = ReidAutoBackend(\n        weights=reid_model, device=device, half=False\n    )\n    r = rab.get_backend()\n\n    if torch.cuda.is_available():\n        assert next(r.model.parameters()).is_cuda\n    else:\n        assert next(r.model.parameters()).device.type == 'cpu'\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_half(reid_model):\n\n    half = True if torch.cuda.is_available() else False\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    rab = ReidAutoBackend(\n        weights=reid_model, device=device, half=Fal","@@ -20,7 +20,7 @@ def test_reidbackend_device(reid_model):\n     rab = ReidAutoBackend(\n         weights=reid_model, device=device, half=False\n     )\n-    self.model = rab.get_backend()\n+    r = rab.get_backend()\n \n     if torch.cuda.is_available():\n         assert next(r.model.parameters()).is_cuda\n@@ -33,11 +33,10 @@ def test_reidbackend_half(reid_model):\n \n     half = True if torch.cuda.is_available() else False\n     device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n-    r = ReIDDetectMultiBackend(\n-        weights=reid_model,\n-        device=device,\n-        fp16=half\n+    rab = ReidAutoBackend(\n+        weights=reid_model, device=device, half=False\n     )\n+    r = rab.get_backend()\n \n     if device is 'cpu':\n         expected_dtype = torch.float32\n",add,Add Table mapping for Pixelmator format ( Mac )
426d68171a35db6a675526ddafc9514cc9f87490,fix warning,tests/unit/test_cuda.py,"import cv2\nimport torch\nimport pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import ROOT\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nREID_MODELS = [\n    Path('mobilenetv2_x1_0_market1501.pt'),\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_device(reid_model):\n\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\n    rab = ReidAutoBackend(\n        weights=reid_model, device=device, half=False\n    )\n    r = rab.get_backend()\n\n    if torch.cuda.is_available():\n        assert next(r.model.parameters()).is_cuda\n    else:\n        assert next(r.model.parameters()).device.type == 'cpu'\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_half(reid_model):\n\n    half = True if torch.cuda.is_available() else False\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    rab = ReidAutoBackend(\n        weights=reid_model, device=device, half=Fal","import cv2\nimport torch\nimport pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import ROOT\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nREID_MODELS = [\n    Path('mobilenetv2_x1_0_market1501.pt'),\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_device(reid_model):\n\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\n    rab = ReidAutoBackend(\n        weights=reid_model, device=device, half=False\n    )\n    r = rab.get_backend()\n\n    if torch.cuda.is_available():\n        assert next(r.model.parameters()).is_cuda\n    else:\n        assert next(r.model.parameters()).device.type == 'cpu'\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_half(reid_model):\n\n    half = True if torch.cuda.is_available() else False\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    rab = ReidAutoBackend(\n        weights=reid_model, device=device, half=Fal","@@ -38,7 +38,7 @@ def test_reidbackend_half(reid_model):\n     )\n     r = rab.get_backend()\n \n-    if device is 'cpu':\n+    if device == 'cpu':\n         expected_dtype = torch.float32\n     else:\n         expected_dtype = torch.float16\n",fix,Add note about data volume to enable_metrics_collection
441f0513d2e083ef99ffc2e6f685bc0b2324363d,fix warmup,boxmot/appearance/backends/base_backend.py,"import cv2\nimport torch\nimport gdown\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom boxmot.appearance.backbones import build_model, get_nr_classes\nfrom boxmot.appearance.reid_model_factory import (\n    get_model_name,\n    get_model_url\n)\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights[0] if isinstance(weights, list) else weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n\n        self.download_model(self.weights)\n        self.model_name = get_model_name(self.weights)\n\n        self.model = build_model(\n            self.model_name,\n            num_classes=get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n        )\n        self.load_model(self.weights)\n\n        \n    @abstractmethod\n    def load_model(","import cv2\nimport torch\nimport gdown\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom boxmot.appearance.backbones import build_model, get_nr_classes\nfrom boxmot.appearance.reid_model_factory import (\n    get_model_name,\n    get_model_url\n)\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights[0] if isinstance(weights, list) else weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n\n        self.download_model(self.weights)\n        self.model_name = get_model_name(self.weights)\n\n        self.model = build_model(\n            self.model_name,\n            num_classes=get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n        )\n        self.load_model(self.weights)\n\n        \n    @abstractmethod\n    def load_model(","@@ -91,7 +91,10 @@ class BaseModelBackend:\n         # warmup model by running inference once\n         if self.device.type != """"cpu"""":\n             im = np.random.randint(0, 255, *imgsz, dtype=np.uint8)\n-            im = self.preprocess(xyxys=np.array([[0, 0, 128, 256]]), img=im)\n+            im = self.get_crops(xyxys=np.array(\n+                [[0, 0, 64, 64], [0, 0, 128, 128]]),\n+                img=im\n+            )\n             self.forward(im)  # warmup\n \n     def to_numpy(self, x):\n",add,Add note about data volume to enable_metrics_collection
f2cc9a0b4d26426b0ce9898c924c470ddf216391,fix,boxmot/motion/kalman_filters/deepocsort_kf.py,"# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","@@ -436,7 +436,7 @@ class KalmanFilter(object):\n             new_history = deepcopy(self.history_obs)\n             self.__dict__ = self.attr_saved\n             # self.history_obs = new_history\n-            self.history_obs = self.history_obs[:-1]\n+            self.history_obs = self.history_obs.pop()\n             occur = [int(d is None) for d in new_history]\n             indices = np.where(np.array(occur) == 0)[0]\n             index1 = indices[-2]\n",add,Added STORM - 360 to CHANGELOG
f2cc9a0b4d26426b0ce9898c924c470ddf216391,fix,boxmot/motion/kalman_filters/hybridsort_kf.py,"# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","@@ -393,7 +393,7 @@ class KalmanFilter(object):\n             new_history = deepcopy(self.history_obs)\n             self.__dict__ = self.attr_saved\n             # self.history_obs = new_history \n-            self.history_obs = self.history_obs[:-1]\n+            self.history_obs = deque(list(self.history_obs)[:-1])\n             occur = [int(d is None) for d in new_history]\n             indices = np.where(np.array(occur)==0)[0]\n             index1 = indices[-2]\n",add,Added net . kano . joustsim . oscar . os
f2cc9a0b4d26426b0ce9898c924c470ddf216391,fix,boxmot/motion/kalman_filters/ocsort_kf.py,"# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","@@ -393,7 +393,7 @@ class KalmanFilter(object):\n             new_history = deepcopy(self.history_obs)\n             self.__dict__ = self.attr_saved\n             # self.history_obs = new_history \n-            self.history_obs = self.history_obs[:-1]\n+            self.history_obs = deque(list(self.history_obs)[:-1])\n             occur = [int(d is None) for d in new_history]\n             indices = np.where(np.array(occur)==0)[0]\n             index1 = indices[-2]\n",add,Added net . kano . joustsim . oscar . os
939dbc651987f85679b527c7cb432f1f3d1db4e1,fix,boxmot/motion/kalman_filters/deepocsort_kf.py,"# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","@@ -436,7 +436,7 @@ class KalmanFilter(object):\n             new_history = deepcopy(self.history_obs)\n             self.__dict__ = self.attr_saved\n             # self.history_obs = new_history\n-            self.history_obs = self.history_obs.pop()\n+            self.history_obs = deque(list(self.history_obs)[:-1])\n             occur = [int(d is None) for d in new_history]\n             indices = np.where(np.array(occur) == 0)[0]\n             index1 = indices[-2]\n",add,Added STORM - 360 to CHANGELOG
e1a1d98478219ea9c6369b8bfcd1f77f1125bbb2,fix deque indexing,boxmot/motion/kalman_filters/deepocsort_kf.py,"# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","@@ -433,7 +433,7 @@ class KalmanFilter(object):\n \n     def unfreeze(self):\n         if self.attr_saved is not None:\n-            new_history = deepcopy(self.history_obs)\n+            new_history = deepcopy(list(self.history_obs))\n             self.__dict__ = self.attr_saved\n             # self.history_obs = new_history\n             self.history_obs = deque(list(self.history_obs)[:-1])\n",add,Add note about data volume to enable_metrics_collection
e1a1d98478219ea9c6369b8bfcd1f77f1125bbb2,fix deque indexing,boxmot/motion/kalman_filters/hybridsort_kf.py,"# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","@@ -390,7 +390,7 @@ class KalmanFilter(object):\n \n     def unfreeze(self):\n         if self.attr_saved is not None:\n-            new_history = deepcopy(self.history_obs)\n+            new_history = deepcopy(list(self.history_obs))\n             self.__dict__ = self.attr_saved\n             # self.history_obs = new_history \n             self.history_obs = deque(list(self.history_obs)[:-1])\n",add,Add warning about data volume to enable_metrics_collection
e1a1d98478219ea9c6369b8bfcd1f77f1125bbb2,fix deque indexing,boxmot/motion/kalman_filters/ocsort_kf.py,"# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","@@ -390,7 +390,7 @@ class KalmanFilter(object):\n \n     def unfreeze(self):\n         if self.attr_saved is not None:\n-            new_history = deepcopy(self.history_obs)\n+            new_history = deepcopy(list(self.history_obs))\n             self.__dict__ = self.attr_saved\n             # self.history_obs = new_history \n             self.history_obs = deque(list(self.history_obs)[:-1])\n",add,Add warning about data volume to enable_metrics_collection
89a43c56a64f84f2074d61448b910e34d1382105,per class frame count fix,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def wr","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def wr","@@ -42,8 +42,14 @@ class PerClassDecorator:\n \n                 # Initialize an array to store the tracks for each class\n                 per_class_tracks = []\n+                \n+                frame_count = instance.frame_count\n \n-                for cls_id in range(self.nr_classes):\n+                for i, cls_id in enumerate(range(self.nr_classes)):\n+                    \n+                    # reset frame count\n+                    if i != 0:\n+                        instance.frame_count = frame_count\n                     if dets.size > 0:\n                         class_dets = dets[dets[:, 5] == cls_id]\n                     else:\n@@ -64,6 +70,8 @@ class PerClassDecorator:\n                 \n                 # when all active tracks lists have been updated\n                 instance.per_class_active_tracks = self.per_class_active_tracks\n+                \n+                instance.frame_count = instance.frame_count - 1\n \n                 tracks = np.vstack(per_class_tracks) if per_class_tracks else np.empty((0, 8))\n             else:\n",add,Added STORM - 2056 to Changelog
f99ddea27cf57d9edaa9edffd862dfdff3682e7f,fix device arg handling,boxmot/appearance/reid_auto_backend.py,"import torch\nfrom pathlib import Path\nfrom typing import Union, Tuple\n\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backends.onnx_backend import ONNXBackend\nfrom boxmot.appearance.backends.openvino_backend import OpenVinoBackend\nfrom boxmot.appearance.backends.pytorch_backend import PyTorchBackend\nfrom boxmot.appearance.backends.tensorrt_backend import TensorRTBackend\nfrom boxmot.appearance.backends.tflite_backend import TFLiteBackend\nfrom boxmot.appearance.backends.torchscript_backend import TorchscriptBackend\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\n\n\n\nclass ReidAutoBackend():\n    def __init__(\n        self,\n        weights: Path = WEIGHTS / """"osnet_x0_25_msmt17.pt"""",\n        device: torch.device = torch.device(""""cpu""""),\n        half: bool = False) -> None:\n        """"""""""""\n        Initializes the ReidAutoBackend instance with specified w","import torch\nfrom pathlib import Path\nfrom typing import Union, Tuple\n\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.appearance import export_formats\nfrom boxmot.appearance.backends.onnx_backend import ONNXBackend\nfrom boxmot.appearance.backends.openvino_backend import OpenVinoBackend\nfrom boxmot.appearance.backends.pytorch_backend import PyTorchBackend\nfrom boxmot.appearance.backends.tensorrt_backend import TensorRTBackend\nfrom boxmot.appearance.backends.tflite_backend import TFLiteBackend\nfrom boxmot.appearance.backends.torchscript_backend import TorchscriptBackend\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\n\n\n\nclass ReidAutoBackend():\n    def __init__(\n        self,\n        weights: Path = WEIGHTS / """"osnet_x0_25_msmt17.pt"""",\n        device: torch.device = torch.device(""""cpu""""),\n        half: bool = False) -> None:\n        """"""""""""\n        Initi","@@ -4,6 +4,7 @@ from typing import Union, Tuple\n \n from boxmot.utils import WEIGHTS\n from boxmot.utils import logger as LOGGER\n+from boxmot.utils.torch_utils import select_device\n from boxmot.appearance import export_formats\n from boxmot.appearance.backends.onnx_backend import ONNXBackend\n from boxmot.appearance.backends.openvino_backend import OpenVinoBackend\n@@ -41,7 +42,7 @@ class ReidAutoBackend():\n         ) = self.model_type(w)  # get backend\n \n         self.weights = weights\n-        self.device = device\n+        self.device = select_device(device)\n         self.half = half\n \n \n",add,Added TypeSpec . h for layers .
f99ddea27cf57d9edaa9edffd862dfdff3682e7f,fix device arg handling,boxmot/utils/torch_utils.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport platform\n\nimport torch\n\nfrom .. import __version__\nfrom . import logger as LOGGER\n\n\ndef select_device(device="""""""", batch=0, newline=False, verbose=True):\n    """"""""""""Selects PyTorch Device. Options are device = None or 'cpu' or 0 or '0' or '0,1,2,3'.""""""""""""\n    s = f""""Yolo Tracking v{__version__} ð Python-{platform.python_version()} torch-{torch.__version__} """"\n    device = str(device).lower()\n    for remove in """"cuda:"""", """"none"""", """"("""", """")"""", """"["""", """"]"""", """"'"""", """" """":\n        device = device.replace(\n            remove, """"""""\n        )  # to string, 'cuda:0' -> '0' and '(0, 1)' -> '0,1'\n    cpu = device == """"cpu""""\n    mps = device == """"mps""""  # Apple Metal Performance Shaders (MPS)\n    if cpu or mps:\n        os.environ[\n            """"CUDA_VISIBLE_DEVICES""""\n        ] = """"-1""""  # force torch.cuda.is_available() = False\n    elif device:  # non-cpu device requested\n        visible = os.environ.","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport platform\nimport toml\nimport torch\n\nfrom .. import __version__\nfrom . import logger as LOGGER\nfrom boxmot.utils import ROOT\n\n\ndef select_device(device="""""""", batch=0, newline=False, verbose=True):\n    """"""""""""Selects PyTorch Device. Options are device = None or 'cpu' or 0 or '0' or '0,1,2,3'.""""""""""""\n    s = f""""Yolo Tracking v{get_version_from_pyproject()} ð Python-{platform.python_version()} torch-{torch.__version__} """"\n    device = str(device).lower()\n    for remove in """"cuda:"""", """"none"""", """"("""", """")"""", """"["""", """"]"""", """"'"""", """" """":\n        device = device.replace(\n            remove, """"""""\n        )  # to string, 'cuda:0' -> '0' and '(0, 1)' -> '0,1'\n    cpu = device == """"cpu""""\n    mps = device == """"mps""""  # Apple Metal Performance Shaders (MPS)\n    if cpu or mps:\n        os.environ[\n            """"CUDA_VISIBLE_DEVICES""""\n        ] = """"-1""""  # force torch.cuda.is_available() = False\n    elif device:","@@ -2,16 +2,17 @@\n \n import os\n import platform\n-\n+import toml\n import torch\n \n from .. import __version__\n from . import logger as LOGGER\n+from boxmot.utils import ROOT\n \n \n def select_device(device="""""""", batch=0, newline=False, verbose=True):\n     """"""""""""Selects PyTorch Device. Options are device = None or 'cpu' or 0 or '0' or '0,1,2,3'.""""""""""""\n-    s = f""""Yolo Tracking v{__version__} ð Python-{platform.python_version()} torch-{torch.__version__} """"\n+    s = f""""Yolo Tracking v{get_version_from_pyproject()} ð Python-{platform.python_version()} torch-{torch.__version__} """"\n     device = str(device).lower()\n     for remove in """"cuda:"""", """"none"""", """"("""", """")"""", """"["""", """"]"""", """"'"""", """" """":\n         device = device.replace(\n@@ -74,3 +75,12 @@ def select_device(device="""""""", batch=0, newline=False, verbose=True):\n \n     LOGGER.info(s)\n     return torch.device(arg)\n+\n+def get_version_from_pyproject():\n+    # Load the pyproject.toml file\n+    with open(ROOT / """"pyproject.toml"""", """"r"""") as file:\n+        pyproject_data = toml.load(file)\n+\n+    # Extract the version\n+    version = pyproject_data['tool']['poetry']['version']\n+    return version\n\ No newline at end of file\n",add,Add wobly to
f99ddea27cf57d9edaa9edffd862dfdff3682e7f,fix device arg handling,poetry.lock,"# This file is automatically @generated by Poetry 1.8.2 and should not be changed by hand.\n\n[[package]]\nname = """"alembic""""\nversion = """"1.13.1""""\ndescription = """"A database migration tool for SQLAlchemy.""""\noptional = false\npython-versions = """">=3.8""""\nfiles = [\n    {file = """"alembic-1.13.1-py3-none-any.whl"""", hash = """"sha256:2edcc97bed0bd3272611ce3a98d98279e9c209e7186e43e75bbb1b2bdfdbcc43""""},\n    {file = """"alembic-1.13.1.tar.gz"""", hash = """"sha256:4932c8558bf68f2ee92b9bbcb8218671c627064d5b08939437af6d77dc05e595""""},\n]\n\n[package.dependencies]\nimportlib-metadata = {version = """"*"""", markers = """"python_version < \""""3.9\""""""""}\nimportlib-resources = {version = """"*"""", markers = """"python_version < \""""3.9\""""""""}\nMako = """"*""""\nSQLAlchemy = """">=1.3.0""""\ntyping-extensions = """">=4""""\n\n[package.extras]\ntz = [""""backports.zoneinfo""""]\n\n[[package]]\nname = """"beautifulsoup4""""\nversion = """"4.12.3""""\ndescription = """"Screen-scraping library""""\noptional = false\npython-versions = """">=3.6.0""""\nfi","# This file is automatically @generated by Poetry 1.8.2 and should not be changed by hand.\n\n[[package]]\nname = """"alembic""""\nversion = """"1.13.1""""\ndescription = """"A database migration tool for SQLAlchemy.""""\noptional = false\npython-versions = """">=3.8""""\nfiles = [\n    {file = """"alembic-1.13.1-py3-none-any.whl"""", hash = """"sha256:2edcc97bed0bd3272611ce3a98d98279e9c209e7186e43e75bbb1b2bdfdbcc43""""},\n    {file = """"alembic-1.13.1.tar.gz"""", hash = """"sha256:4932c8558bf68f2ee92b9bbcb8218671c627064d5b08939437af6d77dc05e595""""},\n]\n\n[package.dependencies]\nimportlib-metadata = {version = """"*"""", markers = """"python_version < \""""3.9\""""""""}\nimportlib-resources = {version = """"*"""", markers = """"python_version < \""""3.9\""""""""}\nMako = """"*""""\nSQLAlchemy = """">=1.3.0""""\ntyping-extensions = """">=4""""\n\n[package.extras]\ntz = [""""backports.zoneinfo""""]\n\n[[package]]\nname = """"beautifulsoup4""""\nversion = """"4.12.3""""\ndescription = """"Screen-scraping library""""\noptional = false\npython-versions = """">=3.6.0""""\nfi","@@ -1563,11 +1563,11 @@ files = [\n [package.dependencies]\n numpy = [\n     {version = """">=1.21.0"""", markers = """"python_version <= \""""3.9\"""" and platform_system == \""""Darwin\"""" and platform_machine == \""""arm64\"""" and python_version >= \""""3.8\""""""""},\n+    {version = """">=1.23.5"""", markers = """"python_version >= \""""3.11\"""" and python_version < \""""3.12\""""""""},\n     {version = """">=1.21.4"""", markers = """"python_version >= \""""3.10\"""" and platform_system == \""""Darwin\"""" and python_version < \""""3.11\""""""""},\n     {version = """">=1.21.2"""", markers = """"platform_system != \""""Darwin\"""" and python_version >= \""""3.10\"""" and python_version < \""""3.11\""""""""},\n     {version = """">=1.19.3"""", markers = """"platform_system == \""""Linux\"""" and platform_machine == \""""aarch64\"""" and python_version >= \""""3.8\"""" and python_version < \""""3.10\"""" or python_version > \""""3.9\"""" and python_version < \""""3.10\"""" or python_version >= \""""3.9\"""" and platform_system != \""""Darwin\"""" and python_version < \""""3.10\"""" or python_version >= \""""3.9\"""" and platform_machine != \""""arm64\"""" and python_version < \""""3.10\""""""""},\n     {version = """">=1.17.3"""", markers = """"(platform_system != \""""Darwin\"""" and platform_system != \""""Linux\"""") and python_version >= \""""3.8\"""" and python_version < \""""3.9\"""" or platform_system != \""""Darwin\"""" and python_version >= \""""3.8\"""" and python_version < \""""3.9\"""" and platform_machine != \""""aarch64\"""" or platform_machine != \""""arm64\"""" and python_version >= \""""3.8\"""" and python_version < \""""3.9\"""" and platform_system != \""""Linux\"""" or (platform_machine != \""""arm64\"""" and platform_machine != \""""aarch64\"""") and python_version >= \""""3.8\"""" and python_version < \""""3.9\""""""""},\n-    {version = """">=1.23.5"""", markers = """"python_version >= \""""3.11\"""" and python_version < \""""3.12\""""""""},\n ]\n \n [[package]]\n@@ -1719,8 +1719,8 @@ files = [\n [package.dependencies]\n numpy = [\n     {version = """">=1.20.3"""", markers = """"python_version < \""""3.10\""""""""},\n-    {version = """">=1.21.0"""", markers = """"python_vers",add,Add note about data volume to enable_metrics_collection
f99ddea27cf57d9edaa9edffd862dfdff3682e7f,fix device arg handling,pyproject.toml,"[tool.poetry]\nname = """"boxmot""""\nversion = """"10.0.65""""\ndescription = """"BoxMOT: pluggable SOTA tracking modules for segmentation, object detection and pose estimation models""""\nauthors = [""""Mikel BrostrÃ¶m""""]\nlicense = """"AGPL-3.0""""\nreadme = """"README.md""""\nclassifiers = [\n    'Development Status :: 4 - Beta',\n    'Intended Audience :: Developers',\n    'Intended Audience :: Education',\n    'Intended Audience :: Science/Research',\n    'License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)',\n    'Programming Language :: Python :: 3',\n    'Programming Language :: Python :: 3.8',\n    'Programming Language :: Python :: 3.9',\n    'Programming Language :: Python :: 3.10',\n    'Programming Language :: Python :: 3.11',\n    'Topic :: Software Development',\n    'Topic :: Scientific/Engineering',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'Topic :: Scientific/Engineering :: Image Recognition',\n    'Topic :: Scientific/Engineeri","[tool.poetry]\nname = """"boxmot""""\nversion = """"10.0.65""""\ndescription = """"BoxMOT: pluggable SOTA tracking modules for segmentation, object detection and pose estimation models""""\nauthors = [""""Mikel BrostrÃ¶m""""]\nlicense = """"AGPL-3.0""""\nreadme = """"README.md""""\nclassifiers = [\n    'Development Status :: 4 - Beta',\n    'Intended Audience :: Developers',\n    'Intended Audience :: Education',\n    'Intended Audience :: Science/Research',\n    'License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)',\n    'Programming Language :: Python :: 3',\n    'Programming Language :: Python :: 3.8',\n    'Programming Language :: Python :: 3.9',\n    'Programming Language :: Python :: 3.10',\n    'Programming Language :: Python :: 3.11',\n    'Topic :: Software Development',\n    'Topic :: Scientific/Engineering',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'Topic :: Scientific/Engineering :: Image Recognition',\n    'Topic :: Scientific/Engineeri","@@ -48,6 +48,7 @@ torchvision = [\n     {version = """"^0.17.1"""", source = """"torchcpu"""", markers = """"sys_platform == 'linux'""""}\n ]\n gitpython = """"^3.1.42""""\n+toml = """"^0.10.2""""\n \n \n [[tool.poetry.source]]\n",add,Parallelizing the build
c7b250465bd26c23bf75e43557173561d9bd854d,fix sed command,.github/workflows/publish.yml,name: Publish to PyPI\n\non:\n  push:\n    branches: [main]\n  workflow_dispatch:\n    inputs:\n      pypi:\n        description: 'Target repository (pypi or testpypi)'\n        required: true\n        default: 'pypi'\n\njobs:\n  pypi-upload:\n    name: Publish\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Set up Python environment\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip setuptools wheel poetry\n          poetry config virtualenvs.create false\n          poetry install\n\n      - name: Increase poetry path version and add\n        id: get_version\n        run: |\n          git config --local user.email yolov5.deepsort.pytorch@gmail.com\n          git config --local user.name mikel-brostrom\n          commit_message=$(poetry version patch)\n          new_version=$(ec,name: Publish to PyPI\n\non:\n  push:\n    branches: [main]\n  workflow_dispatch:\n    inputs:\n      pypi:\n        description: 'Target repository (pypi or testpypi)'\n        required: true\n        default: 'pypi'\n\njobs:\n  pypi-upload:\n    name: Publish\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Set up Python environment\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip setuptools wheel poetry\n          poetry config virtualenvs.create false\n          poetry install\n\n      - name: Increase poetry path version and add\n        id: get_version\n        run: |\n          git config --local user.email yolov5.deepsort.pytorch@gmail.com\n          git config --local user.name mikel-brostrom\n          commit_message=$(poetry version patch)\n          new_version=$(ec,"@@ -44,7 +44,7 @@ jobs:\n \n       - name: Update __init__.py version and add\n         run: |\n-          sed -i """"s/__version__ = '.*'/__version__ = '${{ steps.get_version.outputs.new_version }}'/"""" boxmot/__init__.py\n+          sed -i '' """"s/__version__ = '.*'/__version__ = '${{ steps.get_version.outputs.new_version }}'/"""" boxmot/__init__.py\n           git add boxmot/__init__.py\n         if: ${{ success() }}\n \n",add,Add note about data volume to enable
c7b250465bd26c23bf75e43557173561d9bd854d,fix sed command,boxmot/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n__version__ = '10.0.66'\n\nfrom boxmot.postprocessing.gsi import gsi\nfrom boxmot.tracker_zoo import create_tracker, get_tracker_config\nfrom boxmot.trackers.botsort.bot_sort import BoTSORT\nfrom boxmot.trackers.bytetrack.byte_tracker import BYTETracker\nfrom boxmot.trackers.deepocsort.deep_ocsort import DeepOCSort as DeepOCSORT\nfrom boxmot.trackers.hybridsort.hybridsort import HybridSORT\nfrom boxmot.trackers.ocsort.ocsort import OCSort as OCSORT\nfrom boxmot.trackers.strongsort.strong_sort import StrongSORT\n\nTRACKERS = ['bytetrack', 'botsort', 'strongsort', 'ocsort', 'deepocsort', 'hybridsort']\n\n__all__ = (""""__version__"""",\n           """"StrongSORT"""", """"OCSORT"""", """"BYTETracker"""", """"BoTSORT"""", """"DeepOCSORT"""", """"HybridSORT"""",\n           """"create_tracker"""", """"get_tracker_config"""", """"gsi"""")\n","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n__version__ = '10.1.67'\n\nfrom boxmot.postprocessing.gsi import gsi\nfrom boxmot.tracker_zoo import create_tracker, get_tracker_config\nfrom boxmot.trackers.botsort.bot_sort import BoTSORT\nfrom boxmot.trackers.bytetrack.byte_tracker import BYTETracker\nfrom boxmot.trackers.deepocsort.deep_ocsort import DeepOCSort as DeepOCSORT\nfrom boxmot.trackers.hybridsort.hybridsort import HybridSORT\nfrom boxmot.trackers.ocsort.ocsort import OCSort as OCSORT\nfrom boxmot.trackers.strongsort.strong_sort import StrongSORT\n\nTRACKERS = ['bytetrack', 'botsort', 'strongsort', 'ocsort', 'deepocsort', 'hybridsort']\n\n__all__ = (""""__version__"""",\n           """"StrongSORT"""", """"OCSORT"""", """"BYTETracker"""", """"BoTSORT"""", """"DeepOCSORT"""", """"HybridSORT"""",\n           """"create_tracker"""", """"get_tracker_config"""", """"gsi"""")\n","@@ -1,6 +1,6 @@\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n-__version__ = '10.0.66'\n+__version__ = '10.1.67'\n \n from boxmot.postprocessing.gsi import gsi\n from boxmot.tracker_zoo import create_tracker, get_tracker_config\n",add,Added example for MAP type in documentation
77906e293a88860011d362277585a99845667628,fix frame count,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def wr","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def wr","@@ -43,6 +43,7 @@ class PerClassDecorator:\n                 # Initialize an array to store the tracks for each class\n                 per_class_tracks = []\n                 \n+                # same frame count for all classes\n                 frame_count = instance.frame_count\n \n                 for i, cls_id in enumerate(range(self.nr_classes)):\n@@ -58,6 +59,7 @@ class PerClassDecorator:\n                     \n                     # reset frame count for every class\n                     instance.frame_count = frame_count\n+                    logger.info(instance.frame_count)\n                     \n                     # Update detections using the decorated method\n                     tracks = self.update(instance, class_dets, im)\n@@ -67,11 +69,14 @@ class PerClassDecorator:\n \n                     if tracks.size > 0:\n                         per_class_tracks.append(tracks)\n+                        \n+                    print(cls_id, instance.frame_count)\n                 \n                 # when all active tracks lists have been updated\n                 instance.per_class_active_tracks = self.per_class_active_tracks\n                 \n-                instance.frame_count = instance.frame_count - 1\n+                # increase frame count by 1\n+                instance.frame_count = frame_count + 1\n \n                 tracks = np.vstack(per_class_tracks) if per_class_tracks else np.empty((0, 8))\n             else:\n",add,Add note about data volume to enable_metrics_collection
6bbb7ed8dbbe5da14f40add72f561705ae3f0eac,fix frame count,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def wr","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def wr","@@ -59,7 +59,6 @@ class PerClassDecorator:\n                     \n                     # reset frame count for every class\n                     instance.frame_count = frame_count\n-                    logger.info(instance.frame_count)\n                     \n                     # Update detections using the decorated method\n                     tracks = self.update(instance, class_dets, im)\n@@ -69,9 +68,7 @@ class PerClassDecorator:\n \n                     if tracks.size > 0:\n                         per_class_tracks.append(tracks)\n-                        \n-                    print(cls_id, instance.frame_count)\n-                \n+                                        \n                 # when all active tracks lists have been updated\n                 instance.per_class_active_tracks = self.per_class_active_tracks\n                 \n",add,Add note about data volume to enable_metrics_collection
c6bd482b01701c5d286a1ceb8eb18ca4a27c2bc8,"Update generate_mot_results.py

fix cuda device:
RuntimeError: Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.",tracking/generate_mot_results.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom functools import partial\nimport json\nimport torch\n\nfrom tqdm import tqdm\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\n\nfrom ultralytics.utils.files import increment_path \nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils import logger as LOGGER\n\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.utils import convert_to_mot_format, write_mot_results\n\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\n\ndef generate_mot_results(args):\n    tracker = create_tracker(\n        args.tracking_method,\n        TRACKER_CONFIGS / (args.tracking_method + '.yaml'),\n        args.reid_","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom functools import partial\nimport json\nimport torch\n\nfrom tqdm import tqdm\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\n\nfrom ultralytics.utils.files import increment_path \nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\nfrom boxmot.utils.checks import TestRequirements\nfrom boxmot.utils import logger as LOGGER\n\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.utils import convert_to_mot_format, write_mot_results\nfrom boxmot.utils.torch_utils import select_device\n\n__tr = TestRequirements()\n__tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\n\ndef generate_mot_results(args):\n    args.device = select_device(args.device)\n    tracker = create_tracker(\n        args.t","@@ -22,18 +22,19 @@ from ultralytics import YOLO\n from ultralytics.data.utils import VID_FORMATS\n \n from tracking.utils import convert_to_mot_format, write_mot_results\n-\n+from boxmot.utils.torch_utils import select_device\n \n __tr = TestRequirements()\n __tr.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n \n \n def generate_mot_results(args):\n+    args.device = select_device(args.device)\n     tracker = create_tracker(\n         args.tracking_method,\n         TRACKER_CONFIGS / (args.tracking_method + '.yaml'),\n         args.reid_model.with_suffix('.pt'),\n-        'cpu',\n+        args.device,\n         False,\n         False\n     )\n",add,added meteocons cheatsheet
9eb81edc34c68db73e0dfc2f5accba49d95121a8,fix kfs,boxmot/motion/kalman_filters/deepocsort_kf.py,"# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","@@ -282,13 +282,14 @@ class KalmanFilter(object):\n        https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n     """"""""""""\n \n-    def __init__(self, dim_x, dim_z, dim_u=0):\n+    def __init__(self, dim_x, dim_z, dim_u=0, max_obs=50):\n         if dim_x < 1:\n             raise ValueError(""""dim_x must be 1 or greater"""")\n         if dim_z < 1:\n             raise ValueError(""""dim_z must be 1 or greater"""")\n         if dim_u < 0:\n             raise ValueError(""""dim_u must be 0 or greater"""")\n+        self.max_obs = max_obs\n \n         self.dim_x = dim_x\n         self.dim_z = dim_z\n@@ -330,7 +331,7 @@ class KalmanFilter(object):\n         self._mahalanobis = None\n \n         # keep all observations\n-        self.history_obs = deque([], maxlen=50)\n+        self.history_obs = deque([], maxlen=self.max_obs)\n \n         self.inv = np.linalg.inv\n \n@@ -436,7 +437,7 @@ class KalmanFilter(object):\n             new_history = deepcopy(list(self.history_obs))\n             self.__dict__ = self.attr_saved\n             # self.history_obs = new_history\n-            self.history_obs = deque(list(self.history_obs)[:-1], maxlen=50)\n+            self.history_obs = deque(list(self.history_obs)[:-1], maxlen=self.max_obs)\n             occur = [int(d is None) for d in new_history]\n             indices = np.where(np.array(occur) == 0)[0]\n             index1 = indices[-2]\n",add,Added STORM - 146 to Changelog
9eb81edc34c68db73e0dfc2f5accba49d95121a8,fix kfs,boxmot/motion/kalman_filters/hybridsort_kf.py,"# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","@@ -281,7 +281,7 @@ class KalmanFilter(object):\n        https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n     """"""""""""\n \n-    def __init__(self, dim_x, dim_z, dim_u=0):\n+    def __init__(self, dim_x, dim_z, dim_u=0, max_obs=50):\n         if dim_x < 1:\n             raise ValueError('dim_x must be 1 or greater')\n         if dim_z < 1:\n@@ -329,7 +329,8 @@ class KalmanFilter(object):\n         self._mahalanobis = None\n \n         # keep all observations \n-        self.history_obs = deque([], maxlen=50)\n+        self.max_obs = max_obs\n+        self.history_obs = deque([], maxlen=self.max_obs)\n \n         self.inv = np.linalg.inv\n \n@@ -393,7 +394,7 @@ class KalmanFilter(object):\n             new_history = deepcopy(list(self.history_obs))\n             self.__dict__ = self.attr_saved\n             # self.history_obs = new_history \n-            self.history_obs = deque(list(self.history_obs)[:-1], maxlen=50)\n+            self.history_obs = deque(list(self.history_obs)[:-1], maxlen=self.max_obs)\n             occur = [int(d is None) for d in new_history]\n             indices = np.where(np.array(occur)==0)[0]\n             index1 = indices[-2]\n",add,Added the user group to the contributors
9eb81edc34c68db73e0dfc2f5accba49d95121a8,fix kfs,boxmot/motion/kalman_filters/ocsort_kf.py,"# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","# -*- coding: utf-8 -*-\n# pylint: disable=invalid-name, too-many-arguments, too-many-branches,\n# pylint: disable=too-many-locals, too-many-instance-attributes, too-many-lines\n\n""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemente","@@ -281,7 +281,7 @@ class KalmanFilter(object):\n        https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n     """"""""""""\n \n-    def __init__(self, dim_x, dim_z, dim_u=0):\n+    def __init__(self, dim_x, dim_z, dim_u=0, max_obs=50):\n         if dim_x < 1:\n             raise ValueError('dim_x must be 1 or greater')\n         if dim_z < 1:\n@@ -327,9 +327,10 @@ class KalmanFilter(object):\n         self._log_likelihood = log(sys.float_info.min)\n         self._likelihood = sys.float_info.min\n         self._mahalanobis = None\n+        self.max_obs = max_obs\n \n         # keep all observations \n-        self.history_obs = deque([], maxlen=50)\n+        self.history_obs = deque([], maxlen=self.max_obs)\n \n         self.inv = np.linalg.inv\n \n@@ -393,7 +394,7 @@ class KalmanFilter(object):\n             new_history = deepcopy(list(self.history_obs))\n             self.__dict__ = self.attr_saved\n             # self.history_obs = new_history \n-            self.history_obs = deque(list(self.history_obs)[:-1], maxlen=50)\n+            self.history_obs = deque(list(self.history_obs)[:-1], maxlen=self.max_obs)\n             occur = [int(d is None) for d in new_history]\n             indices = np.where(np.array(occur)==0)[0]\n             index1 = indices[-2]\n",add,Add note about data volume to enable_metrics_collection
b0795a6c7db5200d27efcfcd88b91d83c8ca6b05,fix test,boxmot/trackers/botsort/bot_sort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, feat=None, feat_history=50, max_obs=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.kalman_filter = None\n    ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, feat=None, feat_history=50, max_obs=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs=max_obs\n        s","@@ -23,12 +23,13 @@ class STrack(BaseTrack):\n         self.conf = det[4]\n         self.cls = det[5]\n         self.det_ind = det[6]\n+        self.max_obs=max_obs\n         self.kalman_filter = None\n         self.mean, self.covariance = None, None\n         self.is_activated = False\n         self.cls_hist = []  # (cls id, freq)\n         self.update_cls(self.cls, self.conf)\n-        self.history_observations = deque([], maxlen=50)\n+        self.history_observations = deque([], maxlen=self.max_obs)\n \n         self.tracklet_len = 0\n \n",add,Added STORM - 360 to CHANGELOG
b0795a6c7db5200d27efcfcd88b91d83c8ca6b05,fix test,boxmot/trackers/bytetrack/byte_tracker.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.motion.kalman_filters.bytetrack_kf import KalmanFilter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, max_obs):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n        self.xyah = tlwh2xyah(self.tlwh)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs=max_obs\n        self.kalman_filter = None\n        self.mean, sel","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.motion.kalman_filters.bytetrack_kf import KalmanFilter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, max_obs):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n        self.xyah = tlwh2xyah(self.tlwh)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs=max_obs\n        self.kalman_filter = None\n        self.mean, sel","@@ -118,7 +118,12 @@ class STrack(BaseTrack):\n \n class BYTETracker(BaseTracker):\n     def __init__(\n-        self, track_thresh=0.45, match_thresh=0.8, track_buffer=25, frame_rate=30, per_class=False,\n+        self,\n+        track_thresh=0.45,\n+        match_thresh=0.8,\n+        track_buffer=25,\n+        frame_rate=30,\n+        per_class=False,\n     ):\n         super(BYTETracker, self).__init__()\n         self.active_tracks = []  # type: list[STrack]\n",add,Added note about data volume to enable_metrics_collection
b0795a6c7db5200d27efcfcd88b91d83c8ca6b05,fix test,tests/unit/test_trackers.py,"import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, get_tracker_config, create_tracker,\n)\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tracker()\n\n\n@pytest.mark.parametrize(""""tracker_type"""", AL","import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, get_tracker_config, create_tracker,\n)\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tracker()\n\n\n@pytest.mark.parametrize(""""tracker_type"""", AL","@@ -52,18 +52,20 @@ def test_tracker_output_size(tracker_type):\n     \n @pytest.mark.parametrize(""""tracker_type"""", ALL_TRACKERS)\n def test_dynamic_max_obs_based_on_max_age(tracker_type):\n-    tracker_conf = get_tracker_config(tracker_type)\n-    tracker = create_tracker(\n-        tracker_type=tracker_type,\n-        tracker_config=tracker_conf,\n-        reid_weights=WEIGHTS / 'mobilenetv2_x1_4_dukemtmcreid.pt',\n-        device='cpu',\n-        half=False,\n-        per_class=False,\n-        max_age=100\n+    max_age = 400\n+    ocsort = OCSort(\n+        per_class=per_class,\n+        det_thresh=cfg.det_thresh,\n+        max_age=max_age,\n+        min_hits=cfg.min_hits,\n+        asso_threshold=cfg.iou_thresh,\n+        delta_t=cfg.delta_t,\n+        asso_func=cfg.asso_func,\n+        inertia=cfg.inertia,\n+        use_byte=cfg.use_byte,\n     )\n \n-    assert tracker.max_obs > tracker.max_age\n+    assert ocsort.max_obs == (max_age + 1)\n \n \n @pytest.mark.parametrize(""""tracker_type"""", PER_CLASS_TRACKERS)\n",add,Add ESC mapping for Pixel C keyboard
a0d3b374eb2aa16b6b1e2a970a880681dba0de23,fix test,tests/unit/test_trackers.py,"import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, get_tracker_config, create_tracker,\n)\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tracker()\n\n\n@pytest.mark.parametrize(""""tracker_type"""", AL","import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, get_tracker_config, create_tracker,\n)\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tracker()\n\n\n@pytest.mark.parametrize(""""tracker_type"""", AL","@@ -53,7 +53,7 @@ def test_tracker_output_size(tracker_type):\n @pytest.mark.parametrize(""""tracker_type"""", ALL_TRACKERS)\n def test_dynamic_max_obs_based_on_max_age(tracker_type):\n     max_age = 400\n-    ocsort = OCSort(\n+    ocsort = OCSORT(\n         per_class=per_class,\n         det_thresh=cfg.det_thresh,\n         max_age=max_age,\n",add,Add note about data volume to enable_metrics_collection
42d872362d24bbd12209457e4b473d44e4fd73ee,fix test,boxmot/trackers/basetracker.py,"import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\n\n\nclass BaseTracker(ABC):\n    def __init__(\n        self, \n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        max_obs: int = 50\n    ):\n        """"""""""""\n        Initialize the BaseTracker object with detection threshold, maximum age, minimum hits, \n        and Intersection Over Union (IOU) threshold for tracking objects in video frames.\n\n        Parameters:\n        - det_thresh (float): Detection threshold for considering detections.\n        - max_age (int): Maximum age of a track before it is considered lost.\n        - min_hits (int): Minimum number of detection hits before a track is considered confirmed.\n        - iou_threshold (float): IOU threshold for determining match between detection and tracks.\n\n        Attributes:\n        - frame","import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\n\n\nclass BaseTracker(ABC):\n    def __init__(\n        self, \n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        max_obs: int = 50\n    ):\n        """"""""""""\n        Initialize the BaseTracker object with detection threshold, maximum age, minimum hits, \n        and Intersection Over Union (IOU) threshold for tracking objects in video frames.\n\n        Parameters:\n        - det_thresh (float): Detection threshold for considering detections.\n        - max_age (int): Maximum age of a track before it is considered lost.\n        - min_hits (int): Minimum number of detection hits before a track is considered confirmed.\n        - iou_threshold (float): IOU threshold for determining match between detection and tracks.\n\n        Attributes:\n        - frame","@@ -41,7 +41,7 @@ class BaseTracker(ABC):\n         \n         if self.max_age >= self.max_obs:\n             LOGGER.warning(""""Max age > max observations, increasing size of max observations..."""")\n-            self.max_obs = self.max_age + 1\n+            self.max_obs = self.max_age + 5\n \n     @abstractmethod\n     def update(self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None) -> np.ndarray:\n",add,Add note about data volume to enable
42d872362d24bbd12209457e4b473d44e4fd73ee,fix test,tests/unit/test_trackers.py,"import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, get_tracker_config, create_tracker,\n)\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tracker()\n\n\n@pytest.mark.parametrize(""""tracker_type"""", AL","import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, get_tracker_config, create_tracker,\n)\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tracker()\n\n\n@pytest.mark.parametrize(""""tracker_type"""", AL","@@ -65,7 +65,7 @@ def test_dynamic_max_obs_based_on_max_age(tracker_type):\n         use_byte=cfg.use_byte,\n     )\n \n-    assert ocsort.max_obs == (max_age + 1)\n+    assert ocsort.max_obs == (max_age + 5)\n \n \n @pytest.mark.parametrize(""""tracker_type"""", PER_CLASS_TRACKERS)\n",add,Add note about data volume to enable_metrics_collection
15e286f580f9a72aa3e5122fcd120fa342bdc820,fix test,tests/unit/test_trackers.py,"import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, get_tracker_config, create_tracker,\n)\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tracker()\n\n\n@pytest.mark.parametrize(""""tracker_type"""", AL","import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, get_tracker_config, create_tracker,\n)\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tracker()\n\n\n@pytest.mark.parametrize(""""tracker_type"""", AL","@@ -50,8 +50,7 @@ def test_tracker_output_size(tracker_type):\n     assert output.shape == (2, 8)  # two inputs should give two outputs\n     \n     \n-@pytest.mark.parametrize(""""tracker_type"""", ALL_TRACKERS)\n-def test_dynamic_max_obs_based_on_max_age(tracker_type):\n+def test_dynamic_max_obs_based_on_max_age():\n     max_age = 400\n     ocsort = OCSORT(\n         per_class=per_class,\n",add,Added setup task
18a38f88247df600849adbd7cd39d6d9cbbca7ec,fix test,tests/unit/test_trackers.py,"import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, get_tracker_config, create_tracker,\n)\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tracker()\n\n\n@pytest.mark.parametrize(""""tracker_type"""", AL","import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, get_tracker_config, create_tracker,\n)\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tracker()\n\n\n@pytest.mark.parametrize(""""tracker_type"""", AL","@@ -53,7 +53,7 @@ def test_tracker_output_size(tracker_type):\n def test_dynamic_max_obs_based_on_max_age():\n     max_age = 400\n     ocsort = OCSORT(\n-        per_class=per_class,\n+        per_class=False,\n         det_thresh=cfg.det_thresh,\n         max_age=max_age,\n         min_hits=cfg.min_hits,\n",add,Added KHR_gl_texture_2D_image extension string
599a05f013741f2d7c1a77127f4e388f4ecc2fa0,fix test,tests/unit/test_trackers.py,"import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, get_tracker_config, create_tracker,\n)\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tracker()\n\n\n@pytest.mark.parametrize(""""tracker_type"""", AL","import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, get_tracker_config, create_tracker,\n)\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        fp16=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tracker()\n\n\n@pytest.mark.parametrize(""""tracker_type"""", AL","@@ -53,15 +53,7 @@ def test_tracker_output_size(tracker_type):\n def test_dynamic_max_obs_based_on_max_age():\n     max_age = 400\n     ocsort = OCSORT(\n-        per_class=False,\n-        det_thresh=cfg.det_thresh,\n-        max_age=max_age,\n-        min_hits=cfg.min_hits,\n-        asso_threshold=cfg.iou_thresh,\n-        delta_t=cfg.delta_t,\n-        asso_func=cfg.asso_func,\n-        inertia=cfg.inertia,\n-        use_byte=cfg.use_byte,\n+        max_age=max_age\n     )\n \n     assert ocsort.max_obs == (max_age + 5)\n",add,Added KHR_gl_texture_2D_image extension string
6cfee3a4b14517405891b0daee3f8f10b737d539,fix test,boxmot/trackers/basetracker.py,"import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\n\n\nclass BaseTracker(ABC):\n    def __init__(\n        self, \n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        max_obs: int = 50\n    ):\n        """"""""""""\n        Initialize the BaseTracker object with detection threshold, maximum age, minimum hits, \n        and Intersection Over Union (IOU) threshold for tracking objects in video frames.\n\n        Parameters:\n        - det_thresh (float): Detection threshold for considering detections.\n        - max_age (int): Maximum age of a track before it is considered lost.\n        - min_hits (int): Minimum number of detection hits before a track is considered confirmed.\n        - iou_threshold (float): IOU threshold for determining match between detection and tracks.\n\n        Attributes:\n        - frame","import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\n\n\nclass BaseTracker(ABC):\n    def __init__(\n        self, \n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        max_obs: int = 50\n    ):\n        """"""""""""\n        Initialize the BaseTracker object with detection threshold, maximum age, minimum hits, \n        and Intersection Over Union (IOU) threshold for tracking objects in video frames.\n\n        Parameters:\n        - det_thresh (float): Detection threshold for considering detections.\n        - max_age (int): Maximum age of a track before it is considered lost.\n        - min_hits (int): Minimum number of detection hits before a track is considered confirmed.\n        - iou_threshold (float): IOU threshold for determining match between detection and tracks.\n\n        Attributes:\n        - frame","@@ -42,6 +42,7 @@ class BaseTracker(ABC):\n         if self.max_age >= self.max_obs:\n             LOGGER.warning(""""Max age > max observations, increasing size of max observations..."""")\n             self.max_obs = self.max_age + 5\n+            print(""""self.max_obs"""", self.max_obs)\n \n     @abstractmethod\n     def update(self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None) -> np.ndarray:\n",add,Add note about data volume to enable_metrics_collection
6cfee3a4b14517405891b0daee3f8f10b737d539,fix test,boxmot/trackers/deepocsort/deep_ocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.deepocsort_kf import KalmanFilter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box an","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.deepocsort_kf import KalmanFilter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box an","@@ -327,7 +327,7 @@ class DeepOCSort(BaseTracker):\n         new_kf_off=False,\n         **kwargs\n     ):\n-        super(DeepOCSort, self).__init__()\n+        super().__init__(max_age=max_age)\n         """"""""""""\n         Sets key parameters for SORT\n         """"""""""""\n",add,Added the user Nishanth Shanakran to the developer section
6cfee3a4b14517405891b0daee3f8f10b737d539,fix test,boxmot/trackers/hybridsort/hybridsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.hybridsort.association import (\n    associate_4_points_with_score, associate_4_points_with_score_with_reid,\n    cal_score_dif_batch_two_score, embedding_distance, linear_assignment)\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observa","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.hybridsort.association import (\n    associate_4_points_with_score, associate_4_points_with_score_with_reid,\n    cal_score_dif_batch_two_score, embedding_distance, linear_assignment)\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observa","@@ -335,7 +335,7 @@ class KalmanBoxTracker(object):\n class HybridSORT(BaseTracker):\n     def __init__(self, reid_weights, device, half, det_thresh, per_class=False, max_age=30, min_hits=3,\n                  iou_threshold=0.3, delta_t=3, asso_func=""""iou"""", inertia=0.2, longterm_reid_weight=0, TCM_first_step_weight=0, use_byte=False):\n-        super(HybridSORT, self).__init__()\n+        super().__init__(max_age=max_age)\n \n         """"""""""""\n         Sets key parameters for SORT\n",add,Added note about data volume to enable_metrics_collection
6cfee3a4b14517405891b0daee3f8f10b737d539,fix test,boxmot/trackers/ocsort/ocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\nfrom collections import deque\n\n\nfrom boxmot.motion.kalman_filters.ocsort_kf import KalmanFilter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.utils.iou import run_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y i","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\nfrom collections import deque\n\n\nfrom boxmot.motion.kalman_filters.ocsort_kf import KalmanFilter\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.utils.iou import run_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y i","@@ -204,7 +204,7 @@ class OCSort(BaseTracker):\n         inertia=0.2,\n         use_byte=False,\n     ):\n-        super(OCSort, self).__init__()\n+        super().__init__(max_age=max_age)\n         """"""""""""\n         Sets key parameters for SORT\n         """"""""""""\n",add,Added net . kano . joustsim . oscar . os
b150783f175bb9927eb28ebe67ba8c7fdd22d870,fix test,boxmot/trackers/botsort/bot_sort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, feat=None, feat_history=50, max_obs=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs=max_obs\n        s","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.botsort_kf import KalmanFilter\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, feat=None, feat_history=50, max_obs=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs=max_obs\n        s","@@ -205,7 +205,7 @@ class BoTSORT(BaseTracker):\n         fuse_first_associate: bool = False,\n         with_reid: bool = True,\n     ):\n-        super(BoTSORT, self).__init__()\n+        super().__init__()\n         self.lost_stracks = []  # type: list[STrack]\n         self.removed_stracks = []  # type: list[STrack]\n         BaseTrack.clear_count()\n",add,Added Type name for DFI ( # 3 )
b150783f175bb9927eb28ebe67ba8c7fdd22d870,fix test,boxmot/trackers/bytetrack/byte_tracker.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.motion.kalman_filters.bytetrack_kf import KalmanFilter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, max_obs):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n        self.xyah = tlwh2xyah(self.tlwh)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs=max_obs\n        self.kalman_filter = None\n        self.mean, sel","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.motion.kalman_filters.bytetrack_kf import KalmanFilter\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilter()\n\n    def __init__(self, det, max_obs):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n        self.xyah = tlwh2xyah(self.tlwh)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs=max_obs\n        self.kalman_filter = None\n        self.mean, sel","@@ -125,7 +125,7 @@ class BYTETracker(BaseTracker):\n         frame_rate=30,\n         per_class=False,\n     ):\n-        super(BYTETracker, self).__init__()\n+        super().__init__()\n         self.active_tracks = []  # type: list[STrack]\n         self.lost_stracks = []  # type: list[STrack]\n         self.removed_stracks = []  # type: list[STrack]\n",add,Added note about dates .
644661e195e4bc5983ba83cf788320d816e378e4,fix xys kf import,boxmot/trackers/deepocsort/deep_ocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.deepocsort_kf import KalmanFilterXYS\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.xys_kf import KalmanFilterXYS\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s ","@@ -5,7 +5,7 @@ from collections import deque\n \n from boxmot.appearance.reid_auto_backend import ReidAutoBackend\n from boxmot.motion.cmc import get_cmc_method\n-from boxmot.motion.kalman_filters.deepocsort_kf import KalmanFilterXYS\n+from boxmot.motion.kalman_filters.xys_kf import KalmanFilterXYS\n from boxmot.utils.association import associate, linear_assignment\n from boxmot.utils.iou import get_asso_func\n from boxmot.trackers.basetracker import BaseTracker\n",add,Added hypest who apparently actually wrote the C # port
644661e195e4bc5983ba83cf788320d816e378e4,fix xys kf import,boxmot/trackers/hybridsort/hybridsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.hybridsort.association import (\n    associate_4_points_with_score, associate_4_points_with_score_with_reid,\n    cal_score_dif_batch_two_score, embedding_distance, linear_assignment)\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observa","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.hybridsort.association import (\n    associate_4_points_with_score, associate_4_points_with_score_with_reid,\n    cal_score_dif_batch_two_score, embedding_distance, linear_assignment)\nfrom boxmot.utils import PerClassDecorator\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observa","@@ -131,7 +131,7 @@ class KalmanBoxTracker(object):\n         """"""""""""\n         # define constant velocity model\n         # if not orig and not args.kalman_GPR:\n-        from boxmot.motion.kalman_filters.hybridsort_kf import KalmanFilterXYS\n+        from boxmot.motion.kalman_filters.xys_kf import KalmanFilterXYS\n         self.kf = KalmanFilterXYS(dim_x=9, dim_z=5, max_obs=max_obs)\n \n         # u, v, s, c, r, ~u, ~v, ~s, ~c\n",add,Added Type name for DFI ( # 57480 )
644661e195e4bc5983ba83cf788320d816e378e4,fix xys kf import,boxmot/trackers/ocsort/ocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\nfrom collections import deque\n\n\nfrom boxmot.motion.kalman_filters.ocsort_kf import KalmanFilterXYS\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.utils.iou import run_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nimport numpy as np\nfrom collections import deque\n\n\nfrom boxmot.motion.kalman_filters.xys_kf import KalmanFilterXYS\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.iou import get_asso_func\nfrom boxmot.utils.iou import run_asso_func\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y i","@@ -7,7 +7,7 @@ import numpy as np\n from collections import deque\n \n \n-from boxmot.motion.kalman_filters.ocsort_kf import KalmanFilterXYS\n+from boxmot.motion.kalman_filters.xys_kf import KalmanFilterXYS\n from boxmot.utils.association import associate, linear_assignment\n from boxmot.utils.iou import get_asso_func\n from boxmot.utils.iou import run_asso_func\n",add,Added the UNSTARTED state to the YouTube PlayerState enum
69e65b6b75bce0caa97959e1b565aabcba635574,fix bytetrack slow KF bug,boxmot/motion/kalman_filters/base_kalman_filter.py,"import numpy as np\nimport scipy.linalg\nfrom typing import Tuple\n\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919\n}\n\nclass BaseKalmanFilter:\n    """"""""""""\n    Base class for Kalman filters tracking bounding boxes in image space.\n    """"""""""""\n\n    def __init__(self, ndim: int):\n        self.ndim = ndim\n        self.dt = 1.\n\n        # Create Kalman filter model matrices.\n        self._motion_mat = np.eye(2 * ndim, 2 * ndim)  # State transition matrix\n        for i in range(ndim):\n            self._motion_mat[i, ndim + i] = self.dt\n        self._update_mat = np.eye(ndim, 2 * ndim)  # Observation matrix\n\n        # Motion and observation uncertain","import numpy as np\nimport scipy.linalg\nfrom typing import Tuple\n\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919\n}\n\nclass BaseKalmanFilter:\n    """"""""""""\n    Base class for Kalman filters tracking bounding boxes in image space.\n    """"""""""""\n\n    def __init__(self, ndim: int):\n        self.ndim = ndim\n        self.dt = 1.\n\n        # Create Kalman filter model matrices.\n        self._motion_mat = np.eye(2 * ndim, 2 * ndim)  # State transition matrix\n        for i in range(ndim):\n            self._motion_mat[i, ndim + i] = self.dt\n        self._update_mat = np.eye(ndim, 2 * ndim)  # Observation matrix\n\n        # Motion and observation uncertain","@@ -64,8 +64,9 @@ class BaseKalmanFilter:\n         std_pos, std_vel = self._get_process_noise_std(mean)\n         motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\n \n-        mean = np.dot(self._motion_mat, mean)\n-        covariance = np.linalg.multi_dot((self._motion_mat, covariance, self._motion_mat.T)) + motion_cov\n+        mean = np.dot(mean, self._motion_mat.T)\n+        covariance = np.linalg.multi_dot((\n+            self._motion_mat, covariance, self._motion_mat.T)) + motion_cov\n \n         return mean, covariance\n \n@@ -94,23 +95,9 @@ class BaseKalmanFilter:\n         innovation_cov = np.diag(np.square(std))\n \n         mean = np.dot(self._update_mat, mean)\n-        covariance = np.linalg.multi_dot((self._update_mat, covariance, self._update_mat.T))\n+        covariance = np.linalg.multi_dot((\n+            self._update_mat, covariance, self._update_mat.T))\n         return mean, covariance + innovation_cov\n-    \n-    def _get_multi_process_noise_std(self, mean: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n-        std_pos = [\n-            self._std_weight_position * mean[:, 2],\n-            self._std_weight_position * mean[:, 3],\n-            self._std_weight_position * mean[:, 2],\n-            self._std_weight_position * mean[:, 3]\n-        ]\n-        std_vel = [\n-            self._std_weight_velocity * mean[:, 2],\n-            self._std_weight_velocity * mean[:, 3],\n-            self._std_weight_velocity * mean[:, 2],\n-            self._std_weight_velocity * mean[:, 3]\n-        ]\n-        return std_pos, std_vel\n \n     def multi_predict(self, mean: np.ndarray, covariance: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n         """"""""""""\n@@ -142,6 +129,13 @@ class BaseKalmanFilter:\n         new_covariance = covariance - np.linalg.multi_dot((kalman_gain, projected_cov, kalman_gain.T))\n         return new_mean, new_covariance\n \n+    def _get_multi_process_noise_std(self, mean: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:",add,Added note about dates .
69e65b6b75bce0caa97959e1b565aabcba635574,fix bytetrack slow KF bug,boxmot/motion/kalman_filters/xyah_kf.py,"import numpy as np\nfrom typing import Tuple\nfrom boxmot.motion.kalman_filters.base_kalman_filter import BaseKalmanFilter\n\n\nclass KalmanFilterXYAH(BaseKalmanFilter):\n    """"""""""""\n    A Kalman filter for tracking bounding boxes in image space with state space:\n        x, y, a, h, vx, vy, va, vh\n    """"""""""""\n\n    def __init__(self):\n        super().__init__(ndim=4)\n\n    def _get_initial_covariance_std(self, measurement: np.ndarray) -> np.ndarray:\n        return [\n            2 * self._std_weight_position * measurement[3],\n            2 * self._std_weight_position * measurement[3],\n            1e-2,\n            2 * self._std_weight_position * measurement[3],\n            10 * self._std_weight_velocity * measurement[3],\n            10 * self._std_weight_velocity * measurement[3],\n            1e-5,\n            10 * self._std_weight_velocity * measurement[3]\n        ]\n\n    def _get_process_noise_std(self, mean: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        std_po","import numpy as np\nfrom typing import Tuple\nfrom boxmot.motion.kalman_filters.base_kalman_filter import BaseKalmanFilter\n\n\nclass KalmanFilterXYAH(BaseKalmanFilter):\n    """"""""""""\n    A Kalman filter for tracking bounding boxes in image space with state space:\n        x, y, a, h, vx, vy, va, vh\n    """"""""""""\n\n    def __init__(self):\n        super().__init__(ndim=4)\n\n    def _get_initial_covariance_std(self, measurement: np.ndarray) -> np.ndarray:\n        return [\n            2 * self._std_weight_position * measurement[3],\n            2 * self._std_weight_position * measurement[3],\n            1e-2,\n            2 * self._std_weight_position * measurement[3],\n            10 * self._std_weight_velocity * measurement[3],\n            10 * self._std_weight_velocity * measurement[3],\n            1e-5,\n            10 * self._std_weight_velocity * measurement[3]\n        ]\n\n    def _get_process_noise_std(self, mean: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        std_po","@@ -47,3 +47,18 @@ class KalmanFilterXYAH(BaseKalmanFilter):\n             self._std_weight_position * mean[3]\n         ]\n         return std_noise\n+\n+    def _get_multi_process_noise_std(self, mean: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n+        std_pos = [\n+            self._std_weight_position * mean[:, 3],\n+            self._std_weight_position * mean[:, 3],\n+            1e-2 * np.ones_like(mean[:, 3]),\n+            self._std_weight_position * mean[:, 3]\n+        ]\n+        std_vel = [\n+            self._std_weight_velocity * mean[:, 3],\n+            self._std_weight_velocity * mean[:, 3],\n+            1e-5 * np.ones_like(mean[:, 3]),\n+            self._std_weight_velocity * mean[:, 3]\n+        ]\n+        return std_pos, std_vel\n",add,Added note about data volume to enable_metrics_collection
69e65b6b75bce0caa97959e1b565aabcba635574,fix bytetrack slow KF bug,boxmot/motion/kalman_filters/xywh_kf.py,"import numpy as np\nfrom typing import Tuple\nfrom boxmot.motion.kalman_filters.base_kalman_filter import BaseKalmanFilter\n\n\nclass KalmanFilterXYWH(BaseKalmanFilter):\n    """"""""""""\n    A Kalman filter for tracking bounding boxes in image space with state space:\n        x, y, w, h, vx, vy, vw, vh\n    """"""""""""\n\n    def __init__(self):\n        super().__init__(ndim=4)\n\n    def _get_initial_covariance_std(self, measurement: np.ndarray) -> np.ndarray:\n        return [\n            2 * self._std_weight_position * measurement[2],\n            2 * self._std_weight_position * measurement[3],\n            2 * self._std_weight_position * measurement[2],\n            2 * self._std_weight_position * measurement[3],\n            10 * self._std_weight_velocity * measurement[2],\n            10 * self._std_weight_velocity * measurement[3],\n            10 * self._std_weight_velocity * measurement[2],\n            10 * self._std_weight_velocity * measurement[3]\n        ]\n\n    def _get_proces","import numpy as np\nfrom typing import Tuple\nfrom boxmot.motion.kalman_filters.base_kalman_filter import BaseKalmanFilter\n\n\nclass KalmanFilterXYWH(BaseKalmanFilter):\n    """"""""""""\n    A Kalman filter for tracking bounding boxes in image space with state space:\n        x, y, w, h, vx, vy, vw, vh\n    """"""""""""\n\n    def __init__(self):\n        super().__init__(ndim=4)\n\n    def _get_initial_covariance_std(self, measurement: np.ndarray) -> np.ndarray:\n        return [\n            2 * self._std_weight_position * measurement[2],\n            2 * self._std_weight_position * measurement[3],\n            2 * self._std_weight_position * measurement[2],\n            2 * self._std_weight_position * measurement[3],\n            10 * self._std_weight_velocity * measurement[2],\n            10 * self._std_weight_velocity * measurement[3],\n            10 * self._std_weight_velocity * measurement[2],\n            10 * self._std_weight_velocity * measurement[3]\n        ]\n\n    def _get_proces","@@ -47,3 +47,18 @@ class KalmanFilterXYWH(BaseKalmanFilter):\n             self._std_weight_position * mean[3]\n         ]\n         return std_noise\n+    \n+    def _get_multi_process_noise_std(self, mean: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n+        std_pos = [\n+            self._std_weight_position * mean[:, 2],\n+            self._std_weight_position * mean[:, 3],\n+            self._std_weight_position * mean[:, 2],\n+            self._std_weight_position * mean[:, 3]\n+        ]\n+        std_vel = [\n+            self._std_weight_velocity * mean[:, 2],\n+            self._std_weight_velocity * mean[:, 3],\n+            self._std_weight_velocity * mean[:, 2],\n+            self._std_weight_velocity * mean[:, 3]\n+        ]\n+        return std_pos, std_vel\n",add,Add note about data volume to enable_metrics_collection
69e65b6b75bce0caa97959e1b565aabcba635574,fix bytetrack slow KF bug,boxmot/trackers/botsort/bot_sort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.xywh_kf import KalmanFilterXYWH\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilterXYWH()\n\n    def __init__(self, det, feat=None, feat_history=50, max_obs=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs=max_obs\n    ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.xywh_kf import KalmanFilterXYWH\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilterXYWH()\n\n    def __init__(self, det, feat=None, feat_history=50, max_obs=50):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs=max_obs\n    ","@@ -129,7 +129,7 @@ class STrack(BaseTrack):\n \n     def re_activate(self, new_track, frame_id, new_id=False):\n         self.mean, self.covariance = self.kalman_filter.update(\n-            self.mean, self.covariance, new_track.xywh\n+            self.mean, self.covariance, new_track.xywh, self.conf\n         )\n         if new_track.curr_feat is not None:\n             self.update_features(new_track.curr_feat)\n",add,Added Type name for DFI ( # 210 )
69e65b6b75bce0caa97959e1b565aabcba635574,fix bytetrack slow KF bug,boxmot/trackers/bytetrack/byte_tracker.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.motion.kalman_filters.xyah_kf import KalmanFilterXYAH\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilterXYAH()\n\n    def __init__(self, det, max_obs):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n        self.xyah = tlwh2xyah(self.tlwh)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs=max_obs\n        self.kalman_filter = None\n        self.mean, ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nfrom collections import deque\n\nfrom boxmot.motion.kalman_filters.xyah_kf import KalmanFilterXYAH\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDecorator\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilterXYAH()\n\n    def __init__(self, det, max_obs):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n        self.xyah = tlwh2xyah(self.tlwh)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs=max_obs\n        self.kalman_filter = None\n        self.mean, ","@@ -68,7 +68,7 @@ class STrack(BaseTrack):\n \n     def re_activate(self, new_track, frame_id, new_id=False):\n         self.mean, self.covariance = self.kalman_filter.update(\n-            self.mean, self.covariance, new_track.xyah\n+            self.mean, self.covariance, new_track.xyah, self.conf\n         )\n         self.tracklet_len = 0\n         self.state = TrackState.Tracked\n",add,Added Type name for DFI ( # 3 )
d57959cf5712368e37e3bd4c102489448c51309f,fix imports,boxmot/appearance/exporters/onnx_exporter.py,"import torch\nimport onnx\nfrom base_exporter import BaseExporter\n\nclass ONNXExporter(BaseExporter):\n    def export(self):\n        try:\n            checker.check_packages((""""onnx==1.14.0"""",))\n            f = self.file.with_suffix("""".onnx"""")\n            LOGGER.info(f""""\nStarting export with onnx {onnx.__version__}..."""")\n\n            dynamic = {""""images"""": {0: """"batch""""}, """"output"""": {0: """"batch""""}} if self.dynamic else None\n\n            torch.onnx.export(\n                self.model.cpu() if self.dynamic else self.model,\n                self.im.cpu() if self.dynamic else self.im,\n                f,\n                verbose=False,\n                opset_version=12,\n                do_constant_folding=True,\n                input_names=[""""images""""],\n                output_names=[""""output""""],\n                dynamic_axes=dynamic,\n            )\n\n            model_onnx = onnx.load(f)\n            onnx.checker.check_model(model_onnx)\n            onnx.save(model_onnx, f)\n\","import torch\nimport onnx\nfrom boxmot.appearance.exporters import BaseExporter\n\nclass ONNXExporter(BaseExporter):\n    def export(self):\n        try:\n            checker.check_packages((""""onnx==1.14.0"""",))\n            f = self.file.with_suffix("""".onnx"""")\n            LOGGER.info(f""""\nStarting export with onnx {onnx.__version__}..."""")\n\n            dynamic = {""""images"""": {0: """"batch""""}, """"output"""": {0: """"batch""""}} if self.dynamic else None\n\n            torch.onnx.export(\n                self.model.cpu() if self.dynamic else self.model,\n                self.im.cpu() if self.dynamic else self.im,\n                f,\n                verbose=False,\n                opset_version=12,\n                do_constant_folding=True,\n                input_names=[""""images""""],\n                output_names=[""""output""""],\n                dynamic_axes=dynamic,\n            )\n\n            model_onnx = onnx.load(f)\n            onnx.checker.check_model(model_onnx)\n            onnx.save(mod","@@ -1,6 +1,6 @@\n import torch\n import onnx\n-from base_exporter import BaseExporter\n+from boxmot.appearance.exporters import BaseExporter\n \n class ONNXExporter(BaseExporter):\n     def export(self):\n",add,Add warning about data volume to enable_metrics_collection
d57959cf5712368e37e3bd4c102489448c51309f,fix imports,boxmot/appearance/exporters/openvino_exporter.py,"import os\nimport openvino.runtime as ov\nfrom openvino.tools import mo\nfrom base_exporter import BaseExporter\n\nclass OpenVINOExporter(BaseExporter):\n    def export(self):\n        checker.check_packages(\n            (""""openvino-dev>=2023.0"""",)\n        )\n        f = str(self.file).replace(self.file.suffix, f""""_openvino_model{os.sep}"""")\n        f_onnx = self.file.with_suffix("""".onnx"""")\n        f_ov = str(Path(f) / self.file.with_suffix("""".xml"""").name)\n        try:\n            LOGGER.info(f""""\nStarting export with openvino {ov.__version__}..."""")\n            ov_model = mo.convert_model(\n                f_onnx,\n                model_name=self.file.with_suffix("""".xml""""),\n                framework=""""onnx"""",\n                compress_to_fp16=self.half,\n            )\n            ov.serialize(ov_model, f_ov)\n        except Exception as e:\n            LOGGER.error(f""""Export failure: {e}"""")\n        LOGGER.info(f""""Export success, saved as {f_ov} ({self.file_size(f_ov):.1f} MB)""","import os\nimport openvino.runtime as ov\nfrom openvino.tools import mo\nfrom boxmot.appearance.exporters import BaseExporter\n\nclass OpenVINOExporter(BaseExporter):\n    def export(self):\n        checker.check_packages(\n            (""""openvino-dev>=2023.0"""",)\n        )\n        f = str(self.file).replace(self.file.suffix, f""""_openvino_model{os.sep}"""")\n        f_onnx = self.file.with_suffix("""".onnx"""")\n        f_ov = str(Path(f) / self.file.with_suffix("""".xml"""").name)\n        try:\n            LOGGER.info(f""""\nStarting export with openvino {ov.__version__}..."""")\n            ov_model = mo.convert_model(\n                f_onnx,\n                model_name=self.file.with_suffix("""".xml""""),\n                framework=""""onnx"""",\n                compress_to_fp16=self.half,\n            )\n            ov.serialize(ov_model, f_ov)\n        except Exception as e:\n            LOGGER.error(f""""Export failure: {e}"""")\n        LOGGER.info(f""""Export success, saved as {f_ov} ({self.file_size(f","@@ -1,7 +1,7 @@\n import os\n import openvino.runtime as ov\n from openvino.tools import mo\n-from base_exporter import BaseExporter\n+from boxmot.appearance.exporters import BaseExporter\n \n class OpenVINOExporter(BaseExporter):\n     def export(self):\n",add,Add warning about data volume to enable_metrics_collection
d57959cf5712368e37e3bd4c102489448c51309f,fix imports,boxmot/appearance/exporters/tensorrt_exporter.py,"import platform\nimport torch\nfrom base_exporter import BaseExporter\n\nclass EngineExporter(BaseExporter):\n    def export(self):\n        try:\n            assert self.im.device.type != """"cpu"""", """"export running on CPU but must be on GPU, i.e. `python export.py --device 0`""""\n            try:\n                import tensorrt as trt\n            except ImportError:\n                if platform.system() == """"Linux"""":\n                    checker.check_packages(\n                        [""""nvidia-tensorrt""""],\n                        cmds=(""""-U --index-url https://pypi.ngc.nvidia.com"""",),\n                    )\n                import tensorrt as trt\n\n            onnx_file = self.export_onnx()\n            LOGGER.info(f""""\nStarting export with TensorRT {trt.__version__}..."""")\n            assert onnx_file.exists(), f""""Failed to export ONNX file: {onnx_file}""""\n            f = self.file.with_suffix("""".engine"""")\n            logger = trt.Logger(trt.Logger.INFO)\n            if self.ver","import platform\nimport torch\nfrom boxmot.appearance.exporters import BaseExporter\n\nclass EngineExporter(BaseExporter):\n    def export(self):\n        try:\n            assert self.im.device.type != """"cpu"""", """"export running on CPU but must be on GPU, i.e. `python export.py --device 0`""""\n            try:\n                import tensorrt as trt\n            except ImportError:\n                if platform.system() == """"Linux"""":\n                    checker.check_packages(\n                        [""""nvidia-tensorrt""""],\n                        cmds=(""""-U --index-url https://pypi.ngc.nvidia.com"""",),\n                    )\n                import tensorrt as trt\n\n            onnx_file = self.export_onnx()\n            LOGGER.info(f""""\nStarting export with TensorRT {trt.__version__}..."""")\n            assert onnx_file.exists(), f""""Failed to export ONNX file: {onnx_file}""""\n            f = self.file.with_suffix("""".engine"""")\n            logger = trt.Logger(trt.Logger.INFO)\n         ","@@ -1,6 +1,6 @@\n import platform\n import torch\n-from base_exporter import BaseExporter\n+from boxmot.appearance.exporters import BaseExporter\n \n class EngineExporter(BaseExporter):\n     def export(self):\n",add,Add warning about data volume to enable_metrics_collection
d57959cf5712368e37e3bd4c102489448c51309f,fix imports,boxmot/appearance/exporters/tflite_exporter.py,"import subprocess\nfrom base_exporter import BaseExporter\n\nclass TFLiteExporter(BaseExporter):\n    def export(self):\n        try:\n            checker.check_packages(\n                (""""onnx2tf>=1.15.4"""", """"tensorflow"""", """"onnx_graphsurgeon>=0.3.26"""", """"sng4onnx>=1.0.1""""),\n                cmds='--extra-index-url https://pypi.ngc.nvidia.com'\n            )\n            import onnx2tf\n\n            LOGGER.info(f""""\nStarting {self.file} export with onnx2tf {onnx2tf.__version__}"""")\n            f = str(self.file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n            cmd = f""""onnx2tf -i {self.file} -o {f} -osd -coion --non_verbose""""\n            subprocess.check_output(cmd.split())\n            LOGGER.info(f""""Export success, results saved in {f} ({self.file_size(f):.1f} MB)"""")\n            return f\n        except Exception as e:\n            LOGGER.error(f""""\nExport failure: {e}"""")","import subprocess\nfrom boxmot.appearance.exporters import BaseExporter\n\nclass TFLiteExporter(BaseExporter):\n    def export(self):\n        try:\n            checker.check_packages(\n                (""""onnx2tf>=1.15.4"""", """"tensorflow"""", """"onnx_graphsurgeon>=0.3.26"""", """"sng4onnx>=1.0.1""""),\n                cmds='--extra-index-url https://pypi.ngc.nvidia.com'\n            )\n            import onnx2tf\n\n            LOGGER.info(f""""\nStarting {self.file} export with onnx2tf {onnx2tf.__version__}"""")\n            f = str(self.file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n            cmd = f""""onnx2tf -i {self.file} -o {f} -osd -coion --non_verbose""""\n            subprocess.check_output(cmd.split())\n            LOGGER.info(f""""Export success, results saved in {f} ({self.file_size(f):.1f} MB)"""")\n            return f\n        except Exception as e:\n            LOGGER.error(f""""\nExport failure: {e}"""")","@@ -1,5 +1,5 @@\n import subprocess\n-from base_exporter import BaseExporter\n+from boxmot.appearance.exporters import BaseExporter\n \n class TFLiteExporter(BaseExporter):\n     def export(self):\n",add,Add _binomial_double_trees by default
d57959cf5712368e37e3bd4c102489448c51309f,fix imports,boxmot/appearance/exporters/torchscript_exporter.py,"import torch\nfrom base_exporter import BaseExporter\n\nclass TorchScriptExporter(BaseExporter):\n    def export(self):\n        try:\n            LOGGER.info(f""""\nStarting export with torch {torch.__version__}..."""")\n            f = self.file.with_suffix("""".torchscript"""")\n            ts = torch.jit.trace(self.model, self.im, strict=False)\n            if self.optimize:\n                torch.utils.mobile_optimizer.optimize_for_mobile(ts)._save_for_lite_interpreter(str(f))\n            else:\n                ts.save(str(f))\n\n            LOGGER.info(f""""Export success, saved as {f} ({self.file_size(f):.1f} MB)"""")\n            return f\n        except Exception as e:\n            LOGGER.error(f""""Export failure: {e}"""")","import torch\nfrom boxmot.appearance.exporters import BaseExporter\n\nclass TorchScriptExporter(BaseExporter):\n    def export(self):\n        try:\n            LOGGER.info(f""""\nStarting export with torch {torch.__version__}..."""")\n            f = self.file.with_suffix("""".torchscript"""")\n            ts = torch.jit.trace(self.model, self.im, strict=False)\n            if self.optimize:\n                torch.utils.mobile_optimizer.optimize_for_mobile(ts)._save_for_lite_interpreter(str(f))\n            else:\n                ts.save(str(f))\n\n            LOGGER.info(f""""Export success, saved as {f} ({self.file_size(f):.1f} MB)"""")\n            return f\n        except Exception as e:\n            LOGGER.error(f""""Export failure: {e}"""")","@@ -1,5 +1,5 @@\n import torch\n-from base_exporter import BaseExporter\n+from boxmot.appearance.exporters import BaseExporter\n \n class TorchScriptExporter(BaseExporter):\n     def export(self):\n",add,Add note about data volume to enable
25ce7f21f092a38183fc4a0f9c9438c723696c12,fix imports,boxmot/appearance/exporters/onnx_exporter.py,"import torch\nimport onnx\nfrom boxmot.appearance.exporters import BaseExporter\n\nclass ONNXExporter(BaseExporter):\n    def export(self):\n        try:\n            checker.check_packages((""""onnx==1.14.0"""",))\n            f = self.file.with_suffix("""".onnx"""")\n            LOGGER.info(f""""\nStarting export with onnx {onnx.__version__}..."""")\n\n            dynamic = {""""images"""": {0: """"batch""""}, """"output"""": {0: """"batch""""}} if self.dynamic else None\n\n            torch.onnx.export(\n                self.model.cpu() if self.dynamic else self.model,\n                self.im.cpu() if self.dynamic else self.im,\n                f,\n                verbose=False,\n                opset_version=12,\n                do_constant_folding=True,\n                input_names=[""""images""""],\n                output_names=[""""output""""],\n                dynamic_axes=dynamic,\n            )\n\n            model_onnx = onnx.load(f)\n            onnx.checker.check_model(model_onnx)\n            onnx.save(mod","import torch\nimport onnx\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\n\nclass ONNXExporter(BaseExporter):\n    def export(self):\n        try:\n            checker.check_packages((""""onnx==1.14.0"""",))\n            f = self.file.with_suffix("""".onnx"""")\n            LOGGER.info(f""""\nStarting export with onnx {onnx.__version__}..."""")\n\n            dynamic = {""""images"""": {0: """"batch""""}, """"output"""": {0: """"batch""""}} if self.dynamic else None\n\n            torch.onnx.export(\n                self.model.cpu() if self.dynamic else self.model,\n                self.im.cpu() if self.dynamic else self.im,\n                f,\n                verbose=False,\n                opset_version=12,\n                do_constant_folding=True,\n                input_names=[""""images""""],\n                output_names=[""""output""""],\n                dynamic_axes=dynamic,\n            )\n\n            model_onnx = onnx.load(f)\n            onnx.checker.check_model(model_onnx)\n           ","@@ -1,6 +1,6 @@\n import torch\n import onnx\n-from boxmot.appearance.exporters import BaseExporter\n+from boxmot.appearance.exporters.base_exporter import BaseExporter\n \n class ONNXExporter(BaseExporter):\n     def export(self):\n",add,Add warning about data volume to enable_metrics_collection
25ce7f21f092a38183fc4a0f9c9438c723696c12,fix imports,boxmot/appearance/exporters/openvino_exporter.py,"import os\nimport openvino.runtime as ov\nfrom openvino.tools import mo\nfrom boxmot.appearance.exporters import BaseExporter\n\nclass OpenVINOExporter(BaseExporter):\n    def export(self):\n        checker.check_packages(\n            (""""openvino-dev>=2023.0"""",)\n        )\n        f = str(self.file).replace(self.file.suffix, f""""_openvino_model{os.sep}"""")\n        f_onnx = self.file.with_suffix("""".onnx"""")\n        f_ov = str(Path(f) / self.file.with_suffix("""".xml"""").name)\n        try:\n            LOGGER.info(f""""\nStarting export with openvino {ov.__version__}..."""")\n            ov_model = mo.convert_model(\n                f_onnx,\n                model_name=self.file.with_suffix("""".xml""""),\n                framework=""""onnx"""",\n                compress_to_fp16=self.half,\n            )\n            ov.serialize(ov_model, f_ov)\n        except Exception as e:\n            LOGGER.error(f""""Export failure: {e}"""")\n        LOGGER.info(f""""Export success, saved as {f_ov} ({self.file_size(f","import os\nimport openvino.runtime as ov\nfrom openvino.tools import mo\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\n\nclass OpenVINOExporter(BaseExporter):\n    def export(self):\n        checker.check_packages(\n            (""""openvino-dev>=2023.0"""",)\n        )\n        f = str(self.file).replace(self.file.suffix, f""""_openvino_model{os.sep}"""")\n        f_onnx = self.file.with_suffix("""".onnx"""")\n        f_ov = str(Path(f) / self.file.with_suffix("""".xml"""").name)\n        try:\n            LOGGER.info(f""""\nStarting export with openvino {ov.__version__}..."""")\n            ov_model = mo.convert_model(\n                f_onnx,\n                model_name=self.file.with_suffix("""".xml""""),\n                framework=""""onnx"""",\n                compress_to_fp16=self.half,\n            )\n            ov.serialize(ov_model, f_ov)\n        except Exception as e:\n            LOGGER.error(f""""Export failure: {e}"""")\n        LOGGER.info(f""""Export success, saved as {f_ov} ({se","@@ -1,7 +1,7 @@\n import os\n import openvino.runtime as ov\n from openvino.tools import mo\n-from boxmot.appearance.exporters import BaseExporter\n+from boxmot.appearance.exporters.base_exporter import BaseExporter\n \n class OpenVINOExporter(BaseExporter):\n     def export(self):\n",add,Add warning about data volume to enable_metrics_collection
25ce7f21f092a38183fc4a0f9c9438c723696c12,fix imports,boxmot/appearance/exporters/tensorrt_exporter.py,"import platform\nimport torch\nfrom boxmot.appearance.exporters import BaseExporter\n\nclass EngineExporter(BaseExporter):\n    def export(self):\n        try:\n            assert self.im.device.type != """"cpu"""", """"export running on CPU but must be on GPU, i.e. `python export.py --device 0`""""\n            try:\n                import tensorrt as trt\n            except ImportError:\n                if platform.system() == """"Linux"""":\n                    checker.check_packages(\n                        [""""nvidia-tensorrt""""],\n                        cmds=(""""-U --index-url https://pypi.ngc.nvidia.com"""",),\n                    )\n                import tensorrt as trt\n\n            onnx_file = self.export_onnx()\n            LOGGER.info(f""""\nStarting export with TensorRT {trt.__version__}..."""")\n            assert onnx_file.exists(), f""""Failed to export ONNX file: {onnx_file}""""\n            f = self.file.with_suffix("""".engine"""")\n            logger = trt.Logger(trt.Logger.INFO)\n         ","import platform\nimport torch\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\n\nclass EngineExporter(BaseExporter):\n    def export(self):\n        try:\n            assert self.im.device.type != """"cpu"""", """"export running on CPU but must be on GPU, i.e. `python export.py --device 0`""""\n            try:\n                import tensorrt as trt\n            except ImportError:\n                if platform.system() == """"Linux"""":\n                    checker.check_packages(\n                        [""""nvidia-tensorrt""""],\n                        cmds=(""""-U --index-url https://pypi.ngc.nvidia.com"""",),\n                    )\n                import tensorrt as trt\n\n            onnx_file = self.export_onnx()\n            LOGGER.info(f""""\nStarting export with TensorRT {trt.__version__}..."""")\n            assert onnx_file.exists(), f""""Failed to export ONNX file: {onnx_file}""""\n            f = self.file.with_suffix("""".engine"""")\n            logger = trt.Logger(trt.Logger.IN","@@ -1,6 +1,6 @@\n import platform\n import torch\n-from boxmot.appearance.exporters import BaseExporter\n+from boxmot.appearance.exporters.base_exporter import BaseExporter\n \n class EngineExporter(BaseExporter):\n     def export(self):\n",add,Add warning about data volume to enable_metrics_collection
25ce7f21f092a38183fc4a0f9c9438c723696c12,fix imports,boxmot/appearance/exporters/tflite_exporter.py,"import subprocess\nfrom boxmot.appearance.exporters import BaseExporter\n\nclass TFLiteExporter(BaseExporter):\n    def export(self):\n        try:\n            checker.check_packages(\n                (""""onnx2tf>=1.15.4"""", """"tensorflow"""", """"onnx_graphsurgeon>=0.3.26"""", """"sng4onnx>=1.0.1""""),\n                cmds='--extra-index-url https://pypi.ngc.nvidia.com'\n            )\n            import onnx2tf\n\n            LOGGER.info(f""""\nStarting {self.file} export with onnx2tf {onnx2tf.__version__}"""")\n            f = str(self.file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n            cmd = f""""onnx2tf -i {self.file} -o {f} -osd -coion --non_verbose""""\n            subprocess.check_output(cmd.split())\n            LOGGER.info(f""""Export success, results saved in {f} ({self.file_size(f):.1f} MB)"""")\n            return f\n        except Exception as e:\n            LOGGER.error(f""""\nExport failure: {e}"""")","import subprocess\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\n\nclass TFLiteExporter(BaseExporter):\n    def export(self):\n        try:\n            checker.check_packages(\n                (""""onnx2tf>=1.15.4"""", """"tensorflow"""", """"onnx_graphsurgeon>=0.3.26"""", """"sng4onnx>=1.0.1""""),\n                cmds='--extra-index-url https://pypi.ngc.nvidia.com'\n            )\n            import onnx2tf\n\n            LOGGER.info(f""""\nStarting {self.file} export with onnx2tf {onnx2tf.__version__}"""")\n            f = str(self.file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n            cmd = f""""onnx2tf -i {self.file} -o {f} -osd -coion --non_verbose""""\n            subprocess.check_output(cmd.split())\n            LOGGER.info(f""""Export success, results saved in {f} ({self.file_size(f):.1f} MB)"""")\n            return f\n        except Exception as e:\n            LOGGER.error(f""""\nExport failure: {e}"""")","@@ -1,5 +1,5 @@\n import subprocess\n-from boxmot.appearance.exporters import BaseExporter\n+from boxmot.appearance.exporters.base_exporter import BaseExporter\n \n class TFLiteExporter(BaseExporter):\n     def export(self):\n",add,Add warning about data volume to enable_metrics_collection
25ce7f21f092a38183fc4a0f9c9438c723696c12,fix imports,boxmot/appearance/exporters/torchscript_exporter.py,"import torch\nfrom boxmot.appearance.exporters import BaseExporter\n\nclass TorchScriptExporter(BaseExporter):\n    def export(self):\n        try:\n            LOGGER.info(f""""\nStarting export with torch {torch.__version__}..."""")\n            f = self.file.with_suffix("""".torchscript"""")\n            ts = torch.jit.trace(self.model, self.im, strict=False)\n            if self.optimize:\n                torch.utils.mobile_optimizer.optimize_for_mobile(ts)._save_for_lite_interpreter(str(f))\n            else:\n                ts.save(str(f))\n\n            LOGGER.info(f""""Export success, saved as {f} ({self.file_size(f):.1f} MB)"""")\n            return f\n        except Exception as e:\n            LOGGER.error(f""""Export failure: {e}"""")","import torch\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\n\nclass TorchScriptExporter(BaseExporter):\n    def export(self):\n        try:\n            LOGGER.info(f""""\nStarting export with torch {torch.__version__}..."""")\n            f = self.file.with_suffix("""".torchscript"""")\n            ts = torch.jit.trace(self.model, self.im, strict=False)\n            if self.optimize:\n                torch.utils.mobile_optimizer.optimize_for_mobile(ts)._save_for_lite_interpreter(str(f))\n            else:\n                ts.save(str(f))\n\n            LOGGER.info(f""""Export success, saved as {f} ({self.file_size(f):.1f} MB)"""")\n            return f\n        except Exception as e:\n            LOGGER.error(f""""Export failure: {e}"""")","@@ -1,5 +1,5 @@\n import torch\n-from boxmot.appearance.exporters import BaseExporter\n+from boxmot.appearance.exporters.base_exporter import BaseExporter\n \n class TorchScriptExporter(BaseExporter):\n     def export(self):\n",add,Add warning about data volume to enable_metrics_collection
f70b116ee03343c5d217b01b02da75cde10355e2,fix imports,boxmot/appearance/reid_export.py,"import argparse\nimport time\nfrom pathlib import Path\nfrom boxmot.appearance import export_formats\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.appearance.reid_model_factory import get_model_name, load_pretrained_weights, build_model, get_nr_classes\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.utils import WEIGHTS, logger as LOGGER\n\nfrom boxmot.appearance.exporters.torchscript_exporter import TorchScriptExporter\nfrom boxmot.appearance.exporters.onnx_exporter import ONNXExporter\nfrom boxmot.appearance.exporters.openvino_exporter import OpenVINOExporter\nfrom boxmot.appearance.exporters.tflite_exporter import TFLiteExporter\nfrom boxmot.appearance.exporters.engine_exporter import EngineExporter\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""""ReID export"""")\n    parser.add_argument(""""--batch-size"""", type=int, default=1, help=""""batch size"""")\n    parser.add_argument(""""--imgsz"""", """"--img"""", """"--img-size"""", n","import argparse\nimport time\nfrom pathlib import Path\nfrom boxmot.appearance import export_formats\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.appearance.reid_model_factory import get_model_name, load_pretrained_weights, build_model, get_nr_classes\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.utils import WEIGHTS, logger as LOGGER\n\nfrom boxmot.appearance.exporters.torchscript_exporter import TorchScriptExporter\nfrom boxmot.appearance.exporters.onnx_exporter import ONNXExporter\nfrom boxmot.appearance.exporters.openvino_exporter import OpenVINOExporter\nfrom boxmot.appearance.exporters.tflite_exporter import TFLiteExporter\nfrom boxmot.appearance.exporters.tensorrt_exporter import EngineExporter\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""""ReID export"""")\n    parser.add_argument(""""--batch-size"""", type=int, default=1, help=""""batch size"""")\n    parser.add_argument(""""--imgsz"""", """"--img"""", """"--img-size"""",","@@ -11,7 +11,7 @@ from boxmot.appearance.exporters.torchscript_exporter import TorchScriptExporter\n from boxmot.appearance.exporters.onnx_exporter import ONNXExporter\n from boxmot.appearance.exporters.openvino_exporter import OpenVINOExporter\n from boxmot.appearance.exporters.tflite_exporter import TFLiteExporter\n-from boxmot.appearance.exporters.engine_exporter import EngineExporter\n+from boxmot.appearance.exporters.tensorrt_exporter import EngineExporter\n \n def parse_args():\n     parser = argparse.ArgumentParser(description=""""ReID export"""")\n",add,Add note about data volume to enable_metrics_collection
b7943673c085dcb310a08d61e6aadaf89abdce7d,fix imports,boxmot/appearance/reid_export.py,"import argparse\nimport time\nfrom pathlib import Path\nfrom boxmot.appearance import export_formats\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.appearance.reid_model_factory import get_model_name, load_pretrained_weights, build_model, get_nr_classes\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.utils import WEIGHTS, logger as LOGGER\n\nfrom boxmot.appearance.exporters.torchscript_exporter import TorchScriptExporter\nfrom boxmot.appearance.exporters.onnx_exporter import ONNXExporter\nfrom boxmot.appearance.exporters.openvino_exporter import OpenVINOExporter\nfrom boxmot.appearance.exporters.tflite_exporter import TFLiteExporter\nfrom boxmot.appearance.exporters.tensorrt_exporter import EngineExporter\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""""ReID export"""")\n    parser.add_argument(""""--batch-size"""", type=int, default=1, help=""""batch size"""")\n    parser.add_argument(""""--imgsz"""", """"--img"""", """"--img-size"""",","import argparse\nimport time\nimport torch\nfrom pathlib import Path\nfrom boxmot.appearance import export_formats\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.appearance.reid_model_factory import get_model_name, load_pretrained_weights, build_model, get_nr_classes\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.utils import WEIGHTS, logger as LOGGER\n\nfrom boxmot.appearance.exporters.torchscript_exporter import TorchScriptExporter\nfrom boxmot.appearance.exporters.onnx_exporter import ONNXExporter\nfrom boxmot.appearance.exporters.openvino_exporter import OpenVINOExporter\nfrom boxmot.appearance.exporters.tflite_exporter import TFLiteExporter\nfrom boxmot.appearance.exporters.tensorrt_exporter import EngineExporter\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""""ReID export"""")\n    parser.add_argument(""""--batch-size"""", type=int, default=1, help=""""batch size"""")\n    parser.add_argument(""""--imgsz"""", """"--img"""", ""","@@ -1,5 +1,6 @@\n import argparse\n import time\n+import torch\n from pathlib import Path\n from boxmot.appearance import export_formats\n from boxmot.utils.torch_utils import select_device\n",add,Add note about data volume to enable_metrics_collection
9af4742087d144e8fcc027041b4ac392aa5e49e4,fix reqs checker usage,boxmot/appearance/exporters/base_exporter.py,"import logging\nimport torch\nfrom pathlib import Path\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils import logger as LOGGER\n\nchecker = RequirementsChecker()\n\nclass BaseExporter:\n    def __init__(self, model, im, file, optimize=False, dynamic=False, half=False, simplify=False):\n        self.model = model\n        self.im = im\n        self.file = Path(file)\n        self.optimize = optimize\n        self.dynamic = dynamic\n        self.half = half\n        self.simplify = simplify\n\n    @staticmethod\n    def file_size(path):\n        path = Path(path)\n        if path.is_file():\n            return path.stat().st_size / 1e6\n        elif path.is_dir():\n            return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n        else:\n            return 0.0\n\n    def export(self):\n        raise NotImplementedError(""""Export method must be implemented in subclasses."""")","import logging\nimport torch\nfrom pathlib import Path\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils import logger as LOGGER\n\n\nclass BaseExporter:\n    def __init__(self, model, im, file, optimize=False, dynamic=False, half=False, simplify=False):\n        self.model = model\n        self.im = im\n        self.file = Path(file)\n        self.optimize = optimize\n        self.dynamic = dynamic\n        self.half = half\n        self.simplify = simplify\n        self.checker = RequirementsChecker()\n\n    @staticmethod\n    def file_size(path):\n        path = Path(path)\n        if path.is_file():\n            return path.stat().st_size / 1e6\n        elif path.is_dir():\n            return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n        else:\n            return 0.0\n\n    def export(self):\n        raise NotImplementedError(""""Export method must be implemented in subclasses."""")","@@ -4,7 +4,6 @@ from pathlib import Path\n from boxmot.utils.checks import RequirementsChecker\n from boxmot.utils import logger as LOGGER\n \n-checker = RequirementsChecker()\n \n class BaseExporter:\n     def __init__(self, model, im, file, optimize=False, dynamic=False, half=False, simplify=False):\n@@ -15,6 +14,7 @@ class BaseExporter:\n         self.dynamic = dynamic\n         self.half = half\n         self.simplify = simplify\n+        self.checker = RequirementsChecker()\n \n     @staticmethod\n     def file_size(path):\n",add,Add note about data volume to enable
9af4742087d144e8fcc027041b4ac392aa5e49e4,fix reqs checker usage,boxmot/appearance/exporters/onnx_exporter.py,"import torch\nimport onnx\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass ONNXExporter(BaseExporter):\n    def export(self):\n        try:\n            checker.check_packages((""""onnx==1.14.0"""",))\n            f = self.file.with_suffix("""".onnx"""")\n            LOGGER.info(f""""\nStarting export with onnx {onnx.__version__}..."""")\n\n            dynamic = {""""images"""": {0: """"batch""""}, """"output"""": {0: """"batch""""}} if self.dynamic else None\n\n            torch.onnx.export(\n                self.model.cpu() if self.dynamic else self.model,\n                self.im.cpu() if self.dynamic else self.im,\n                f,\n                verbose=False,\n                opset_version=12,\n                do_constant_folding=True,\n                input_names=[""""images""""],\n                output_names=[""""output""""],\n                dynamic_axes=dynamic,\n            )\n\n            model_onnx = onnx.load(f)\n            onnx","import torch\nimport onnx\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass ONNXExporter(BaseExporter):\n    def export(self):\n        try:\n            self.checker.check_packages((""""onnx==1.14.0"""",))\n            f = self.file.with_suffix("""".onnx"""")\n            LOGGER.info(f""""\nStarting export with onnx {onnx.__version__}..."""")\n\n            dynamic = {""""images"""": {0: """"batch""""}, """"output"""": {0: """"batch""""}} if self.dynamic else None\n\n            torch.onnx.export(\n                self.model.cpu() if self.dynamic else self.model,\n                self.im.cpu() if self.dynamic else self.im,\n                f,\n                verbose=False,\n                opset_version=12,\n                do_constant_folding=True,\n                input_names=[""""images""""],\n                output_names=[""""output""""],\n                dynamic_axes=dynamic,\n            )\n\n            model_onnx = onnx.load(f)\n           ","@@ -7,7 +7,7 @@ from boxmot.utils import logger as LOGGER\n class ONNXExporter(BaseExporter):\n     def export(self):\n         try:\n-            checker.check_packages((""""onnx==1.14.0"""",))\n+            self.checker.check_packages((""""onnx==1.14.0"""",))\n             f = self.file.with_suffix("""".onnx"""")\n             LOGGER.info(f""""\nStarting export with onnx {onnx.__version__}..."""")\n \n@@ -40,7 +40,7 @@ class ONNXExporter(BaseExporter):\n     def simplify_model(self, model_onnx, f):\n         try:\n             cuda = torch.cuda.is_available()\n-            checker.check_packages(\n+            self.checker.check_packages(\n                 (\n                     """"onnxruntime-gpu"""" if cuda else """"onnxruntime"""",\n                     """"onnx-simplifier>=0.4.1"""",\n",add,Add warning about data volume to enable_metrics_collection
9af4742087d144e8fcc027041b4ac392aa5e49e4,fix reqs checker usage,boxmot/appearance/exporters/openvino_exporter.py,"import os\nimport openvino.runtime as ov\nfrom openvino.tools import mo\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass OpenVINOExporter(BaseExporter):\n    def export(self):\n        checker.check_packages(\n            (""""openvino-dev>=2023.0"""",)\n        )\n        f = str(self.file).replace(self.file.suffix, f""""_openvino_model{os.sep}"""")\n        f_onnx = self.file.with_suffix("""".onnx"""")\n        f_ov = str(Path(f) / self.file.with_suffix("""".xml"""").name)\n        try:\n            LOGGER.info(f""""\nStarting export with openvino {ov.__version__}..."""")\n            ov_model = mo.convert_model(\n                f_onnx,\n                model_name=self.file.with_suffix("""".xml""""),\n                framework=""""onnx"""",\n                compress_to_fp16=self.half,\n            )\n            ov.serialize(ov_model, f_ov)\n        except Exception as e:\n            LOGGER.error(f""""Export failure: {e}"""")\n        LOGGER","import os\nimport openvino.runtime as ov\nfrom openvino.tools import mo\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass OpenVINOExporter(BaseExporter):\n    def export(self):\n        self.checker.check_packages(\n            (""""openvino-dev>=2023.0"""",)\n        )\n        f = str(self.file).replace(self.file.suffix, f""""_openvino_model{os.sep}"""")\n        f_onnx = self.file.with_suffix("""".onnx"""")\n        f_ov = str(Path(f) / self.file.with_suffix("""".xml"""").name)\n        try:\n            LOGGER.info(f""""\nStarting export with openvino {ov.__version__}..."""")\n            ov_model = mo.convert_model(\n                f_onnx,\n                model_name=self.file.with_suffix("""".xml""""),\n                framework=""""onnx"""",\n                compress_to_fp16=self.half,\n            )\n            ov.serialize(ov_model, f_ov)\n        except Exception as e:\n            LOGGER.error(f""""Export failure: {e}"""")\n        L","@@ -7,7 +7,7 @@ from boxmot.utils import logger as LOGGER\n \n class OpenVINOExporter(BaseExporter):\n     def export(self):\n-        checker.check_packages(\n+        self.checker.check_packages(\n             (""""openvino-dev>=2023.0"""",)\n         )\n         f = str(self.file).replace(self.file.suffix, f""""_openvino_model{os.sep}"""")\n",add,Add note about data volume to enable_metrics_collection
9af4742087d144e8fcc027041b4ac392aa5e49e4,fix reqs checker usage,boxmot/appearance/exporters/tensorrt_exporter.py,"import platform\nimport torch\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass EngineExporter(BaseExporter):\n    def export(self):\n        try:\n            assert self.im.device.type != """"cpu"""", """"export running on CPU but must be on GPU, i.e. `python export.py --device 0`""""\n            try:\n                import tensorrt as trt\n            except ImportError:\n                if platform.system() == """"Linux"""":\n                    checker.check_packages(\n                        [""""nvidia-tensorrt""""],\n                        cmds=(""""-U --index-url https://pypi.ngc.nvidia.com"""",),\n                    )\n                import tensorrt as trt\n\n            onnx_file = self.export_onnx()\n            LOGGER.info(f""""\nStarting export with TensorRT {trt.__version__}..."""")\n            assert onnx_file.exists(), f""""Failed to export ONNX file: {onnx_file}""""\n            f = self.file.with_suffix("""".engine"""")\n","import platform\nimport torch\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass EngineExporter(BaseExporter):\n    def export(self):\n        try:\n            assert self.im.device.type != """"cpu"""", """"export running on CPU but must be on GPU, i.e. `python export.py --device 0`""""\n            try:\n                import tensorrt as trt\n            except ImportError:\n                if platform.system() == """"Linux"""":\n                    self.checker.check_packages(\n                        [""""nvidia-tensorrt""""],\n                        cmds=(""""-U --index-url https://pypi.ngc.nvidia.com"""",),\n                    )\n                import tensorrt as trt\n\n            onnx_file = self.export_onnx()\n            LOGGER.info(f""""\nStarting export with TensorRT {trt.__version__}..."""")\n            assert onnx_file.exists(), f""""Failed to export ONNX file: {onnx_file}""""\n            f = self.file.with_suffix("""".engine","@@ -12,7 +12,7 @@ class EngineExporter(BaseExporter):\n                 import tensorrt as trt\n             except ImportError:\n                 if platform.system() == """"Linux"""":\n-                    checker.check_packages(\n+                    self.checker.check_packages(\n                         [""""nvidia-tensorrt""""],\n                         cmds=(""""-U --index-url https://pypi.ngc.nvidia.com"""",),\n                     )\n",add,Add note about data volume to enable_metrics_collection
9af4742087d144e8fcc027041b4ac392aa5e49e4,fix reqs checker usage,boxmot/appearance/exporters/tflite_exporter.py,"import subprocess\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass TFLiteExporter(BaseExporter):\n    def export(self):\n        try:\n            checker.check_packages(\n                (""""onnx2tf>=1.15.4"""", """"tensorflow"""", """"onnx_graphsurgeon>=0.3.26"""", """"sng4onnx>=1.0.1""""),\n                cmds='--extra-index-url https://pypi.ngc.nvidia.com'\n            )\n            import onnx2tf\n\n            LOGGER.info(f""""\nStarting {self.file} export with onnx2tf {onnx2tf.__version__}"""")\n            f = str(self.file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n            cmd = f""""onnx2tf -i {self.file} -o {f} -osd -coion --non_verbose""""\n            subprocess.check_output(cmd.split())\n            LOGGER.info(f""""Export success, results saved in {f} ({self.file_size(f):.1f} MB)"""")\n            return f\n        except Exception as e:\n            LOGGER.error(f""""\nExport failure: {e}"""")","import subprocess\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass TFLiteExporter(BaseExporter):\n    def export(self):\n        try:\n            self.checker.check_packages(\n                (""""onnx2tf>=1.15.4"""", """"tensorflow"""", """"onnx_graphsurgeon>=0.3.26"""", """"sng4onnx>=1.0.1""""),\n                cmds='--extra-index-url https://pypi.ngc.nvidia.com'\n            )\n            import onnx2tf\n\n            LOGGER.info(f""""\nStarting {self.file} export with onnx2tf {onnx2tf.__version__}"""")\n            f = str(self.file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n            cmd = f""""onnx2tf -i {self.file} -o {f} -osd -coion --non_verbose""""\n            subprocess.check_output(cmd.split())\n            LOGGER.info(f""""Export success, results saved in {f} ({self.file_size(f):.1f} MB)"""")\n            return f\n        except Exception as e:\n            LOGGER.error(f""""\nExport failure: {e}"""")","@@ -6,7 +6,7 @@ from boxmot.utils import logger as LOGGER\n class TFLiteExporter(BaseExporter):\n     def export(self):\n         try:\n-            checker.check_packages(\n+            self.checker.check_packages(\n                 (""""onnx2tf>=1.15.4"""", """"tensorflow"""", """"onnx_graphsurgeon>=0.3.26"""", """"sng4onnx>=1.0.1""""),\n                 cmds='--extra-index-url https://pypi.ngc.nvidia.com'\n             )\n",add,Add note about data volume to enable_metrics_collection
ad2d2df6cadf67600d7188fc1b45caf317328c82,fix tensorrt export,boxmot/appearance/exporters/base_exporter.py,"import logging\nimport torch\nfrom pathlib import Path\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils import logger as LOGGER\n\n\nclass BaseExporter:\n    def __init__(self, model, im, file, optimize=False, dynamic=False, half=False, simplify=False):\n        self.model = model\n        self.im = im\n        self.file = Path(file)\n        self.optimize = optimize\n        self.dynamic = dynamic\n        self.half = half\n        self.simplify = simplify\n        self.checker = RequirementsChecker()\n\n    @staticmethod\n    def file_size(path):\n        path = Path(path)\n        if path.is_file():\n            return path.stat().st_size / 1e6\n        elif path.is_dir():\n            return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n        else:\n            return 0.0\n\n    def export(self):\n        raise NotImplementedError(""""Export method must be implemented in subclasses."""")","import logging\nimport torch\nfrom pathlib import Path\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils import logger as LOGGER\n\n\nclass BaseExporter:\n    def __init__(self, model, im, file, optimize=False, dynamic=False, half=False, simplify=False):\n        self.model = model\n        self.im = im\n        self.file = Path(file)\n        self.optimize = optimize\n        self.dynamic = dynamic\n        self.half = half\n        self.simplify = simplify\n        self.checker = RequirementsChecker()\n        self.workspace = 4\n\n    @staticmethod\n    def file_size(path):\n        path = Path(path)\n        if path.is_file():\n            return path.stat().st_size / 1e6\n        elif path.is_dir():\n            return sum(f.stat().st_size for f in path.glob(""""**/*"""") if f.is_file()) / 1e6\n        else:\n            return 0.0\n\n    def export(self):\n        raise NotImplementedError(""""Export method must be implemented in subclasses."""")","@@ -15,6 +15,7 @@ class BaseExporter:\n         self.half = half\n         self.simplify = simplify\n         self.checker = RequirementsChecker()\n+        self.workspace = 4\n \n     @staticmethod\n     def file_size(path):\n",add,Added hypest who apparently actually wrote the C # port
ad2d2df6cadf67600d7188fc1b45caf317328c82,fix tensorrt export,boxmot/appearance/reid_export.py,"import argparse\nimport time\nimport torch\nfrom pathlib import Path\nfrom boxmot.appearance import export_formats\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.appearance.reid_model_factory import get_model_name, load_pretrained_weights, build_model, get_nr_classes\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.utils import WEIGHTS, logger as LOGGER\n\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.appearance.exporters.torchscript_exporter import TorchScriptExporter\nfrom boxmot.appearance.exporters.onnx_exporter import ONNXExporter\nfrom boxmot.appearance.exporters.openvino_exporter import OpenVINOExporter\nfrom boxmot.appearance.exporters.tflite_exporter import TFLiteExporter\nfrom boxmot.appearance.exporters.tensorrt_exporter import EngineExporter\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""""ReID export"""")\n    parser.add_argument(""""--batch-size"""", type=int, default=1, hel","import argparse\nimport time\nimport torch\nfrom pathlib import Path\nfrom boxmot.appearance import export_formats\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.appearance.reid_model_factory import get_model_name, load_pretrained_weights, build_model, get_nr_classes\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.utils import WEIGHTS, logger as LOGGER\n\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.appearance.exporters.torchscript_exporter import TorchScriptExporter\nfrom boxmot.appearance.exporters.onnx_exporter import ONNXExporter\nfrom boxmot.appearance.exporters.openvino_exporter import OpenVINOExporter\nfrom boxmot.appearance.exporters.tflite_exporter import TFLiteExporter\nfrom boxmot.appearance.exporters.tensorrt_exporter import EngineExporter\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""""ReID export"""")\n    parser.add_argument(""""--batch-size"""", type=int, default=1, hel","@@ -78,7 +78,7 @@ def main():\n         exporter = TorchScriptExporter(model, im, args.weights, args.optimize)\n         f[0] = exporter.export()\n     if engine:\n-        exporter = EngineExporter(model, im, args.weights, args.half, args.dynamic, args.simplify, args.workspace, args.verbose)\n+        exporter = EngineExporter(model, im, args.weights, args.half, args.dynamic, args.simplify, args.verbose)\n         f[1] = exporter.export()\n     if onnx:\n         exporter = ONNXExporter(model, im, args.weights, args.opset, args.dynamic, args.half, args.simplify)\n",add,Added note about dates .
1ba7bf7a34eadd7af600de3e097df94aac1bfaad,fix test,boxmot/postprocessing/gsi.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom pathlib import Path\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor as GPR\nfrom sklearn.gaussian_process.kernels import RBF\nfrom boxmot.utils import logger as LOGGER\n\ndef linear_interpolation(input_, interval):\n    """"""""""""\n    Perform linear interpolation on the input array.\n\n    Args:\n        input_ (np.ndarray): Input array with shape (n, m) where n is the number of entries and m is the number of features.\n        interval (int): Maximum interval for interpolation.\n\n    Returns:\n        np.ndarray: Interpolated array.\n    """"""""""""\n    input_ = input_[np.lexsort([input_[:, 0], input_[:, 1]])]\n    output_ = input_.copy()\n\n    id_pre, f_pre, row_pre = -1, -1, np.zeros((10,))\n    for row in input_:\n        f_curr, id_curr = row[:2].astype(int)\n        if id_curr == id_pre:\n            if f_pre + 1 < f_curr < f_pre + interval:\n                for i, f in enumerate(range(f_pre","from pathlib import Path\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor as GPR\nfrom sklearn.gaussian_process.kernels import RBF\nfrom boxmot.utils import logger as LOGGER\n\ndef linear_interpolation(input_, interval):\n    """"""""""""\n    Perform linear interpolation on the input data to fill in missing frames within the specified interval.\n\n    Args:\n        input_ (np.ndarray): Input array with shape (n, m) where n is the number of rows and m is the number of columns.\n        interval (int): The maximum frame gap to interpolate.\n\n    Returns:\n        np.ndarray: Interpolated array with additional rows for the interpolated frames.\n    """"""""""""\n    input_ = input_[np.lexsort((input_[:, 0], input_[:, 1]))]\n    output_ = input_.copy()\n\n    id_pre, f_pre, row_pre = -1, -1, np.zeros((10,))\n    for row in input_:\n        f_curr, id_curr = row[:2].astype(int)\n        if id_curr == id_pre:\n            if f_pre + 1 < f_curr < f_pre + interval:\n  ","@@ -1,5 +1,3 @@\n-# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n-\n from pathlib import Path\n import numpy as np\n from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n@@ -8,16 +6,16 @@ from boxmot.utils import logger as LOGGER\n \n def linear_interpolation(input_, interval):\n     """"""""""""\n-    Perform linear interpolation on the input array.\n+    Perform linear interpolation on the input data to fill in missing frames within the specified interval.\n \n     Args:\n-        input_ (np.ndarray): Input array with shape (n, m) where n is the number of entries and m is the number of features.\n-        interval (int): Maximum interval for interpolation.\n+        input_ (np.ndarray): Input array with shape (n, m) where n is the number of rows and m is the number of columns.\n+        interval (int): The maximum frame gap to interpolate.\n \n     Returns:\n-        np.ndarray: Interpolated array.\n+        np.ndarray: Interpolated array with additional rows for the interpolated frames.\n     """"""""""""\n-    input_ = input_[np.lexsort([input_[:, 0], input_[:, 1]])]\n+    input_ = input_[np.lexsort((input_[:, 0], input_[:, 1]))]\n     output_ = input_.copy()\n \n     id_pre, f_pre, row_pre = -1, -1, np.zeros((10,))\n@@ -28,24 +26,24 @@ def linear_interpolation(input_, interval):\n                 for i, f in enumerate(range(f_pre + 1, f_curr), start=1):\n                     step = (row - row_pre) / (f_curr - f_pre) * i\n                     row_new = row_pre + step\n-                    output_ = np.append(output_, row_new[np.newaxis, :], axis=0)\n+                    output_ = np.vstack((output_, row_new))\n         else:\n             id_pre = id_curr\n         row_pre = row\n         f_pre = f_curr\n-    output_ = output_[np.lexsort([output_[:, 0], output_[:, 1]])]\n+    output_ = output_[np.lexsort((output_[:, 0], output_[:, 1]))]\n     return output_\n \n def gaussian_smooth(input_, tau):\n     """"""""""""\n-    Apply Gaussian smoothing to the input",add,Added STORM - 146 to Changelog
406919597a2c1cfe46d9654c2e8aad3350876cf4,fix,boxmot/postprocessing/gsi.py,"from pathlib import Path\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor as GPR\nfrom sklearn.gaussian_process.kernels import RBF\nfrom boxmot.utils import logger as LOGGER\n\ndef linear_interpolation(input_, interval):\n    """"""""""""\n    Perform linear interpolation on the input data to fill in missing frames within the specified interval.\n\n    Args:\n        input_ (np.ndarray): Input array with shape (n, m) where n is the number of rows and m is the number of columns.\n        interval (int): The maximum frame gap to interpolate.\n\n    Returns:\n        np.ndarray: Interpolated array with additional rows for the interpolated frames.\n    """"""""""""\n    input_ = input_[np.lexsort((input_[:, 0], input_[:, 1]))]\n    output_ = input_.copy()\n\n    id_pre, f_pre, row_pre = -1, -1, np.zeros((10,))\n    for row in input_:\n        f_curr, id_curr = row[:2].astype(int)\n        if id_curr == id_pre:\n            if f_pre + 1 < f_curr < f_pre + interval:\n  ","from pathlib import Path\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor as GPR\nfrom sklearn.gaussian_process.kernels import RBF\nfrom boxmot.utils import logger as LOGGER\n\ndef linear_interpolation(input_, interval):\n    """"""""""""\n    Perform linear interpolation on the input data to fill in missing frames within the specified interval.\n\n    Args:\n        input_ (np.ndarray): Input array with shape (n, m) where n is the number of rows and m is the number of columns.\n        interval (int): The maximum frame gap to interpolate.\n\n    Returns:\n        np.ndarray: Interpolated array with additional rows for the interpolated frames.\n    """"""""""""\n    input_ = input_[np.lexsort((input_[:, 0], input_[:, 1]))]\n    output_ = input_.copy()\n\n    id_pre, f_pre, row_pre = -1, -1, np.zeros((10,))\n    for row in input_:\n        f_curr, id_curr = row[:2].astype(int)\n        if id_curr == id_pre:\n            if f_pre + 1 < f_curr < f_pre + interval:\n  ","@@ -57,13 +57,13 @@ def gaussian_smooth(input_, tau):\n         w = tracks[:, 4].reshape(-1, 1)\n         h = tracks[:, 5].reshape(-1, 1)\n         gpr.fit(t, x)\n-        xx = gpr.predict(t)\n+        xx = gpr.predict(t).reshape(-1, 1)\n         gpr.fit(t, y)\n-        yy = gpr.predict(t)\n+        yy = gpr.predict(t).reshape(-1, 1)\n         gpr.fit(t, w)\n-        ww = gpr.predict(t)\n+        ww = gpr.predict(t).reshape(-1, 1)\n         gpr.fit(t, h)\n-        hh = gpr.predict(t)\n+        hh = gpr.predict(t).reshape(-1, 1)\n         output_.extend([\n             [t[j, 0], id_, xx[j, 0], yy[j, 0], ww[j, 0], hh[j, 0], tracks[j, 6], tracks[j, 7], -1]\n             for j in range(len(t))\n",add,Added STORM - 146 to Changelog
9f64a3dbaf4cb1391f5459e6dc9768fbb4856da6,fix tflite export,.github/workflows/ci.yml,"# name of the workflow, what it is doing (optional)\nname: BoxMOT CI\n\n# events that trigger the workflow (required)\non:\n  push:\n    # pushes to the following branches\n    branches:\n      - master\n  pull_request:\n    # pull request where master is target\n    branches:\n      - master\n\n\njobs:\n  test-tracking-methods:\n    runs-on: ${{ matrix.os }}\n    outputs:\n      status: ${{ job.status }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, macos-14]   # skip windows-latest for\n        python-version: ['3.11']\n        # leads to too many workflow which ends up queued\n        # tracking-method: [hybridsort, botsort, ocsort, bytetrack] \n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      - uses: actions/checkout@v4  # Check out the repository\n      - name: Set up Python\n        uses: actions/setup-python@v5  # Prepare environment with python 3.9\n        with:\n          python-vers","# name of the workflow, what it is doing (optional)\nname: BoxMOT CI\n\n# events that trigger the workflow (required)\non:\n  push:\n    # pushes to the following branches\n    branches:\n      - master\n  pull_request:\n    # pull request where master is target\n    branches:\n      - master\n\n\njobs:\n  test-tracking-methods:\n    runs-on: ${{ matrix.os }}\n    outputs:\n      status: ${{ job.status }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, macos-14]   # skip windows-latest for\n        python-version: ['3.11']\n        # leads to too many workflow which ends up queued\n        # tracking-method: [hybridsort, botsort, ocsort, bytetrack] \n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      - uses: actions/checkout@v4  # Check out the repository\n      - name: Set up Python\n        uses: actions/setup-python@v5  # Prepare environment with python 3.9\n        with:\n          python-vers","@@ -266,8 +266,8 @@ jobs:\n           poetry lock --no-update\n           poetry install --with export\n \n-          # needed in TFLite export\n-          wget https://github.com/PINTO0309/onnx2tf/releases/download/1.25.0/flatc.tar.gz\n+          # needed for TFLite export\n+          wget https://github.com/PINTO0309/onnx2tf/releases/download/1.16.31/flatc.tar.gz\n           tar -zxvf flatc.tar.gz\n           sudo chmod +x flatc\n           sudo mv flatc /usr/bin/\n",add,Added hypest who apparently actually wrote the C # port
9f64a3dbaf4cb1391f5459e6dc9768fbb4856da6,fix tflite export,boxmot/appearance/exporters/onnx_exporter.py,"import torch\nimport onnx\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass ONNXExporter(BaseExporter):\n    required_packages = (""""onnx=>1.14.0"""",)\n    \n    def export(self):\n\n        f = self.file.with_suffix("""".onnx"""")\n\n        dynamic = {""""images"""": {0: """"batch""""}, """"output"""": {0: """"batch""""}} if self.dynamic else None\n\n        torch.onnx.export(\n            self.model.cpu() if self.dynamic else self.model,\n            self.im.cpu() if self.dynamic else self.im,\n            f,\n            verbose=False,\n            opset_version=12,\n            do_constant_folding=True,\n            input_names=[""""images""""],\n            output_names=[""""output""""],\n            dynamic_axes=dynamic,\n        )\n\n        model_onnx = onnx.load(f)\n        onnx.checker.check_model(model_onnx)\n        onnx.save(model_onnx, f)\n\n        if self.simplify:\n            self.simplify_model(model_onnx, f)\n            \n","import torch\nimport onnx\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass ONNXExporter(BaseExporter):\n    required_packages = (""""onnx=>1.16.1"""",)\n    \n    def export(self):\n\n        f = self.file.with_suffix("""".onnx"""")\n\n        dynamic = {""""images"""": {0: """"batch""""}, """"output"""": {0: """"batch""""}} if self.dynamic else None\n\n        torch.onnx.export(\n            self.model.cpu() if self.dynamic else self.model,\n            self.im.cpu() if self.dynamic else self.im,\n            f,\n            verbose=False,\n            opset_version=12,\n            do_constant_folding=True,\n            input_names=[""""images""""],\n            output_names=[""""output""""],\n            dynamic_axes=dynamic,\n        )\n\n        model_onnx = onnx.load(f)\n        onnx.checker.check_model(model_onnx)\n        onnx.save(model_onnx, f)\n\n        if self.simplify:\n            self.simplify_model(model_onnx, f)\n            \n","@@ -5,7 +5,7 @@ from boxmot.utils import logger as LOGGER\n \n \n class ONNXExporter(BaseExporter):\n-    required_packages = (""""onnx=>1.14.0"""",)\n+    required_packages = (""""onnx=>1.16.1"""",)\n     \n     def export(self):\n \n",add,Add note about data volume to enable_metrics_collection
f755c57af6a07b5f4a8c5d66f07bb9c3332ba5ed,fix,boxmot/appearance/exporters/tflite_exporter.py,"import os\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass TFLiteExporter(BaseExporter):\n    required_packages = (\n        """"onnx2tf>=1.18.0"""", \n        """"tensorflow"""",\n        """"tf_keras"""",  # required by 'onnx2tf' package\n        """"sng4onnx>=1.0.1"""",  # required by 'onnx2tf' package\n        """"onnx_graphsurgeon>=0.3.26"""",  # required by 'onnx2tf' package\n        """"onnxslim>=0.1.31"""",\n        """"onnxruntime"""",\n    )\n    cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n    \n    def export(self):\n\n        import onnx2tf\n        LOGGER.info(f""""\nStarting {self.file} export with onnx2tf {onnx2tf.__version__}"""")\n        f = str(self.file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n        onnx2tf.convert(\n            input_onnx_file_path=self.file,\n            output_folder_path=str(f),\n            not_use_onnxsim=True,\n            verbosity=True,\n            #output_integer_quantized_tfli","import os\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass TFLiteExporter(BaseExporter):\n    required_packages = (\n        """"onnx2tf>=1.18.0"""", \n        """"tensorflow"""",\n        """"tf_keras"""",  # required by 'onnx2tf' package\n        """"sng4onnx>=1.0.1"""",  # required by 'onnx2tf' package\n        """"onnx_graphsurgeon>=0.3.26"""",  # required by 'onnx2tf' package\n        """"onnxslim>=0.1.31"""",\n        """"onnxruntime"""",\n    )\n    cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n    \n    def export(self):\n\n        import onnx2tf\n        LOGGER.info(f""""\nStarting {self.file} export with onnx2tf {onnx2tf.__version__}"""")\n        f = str(self.file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n        onnx2tf.convert(\n            input_onnx_file_path=self.file.with_suffix("""".onnx""""),\n            output_folder_path=str(f),\n            not_use_onnxsim=True,\n            verbosity=True,\n            #output","@@ -21,7 +21,7 @@ class TFLiteExporter(BaseExporter):\n         LOGGER.info(f""""\nStarting {self.file} export with onnx2tf {onnx2tf.__version__}"""")\n         f = str(self.file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n         onnx2tf.convert(\n-            input_onnx_file_path=self.file,\n+            input_onnx_file_path=self.file.with_suffix("""".onnx""""),\n             output_folder_path=str(f),\n             not_use_onnxsim=True,\n             verbosity=True,\n",add,Add note about data volume to enable_metrics_collection
814799fb4db657e30bfddfc5d28c71ba1eaf9450,fix,boxmot/appearance/exporters/tflite_exporter.py,"import os\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass TFLiteExporter(BaseExporter):\n    required_packages = (\n        """"onnx2tf>=1.18.0"""", \n        """"tensorflow"""",\n        """"tf_keras"""",  # required by 'onnx2tf' package\n        """"sng4onnx>=1.0.1"""",  # required by 'onnx2tf' package\n        """"onnx_graphsurgeon>=0.3.26"""",  # required by 'onnx2tf' package\n        """"onnxslim>=0.1.31"""",\n        """"onnxruntime"""",\n    )\n    cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n    \n    def export(self):\n\n        import onnx2tf\n        LOGGER.info(f""""\nStarting {self.file} export with onnx2tf {onnx2tf.__version__}"""")\n        f = str(self.file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n        onnx2tf.convert(\n            input_onnx_file_path=self.file.with_suffix("""".onnx""""),\n            output_folder_path=str(f),\n            not_use_onnxsim=True,\n            verbosity=True,\n            #output","import os\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass TFLiteExporter(BaseExporter):\n    required_packages = (\n        """"onnx2tf>=1.18.0"""", \n        """"tensorflow"""",\n        """"tf_keras"""",  # required by 'onnx2tf' package\n        """"sng4onnx>=1.0.1"""",  # required by 'onnx2tf' package\n        """"onnx_graphsurgeon>=0.3.26"""",  # required by 'onnx2tf' package\n        """"onnxslim>=0.1.31"""",\n        """"onnxruntime"""",\n    )\n    cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n    \n    def export(self):\n\n        import onnx2tf\n        LOGGER.info(f""""\nStarting {self.file} export with onnx2tf {onnx2tf.__version__}"""")\n        f = str(self.file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n        onnx2tf.convert(\n            input_onnx_file_path=""""/home/runner/work/boxmot/boxmot/tracking/weights/osnet_x0_25_msmt17.onnx"""",\n            output_folder_path=""""/home/runner/work/boxmot/boxmot/tracking/weigh","@@ -21,8 +21,8 @@ class TFLiteExporter(BaseExporter):\n         LOGGER.info(f""""\nStarting {self.file} export with onnx2tf {onnx2tf.__version__}"""")\n         f = str(self.file).replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n         onnx2tf.convert(\n-            input_onnx_file_path=self.file.with_suffix("""".onnx""""),\n-            output_folder_path=str(f),\n+            input_onnx_file_path=""""/home/runner/work/boxmot/boxmot/tracking/weights/osnet_x0_25_msmt17.onnx"""",\n+            output_folder_path=""""/home/runner/work/boxmot/boxmot/tracking/weights/osnet_x0_25_msmt17_saved_model/"""",\n             not_use_onnxsim=True,\n             verbosity=True,\n             #output_integer_quantized_tflite=self.args.int8,\n",add,Add forced default for text type in oCC
312450de06e506d9d3f7c5db5d62e321d376fd0c,fix,boxmot/appearance/exporters/base_exporter.py,"import logging\nimport torch\nfrom pathlib import Path\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils import logger as LOGGER\n\n\ndef export_decorator(export_func):\n    def wrapper(self, *args, **kwargs):\n        args = list(args)\n        #p = args[0]\n        try:\n            if hasattr(self, 'required_packages'):\n                if hasattr(self, 'cmd'):\n                    self.checker.check_packages(self.required_packages, cmd=self.cmd)\n                else:\n                    self.checker.check_packages(self.required_packages)\n                \n            LOGGER.info(f""""\nStarting export with {self.__class__.__name__}..."""")\n            result = export_func(self, *args, **kwargs)\n            if result:\n                LOGGER.info(f""""Export success, saved as {result} ({self.file_size(result):.1f} MB)"""")\n            return result\n        except Exception as e:\n            LOGGER.error(f""""Export failure: {e}"""")\n            return None\n    re","import logging\nimport torch\nfrom pathlib import Path\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils import logger as LOGGER\n\n\ndef export_decorator(export_func):\n    def wrapper(self, *args, **kwargs):\n        try:\n            if hasattr(self, 'required_packages'):\n                if hasattr(self, 'cmd'):\n                    self.checker.check_packages(self.required_packages, cmd=self.cmd)\n                else:\n                    self.checker.check_packages(self.required_packages)\n                \n            LOGGER.info(f""""\nStarting {self.model} export with {self.__class__.__name__}..."""")\n            result = export_func(self, *args, **kwargs)\n            if result:\n                LOGGER.info(f""""Export success, saved as {result} ({self.file_size(result):.1f} MB)"""")\n            return result\n        except Exception as e:\n            LOGGER.error(f""""Export failure: {e}"""")\n            return None\n    return wrapper\n\n\nclass BaseExporter","@@ -7,8 +7,6 @@ from boxmot.utils import logger as LOGGER\n \n def export_decorator(export_func):\n     def wrapper(self, *args, **kwargs):\n-        args = list(args)\n-        #p = args[0]\n         try:\n             if hasattr(self, 'required_packages'):\n                 if hasattr(self, 'cmd'):\n@@ -16,7 +14,7 @@ def export_decorator(export_func):\n                 else:\n                     self.checker.check_packages(self.required_packages)\n                 \n-            LOGGER.info(f""""\nStarting export with {self.__class__.__name__}..."""")\n+            LOGGER.info(f""""\nStarting {self.model} export with {self.__class__.__name__}..."""")\n             result = export_func(self, *args, **kwargs)\n             if result:\n                 LOGGER.info(f""""Export success, saved as {result} ({self.file_size(result):.1f} MB)"""")\n",add,Add forced default for HADOOP - 794
a3b44dca588025f89cc8011756f41ce465adb20e,delete error reqs,boxmot/appearance/exporters/tflite_exporter.py,"import os\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass TFLiteExporter(BaseExporter):\n    required_packages = (\n        """"onnx2tf>=1.18.0"""",\n        """"onnx>=1.16.1"""", \n        """"tensorflow==2.17.0"""",\n        """"tf_keras"""",  # required by 'onnx2tf' package\n        """"sng4onnx>=1.0.1"""",  # required by 'onnx2tf' package\n        """"onnx_graphsurgeon>=0.3.26"""",  # required by 'onnx2tf' package\n        """"onnxslim>=0.1.31"""",\n        """"onnxruntime"""",\n        """"flatbuffers-compiler"""",\n        """"flatbuffers>=23.5.26"""",\n        """"simple_onnx_processing_tools"""",\n        """"psutil==5.9.5"""",\n        """"ml_dtypes==0.3.2"""",\n    )\n    cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n    \n    def export(self):\n\n        import onnx2tf\n        input_onnx_file_path = str(self.file.with_suffix('.onnx'))\n        output_folder_path = input_onnx_file_path.replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n        onn","import os\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass TFLiteExporter(BaseExporter):\n    required_packages = (\n        """"onnx2tf>=1.18.0"""",\n        """"onnx>=1.16.1"""", \n        """"tensorflow==2.17.0"""",\n        """"tf_keras"""",  # required by 'onnx2tf' package\n        """"sng4onnx>=1.0.1"""",  # required by 'onnx2tf' package\n        """"onnx_graphsurgeon>=0.3.26"""",  # required by 'onnx2tf' package\n        """"onnxslim>=0.1.31"""",\n        """"onnxruntime"""",\n        """"flatbuffers>=23.5.26"""",\n        """"psutil==5.9.5"""",\n        """"ml_dtypes==0.3.2"""",\n    )\n    cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n    \n    def export(self):\n\n        import onnx2tf\n        input_onnx_file_path = str(self.file.with_suffix('.onnx'))\n        output_folder_path = input_onnx_file_path.replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n        onnx2tf.convert(\n            input_onnx_file_path=input_onnx_file_path,\n       ","@@ -13,9 +13,7 @@ class TFLiteExporter(BaseExporter):\n         """"onnx_graphsurgeon>=0.3.26"""",  # required by 'onnx2tf' package\n         """"onnxslim>=0.1.31"""",\n         """"onnxruntime"""",\n-        """"flatbuffers-compiler"""",\n         """"flatbuffers>=23.5.26"""",\n-        """"simple_onnx_processing_tools"""",\n         """"psutil==5.9.5"""",\n         """"ml_dtypes==0.3.2"""",\n     )\n",add,Add note about data volume to enable_metrics_collection
2034bd3d5d55e094d913a992adc569a906417b82,fix tflite inference,boxmot/appearance/backends/tflite_backend.py,"import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\n\nclass TFLiteBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n\n    def load_model(self, w):\n        checker.check_packages((""""tensorflow"""",))\n\n        LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n        \n        try:\n            import tensorflow as tf\n            interpreter = tf.lite.Interpreter(model_path=str(w))\n            self.tf_lite_model = interpreter.get_signature_runner()\n        except Exception as e:\n            LOGGER.error(f'{e}. If SignatureDef error. Export you model with the official onn2tf docker')\n            exit()\n\n    def forward(self, im_ba","import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\n\nclass TFLiteBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.interpreter = None\n        # Variable to keep track of the current allocated batch size\n        self.current_allocated_batch_size = None\n        \n    def load_model(self, w):\n        checker.check_packages((""""tensorflow"""",))\n\n        LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n        \n        try:\n            import tensorflow as tf\n            Interpreter = tf.lite.Interpreter\n            self.interpreter = tf.lite.Interpreter(model_path=str(w))\n        except Exception as e:\n            ","@@ -15,7 +15,10 @@ class TFLiteBackend(BaseModelBackend):\n         super().__init__(weights, device, half)\n         self.nhwc = False\n         self.half = half\n-\n+        self.interpreter = None\n+        # Variable to keep track of the current allocated batch size\n+        self.current_allocated_batch_size = None\n+        \n     def load_model(self, w):\n         checker.check_packages((""""tensorflow"""",))\n \n@@ -23,17 +26,37 @@ class TFLiteBackend(BaseModelBackend):\n         \n         try:\n             import tensorflow as tf\n-            interpreter = tf.lite.Interpreter(model_path=str(w))\n-            self.tf_lite_model = interpreter.get_signature_runner()\n+            Interpreter = tf.lite.Interpreter\n+            self.interpreter = tf.lite.Interpreter(model_path=str(w))\n         except Exception as e:\n             LOGGER.error(f'{e}. If SignatureDef error. Export you model with the official onn2tf docker')\n             exit()\n+            \n+        self.interpreter.allocate_tensors()  # allocate\n+        self.input_details = self.interpreter.get_input_details()  # inputs\n+        self.output_details = self.interpreter.get_output_details()  # outputs\n+        self.current_allocated_batch_size = input_details[0]['shape'][0]\n+\n \n     def forward(self, im_batch):\n         im_batch = im_batch.cpu().numpy()\n-        inputs = {\n-            'images': im_batch,\n-        }\n-        tf_lite_output = self.tf_lite_model(**inputs)\n-        features = tf_lite_output['output']\n+        # Extract batch size from im_batch\n+        batch_size = im_batch.shape[0]\n+\n+        # Resize tensors if the new batch size is different from the current allocated batch size\n+        if batch_size != self.current_allocated_batch_size:\n+            print(f""""Resizing tensor input to batch size {batch_size}"""")\n+            interpreter.resize_tensor_input(input_details[0]['index'], [batch_size, 256, 128, 3])\n+            interpreter.allocate_tensors()\n+    ",add,Added Type name for DFI ( # 57480 )
07a1dedb626d15424d276a9d7712292a54c20eff,fix,boxmot/appearance/backends/tflite_backend.py,"import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\n\nclass TFLiteBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.interpreter = None\n        # Variable to keep track of the current allocated batch size\n        self.current_allocated_batch_size = None\n        \n    def load_model(self, w):\n        checker.check_packages((""""tensorflow"""",))\n\n        LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n        \n        try:\n            import tensorflow as tf\n            Interpreter = tf.lite.Interpreter\n            self.interpreter = tf.lite.Interpreter(model_path=str(w))\n        except Exception as e:\n            ","import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\n\nclass TFLiteBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.interpreter = None\n        # Variable to keep track of the current allocated batch size\n        self.current_allocated_batch_size = None\n        \n    def load_model(self, w):\n        checker.check_packages((""""tensorflow"""",))\n\n        LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n        \n        try:\n            import tensorflow as tf\n            Interpreter = tf.lite.Interpreter\n            self.interpreter = tf.lite.Interpreter(model_path=str(w))\n        except Exception as e:\n            ","@@ -46,17 +46,17 @@ class TFLiteBackend(BaseModelBackend):\n         # Resize tensors if the new batch size is different from the current allocated batch size\n         if batch_size != self.current_allocated_batch_size:\n             print(f""""Resizing tensor input to batch size {batch_size}"""")\n-            interpreter.resize_tensor_input(input_details[0]['index'], [batch_size, 256, 128, 3])\n+            interpreter.resize_tensor_input(self.input_details[0]['index'], [batch_size, 256, 128, 3])\n             interpreter.allocate_tensors()\n             current_allocated_batch_size = batch_size\n \n         # Set the tensor to point to the input data\n-        interpreter.set_tensor(input_details[0]['index'], input_data)\n+        interpreter.set_tensor(self.input_details[0]['index'], input_data)\n \n         # Run inference\n         interpreter.invoke()\n \n         # Get the output data\n-        features = interpreter.get_tensor(output_details[0]['index'])\n+        features = interpreter.get_tensor(self.output_details[0]['index'])\n \n         return features\n",add,Add Rossen
2d8a03648388c29142a7661fc1ba2c1a0d9ec98c,fix,boxmot/appearance/backends/tflite_backend.py,"import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\n\nclass TFLiteBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.interpreter = None\n        # Variable to keep track of the current allocated batch size\n        self.current_allocated_batch_size = None\n        \n    def load_model(self, w):\n        checker.check_packages((""""tensorflow"""",))\n\n        LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n        \n        try:\n            import tensorflow as tf\n            Interpreter = tf.lite.Interpreter\n            self.interpreter = tf.lite.Interpreter(model_path=str(w))\n        except Exception as e:\n            ","import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\n\nclass TFLiteBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.interpreter = None\n        # Variable to keep track of the current allocated batch size\n        self.current_allocated_batch_size = None\n        \n    def load_model(self, w):\n        checker.check_packages((""""tensorflow"""",))\n\n        LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n        \n        try:\n            import tensorflow as tf\n            Interpreter = tf.lite.Interpreter\n            self.interpreter = tf.lite.Interpreter(model_path=str(w))\n        except Exception as e:\n            ","@@ -35,7 +35,7 @@ class TFLiteBackend(BaseModelBackend):\n         self.interpreter.allocate_tensors()  # allocate\n         self.input_details = self.interpreter.get_input_details()  # inputs\n         self.output_details = self.interpreter.get_output_details()  # outputs\n-        self.current_allocated_batch_size = input_details[0]['shape'][0]\n+        self.current_allocated_batch_size = self.input_details[0]['shape'][0]\n \n \n     def forward(self, im_batch):\n",add,Add note about data volume to enable_metrics_collection
4b6192f06762138621bd569205cc9cea3041a225,fix,boxmot/appearance/backends/tflite_backend.py,"import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\n\nclass TFLiteBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.interpreter = None\n        # Variable to keep track of the current allocated batch size\n        self.current_allocated_batch_size = None\n        \n    def load_model(self, w):\n        checker.check_packages((""""tensorflow"""",))\n\n        LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n        \n        try:\n            import tensorflow as tf\n            Interpreter = tf.lite.Interpreter\n            self.interpreter = tf.lite.Interpreter(model_path=str(w))\n        except Exception as e:\n            ","import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\n\nclass TFLiteBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.interpreter = None\n        # Variable to keep track of the current allocated batch size\n        self.current_allocated_batch_size = None\n        \n    def load_model(self, w):\n        checker.check_packages((""""tensorflow"""",))\n\n        LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n        \n        try:\n            import tensorflow as tf\n            Interpreter = tf.lite.Interpreter\n            self.interpreter = tf.lite.Interpreter(model_path=str(w))\n        except Exception as e:\n            ","@@ -46,17 +46,17 @@ class TFLiteBackend(BaseModelBackend):\n         # Resize tensors if the new batch size is different from the current allocated batch size\n         if batch_size != self.current_allocated_batch_size:\n             print(f""""Resizing tensor input to batch size {batch_size}"""")\n-            interpreter.resize_tensor_input(self.input_details[0]['index'], [batch_size, 256, 128, 3])\n-            interpreter.allocate_tensors()\n+            self.interpreter.resize_tensor_input(self.input_details[0]['index'], [batch_size, 256, 128, 3])\n+            self.interpreter.allocate_tensors()\n             current_allocated_batch_size = batch_size\n \n         # Set the tensor to point to the input data\n-        interpreter.set_tensor(self.input_details[0]['index'], input_data)\n+        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n \n         # Run inference\n-        interpreter.invoke()\n+        self.interpreter.invoke()\n \n         # Get the output data\n-        features = interpreter.get_tensor(self.output_details[0]['index'])\n+        features = self.interpreter.get_tensor(self.output_details[0]['index'])\n \n         return features\n",add,Add Rossen
eceb0b78fd42847b24cb9eeb2047ec9bc3d8b2fe,fix,boxmot/appearance/backends/tflite_backend.py,"import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\n\nclass TFLiteBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.interpreter = None\n        # Variable to keep track of the current allocated batch size\n        self.current_allocated_batch_size = None\n        \n    def load_model(self, w):\n        checker.check_packages((""""tensorflow"""",))\n\n        LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n        \n        try:\n            import tensorflow as tf\n            Interpreter = tf.lite.Interpreter\n            self.interpreter = tf.lite.Interpreter(model_path=str(w))\n        except Exception as e:\n            ","import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\n\nclass TFLiteBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.interpreter = None\n        self.current_allocated_batch_size = None\n\n    def load_model(self, w):\n        checker.check_packages((""""tensorflow"""",))\n\n        LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n        \n        try:\n            Interpreter = tf.lite.Interpreter\n            self.interpreter = tf.lite.Interpreter(model_path=str(w))\n        except Exception as e:\n            LOGGER.error(f'{e}. If SignatureDef error. Export your model with the official onnx2tf docker')\n            exit()","@@ -16,20 +16,18 @@ class TFLiteBackend(BaseModelBackend):\n         self.nhwc = False\n         self.half = half\n         self.interpreter = None\n-        # Variable to keep track of the current allocated batch size\n         self.current_allocated_batch_size = None\n-        \n+\n     def load_model(self, w):\n         checker.check_packages((""""tensorflow"""",))\n \n         LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n         \n         try:\n-            import tensorflow as tf\n             Interpreter = tf.lite.Interpreter\n             self.interpreter = tf.lite.Interpreter(model_path=str(w))\n         except Exception as e:\n-            LOGGER.error(f'{e}. If SignatureDef error. Export you model with the official onn2tf docker')\n+            LOGGER.error(f'{e}. If SignatureDef error. Export your model with the official onnx2tf docker')\n             exit()\n             \n         self.interpreter.allocate_tensors()  # allocate\n@@ -37,7 +35,6 @@ class TFLiteBackend(BaseModelBackend):\n         self.output_details = self.interpreter.get_output_details()  # outputs\n         self.current_allocated_batch_size = self.input_details[0]['shape'][0]\n \n-\n     def forward(self, im_batch):\n         im_batch = im_batch.cpu().numpy()\n         # Extract batch size from im_batch\n@@ -48,10 +45,10 @@ class TFLiteBackend(BaseModelBackend):\n             print(f""""Resizing tensor input to batch size {batch_size}"""")\n             self.interpreter.resize_tensor_input(self.input_details[0]['index'], [batch_size, 256, 128, 3])\n             self.interpreter.allocate_tensors()\n-            current_allocated_batch_size = batch_size\n+            self.current_allocated_batch_size = batch_size\n \n         # Set the tensor to point to the input data\n-        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n+        self.interpreter.set_tensor(self.input_details[0]['index'], im_batch)\n \n         # Run inference\n         self.inte",add,Added Type name for DFI ( # 57480 )
38a118c34bfc99eaf52eb09e3b55675667024934,fix,boxmot/appearance/backends/tflite_backend.py,"import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\n\nclass TFLiteBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.interpreter = None\n        self.current_allocated_batch_size = None\n\n    def load_model(self, w):\n        checker.check_packages((""""tensorflow"""",))\n\n        LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n        \n        try:\n            Interpreter = tf.lite.Interpreter\n            self.interpreter = tf.lite.Interpreter(model_path=str(w))\n        except Exception as e:\n            LOGGER.error(f'{e}. If SignatureDef error. Export your model with the official onnx2tf docker')\n            exit()","import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\n\nclass TFLiteBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.interpreter = None\n        self.current_allocated_batch_size = None\n\n    def load_model(self, w):\n        checker.check_packages((""""tensorflow"""",))\n\n        LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n        \n        try:\n            import tensorflow as tf\n            self.interpreter = tf.lite.Interpreter(model_path=str(w))\n        except Exception as e:\n            LOGGER.error(f'{e}. If SignatureDef error. Export your model with the official onnx2tf docker')\n            exit()\n        ","@@ -24,7 +24,7 @@ class TFLiteBackend(BaseModelBackend):\n         LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n         \n         try:\n-            Interpreter = tf.lite.Interpreter\n+            import tensorflow as tf\n             self.interpreter = tf.lite.Interpreter(model_path=str(w))\n         except Exception as e:\n             LOGGER.error(f'{e}. If SignatureDef error. Export your model with the official onnx2tf docker')\n",add,Added hypest who apparently actually wrote the C # port
4a96cf512a3d81b916cceae1417f00db789591e9,fix,boxmot/appearance/backends/tflite_backend.py,"import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\n\nclass TFLiteBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.interpreter = None\n        self.current_allocated_batch_size = None\n\n    def load_model(self, w):\n        checker.check_packages((""""tensorflow"""",))\n\n        LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n        \n        try:\n            import tensorflow as tf\n            self.interpreter = tf.lite.Interpreter(model_path=str(w))\n        except Exception as e:\n            LOGGER.error(f'{e}. If SignatureDef error. Export your model with the official onnx2tf docker')\n            exit()\n        ","import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\n\nclass TFLiteBackend(BaseModelBackend):\n    """"""""""""\n    A class to handle TensorFlow Lite model inference with dynamic batch size support.\n\n    Attributes:\n        nhwc (bool): A flag indicating the order of dimensions.\n        half (bool): A flag to indicate if half precision is used.\n        interpreter (tf.lite.Interpreter): The TensorFlow Lite interpreter.\n        current_allocated_batch_size (int): The current batch size allocated in the interpreter.\n    """"""""""""\n\n    def __init__(self, weights: str, device: str, half: bool):\n        """"""""""""\n        Initializes the TFLiteBackend with given weights, device, and precision flag.\n\n        Args:\n            weights (str): Path to the TFLite model file.\n         ","@@ -10,32 +10,64 @@ checker = RequirementsChecker()\n \n \n class TFLiteBackend(BaseModelBackend):\n+    """"""""""""\n+    A class to handle TensorFlow Lite model inference with dynamic batch size support.\n \n-    def __init__(self, weights, device, half):\n+    Attributes:\n+        nhwc (bool): A flag indicating the order of dimensions.\n+        half (bool): A flag to indicate if half precision is used.\n+        interpreter (tf.lite.Interpreter): The TensorFlow Lite interpreter.\n+        current_allocated_batch_size (int): The current batch size allocated in the interpreter.\n+    """"""""""""\n+\n+    def __init__(self, weights: str, device: str, half: bool):\n+        """"""""""""\n+        Initializes the TFLiteBackend with given weights, device, and precision flag.\n+\n+        Args:\n+            weights (str): Path to the TFLite model file.\n+            device (str): Device type (e.g., 'cpu', 'gpu').\n+            half (bool): Flag to indicate if half precision is used.\n+        """"""""""""\n         super().__init__(weights, device, half)\n         self.nhwc = False\n         self.half = half\n-        self.interpreter = None\n-        self.current_allocated_batch_size = None\n+        self.interpreter: tf.lite.Interpreter = None\n+        self.current_allocated_batch_size: int = None\n \n-    def load_model(self, w):\n+    def load_model(self, w: str) -> None:\n+        """"""""""""\n+        Loads the TensorFlow Lite model and initializes the interpreter.\n+\n+        Args:\n+            w (str): Path to the TFLite model file.\n+        """"""""""""\n         checker.check_packages((""""tensorflow"""",))\n \n         LOGGER.info(f""""Loading {w} for TensorFlow Lite inference..."""")\n-        \n+\n         try:\n             import tensorflow as tf\n             self.interpreter = tf.lite.Interpreter(model_path=str(w))\n         except Exception as e:\n             LOGGER.error(f'{e}. If SignatureDef error. Export your model with the official onnx2tf docker')\n             exit()\n-        ",add,Add note about data volume to enable_metrics_collection
e163475d0301c07f94771b533ff4bc5b1e477771,Add fixes,.gitignore,.vscode/\n.venv/\n.eggs/\nenv/\n*.DS_Store\n.coverage\nhtmlcov/\n# interpreter bytecode\n__pycache__/\n\n# experiment results\nruns/\nvideos/\n\n# Downloaded weights\nweights/*.pt\nweights/*.torchscript\nweights/*.onnx\nweights/*_openvino_model\nweights/*.engine\nweights/*.tflite\n\n# evaluation tools folder\nval_utils/\n\n# zip files\n*.zip\n\n# exports\n*_openvino_model\n*.torchscript\n*.pt\n*.pth\n*.onnx\n*.engine\n\n# interactive plots\n*.html\n\n# tf models\n*saved_model/\ntf_models/\n\n# distribution / packaging\nbuild/\ndist/\n*.egg-info/\n\n# cmc debug images\nboxmot/motion/cmc/*.jpg\n,.vscode/\n.venv/\n.eggs/\nenv/\n*.DS_Store\n.coverage\nhtmlcov/\n# interpreter bytecode\n__pycache__/\n\n# experiment results\nruns/\nvideos/\n\n# experiment files\n*.pkl\n\n# Downloaded weights\nweights/*.pt\nweights/*.torchscript\nweights/*.onnx\nweights/*_openvino_model\nweights/*.engine\nweights/*.tflite\n\n# evaluation tools folder\nval_utils/\n\n# zip files\n*.zip\n\n# exports\n*_openvino_model\n*.torchscript\n*.pt\n*.pth\n*.onnx\n*.engine\n\n# interactive plots\n*.html\n\n# tf models\n*saved_model/\ntf_models/\n\n# distribution / packaging\nbuild/\ndist/\n*.egg-info/\n\n# cmc debug images\nboxmot/motion/cmc/*.jpg\n,"@@ -12,6 +12,9 @@ __pycache__/\n runs/\n videos/\n \n+# experiment files\n+*.pkl\n+\n # Downloaded weights\n weights/*.pt\n weights/*.torchscript\n",add,Add note about data volume to enable
e163475d0301c07f94771b533ff4bc5b1e477771,Add fixes,boxmot/configs/botsort.yaml,appearance_thresh: 0.7636154550757781\ncmc_method: sparseOptFlow\nframe_rate: 30\nlambda_: 0.9764964093204093\nmatch_thresh: 0.22182609980772128\nnew_track_thresh: 0.25855770885476126\nproximity_thresh: 0.37262205760353817\ntrack_buffer: 40\ntrack_high_thresh: 0.5\ntrack_low_thresh: 0.1\n,"# Trial number:      0\n# HOTA, MOTA, IDF1:  [37.804, 19.34, 34.483]\nappearance_thresh: 0.7636154550757781\ncmc_method: sparseOptFlow\nconf: 0.4828271520496597\nframe_rate: 30\nlambda_: 0.9764964093204093\nmatch_thresh: 0.22182609980772128\nnew_track_thresh: 0.25855770885476126\nproximity_thresh: 0.37262205760353817\ntrack_buffer: 40\ntrack_high_thresh: 0.5\ntrack_low_thresh: 0.1\n","@@ -1,5 +1,8 @@\n+# Trial number:      0\n+# HOTA, MOTA, IDF1:  [37.804, 19.34, 34.483]\n appearance_thresh: 0.7636154550757781\n cmc_method: sparseOptFlow\n+conf: 0.4828271520496597\n frame_rate: 30\n lambda_: 0.9764964093204093\n match_thresh: 0.22182609980772128\n",add,Added STORM - 862 to Changelog
e163475d0301c07f94771b533ff4bc5b1e477771,Add fixes,boxmot/configs/imprassoc.yaml,appearance_thresh: 0.45026460227060683\ncmc_method: sparseOptFlow\nframe_rate: 30\niou_weight: 0.23952360206611586\nlambda_: 0.9868313792314138\nmatch_thresh: 0.1679093699779095\nnew_track_thresh: 0.49929306257998685\noverlap_thresh: 0.5486223573956148\nproximity_thresh: 0.3769494835487784\nsecond_match_thresh: 0.31480506616083626\ntrack_buffer: 30\ntrack_high_thresh: 0.6975266597432511\ntrack_low_thresh: 0.1915594386319488\n,"# Trial number:      3\n# HOTA, MOTA, IDF1:  [37.787, 19.575, 35.048]\nappearance_thresh: 0.48396589323963035\ncmc_method: sparseOptFlow\nconf: 0.4336934087443602\nframe_rate: 30\niou_weight: 0.4570211918252064\nlambda_: 0.9727356926625315\nmatch_thresh: 0.3853661949420558\nnew_track_thresh: 0.2775399236098679\noverlap_thresh: 0.32722172170799374\nproximity_thresh: 0.5561104565730095\nsecond_match_thresh: 0.3316381346568539\ntrack_buffer: 30\ntrack_high_thresh: 0.5475880676693929\ntrack_low_thresh: 0.22917350812104917\n","@@ -1,13 +1,16 @@\n-appearance_thresh: 0.45026460227060683\n+# Trial number:      3\n+# HOTA, MOTA, IDF1:  [37.787, 19.575, 35.048]\n+appearance_thresh: 0.48396589323963035\n cmc_method: sparseOptFlow\n+conf: 0.4336934087443602\n frame_rate: 30\n-iou_weight: 0.23952360206611586\n-lambda_: 0.9868313792314138\n-match_thresh: 0.1679093699779095\n-new_track_thresh: 0.49929306257998685\n-overlap_thresh: 0.5486223573956148\n-proximity_thresh: 0.3769494835487784\n-second_match_thresh: 0.31480506616083626\n+iou_weight: 0.4570211918252064\n+lambda_: 0.9727356926625315\n+match_thresh: 0.3853661949420558\n+new_track_thresh: 0.2775399236098679\n+overlap_thresh: 0.32722172170799374\n+proximity_thresh: 0.5561104565730095\n+second_match_thresh: 0.3316381346568539\n track_buffer: 30\n-track_high_thresh: 0.6975266597432511\n-track_low_thresh: 0.1915594386319488\n+track_high_thresh: 0.5475880676693929\n+track_low_thresh: 0.22917350812104917\n",add,Add note about data volume to enable_metrics_collection
e163475d0301c07f94771b533ff4bc5b1e477771,Add fixes,boxmot/trackers/imprassoc/impr_assoc_tracker.py,"# Raif Olson\n\nimport numpy as np\nfrom collections import deque\n\n# from Impr_Assoc_Track.matching import iou_distance, d_iou_distance, embedding_distance, fuse_motion, fuse_score, linear_assignment, ious, fuse_iou\n# from tracker.gmc import GMC\n# from Impr_Assoc_Track.basetrack import BaseTrack, TrackState\n# from Impr_Assoc_Track.kalman_filter import KalmanFilter\n# from supervision.detection.core import Detections\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.xywh_kf import KalmanFilterXYWH\nfrom boxmot.trackers.imprassoc.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment,\n                                   d_iou_distance)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDeco","# Raif Olson\n\nimport numpy as np\nfrom collections import deque\n\n# from Impr_Assoc_Track.matching import iou_distance, d_iou_distance, embedding_distance, fuse_motion, fuse_score, linear_assignment, ious, fuse_iou\n# from tracker.gmc import GMC\n# from Impr_Assoc_Track.basetrack import BaseTrack, TrackState\n# from Impr_Assoc_Track.kalman_filter import KalmanFilter\n# from supervision.detection.core import Detections\n\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.motion.kalman_filters.xywh_kf import KalmanFilterXYWH\nfrom boxmot.trackers.imprassoc.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment,\n                                   d_iou_distance)\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils import PerClassDeco","@@ -26,8 +26,8 @@ class STrack(BaseTrack):\n \n     def __init__(self, det, feat=None, feat_history=15, max_obs=15):\n         # wait activate\n-        print(f""""det: {det}"""")\n-        print(f""""box: {det[0:4]}"""")\n+        # print(f""""det: {det}"""")\n+        # print(f""""box: {det[0:4]}"""")\n         self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n         self.conf = det[4]\n         self.cls = det[5]\n@@ -122,7 +122,7 @@ class STrack(BaseTrack):\n                 stracks[i].mean = mean\n                 stracks[i].covariance = cov\n \n-    def activate(self, kalman_filter, frame_id):\n+    def activate(self, kalman_filter, frame_count):\n         """"""""""""Start a new tracklet""""""""""""\n         self.kalman_filter = kalman_filter\n         self.id = self.next_id()\n@@ -133,19 +133,19 @@ class STrack(BaseTrack):\n         self.state = TrackState.Tracked\n         # from OAI track, no unconfirmed tracks.\n         self.is_activated = True\n-        self.frame_id = frame_id\n-        self.start_frame = frame_id\n+        self.frame_count = frame_count\n+        self.start_frame = frame_count\n \n-    def re_activate(self, new_track, frame_id, new_id=False):\n+    def re_activate(self, new_track, frame_count, new_id=False):\n         self.mean, self.covariance = self.kalman_filter.update(\n-            self.mean, self.covariance, new_track.xywh, self.score\n+            self.mean, self.covariance, new_track.xywh, self.conf\n         )\n         if new_track.curr_feat is not None:\n             self.update_features(new_track.curr_feat)\n         self.tracklet_len = 0\n         self.state = TrackState.Tracked\n         self.is_activated = True\n-        self.frame_id = frame_id\n+        self.frame_count = frame_count\n         if new_id:\n             self.id = self.next_id()\n         self.conf = new_track.conf\n@@ -154,21 +154,21 @@ class STrack(BaseTrack):\n \n         self.update_cls(new_track.cls, new_track.conf)\n \n-    def update(self, new_track, ",add,Added the user group to the contributor list
e163475d0301c07f94771b533ff4bc5b1e477771,Add fixes,tracking/generate_mot_results.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom functools import partial\nimport json\nimport torch\n\nfrom tqdm import tqdm\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\n\nfrom ultralytics.utils.files import increment_path \nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils import logger as LOGGER\n\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.utils import convert_to_mot_format, write_mot_results\nfrom boxmot.utils.torch_utils import select_device\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\n\ndef generate_mot_results(args):\n    args.device = select_device(args.device)\n    tracker = create_tracker(\n  ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nfrom functools import partial\nimport json\nimport torch\n\nfrom tqdm import tqdm\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\n\nfrom ultralytics.utils.files import increment_path \nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils import logger as LOGGER\n\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics import YOLO\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.utils import convert_to_mot_format, write_mot_results\nfrom boxmot.utils.torch_utils import select_device\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\n\ndef generate_mot_results(args):\n    args.device = select_device(args.device)\n    tracker = create_tracker(\n  ","@@ -156,6 +156,7 @@ def run_generate_mot_results(opt):\n         opt = opt\n \n     exp_folder_path = opt.project / (str(opt.dets) + """"_"""" + str(opt.embs) + """"_"""" + str(opt.tracking_method))\n+    print(f""""experiment folder: {exp_folder_path}"""")\n     exp_folder_path = increment_path(path=exp_folder_path, sep=""""_"""", exist_ok=False)\n     opt.exp_folder_path = exp_folder_path\n     dets_file_paths = [item for item in (opt.project.parent / """"dets_n_embs"""" / opt.dets / 'dets').glob('*.txt') if not item.name.startswith('.')]\n",add,Add warning about data volume to enable_metrics_collection
55548b5f38b60c3bb2a6c3172147fa99ae2528b0,debug,.github/workflows/ci.yml,"# name of the workflow, what it is doing (optional)\nname: BoxMOT CI\n\n# events that trigger the workflow (required)\non:\n  push:\n    # pushes to the following branches\n    branches:\n      - master\n  pull_request:\n    # pull request where master is target\n    branches:\n      - master\n\n\njobs:\n  test-tracking-methods:\n    runs-on: ${{ matrix.os }}\n    outputs:\n      status: ${{ job.status }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, macos-14]   # skip windows-latest for\n        python-version: ['3.11']\n        # leads to too many workflow which ends up queued\n        # tracking-method: [hybridsort, botsort, ocsort, bytetrack] \n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      - uses: actions/checkout@v4  # Check out the repository\n      - name: Set up Python\n        uses: actions/setup-python@v5  # Prepare environment with python 3.9\n        with:\n          python-vers","# name of the workflow, what it is doing (optional)\nname: BoxMOT CI\n\n# events that trigger the workflow (required)\non:\n  push:\n    # pushes to the following branches\n    branches:\n      - master\n  pull_request:\n    # pull request where master is target\n    branches:\n      - master\n\n\njobs:\n  test-tracking-methods:\n    runs-on: ${{ matrix.os }}\n    outputs:\n      status: ${{ job.status }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, macos-14]   # skip windows-latest for\n        python-version: ['3.11']\n        # leads to too many workflow which ends up queued\n        # tracking-method: [hybridsort, botsort, ocsort, bytetrack] \n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      - uses: actions/checkout@v4  # Check out the repository\n      - name: Set up Python\n        uses: actions/setup-python@v5  # Prepare environment with python 3.9\n        with:\n          python-vers","@@ -147,6 +147,13 @@ jobs:\n           python tracking/generate_dets_n_embs.py --source ./assets/MOT17-mini/train --yolo-model yolov8n.pt --reid-model osnet_x0_25_msmt17.pt --imgsz 320\n           python tracking/generate_mot_results.py --dets yolov8n --reid osnet_x0_25_msmt17 --tracking-method ocsort\n \n+      - name: Debug Output\n+        run: |\n+          for tracker in """"${TRACKERS[@]}""""; do\n+            echo """"Output for $tracker:""""\n+            cat ${tracker}_output.json\n+          done\n+\n       - name: Evaluation and Summarize Results\n         shell: bash\n         run: |\n",add,Add note about data volume to enable_metrics_collection
55548b5f38b60c3bb2a6c3172147fa99ae2528b0,debug,tracking/val.py,"import re\nimport sys\nimport json\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        # pack everything in dict\n        combined_results = {key: value for key, value in zip([""""HOTA"""", """"MOTA"""", """"IDF1""""], combined_","import re\nimport sys\nimport json\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        # pack everything in dict\n        combined_results = {key: value for key, value in zip([""""HOTA"""", """"MOTA"""", """"IDF1""""], combined_","@@ -171,7 +171,7 @@ def run_trackeval(opt):\n     seq_paths, save_dir, MOT_results_folder, gt_folder = eval_setup(opt, val_tools_path)\n     results = trackeval(opt, seq_paths, save_dir, MOT_results_folder, gt_folder)\n     combined_results = parse_mot_results(results)\n-    print(combined_results)\n+    print(json.dumps(combined_results))\n     return json.dumps(combined_results)\n \n if __name__ == """"__main__"""":\n",fix,Add note about data volume to enable_metrics_collection
f2dd5d1dd47eb74fbac7aa8163a283938c465151,fix,tracking/val.py,"import re\nimport sys\nimport {}{}}\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        # pack everything in dict\n        results_dict = {}\n        combined_results[""""trackeval""""] = results\n        results_dict[ke","import re\nimport sys\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        # pack everything in dict\n        results_dict = {}\n        combined_results[""""trackeval""""] = results\n        results_dict[key]: value for ","@@ -1,6 +1,5 @@\n import re\n import sys\n-import {}{}}\n import argparse\n import subprocess\n from boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\n",add,Add note about data volume to enable
9bb9a5080657b5c0f02d6f61e53648f64efdf9f6,fix,tracking/val.py,"import re\nimport sys\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        # pack everything in dict\n        results_dict = {}\n        combined_results[""""trackeval""""] = results\n        results_dict[key]: value for ","import re\nimport sys\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        \n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        \n        # pack everything in dict\n        results_dict = {}\n        combined_results[""""trackeval""""] = results  #trackeval result","@@ -25,12 +25,15 @@ def parse_mot_results(results):\n             (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n         """"""""""""\n         combined_results = results.split('COMBINED')[2:-1]\n+        \n         # robust way of getting first ints/float in string\n         combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n+        \n         # pack everything in dict\n         results_dict = {}\n-        combined_results[""""trackeval""""] = results\n-        results_dict[key]: value for key, value in zip([""""HOTA"""", """"MOTA"""", """"IDF1""""], combined_results)\n+        combined_results[""""trackeval""""] = results  #trackeval results\n+        for key, value in zip([""""HOTA"""", """"MOTA"""", """"IDF1""""], combined_results):\n+            results_dict[key]= value \n         \n         return combined_results\n \n",fix,Add note about data volume to enable_metrics_collection
649e827e67d7668d7e3ad0d5499976990e6893f4,fix,tracking/val.py,"import re\nimport sys\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        \n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        \n        # pack everything in dict\n        results_dict = {}\n        combined_results[""""trackeval""""] = results  #trackeval result","import re\nimport sys\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        \n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        \n        # pack everything in dict\n        results_dict = {}\n        combined_results[""""trackeval""""] = results  #trackeval result","@@ -174,8 +174,8 @@ def run_trackeval(opt):\n     seq_paths, save_dir, MOT_results_folder, gt_folder = eval_setup(opt, val_tools_path)\n     results = trackeval(opt, seq_paths, save_dir, MOT_results_folder, gt_folder)\n     combined_results = parse_mot_results(results)\n-    print({}{}}.dumps(combined_results))\n-    return {}{}}.dumps(combined_results)\n+    print(json.dumps(combined_results))\n+    return json.dumps(combined_results)\n \n if __name__ == """"__main__"""":\n     run_trackeval(None)\n",fix,Add note about data volume to enable_metrics_collection
cd384213e54c6adf91f2188960dc08348e6db767,fix,tracking/val.py,"import re\nimport sys\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        \n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        \n        # pack everything in dict\n        results_dict = {}\n        combined_results[""""trackeval""""] = results  #trackeval result","import re\nimport sys\nimport json\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        \n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        \n        # pack everything in dict\n        results_dict = {}\n        results_dict[""""trackeval""""] = results  #trackev","@@ -1,5 +1,6 @@\n import re\n import sys\n+import json\n import argparse\n import subprocess\n from boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\n@@ -31,11 +32,11 @@ def parse_mot_results(results):\n         \n         # pack everything in dict\n         results_dict = {}\n-        combined_results[""""trackeval""""] = results  #trackeval results\n+        results_dict[""""trackeval""""] = results  #trackeval results\n         for key, value in zip([""""HOTA"""", """"MOTA"""", """"IDF1""""], combined_results):\n             results_dict[key]= value \n         \n-        return combined_results\n+        return results_dict\n \n \n def trackeval(\n",add,Add note about data volume to enable_metrics_collection
98b1a5b9986340823703992fbb474818db6e13cd,fix,tracking/val.py,"import re\nimport sys\nimport json\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        \n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        \n        # pack everything in dict\n        results_dict = {}\n        results_dict[""""trackeval""""] = results  #trackev","import re\nimport sys\nimport json\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        \n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        \n        # pack everything in dict\n        results_dict = {}\n        results_dict[""""trackeval""""] = results  #trackev","@@ -175,7 +175,7 @@ def run_trackeval(opt):\n     seq_paths, save_dir, MOT_results_folder, gt_folder = eval_setup(opt, val_tools_path)\n     results = trackeval(opt, seq_paths, save_dir, MOT_results_folder, gt_folder)\n     combined_results = parse_mot_results(results)\n-    print(json.dumps(combined_results))\n+    #print(json.dumps(combined_results))\n     return json.dumps(combined_results)\n \n if __name__ == """"__main__"""":\n",fix,Add note about data volume to enable_metrics_collection
d4b98c7d4876ef03422c9dfb051da398d72456cf,fix,tracking/val.py,"import re\nimport sys\nimport json\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        \n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        \n        # pack everything in dict\n        results_dict = {}\n        results_dict[""""trackeval""""] = results  #trackev","import re\nimport sys\nimport json\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        \n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        \n        # pack everything in dict\n        results_dict = {}\n        results_dict[""""trackeval""""] = results  #trackev","@@ -175,7 +175,7 @@ def run_trackeval(opt):\n     seq_paths, save_dir, MOT_results_folder, gt_folder = eval_setup(opt, val_tools_path)\n     results = trackeval(opt, seq_paths, save_dir, MOT_results_folder, gt_folder)\n     combined_results = parse_mot_results(results)\n-    #print(json.dumps(combined_results))\n+    print(json.dumps(combined_results))\n     return json.dumps(combined_results)\n \n if __name__ == """"__main__"""":\n",fix,Add note about data volume to enable_metrics_collection
ff6937a39c80c6012c40fb2c1c9206b4e5258ee1,fix evolution,tracking/val.py,"import re\nimport sys\nimport json\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        \n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        \n        # pack everything in dict\n        results_dict = {}\n        for key, value in zip([""""HOTA"""", """"MOTA"""", """"ID","import re\nimport sys\nimport json\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        \n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        \n        # pack everything in dict\n        results_dict = {}\n        for key, value in zip([""""HOTA"""", """"MOTA"""", """"ID","@@ -179,7 +179,7 @@ def run_trackeval(opt):\n         with open(opt.tracking_method + """"_output.json"""", """"w"""") as outfile:\n             outfile.write(json.dumps(hota_mota_idf1))\n     print(json.dumps(hota_mota_idf1))\n-    return hota_mota_idf1\n+    return json.dumps(hota_mota_idf1)\n \n if __name__ == """"__main__"""":\n     run_trackeval(None)\n",fix,Don ' t define the dependency on real_commad twice
5745ca5e4ce8c74be59893bce53a9093dab4bd9f,fix evolution,tracking/evolve.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport argparse\n\nimport yaml\nfrom ultralytics.utils.checks import check_requirements, print_args\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, logger\nfrom tracking.val import run_trackeval\nfrom tracking.generate_mot_results import run_generate_mot_results\n\n\nclass Objective():\n    """"""""""""Objective function to evolve best set of hyperparams for\n\n    This object is passed to an objective function and provides inte","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport argparse\n\nimport yaml\nfrom ultralytics.utils.checks import check_requirements, print_args\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, logger\nfrom tracking.val import run_trackeval\nfrom tracking.generate_mot_results import run_generate_mot_results\n\n\nclass Objective():\n    """"""""""""Objective function to evolve best set of hyperparams for\n\n    This object is passed to an objective function and provides inte","@@ -335,6 +335,8 @@ def parse_opt():\n                         help='how many subprocesses can be invoked per GPU (to manage memory consumption)')\n     parser.add_argument('--objectives', type=str, default='HOTA,MOTA,IDF1',\n                         help='set of objective metrics: HOTA,MOTA,IDF1')\n+    parser.add_argument('--verbose', default=False, action='store_true',\n+                        help='print results per frame')\n \n     opt = parser.parse_args()\n     opt.tracking_config = ROOT / 'boxmot' / 'configs' / (opt.tracking_method + '.yaml')\n",add,Add note about data volume to enable_metrics_collection
dbaf0586904820d5bf931838efca4a315256d8a1,fix evolution,tracking/val.py,"import re\nimport sys\nimport json\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        \n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        \n        # pack everything in dict\n        results_dict = {}\n        for key, value in zip([""""HOTA"""", """"MOTA"""", """"ID","import re\nimport sys\nimport json\nimport argparse\nimport subprocess\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, DATA\nfrom pathlib import Path\nfrom tracking.utils import (\n    download_mot_eval_tools,\n    download_mot_dataset,\n    unzip_mot_dataset,\n    eval_setup\n)\nfrom ultralytics.utils.files import increment_path \n\n\n\ndef parse_mot_results(results):\n        """"""""""""Extract the COMBINED HOTA, MOTA, IDF1 from the results generate by the\n           run_mot_challenge.py script.\n\n        Args:\n            str: mot_results\n\n        Returns:\n            (dict): {'HOTA': x, 'MOTA':y, 'IDF1':z}\n        """"""""""""\n        combined_results = results.split('COMBINED')[2:-1]\n        \n        # robust way of getting first ints/float in string\n        combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0]) for f in combined_results]\n        \n        # pack everything in dict\n        results_dict = {}\n        for key, value in zip([""""HOTA"""", """"MOTA"""", """"ID","@@ -179,7 +179,7 @@ def run_trackeval(opt):\n         with open(opt.tracking_method + """"_output.json"""", """"w"""") as outfile:\n             outfile.write(json.dumps(hota_mota_idf1))\n     print(json.dumps(hota_mota_idf1))\n-    return json.dumps(hota_mota_idf1)\n+    return hota_mota_idf1\n \n if __name__ == """"__main__"""":\n     run_trackeval(None)\n",fix,Don ' t define the dependency on real_commad twice
c922139c6baf8fb7667451d93cf06e2f4d19a86e,fix import,boxmot/appearance/backends/base_backend.py,"import cv2\nimport torch\nimport gdown\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.reid_model_factory import (\n    get_model_name,\n    get_model_url,\n    build_model,\n    get_nr_classes\n)\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights[0] if isinstance(weights, list) else weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n\n        self.download_model(self.weights)\n        self.model_name = get_model_name(self.weights)\n\n        self.model = build_model(\n            self.model_name,\n            num_classes=get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n        )\n        self.load_model(self.weights)\n\n        \n    @abstractmethod\n    de","import cv2\nimport torch\nimport gdown\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.reid_model_factory import (\n    get_model_name,\n    get_model_url,\n    build_model,\n    get_nr_classes,\n    show_downloadable_models\n)\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights[0] if isinstance(weights, list) else weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n\n        self.download_model(self.weights)\n        self.model_name = get_model_name(self.weights)\n\n        self.model = build_model(\n            self.model_name,\n            num_classes=get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n        )\n        self.load_model(self.weights)\n\n      ","@@ -8,7 +8,8 @@ from boxmot.appearance.reid_model_factory import (\n     get_model_name,\n     get_model_url,\n     build_model,\n-    get_nr_classes\n+    get_nr_classes,\n+    show_downloadable_models\n )\n \n class BaseModelBackend:\n",add,Add - lpam when linking pam_google_authenticator module
98f88d9f22180060ef1400a9426f16e47e3219a5,fix,boxmot/appearance/backends/base_backend.py,"import cv2\nimport torch\nimport gdown\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.reid_model_factory import (\n    get_model_name,\n    get_model_url,\n    build_model,\n    get_nr_classes,\n    show_downloadable_models\n)\nfrom boxmot.utils.checks import RequirementsChecker\n\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights[0] if isinstance(weights, list) else weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n\n        self.download_model(self.weights)\n        self.model_name = get_model_name(self.weights)\n\n        self.model = build_model(\n            self.model_name,\n            num_classes=get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n   ","import cv2\nimport torch\nimport gdown\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.reid_model_factory import (\n    get_model_name,\n    get_model_url,\n    build_model,\n    get_nr_classes,\n    show_downloadable_models\n)\nfrom boxmot.utils.checks import RequirementsChecker\n\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights[0] if isinstance(weights, list) else weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n\n        self.download_model(self.weights)\n        self.model_name = get_model_name(self.weights)\n\n        self.model = build_model(\n            self.model_name,\n            num_classes=get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n   ","@@ -34,11 +34,6 @@ class BaseModelBackend:\n         self.checker = RequirementsChecker()\n         self.load_model(self.weights)\n \n-        \n-    @abstractmethod\n-    def load_model(self):\n-        raise NotImplementedError(""""This method should be implemented by subclasses."""")\n-\n     def get_crops(self, xyxys, img):\n         crops = []\n         h, w = img.shape[:2]\n@@ -123,11 +118,11 @@ class BaseModelBackend:\n \n     @abstractmethod\n     def forward(self, im_batch):\n-        pass\n+        raise NotImplementedError(""""This method should be implemented by subclasses."""")\n \n     @abstractmethod\n     def load_model(self, w):\n-        pass\n+        raise NotImplementedError(""""This method should be implemented by subclasses."""")\n \n \n     def download_model(self, w):\n",add,Added Type name for DFI ( # 3 )
3a20778ef69b8e9a1d84cb4a9d10f5f7e5511237,fix bs,boxmot/appearance/backends/tflite_backend.py,"import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\n\n\nclass TFLiteBackend(BaseModelBackend):\n    """"""""""""\n    A class to handle TensorFlow Lite model inference with dynamic batch size support.\n\n    Attributes:\n        nhwc (bool): A flag indicating the order of dimensions.\n        half (bool): A flag to indicate if half precision is used.\n        interpreter (tf.lite.Interpreter): The TensorFlow Lite interpreter.\n        current_allocated_batch_size (int): The current batch size allocated in the interpreter.\n    """"""""""""\n\n    def __init__(self, weights: Path, device: str, half: bool):\n        """"""""""""\n        Initializes the TFLiteBackend with given weights, device, and precision flag.\n\n        Args:\n            weights (Path): Path to the TFLite model file.\n            device (str): Device type (e.g., 'cpu', 'gpu').\n            half (bool): Flag to i","import torch\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import logger as LOGGER\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\n\n\nclass TFLiteBackend(BaseModelBackend):\n    """"""""""""\n    A class to handle TensorFlow Lite model inference with dynamic batch size support.\n\n    Attributes:\n        nhwc (bool): A flag indicating the order of dimensions.\n        half (bool): A flag to indicate if half precision is used.\n        interpreter (tf.lite.Interpreter): The TensorFlow Lite interpreter.\n        current_allocated_batch_size (int): The current batch size allocated in the interpreter.\n    """"""""""""\n\n    def __init__(self, weights: Path, device: str, half: bool):\n        """"""""""""\n        Initializes the TFLiteBackend with given weights, device, and precision flag.\n\n        Args:\n            weights (Path): Path to the TFLite model file.\n            device (str): Device type (e.g., 'cpu', 'gpu').\n            half (bool): Flag to i","@@ -30,7 +30,7 @@ class TFLiteBackend(BaseModelBackend):\n         self.nhwc = False\n         self.half = half\n         # self.interpreter: tf.lite.Interpreter = None\n-        self.current_allocated_batch_size: int = None\n+        # self.current_allocated_batch_size: int = None\n \n     def load_model(self, w):\n         """"""""""""\n",add,Add note about data volume to enable EGL
4dad1d8aaafd5cfcda429d1649ad70d47a810ea9,fix user input,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport re\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nchecker = Requirement","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBac","@@ -6,6 +6,8 @@ from pathlib import Path\n import numpy as np\n from tqdm import tqdm\n import json\n+import queue\n+import select\n import re\n import torch\n from functools import partial\n@@ -30,29 +32,23 @@ from boxmot.appearance.reid_auto_backend import ReidAutoBackend\n checker = RequirementsChecker()\n checker.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n \n-def prompt_overwrite(path_type, path, auto_overwrite=False):\n-    if auto_overwrite:\n-        print(f""""{path_type} {path} already exists. Auto-overwriting due to no UI mode."""")\n-        return True\n+\n+def prompt_overwrite(path_type, path, ci=False):\n+    if ci:\n+        print(f""""{path_type} {path} already exists. Use existing due to no UI mode."""")\n+        return False\n     \n     def input_with_timeout(prompt, timeout=3.0):\n         print(prompt, end='', flush=True)\n-        result = [None]\n-        \n-        def inner_input():\n-            result[0] = input()\n-        \n-        thread = threading.Thread(target=inner_input)\n-        thread.start()\n-        thread.join(timeout)\n-        if thread.is_alive():\n+        inputs, _, _ = select.select([sys.stdin], [], [], timeout)\n+        if inputs:\n+            result = sys.stdin.readline().strip().lower()\n+            return result in ['y', 'yes']\n+        else:\n             print(""""\nNo response, proceeding with overwrite..."""")\n             return True\n-        return result[0].strip().lower() in ['y', 'yes']\n     \n-    if input_with_timeout(f""""{path_type} {path} already exists. Overwrite? [y/N]: """"):\n-        return True\n-    return False\n+    return input_with_timeout(f""""{path_type} {path} already exists. Overwrite? [y/N]: """")\n \n def generate_dets_embs(args, y):\n     WEIGHTS.mkdir(parents=True, exist_ok=True)\n@@ -318,8 +314,8 @@ def parse_opt():\n                         help='MOT16, MOT17, MOT20')\n     parser.add_argument('--split', type=str, default='train',\n ",add,Add forced default for text type in oCC
2235d750d688ec0a35b671dcaccdd93949cccfa5,fix imports,tracking/evolve.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport argparse\n\nimport yaml\nfrom ultralytics.utils.checks import check_requirements, print_args\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, logger\nfrom tracking.val import run_trackeval\nfrom tracking.generate_mot_results import run_generate_mot_results\n\n\nclass Objective():\n    """"""""""""Objective function to evolve best set of hyperparams for\n\n    This object is passed to an objective function and provides inte","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport argparse\n\nimport yaml\nfrom ultralytics.utils.checks import check_requirements, print_args\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, logger\nfrom tracking.val import run_trackeval\nfrom tracking.val import run_generate_mot_results, run_trackeval\n\n\n\nclass Objective():\n    """"""""""""Objective function to evolve best set of hyperparams for\n\n    This object is passed to an objective function and provides inte","@@ -19,7 +19,8 @@ from ultralytics.utils.checks import check_requirements, print_args\n \n from boxmot.utils import EXAMPLES, ROOT, WEIGHTS, logger\n from tracking.val import run_trackeval\n-from tracking.generate_mot_results import run_generate_mot_results\n+from tracking.val import run_generate_mot_results, run_trackeval\n+\n \n \n class Objective():\n",add,Add note about data volume to enable_metrics_collection
edae8e430e4c431eef1715c1f92f712c562d7de9,fix evolve params,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","@@ -350,6 +350,10 @@ def parse_opt():\n     parser.add_argument('--split', type=str, default='train', help='existing project/name ok, do not increment')\n     parser.add_argument('--verbose', action='store_true', help='print results')\n     parser.add_argument('--agnostic-nms', default=False, action='store_true', help='class-agnostic NMS')\n+    parser.add_argument('--n-trials', type=int, default=4, help='nr of trials for evolution')\n+    parser.add_argument('--resume', action='store_true', help='resume hparam search')\n+    parser.add_argument('--processes-per-device', type=int, default=2, help='how many subprocesses can be invoked per GPU (to manage memory consumption)')\n+    parser.add_argument('--objectives', type=str, default='HOTA,MOTA,IDF1', help='set of objective metrics: HOTA,MOTA,IDF1')\n \n     subparsers = parser.add_subparsers(dest='command')\n \n",add,Add note about data volume to enable_metrics_collection
6bd4363a771443fa650d19da787130aabb3addd7,fix evolve,tracking/evolve.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport argparse\n\nimport yaml\nfrom ultralytics.utils.checks import check_requirements, print_args\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, logger\nfrom tracking.val import run_trackeval\nfrom tracking.val import run_generate_mot_results, run_trackeval, parse_opt as parse_optt\n\n\n\nclass Objective():\n    """"""""""""Objective function to evolve best set of hyperparams for\n\n    This object is passed to an objective f","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nEvolve hyperparameters for the specific selected tracking method and a specific dataset.\nThe best set of hyperparameters is written to the config file of the selected tracker\n(trackers/<tracking-method>/configs). Tracker parameter importance and pareto front plots\nare generated as well.\n\nUsage:\n\n    $ python3 evolve.py --tracking-method strongsort --benchmark MOT17 --device 0,1,2,3 --n-trials 100\n                        --tracking-method ocsort     --benchmark MOT16 --n-trials 1000\n""""""""""""\n\nimport argparse\n\nimport yaml\nfrom ultralytics.utils.checks import check_requirements, print_args\n\nfrom boxmot.utils import EXAMPLES, ROOT, WEIGHTS, logger\nfrom tracking.val import run_trackeval\nfrom tracking.val import run_generate_mot_results, run_trackeval, parse_opt as parse_optt\n\n\n\nclass Objective():\n    """"""""""""Objective function to evolve best set of hyperparams for\n\n    This object is passed to an objective f","@@ -328,7 +328,6 @@ def parse_opt():\n     reid_model_stem = (opt.reid_model[0]).stem\n     default_name = f""""{yolo_model_stem}_{reid_model_stem}""""\n     opt.name = default_name\n-    opt.yolo_model = opt.yolo_model[0]\n     \n     opt.tracking_config = ROOT / 'boxmot' / 'configs' / (opt.tracking_method + '.yaml')\n     opt.objectives = opt.objectives.split("""","""")\n",fix,Add KHR_gl_texture_2D_image extension string .
e110319b04b7fe545551b1b66ed0c67a8ff8fcb8,fix MOTXX dataset extraction location,tracking/utils.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nfrom ultralytics.utils import ops\nfrom ultralytics.engine.results import Results\nfrom typing import Union\nfrom pathlib import Path\nimport os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\nfrom boxmot.utils import EXAMPLES, ROOT\n\n\ndef download_mot_eval_tools(val_tools_path):\n    """"""""""""\n    Download the official evaluation tools for MOT metrics from the GitHub repository.\n    \n    Parameters:\n        val_tools_path (Path): Path to the destination folder where the evaluation tools will be downloaded.\n    \n    Returns:\n        None. Clones the evaluation tools repository and updates deprecated numpy types.\n    """"""""""""\n    val_tools_url = """"https://github.com/JonathonLuiten/TrackEval""""\n\n    try:\n        # Clone the repository\n        Repo.clone_from(val_tools_url","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nfrom ultralytics.utils import ops\nfrom ultralytics.engine.results import Results\nfrom typing import Union\nfrom pathlib import Path\nimport os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\nfrom boxmot.utils import EXAMPLES, ROOT\n\n\ndef download_mot_eval_tools(val_tools_path):\n    """"""""""""\n    Download the official evaluation tools for MOT metrics from the GitHub repository.\n    \n    Parameters:\n        val_tools_path (Path): Path to the destination folder where the evaluation tools will be downloaded.\n    \n    Returns:\n        None. Clones the evaluation tools repository and updates deprecated numpy types.\n    """"""""""""\n    val_tools_url = """"https://github.com/JonathonLuiten/TrackEval""""\n\n    try:\n        # Clone the repository\n        Repo.clone_from(val_tools_url","@@ -121,7 +121,8 @@ def unzip_mot_dataset(zip_path, val_tools_path, benchmark):\n     if not extract_path.exists():\n         try:\n             with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n-                zip_ref.extractall(extract_path)\n+                # folder will be called as the original fetched file\n+                zip_ref.extractall(val_tools_path / 'data')\n \n             LOGGER.info(f'{benchmark}.zip unzipped successfully.')\n         except zipfile.BadZipFile:\n",add,Add warning about data volume to enable_metrics_collection
5eef0e1471015e2a28037371a5e670c20ad8098c,not overwrite dets and embs as default,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","@@ -46,11 +46,12 @@ def prompt_overwrite(path_type, path, ci=False):\n             result = sys.stdin.readline().strip().lower()\n             return result in ['y', 'yes']\n         else:\n-            print(""""\nNo response, proceeding with overwrite..."""")\n-            return True\n+            print(""""\nNo response, not proceeding with overwrite..."""")\n+            return False\n     \n     return input_with_timeout(f""""{path_type} {path} already exists. Overwrite? [y/N]: """")\n \n+\n def generate_dets_embs(args, y):\n     WEIGHTS.mkdir(parents=True, exist_ok=True)\n \n",add,Add note about data volume to enable_metrics_collection
880ea6b904e5d0a4a5f1eb87399dadae88f71160,fix tensorrt export,boxmot/appearance/exporters/tensorrt_exporter.py,"import platform\nimport torch\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass EngineExporter(BaseExporter):\n    required_packages = (""""nvidia-tensorrt"""")\n    cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n    \n    def export(self):\n\n        assert self.im.device.type != """"cpu"""", """"export running on CPU but must be on GPU, i.e. `python export.py --device 0`""""\n        try:\n            import tensorrt as trt\n        except ImportError:\n            import tensorrt as trt\n\n        onnx_file = self.export_onnx()\n        LOGGER.info(f""""\nStarting export with TensorRT {trt.__version__}..."""")\n        assert onnx_file.exists(), f""""Failed to export ONNX file: {onnx_file}""""\n        f = self.file.with_suffix("""".engine"""")\n        logger = trt.Logger(trt.Logger.INFO)\n        if self.verbose:\n            logger.min_severity = trt.Logger.Severity.VERBOSE\n\n        builder = trt.Builder(logger)\n        ","import platform\nimport torch\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.appearance.exporters.onnx_exporter import ONNXExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass EngineExporter(BaseExporter):\n    required_packages = (""""nvidia-tensorrt"""",)\n    cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n    \n    def export(self):\n\n        assert self.im.device.type != """"cpu"""", """"export running on CPU but must be on GPU, i.e. `python export.py --device 0`""""\n        try:\n            import tensorrt as trt\n        except ImportError:\n            import tensorrt as trt\n\n        onnx_file = self.export_onnx()\n        LOGGER.info(f""""\nStarting export with TensorRT {trt.__version__}..."""")\n        assert onnx_file.exists(), f""""Failed to export ONNX file: {onnx_file}""""\n        f = self.file.with_suffix("""".engine"""")\n        logger = trt.Logger(trt.Logger.INFO)\n        if self.verbose:\n            logger.min_severity = trt.Logge","@@ -1,11 +1,12 @@\n import platform\n import torch\n from boxmot.appearance.exporters.base_exporter import BaseExporter\n+from boxmot.appearance.exporters.onnx_exporter import ONNXExporter\n from boxmot.utils import logger as LOGGER\n \n \n class EngineExporter(BaseExporter):\n-    required_packages = (""""nvidia-tensorrt"""")\n+    required_packages = (""""nvidia-tensorrt"""",)\n     cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n     \n     def export(self):\n",add,Add note about data volume to enable_metrics_collection
cc34c7206422bef42d0ccabbe69d47871049e8df,fix evolve,.github/workflows/ci.yml,"# name of the workflow, what it is doing (optional)\nname: BoxMOT CI\n\n# events that trigger the workflow (required)\non:\n  push:\n    # pushes to the following branches\n    branches:\n      - master\n  pull_request:\n    # pull request where master is target\n    branches:\n      - master\n\n\njobs:\n  tracking-methods:\n    runs-on: ${{ matrix.os }}\n    outputs:\n      status: ${{ job.status }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, macos-14]   # skip windows-latest for\n        python-version: ['3.11']\n        # leads to too many workflow which ends up queued\n        # tracking-method: [hybridsort, botsort, ocsort, bytetrack] \n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      - uses: actions/checkout@v4  # Check out the repository\n      - name: Set up Python\n        uses: actions/setup-python@v5  # Prepare environment with python 3.9\n        with:\n          python-version: ","# name of the workflow, what it is doing (optional)\nname: BoxMOT CI\n\n# events that trigger the workflow (required)\non:\n  push:\n    # pushes to the following branches\n    branches:\n      - master\n  pull_request:\n    # pull request where master is target\n    branches:\n      - master\n\n\njobs:\n  tracking-methods:\n    runs-on: ${{ matrix.os }}\n    outputs:\n      status: ${{ job.status }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, macos-14]   # skip windows-latest for\n        python-version: ['3.11']\n        # leads to too many workflow which ends up queued\n        # tracking-method: [hybridsort, botsort, ocsort, bytetrack] \n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      - uses: actions/checkout@v4  # Check out the repository\n      - name: Set up Python\n        uses: actions/setup-python@v5  # Prepare environment with python 3.9\n        with:\n          python-version: ","@@ -95,13 +95,10 @@ jobs:\n           poetry lock --no-update\n           poetry install --with yolo,evolve\n \n-      - name: Generate detections and embeddings\n-        run: |\n-          python tracking/val.py generate_dets_embs --source ./assets/MOT17-mini/train  --yolo-model yolov8n.pt --reid-model osnet_x0_25_msmt17.pt --imgsz 320\n-\n       - name: Evolve set of parameters for selected tracking method\n         run: |\n-          python tracking/evolve.py --benchmark MOT17-mini --yolo-model yolov8n.pt --reid-model osnet_x0_25_msmt17.pt --n-trials 9 --tracking-method bytetrack\n+          pwd\n+          python tracking/evolve.py --benchmark MOT17-mini --yolo-model yolov8n.pt --reid-model osnet_x0_25_msmt17.pt --n-trials 1 --tracking-method strongsort --source ./assets/MOT17-mini/train\n \n \n   mot-metrics-benchmark:\n",add,Add note about data volume to enable EGL
cc34c7206422bef42d0ccabbe69d47871049e8df,fix evolve,tracking/evolve.py,"\nimport os\nimport yaml\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.val import run_generate_mot_results, run_trackeval, parse_opt as parse_optt\nfrom boxmot.utils import ROOT\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n        self.parameters = parameters\n\n    def get_new_config(self, config):\n        # Update the options with the current config\n        self.parameters.update(config)\n        # Overwrite the local file with the new parameters\n        tracking_config = ROOT / 'boxmot' / 'configs' / (self.opt.tracking_method + '.yaml')\n        with open(tracking_config, 'w') as f:\n            yaml.dump(self.parameters, f)\n\n    def objective_function(self, config):\n        # Generate new set of params\n        # sel","\nimport os\nimport yaml\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt\n)\nfrom boxmot.utils import ROOT\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n        self.parameters = parameters\n\n    def get_new_config(self, config):\n        # Update the options with the current config\n        self.parameters.update(config)\n        # Overwrite the local file with the new parameters\n        tracking_config = ROOT / 'boxmot' / 'configs' / (self.opt.tracking_method + '.yaml')\n        with open(tracking_config, 'w') as f:\n            yaml.dump(self.parameters, f)\n\n    def objective_function(self, config):\n  ","@@ -2,7 +2,12 @@\n import os\n import yaml\n from boxmot.utils.checks import RequirementsChecker\n-from tracking.val import run_generate_mot_results, run_trackeval, parse_opt as parse_optt\n+from tracking.val import (\n+    run_generate_dets_embs,\n+    run_generate_mot_results,\n+    run_trackeval,\n+    parse_opt as parse_optt\n+)\n from boxmot.utils import ROOT\n \n checker = RequirementsChecker()\n@@ -29,14 +34,15 @@ class Tracker:\n \n     def objective_function(self, config):\n         # Generate new set of params\n-        # self.get_new_config(config)\n-        # # Run trial, get HOTA, MOTA, IDF1 combined results\n-        # run_generate_mot_results(self.opt)\n-        # results = run_trackeval(self.opt)\n-        # # Extract objective results of the current trial\n-        # combined_results = {key: results.get(key) for key in self.opt['objectives']}\n-        #return combined_results\n-        return {""""HOTA"""": 0.1, """"MOTA"""": 0.1, """"IDF1"""": 0.1}\n+        self.get_new_config(config)\n+        # Run trial, get HOTA, MOTA, IDF1 combined results\n+        run_generate_dets_embs(self.opt)\n+        run_generate_mot_results(self.opt)\n+        results = run_trackeval(self.opt)\n+        # Extract objective results of the current trial\n+        combined_results = {key: results.get(key) for key in self.opt.objectives}\n+        return combined_results\n+        #return {""""HOTA"""": 0.1, """"MOTA"""": 0.1, """"IDF1"""": 0.1}\n \n # Define the search space for hyperparameters\n search_space = {\n@@ -74,7 +80,7 @@ results_dir = os.path.abspath(""""results/"""")\n tuner = tune.Tuner(\n     tune.with_resources(train, {""""cpu"""": 1, """"gpu"""": 0}),  # Adjust resources as needed\n     param_space=search_space,\n-    tune_config=tune.TuneConfig(scheduler=asha_scheduler, num_samples=10),\n+    tune_config=tune.TuneConfig(scheduler=asha_scheduler, num_samples=opt.n_trials),\n     run_config=RunConfig(storage_path=results_dir)\n )\n \n",add,Add note about data volume to enable_metrics_collection
d5e2f4af2a92dce2f80e25a9c58e96d88297e70d,fix mot results append bug for each trial,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n\n    def __get__(self, instance, owner):\n        # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n        def wr","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of BoxMOT multiprocessing threads\n\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n\n    def __get__(self, instance, owner):\n     ","@@ -1,5 +1,6 @@\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n+import os\n import sys\n from pathlib import Path\n \n@@ -14,6 +15,9 @@ TRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\n WEIGHTS = ROOT / """"tracking"""" / """"weights""""\n REQUIREMENTS = ROOT / """"requirements.txt""""\n \n+NUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of BoxMOT multiprocessing threads\n+\n+\n # global logger\n from loguru import logger\n \n",add,only show interlineWithPreviousLeg where true
d5e2f4af2a92dce2f80e25a9c58e96d88297e70d,fix mot results append bug for each trial,tracking/evolve.py,"\nimport os\nimport yaml\nfrom pathlib import Path\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt\n)\nfrom boxmot.utils import ROOT\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n        self.parameters = parameters\n\n    def get_new_config(self, config):\n        # Update the options with the current config\n        self.parameters.update(config)\n        # Overwrite the local file with the new parameters\n        tracking_config = ROOT / 'boxmot' / 'configs' / (self.opt.tracking_method + '.yaml')\n        with open(tracking_config, 'w') as f:\n            yaml.dump(self.parameters, f)\n\n    def objective_f","\nimport os\nimport yaml\nfrom pathlib import Path\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt\n)\nfrom boxmot.utils import ROOT, NUM_THREADS\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n        self.parameters = parameters\n\n    def get_new_config(self, config):\n        # Update the options with the current config\n        self.parameters.update(config)\n        # Overwrite the local file with the new parameters\n        tracking_config = ROOT / 'boxmot' / 'configs' / (self.opt.tracking_method + '.yaml')\n        with open(tracking_config, 'w') as f:\n            yaml.dump(self.parameters, f)\n\n    de","@@ -9,7 +9,7 @@ from tracking.val import (\n     run_trackeval,\n     parse_opt as parse_optt\n )\n-from boxmot.utils import ROOT\n+from boxmot.utils import ROOT, NUM_THREADS\n \n checker = RequirementsChecker()\n checker.check_packages(('ray[tune]',))  # install\n@@ -37,7 +37,6 @@ class Tracker:\n         # Generate new set of params\n         self.get_new_config(config)\n         # Run trial, get HOTA, MOTA, IDF1 combined results\n-        run_generate_dets_embs(self.opt)\n         run_generate_mot_results(self.opt)\n         results = run_trackeval(self.opt)\n         # Extract objective results of the current trial\n@@ -62,6 +61,7 @@ search_space = {\n opt = parse_optt()\n opt.source = Path(opt.source).resolve()\n tracker = Tracker(opt, search_space)\n+run_generate_dets_embs(opt)\n \n def train(config):\n     return tracker.objective_function(config)\n@@ -80,7 +80,7 @@ asha_scheduler = ASHAScheduler(\n results_dir = os.path.abspath(""""results/"""")\n # Run Ray Tune\n tuner = tune.Tuner(\n-    tune.with_resources(train, {""""cpu"""": 1, """"gpu"""": 0}),  # Adjust resources as needed\n+    tune.with_resources(train, {""""cpu"""": NUM_THREADS, """"gpu"""": 0}),  # Adjust resources as needed\n     param_space=search_space,\n     tune_config=tune.TuneConfig(scheduler=asha_scheduler, num_samples=opt.n_trials),\n     run_config=RunConfig(storage_path=results_dir)\n",add,Added STORM - 1270 to Changelog
d5e2f4af2a92dce2f80e25a9c58e96d88297e70d,fix mot results append bug for each trial,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","@@ -168,10 +168,14 @@ def generate_mot_results(args):\n \n     dets_n_embs = np.concatenate([dets, embs], axis=1)\n \n-    print('args.source', args.source)\n     dataset = LoadImages(args.source)\n \n+    # create new MOT folder if txt file already exists\n     txt_path = args.exp_folder_path / (Path(args.source).parent.name + '.txt')\n+    if txt_path.exists():\n+        args.exp_folder_path = increment_path(path=args.exp_folder_path, sep=""""_"""", exist_ok=False)\n+        txt_path = args.exp_folder_path / (Path(args.source).parent.name + '.txt')\n+        \n     all_mot_results = []\n \n     for frame_idx, d in enumerate(tqdm(dataset, desc=""""Frames"""")):\n",fix,Add warning about data volume to enable_metrics_collection
6b18fc6f01d32d79a9e83b5e20eeba0fbc8f52de,fix config usage in objective function,tracking/evolve.py,"\nimport os\nimport yaml\nfrom pathlib import Path\n\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt\n)\nfrom boxmot.utils import ROOT, NUM_THREADS\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n\n    def objective_function(self, config):\n        config.update(config)\n        # Generate new set of params\n        # Run trial, get HOTA, MOTA, IDF1 combined results\n        run_generate_mot_results(self.opt, config)\n        results = run_trackeval(self.opt)\n        # Extract objective results of the current trial\n        combined_results = {key: results.get(key) for key in self.opt.objectives}\n        retur","\nimport os\nimport yaml\nfrom pathlib import Path\n\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt\n)\nfrom boxmot.utils import ROOT, NUM_THREADS\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n\n    def objective_function(self, config):\n        # Generate new set of params\n        # Run trial, get HOTA, MOTA, IDF1 combined results\n        run_generate_mot_results(self.opt, config)\n        results = run_trackeval(self.opt)\n        # Extract objective results of the current trial\n        combined_results = {key: results.get(key) for key in self.opt.objectives}\n        return combined_results\n        #re","@@ -26,7 +26,6 @@ class Tracker:\n         self.opt = opt\n \n     def objective_function(self, config):\n-        config.update(config)\n         # Generate new set of params\n         # Run trial, get HOTA, MOTA, IDF1 combined results\n         run_generate_mot_results(self.opt, config)\n",add,Add note about data volume to enable_metrics_collection
c8bbbf601a467ea91e76ba90a5b22b016f612ec4,fix execution order,tracking/evolve.py,"\nimport os\nimport yaml\nfrom pathlib import Path\n\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt\n)\nfrom boxmot.utils import ROOT, NUM_THREADS\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n\n    def objective_function(self, config):\n        # generate new set of mot challenge compliant results with\n        # new set of generated tracker parameters\n        run_generate_mot_results(self.opt, config)\n        # get MOTA, HOTA, IDF1 results\n        results = run_trackeval(self.opt)\n        # Extract objective results\n        combined_results = {key: results.get(key) for key in self.opt.objectives}\n    ","\nimport os\nimport yaml\nfrom pathlib import Path\n\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt\n)\nfrom boxmot.utils import ROOT, NUM_THREADS\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n\n    def objective_function(self, config):\n        # generate new set of mot challenge compliant results with\n        # new set of generated tracker parameters\n        run_generate_mot_results(self.opt, config)\n        # get MOTA, HOTA, IDF1 results\n        results = run_trackeval(self.opt)\n        # Extract objective results\n        combined_results = {key: results.get(key) for key in self.opt.objectives}\n    ","@@ -117,7 +117,7 @@ def get_search_space(tracking_method):\n             """"track_high_thresh"""": tune.uniform(0.3, 0.7),\n             """"track_low_thresh"""": tune.uniform(0.1, 0.3),\n             """"new_track_thresh"""": tune.uniform(0.1, 0.8),\n-            """"track_buffer"""": tune.randint(20, 81, 10),  # The upper bound is exclusive in randint\n+            """"track_buffer"""": tune.qrandint(20, 80, 10),  # The upper bound is exclusive in randint\n             """"match_thresh"""": tune.uniform(0.1, 0.9),\n             """"second_match_thresh"""": tune.uniform(0.1, 0.4),\n             """"overlap_thresh"""": tune.uniform(0.3, 0.6),\n",add,Add note about data volume to enable_metrics_collection
c8bbbf601a467ea91e76ba90a5b22b016f612ec4,fix execution order,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","@@ -34,11 +34,22 @@ checker = RequirementsChecker()\n checker.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n \n \n-def prompt_overwrite(path_type, path, ci=False):\n+def prompt_overwrite(path_type: str, path: str, ci: bool = False) -> bool:\n+    """"""""""""\n+    Prompts the user to confirm overwriting an existing file.\n+\n+    Args:\n+        path_type (str): Type of the path (e.g., 'Detections and Embeddings', 'MOT Result').\n+        path (str): The path to check.\n+        ci (bool): If True, automatically reuse existing file without prompting (for CI environments).\n+\n+    Returns:\n+        bool: True if user confirms to overwrite, False otherwise.\n+    """"""""""""\n     if ci:\n         print(f""""{path_type} {path} already exists. Use existing due to no UI mode."""")\n         return False\n-    \n+\n     def input_with_timeout(prompt, timeout=3.0):\n         print(prompt, end='', flush=True)\n         inputs, _, _ = select.select([sys.stdin], [], [], timeout)\n@@ -48,16 +59,21 @@ def prompt_overwrite(path_type, path, ci=False):\n         else:\n             print(""""\nNo response, not proceeding with overwrite..."""")\n             return False\n-    \n+\n     return input_with_timeout(f""""{path_type} {path} already exists. Overwrite? [y/N]: """")\n \n \n-def generate_dets_embs(args, y):\n+def generate_dets_embs(args: argparse.Namespace, y: Path) -> None:\n+    """"""""""""\n+    Generates detections and embeddings for the specified YOLO model and arguments.\n+\n+    Args:\n+        args (Namespace): Parsed command line arguments.\n+        y (Path): Path to the YOLO model file.\n+    """"""""""""\n     WEIGHTS.mkdir(parents=True, exist_ok=True)\n \n-    yolo = YOLO(\n-        y if 'yolov8' in str(y) else 'yolov8n.pt',\n-    )\n+    yolo = YOLO(y if 'yolov8' in str(y) else 'yolov8n.pt')\n \n     results = yolo(\n         source=args.source,\n@@ -76,58 +92,44 @@ def generate_dets_embs(args, y):\n     )\n \n     if 'yolov8' not in ",add,Don ' t need the Reader topics selector if our activity has finished .
de352989e3e26107ce77e84b80a9ce95f0123ab8,fix val tools path bug,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","@@ -325,6 +325,7 @@ def run_trackeval(opt: argparse.Namespace) -> dict:\n     Args:\n         opt (Namespace): Parsed command line arguments.\n     """"""""""""\n+    opt.val_tools_path = EXAMPLES / 'val_utils'\n     seq_paths, save_dir, MOT_results_folder, gt_folder = eval_setup(opt, opt.val_tools_path)\n     trackeval_results = trackeval(opt, seq_paths, save_dir, MOT_results_folder, gt_folder)\n     hota_mota_idf1 = parse_mot_results(trackeval_results)\n",add,Fix typo
b1449205fff740565dc4170d21f659df1c760476,fix val tools path bug,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","@@ -325,7 +325,6 @@ def run_trackeval(opt: argparse.Namespace) -> dict:\n     Args:\n         opt (Namespace): Parsed command line arguments.\n     """"""""""""\n-    opt.val_tools_path = EXAMPLES / 'val_utils'\n     seq_paths, save_dir, MOT_results_folder, gt_folder = eval_setup(opt, opt.val_tools_path)\n     trackeval_results = trackeval(opt, seq_paths, save_dir, MOT_results_folder, gt_folder)\n     hota_mota_idf1 = parse_mot_results(trackeval_results)\n@@ -377,6 +376,7 @@ def parse_opt() -> argparse.Namespace:\n     parser.add_argument('--agnostic-nms', default=False, action='store_true', help='class-agnostic NMS')\n     parser.add_argument('--n-trials', type=int, default=4, help='nr of trials for evolution')\n     parser.add_argument('--objectives', type=str, nargs='+', default=[""""HOTA"""", """"MOTA"""", """"IDF1""""], help='set of objective metrics: HOTA,MOTA,IDF1')\n+    parser.add_argument('--val-tools-path', type=Path, default=EXAMPLES / 'val_utils', help='path to trackeval repo')\n \n     subparsers = parser.add_subparsers(dest='command')\n \n@@ -406,7 +406,6 @@ if __name__ == """"__main__"""":\n     opt = parse_opt()\n \n     # download MOT benchmark\n-    opt.val_tools_path = EXAMPLES / 'val_utils'\n     download_mot_eval_tools(opt.val_tools_path)\n     zip_path = download_mot_dataset(opt.val_tools_path, opt.benchmark)\n     unzip_mot_dataset(zip_path, opt.val_tools_path, opt.benchmark)\n",add,Fix typo
776f61b527a8f24c73e2736510fe0d16ab95c74d,fix val tools path bug,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","@@ -325,6 +325,7 @@ def run_trackeval(opt: argparse.Namespace) -> dict:\n     Args:\n         opt (Namespace): Parsed command line arguments.\n     """"""""""""\n+    opt.val_tools_path = EXAMPLES / 'val_utils'\n     seq_paths, save_dir, MOT_results_folder, gt_folder = eval_setup(opt, opt.val_tools_path)\n     trackeval_results = trackeval(opt, seq_paths, save_dir, MOT_results_folder, gt_folder)\n     hota_mota_idf1 = parse_mot_results(trackeval_results)\n@@ -376,7 +377,6 @@ def parse_opt() -> argparse.Namespace:\n     parser.add_argument('--agnostic-nms', default=False, action='store_true', help='class-agnostic NMS')\n     parser.add_argument('--n-trials', type=int, default=4, help='nr of trials for evolution')\n     parser.add_argument('--objectives', type=str, nargs='+', default=[""""HOTA"""", """"MOTA"""", """"IDF1""""], help='set of objective metrics: HOTA,MOTA,IDF1')\n-    parser.add_argument('--val-tools-path', type=Path, default=EXAMPLES / 'val_utils', help='path to trackeval repo')\n \n     subparsers = parser.add_subparsers(dest='command')\n \n@@ -406,6 +406,7 @@ if __name__ == """"__main__"""":\n     opt = parse_opt()\n \n     # download MOT benchmark\n+    opt.val_tools_path = EXAMPLES / 'val_utils'\n     download_mot_eval_tools(opt.val_tools_path)\n     zip_path = download_mot_dataset(opt.val_tools_path, opt.benchmark)\n     unzip_mot_dataset(zip_path, opt.val_tools_path, opt.benchmark)\n",add,Fix typo
bfa5e19b4324912bb34dd24b14595cd250b1785c,fix,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","@@ -325,7 +325,6 @@ def run_trackeval(opt: argparse.Namespace) -> dict:\n     Args:\n         opt (Namespace): Parsed command line arguments.\n     """"""""""""\n-    opt.val_tools_path = EXAMPLES / 'val_utils'\n     seq_paths, save_dir, MOT_results_folder, gt_folder = eval_setup(opt, opt.val_tools_path)\n     trackeval_results = trackeval(opt, seq_paths, save_dir, MOT_results_folder, gt_folder)\n     hota_mota_idf1 = parse_mot_results(trackeval_results)\n",add,Fix typo
a5b3c32f59b5b821511f6a9e47fad4eba508c142,set default benchmark to MOT17-mini,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","@@ -370,7 +370,7 @@ def parse_opt() -> argparse.Namespace:\n     parser.add_argument('--dets-file-path', type=Path, help='path to detections file')\n     parser.add_argument('--embs-file-path', type=Path, help='path to embeddings file')\n     parser.add_argument('--exp-folder-path', type=Path, help='path to experiment folder')\n-    parser.add_argument('--benchmark', type=str, default='MOT17', help='MOT16, MOT17, MOT20')\n+    parser.add_argument('--benchmark', type=str, default='MOT17-mini', help='MOT16, MOT17, MOT20')\n     parser.add_argument('--split', type=str, default='train', help='existing project/name ok, do not increment')\n     parser.add_argument('--verbose', action='store_true', help='print results')\n     parser.add_argument('--agnostic-nms', default=False, action='store_true', help='class-agnostic NMS')\n",add,Add note about data volume to enable_metrics_collection
7a1fd0e7a1d06e93a69bcbce8258c1e964382a91,fix,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","@@ -376,6 +376,7 @@ def parse_opt() -> argparse.Namespace:\n     parser.add_argument('--agnostic-nms', default=False, action='store_true', help='class-agnostic NMS')\n     parser.add_argument('--n-trials', type=int, default=4, help='nr of trials for evolution')\n     parser.add_argument('--objectives', type=str, nargs='+', default=[""""HOTA"""", """"MOTA"""", """"IDF1""""], help='set of objective metrics: HOTA,MOTA,IDF1')\n+    parser.add_argument('--val-tools-path', type=Path, default=EXAMPLES / 'val_utils', help='path to store trackeval repo in')\n \n     subparsers = parser.add_subparsers(dest='command')\n \n@@ -405,7 +406,6 @@ if __name__ == """"__main__"""":\n     opt = parse_opt()\n \n     # download MOT benchmark\n-    opt.val_tools_path = EXAMPLES / 'val_utils'\n     download_mot_eval_tools(opt.val_tools_path)\n     zip_path = download_mot_dataset(opt.val_tools_path, opt.benchmark)\n     unzip_mot_dataset(zip_path, opt.val_tools_path, opt.benchmark)\n",add,Add note about data volume to enable_metrics_collection
c22ac2230f5392205688554bb55216e47583c9bc,fix,tracking/evolve.py,"\nimport os\nimport yaml\nfrom pathlib import Path\n\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt\n)\nfrom boxmot.utils import ROOT, NUM_THREADS\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n\n    def objective_function(self, config):\n        # generate new set of mot challenge compliant results with\n        # new set of generated tracker parameters\n        run_generate_mot_results(self.opt, config)\n        # get MOTA, HOTA, IDF1 results\n        results = run_trackeval(self.opt)\n        # Extract objective results\n        combined_results = {key: results.get(key) for key in self.opt.objectives}\n    ","\nimport os\nimport yaml\nfrom pathlib import Path\n\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt\n)\nfrom boxmot.utils import ROOT, NUM_THREADS\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n\n    def objective_function(self, config):\n        # generate new set of mot challenge compliant results with\n        # new set of generated tracker parameters\n        run_generate_mot_results(self.opt, config)\n        # get MOTA, HOTA, IDF1 results\n        results = run_trackeval(self.opt)\n        # Extract objective results\n        combined_results = {key: results.get(key) for key in self.opt.objectives}\n    ","@@ -130,6 +130,7 @@ def get_search_space(tracking_method):\n     return search_space\n         \n opt = parse_optt()\n+opt.val_tools_path = EXAMPLES / 'val_utils'\n opt.source = Path(opt.source).resolve()\n opt.yolo_model = [Path(y).resolve() for y in opt.yolo_model]\n opt.reid_model = [Path(r).resolve() for r in opt.reid_model]\n",add,Added hypest who apparently actually wrote the C # port
ff20ea20364d9899143eff55fceb9aab5a4bd9a8,fix,tracking/evolve.py,"\nimport os\nimport yaml\nfrom pathlib import Path\n\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt\n)\nfrom boxmot.utils import ROOT, NUM_THREADS\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n\n    def objective_function(self, config):\n        # generate new set of mot challenge compliant results with\n        # new set of generated tracker parameters\n        run_generate_mot_results(self.opt, config)\n        # get MOTA, HOTA, IDF1 results\n        results = run_trackeval(self.opt)\n        # Extract objective results\n        combined_results = {key: results.get(key) for key in self.opt.objectives}\n    ","\nimport os\nimport yaml\nfrom pathlib import Path\n\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils import EXAMPLES\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt\n)\nfrom boxmot.utils import ROOT, NUM_THREADS\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n\n    def objective_function(self, config):\n        # generate new set of mot challenge compliant results with\n        # new set of generated tracker parameters\n        run_generate_mot_results(self.opt, config)\n        # get MOTA, HOTA, IDF1 results\n        results = run_trackeval(self.opt)\n        # Extract objective results\n        combined_results = {key: results.get(key) fo","@@ -4,6 +4,7 @@ import yaml\n from pathlib import Path\n \n from boxmot.utils.checks import RequirementsChecker\n+from boxmot.utils import EXAMPLES\n from tracking.val import (\n     run_generate_dets_embs,\n     run_generate_mot_results,\n",add,Added KHR_gl_texture_2D_image extension string
cd0745b862d952978be104f6f0527a8066f5e6e7,fix embedding extraction for each class,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of BoxMOT multiprocessing threads\n\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n\n    def __get__(self, instance, owner):\n     ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of BoxMOT multiprocessing threads\n\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n        self.last_emb_size = None\n            \","@@ -33,6 +33,25 @@ class PerClassDecorator:\n         self.per_class_active_tracks = {}\n         for i in range(self.nr_classes):\n             self.per_class_active_tracks[i] = []\n+        self.last_emb_size = None\n+            \n+    def get_class_dets_n_embs(dets, embs, cls_id):\n+        # can be that there are detections but no embeddings\n+        if dets.size > 0:\n+            class_indices = np.where(dets[:, 5] == cls_id)[0]\n+            class_dets = dets[class_indices]\n+            if embs is not None:\n+                if embs.size > 0:\n+                    class_embs = embs[class_indices]\n+                    self.last_emb_size = class_embs.shape[1]  # Update the last known embedding size\n+        else:\n+            class_dets = np.empty((0, 6))\n+            if self.last_emb_size is not None:\n+                class_embs = np.empty((0, self.last_emb_size))  # Use the last known embedding size\n+            else:\n+                class_embs = None\n+        return class_dets, class_embs\n+        \n \n     def __get__(self, instance, owner):\n         # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n@@ -53,14 +72,8 @@ class PerClassDecorator:\n \n                 for i, cls_id in enumerate(range(self.nr_classes)):\n  \n-                    if dets.size > 0:\n-                        # Get the indices of the detections for the current class\n-                        class_indices = np.where(dets[:, 5] == cls_id)[0]\n-                        class_dets = dets[class_indices]\n-                        class_embs = embs[class_indices]  # Filter embeddings based on indices\n-                    else:\n-                        class_dets = np.empty((0, 6))\n-                        class_embs = np.empty((0, embs.shape[1]))  # Assuming embeddings have the same number of columns\n+                    class_dets, class_embs = get_class_dets_n_embs(dets, embs, cls_id)\n+                    \n                     ",add,Fix bytesPerPixel for SurfaceTexture
cd0745b862d952978be104f6f0527a8066f5e6e7,fix embedding extraction for each class,tests/unit/test_trackers.py,"import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, ImprAssocTrack, get_tracker_config, create_tracker,\n)\n\nfrom boxmot.trackers.ocsort.ocsort import KalmanBoxTracker as OCSortKalmanBoxTracker\nfrom boxmot.trackers.deepocsort.deep_ocsort import KalmanBoxTracker as DeepOCSortKalmanBoxTracker\n\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT, ImprAssocTrack]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort', 'imprassoc']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'imprassoc']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt","import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, ImprAssocTrack, get_tracker_config, create_tracker,\n)\n\nfrom boxmot.trackers.ocsort.ocsort import KalmanBoxTracker as OCSortKalmanBoxTracker\nfrom boxmot.trackers.deepocsort.deep_ocsort import KalmanBoxTracker as DeepOCSortKalmanBoxTracker\n\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT, ImprAssocTrack]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort', 'imprassoc']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'imprassoc']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt","@@ -134,9 +134,10 @@ def test_per_class_tracker_output_size(tracker_type):\n     rgb = np.random.randint(255, size=(640, 640, 3), dtype=np.uint8)\n     det = np.array([[144, 212, 578, 480, 0.82, 0],\n                     [425, 281, 576, 472, 0.72, 65]])\n+    embs = np.random.random(size=(2, 512))\n \n-    output = tracker.update(det, rgb)\n-    output = tracker.update(det, rgb)\n+    output = tracker.update(det, rgb, embs)\n+    output = tracker.update(det, rgb, embs)\n     assert output.shape == (2, 8)  # two inputs should give two outputs\n \n \n",add,Added STORM - 204 to Changelog
105922977b7aeaa677087fca56776db5c23404f4,fix embedding extraction for each class,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of BoxMOT multiprocessing threads\n\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n        self.last_emb_size = None\n            \","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of BoxMOT multiprocessing threads\n\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n        self.last_emb_size = None\n            \","@@ -35,12 +35,14 @@ class PerClassDecorator:\n             self.per_class_active_tracks[i] = []\n         self.last_emb_size = None\n             \n-    def get_class_dets_n_embs(dets, embs, cls_id):\n+    def get_class_dets_n_embs(self, dets, embs, cls_id):\n         # can be that there are detections but no embeddings\n         if dets.size > 0:\n             class_indices = np.where(dets[:, 5] == cls_id)[0]\n             class_dets = dets[class_indices]\n             if embs is not None:\n+                # Assert that if embeddings are provided, they have the same number of elements as detections\n+                assert dets.shape[0] == embs.shape[0], """"Detections and embeddings must have the same number of elements""""\n                 if embs.size > 0:\n                     class_embs = embs[class_indices]\n                     self.last_emb_size = class_embs.shape[1]  # Update the last known embedding size\n@@ -72,7 +74,7 @@ class PerClassDecorator:\n \n                 for i, cls_id in enumerate(range(self.nr_classes)):\n  \n-                    class_dets, class_embs = get_class_dets_n_embs(dets, embs, cls_id)\n+                    class_dets, class_embs = self.get_class_dets_n_embs(dets, embs, cls_id)\n                     \n                     logger.debug(f""""Processing class {int(cls_id)}: {class_dets.shape} with embeddings {class_embs.shape}"""")\n \n",add,Added net . kotlin_logger_list_read
e2a02f7bec26e0efd1e5ba4803cf054cd71b57f1,fix embedding extraction for each class,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of BoxMOT multiprocessing threads\n\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n        self.last_emb_size = None\n            \","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of BoxMOT multiprocessing threads\n\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n        self.last_emb_size = None\n            \","@@ -42,10 +42,15 @@ class PerClassDecorator:\n             class_dets = dets[class_indices]\n             if embs is not None:\n                 # Assert that if embeddings are provided, they have the same number of elements as detections\n-                assert dets.shape[0] == embs.shape[0], """"Detections and embeddings must have the same number of elements""""\n+                assert dets.shape[0] == embs.shape[0], """"Detections and embeddings must have the same number of elements when both are provided""""\n                 if embs.size > 0:\n                     class_embs = embs[class_indices]\n                     self.last_emb_size = class_embs.shape[1]  # Update the last known embedding size\n+                else:\n+                    class_embs = None\n+            else:\n+                class_embs = None\n+\n         else:\n             class_dets = np.empty((0, 6))\n             if self.last_emb_size is not None:\n",add,Add network permission
3a4c34eb9e9d0d86fb2ef62f9c0cbfc3430360a9,fix embedding extraction for each class,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of BoxMOT multiprocessing threads\n\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n        self.last_emb_size = None\n            \","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of BoxMOT multiprocessing threads\n\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n        self.last_emb_size = None\n            \","@@ -59,7 +59,6 @@ class PerClassDecorator:\n                 class_embs = None\n         return class_dets, class_embs\n         \n-\n     def __get__(self, instance, owner):\n         # This makes PerClassDecorator a non-data descriptor that binds the method to the instance\n         def wrapper(*args, **kwargs):\n@@ -81,7 +80,7 @@ class PerClassDecorator:\n  \n                     class_dets, class_embs = self.get_class_dets_n_embs(dets, embs, cls_id)\n                     \n-                    logger.debug(f""""Processing class {int(cls_id)}: {class_dets.shape} with embeddings {class_embs.shape}"""")\n+                    logger.debug(f""""Processing class {int(cls_id)}: {class_dets.shape} with embeddings {class_embs.shape if class_embs is not None else None}"""")\n \n                     # activate the specific active tracks for this class id\n                     instance.active_tracks = self.per_class_active_tracks[cls_id]\n",add,Add note about data volume to enable_metrics_collection
395bb4a4b0532cb1288a6fdecebdfeefb7861021,fix,boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of BoxMOT multiprocessing threads\n\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n        self.last_emb_size = None\n            \","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of BoxMOT multiprocessing threads\n\n\n# global logger\nfrom loguru import logger\n\nlogger.remove()\nlogger.add(sys.stderr, colorize=True, level=""""INFO"""")\n\n\nclass PerClassDecorator:\n    def __init__(self, method):\n        # Store the method that will be decorated\n        self.update = method\n        self.nr_classes = 80\n        self.per_class_active_tracks = {}\n        for i in range(self.nr_classes):\n            self.per_class_active_tracks[i] = []\n        self.last_emb_size = None\n            \","@@ -36,27 +36,24 @@ class PerClassDecorator:\n         self.last_emb_size = None\n             \n     def get_class_dets_n_embs(self, dets, embs, cls_id):\n-        # can be that there are detections but no embeddings\n+        # Initialize empty arrays for detections and embeddings\n+        class_dets = np.empty((0, 6))\n+        class_embs = np.empty((0, self.last_emb_size)) if self.last_emb_size is not None else None\n+\n+        # Check if there are detections\n         if dets.size > 0:\n             class_indices = np.where(dets[:, 5] == cls_id)[0]\n             class_dets = dets[class_indices]\n+            \n             if embs is not None:\n                 # Assert that if embeddings are provided, they have the same number of elements as detections\n                 assert dets.shape[0] == embs.shape[0], """"Detections and embeddings must have the same number of elements when both are provided""""\n+                \n                 if embs.size > 0:\n                     class_embs = embs[class_indices]\n                     self.last_emb_size = class_embs.shape[1]  # Update the last known embedding size\n                 else:\n                     class_embs = None\n-            else:\n-                class_embs = None\n-\n-        else:\n-            class_dets = np.empty((0, 6))\n-            if self.last_emb_size is not None:\n-                class_embs = np.empty((0, self.last_emb_size))  # Use the last known embedding size\n-            else:\n-                class_embs = None\n         return class_dets, class_embs\n         \n     def __get__(self, instance, owner):\n",add,Add group has a name
036e140ff866b4f83d37fff7c25a3e193ca6c693,fix not generating mot results for last frame bug,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","@@ -184,7 +184,7 @@ def generate_mot_results(args: argparse.Namespace, config_dict: dict = None) ->\n     all_mot_results = []\n \n     for frame_idx, d in enumerate(tqdm(dataset, desc=""""Frames"""")):\n-        if (frame_idx + 1) == len(dataset):\n+        if frame_idx == len(dataset):\n             break\n \n         im = d[1][0]\n",fix,Add note about data volume to enable_metrics_collection
b7e11d675c7ba2dc8e51ef86b4fcbaf885973a42,fix val in windows,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImages\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backend import ","@@ -52,10 +52,22 @@ def prompt_overwrite(path_type: str, path: str, ci: bool = False) -> bool:\n \n     def input_with_timeout(prompt, timeout=3.0):\n         print(prompt, end='', flush=True)\n-        inputs, _, _ = select.select([sys.stdin], [], [], timeout)\n-        if inputs:\n-            result = sys.stdin.readline().strip().lower()\n-            return result in ['y', 'yes']\n+        \n+        result = []\n+        input_received = threading.Event()\n+\n+        def get_input():\n+            user_input = sys.stdin.readline().strip().lower()\n+            result.append(user_input)\n+            input_received.set()\n+\n+        input_thread = threading.Thread(target=get_input)\n+        input_thread.daemon = True  # Ensure thread does not prevent program exit\n+        input_thread.start()\n+        input_thread.join(timeout)\n+\n+        if input_received.is_set():\n+            return result[0] in ['y', 'yes']\n         else:\n             print(""""\nNo response, not proceeding with overwrite..."""")\n             return False\n",add,Add note about data volume to enable_metrics_collection
6c1b254ccec51131cbbe929407857345ebd03194,fix import,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backen","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backen","@@ -190,7 +190,7 @@ def generate_mot_results(args: argparse.Namespace, config_dict: dict = None) ->\n \n     dets_n_embs = np.concatenate([dets, embs], axis=1)\n \n-    dataset = LoadImages(args.source)\n+    dataset = LoadImagesAndVideos(args.source)\n \n     txt_path = args.exp_folder_path / (Path(args.source).parent.name + '.txt')\n     all_mot_results = []\n",add,Add note about data volume to enable_metrics_collection
722e4dac73c58db536facd43cf49a67b7f071d90,fix evol,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backen","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.reid_auto_backen","@@ -84,9 +84,11 @@ def generate_dets_embs(args: argparse.Namespace, y: Path) -> None:\n         y (Path): Path to the YOLO model file.\n     """"""""""""\n     WEIGHTS.mkdir(parents=True, exist_ok=True)\n+    \n+    ul_models = ['yolov8', 'yolov9', 'yolov10', 'rtdetr', 'sam']\n \n-    yolo = YOLO(y if 'yolov8' in str(y) else 'yolov8n.pt')\n-\n+    yolo = YOLO(y if any(yolo in str(args.yolo_model) for yolo in ul_models) else 'yolov8n.pt')\n+    \n     results = yolo(\n         source=args.source,\n         conf=args.conf,\n@@ -103,7 +105,7 @@ def generate_dets_embs(args: argparse.Namespace, y: Path) -> None:\n         vid_stride=args.vid_stride,\n     )\n \n-    if 'yolov8' not in str(y):\n+    if not any(yolo in str(args.yolo_model) for yolo in ul_models):\n         m = get_yolo_inferer(y)\n         model = m(model=y, device=yolo.predictor.device, args=yolo.predictor.args)\n         yolo.predictor.model = model\n",add,Add note about data volume to enable_metrics_collection
1ffcedde3aa5b74074bd4b0c4247109dc84c6792,fix missing KF functionality in oc & hybrid trackers,boxmot/motion/kalman_filters/xysr_kf.py,"""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","@@ -220,21 +220,47 @@ class KalmanFilterXYSR(object):\n         H : np.array, or None\n             Measurement function. If None, the filter's self.H value is used.\n         """"""""""""\n+        \n+        # set to None to force recompute\n+        self._log_likelihood = None\n+        self._likelihood = None\n+        self._mahalanobis = None\n+\n+        # append the observation\n+        self.history_obs.append(z)\n+        \n         if z is None:\n-            self.history_obs.append(z)\n+            if self.observed:\n+                """"""""""""\n+                Got no observation so freeze the current parameters for future\n+                potential online smoothing.\n+                """"""""""""\n+                self.freeze()\n             self.observed = False\n+            self.z = np.array([[None] * self.dim_z]).T\n+            self.x_post = self.x.copy()\n+            self.P_post = self.P.copy()\n+            self.y = zeros((self.dim_z, 1))\n             return\n \n+        # self.observed = True\n+        if not self.observed:\n+            """"""""""""\n+            Get observation, use online smoothing to re-update parameters\n+            """"""""""""\n+            self.unfreeze()\n         self.observed = True\n+\n         if R is None:\n             R = self.R\n+        elif isscalar(R):\n+            R = eye(self.dim_z) * R\n         if H is None:\n+            z = reshape_z(z, self.dim_z, self.x.ndim)\n             H = self.H\n-        H = np.asarray(H)\n \n         # y = z - Hx\n         # error (residual) between measurement and prediction\n-        z = reshape_z(z, self.dim_z, self.x.ndim)\n         self.y = z - dot(H, self.x)\n \n         # common subexpression for speed\n@@ -321,76 +347,111 @@ class KalmanFilterXYSR(object):\n \n         return self._likelihood\n \n-    def batch_filter(self, zs, us=None, Bs=None, Fs=None, Qs=None, Hs=None, Rs=None):\n-        """""""""""" Batch processes a sequence of measurements.\n \n-        Parameters\n-        ----------\n-    ",add,Add note about data volume to enable
2d105ef38a9545e74305e25c0417e53e9c06ce76,"Revert ""Merge pull request #1603 from mikel-brostrom/fix-oc-n-hybrid-kfs""

This reverts commit 949511c72e9cfa73330102a85d7e4177ece1ba8c, reversing
changes made to 6bc5db65f4dcc711dd7f4b75d5ce56d843a49f9f.",boxmot/motion/kalman_filters/xysr_kf.py,"""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","@@ -220,47 +220,21 @@ class KalmanFilterXYSR(object):\n         H : np.array, or None\n             Measurement function. If None, the filter's self.H value is used.\n         """"""""""""\n-        \n-        # set to None to force recompute\n-        self._log_likelihood = None\n-        self._likelihood = None\n-        self._mahalanobis = None\n-\n-        # append the observation\n-        self.history_obs.append(z)\n-        \n         if z is None:\n-            if self.observed:\n-                """"""""""""\n-                Got no observation so freeze the current parameters for future\n-                potential online smoothing.\n-                """"""""""""\n-                self.freeze()\n+            self.history_obs.append(z)\n             self.observed = False\n-            self.z = np.array([[None] * self.dim_z]).T\n-            self.x_post = self.x.copy()\n-            self.P_post = self.P.copy()\n-            self.y = zeros((self.dim_z, 1))\n             return\n \n-        # self.observed = True\n-        if not self.observed:\n-            """"""""""""\n-            Get observation, use online smoothing to re-update parameters\n-            """"""""""""\n-            self.unfreeze()\n         self.observed = True\n-\n         if R is None:\n             R = self.R\n-        elif isscalar(R):\n-            R = eye(self.dim_z) * R\n         if H is None:\n-            z = reshape_z(z, self.dim_z, self.x.ndim)\n             H = self.H\n+        H = np.asarray(H)\n \n         # y = z - Hx\n         # error (residual) between measurement and prediction\n+        z = reshape_z(z, self.dim_z, self.x.ndim)\n         self.y = z - dot(H, self.x)\n \n         # common subexpression for speed\n@@ -347,111 +321,76 @@ class KalmanFilterXYSR(object):\n \n         return self._likelihood\n \n+    def batch_filter(self, zs, us=None, Bs=None, Fs=None, Qs=None, Hs=None, Rs=None):\n+        """""""""""" Batch processes a sequence of measurements.\n \n-def batch_filter(x, P, zs, Fs, Qs, Hs, Rs, Bs=",add,Added example for MAP type in documentation
3047f293b318408f4edd029101b341da668470cd,fix,boxmot/motion/kalman_filters/xysr_kf.py,"""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","@@ -220,21 +220,47 @@ class KalmanFilterXYSR(object):\n         H : np.array, or None\n             Measurement function. If None, the filter's self.H value is used.\n         """"""""""""\n+        \n+        # set to None to force recompute\n+        self._log_likelihood = None\n+        self._likelihood = None\n+        self._mahalanobis = None\n+\n+        # append the observation\n+        self.history_obs.append(z)\n+        \n         if z is None:\n-            self.history_obs.append(z)\n+            if self.observed:\n+                """"""""""""\n+                Got no observation so freeze the current parameters for future\n+                potential online smoothing.\n+                """"""""""""\n+                self.freeze()\n             self.observed = False\n+            self.z = np.array([[None] * self.dim_z]).T\n+            self.x_post = self.x.copy()\n+            self.P_post = self.P.copy()\n+            self.y = zeros((self.dim_z, 1))\n             return\n \n+        # self.observed = True\n+        if not self.observed:\n+            """"""""""""\n+            Get observation, use online smoothing to re-update parameters\n+            """"""""""""\n+            self.unfreeze()\n         self.observed = True\n+        \n         if R is None:\n             R = self.R\n+        elif isscalar(R):\n+            R = eye(self.dim_z) * R\n         if H is None:\n+            z = reshape_z(z, self.dim_z, self.x.ndim)\n             H = self.H\n-        H = np.asarray(H)\n \n         # y = z - Hx\n         # error (residual) between measurement and prediction\n-        z = reshape_z(z, self.dim_z, self.x.ndim)\n         self.y = z - dot(H, self.x)\n \n         # common subexpression for speed\n@@ -321,76 +347,111 @@ class KalmanFilterXYSR(object):\n \n         return self._likelihood\n \n-    def batch_filter(self, zs, us=None, Bs=None, Fs=None, Qs=None, Hs=None, Rs=None):\n-        """""""""""" Batch processes a sequence of measurements.\n \n-        Parameters\n-        ---------",add,Add note about data volume to enable_metrics_collection
d1bcf6d8986ec5371fdcd6b0e75e23504ccef0da,fix,boxmot/motion/kalman_filters/xysr_kf.py,"""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","@@ -106,6 +106,7 @@ class KalmanFilterXYSR(object):\n \n         self.attr_saved = None\n         self.observed = False\n+        self.last_measurement = None\n \n \n     def apply_affine_correction(self, m, t):\n",add,Add note about data volume to enable_metrics_collection
f70386b2c9551b84e854f4abdeee2a7de4e80e0a,fix,boxmot/motion/kalman_filters/xysr_kf.py,"""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","@@ -109,29 +109,51 @@ class KalmanFilterXYSR(object):\n         self.last_measurement = None\n \n \n-    def apply_affine_correction(self, m, t):\n+    def apply_affine_correction(self, m, t, new_kf):\n         """"""""""""\n         Apply to both last state and last observation for OOS smoothing.\n \n         Messy due to internal logic for kalman filter being messy.\n         """"""""""""\n-        \n-        scale = np.linalg.norm(m[:, 0])\n-        self.x[:2] = m @ self.x[:2] + t\n-        self.x[4:6] = m @ self.x[4:6]\n-\n-        self.P[:2, :2] = m @ self.P[:2, :2] @ m.T\n-        self.P[4:6, 4:6] = m @ self.P[4:6, 4:6] @ m.T\n-\n-        # If frozen, also need to update the frozen state for OOS\n-        if not self.observed and self.attr_saved is not None:\n-            self.attr_saved[""""x""""][:2] = m @ self.attr_saved[""""x""""][:2] + t\n-            self.attr_saved[""""x""""][4:6] = m @ self.attr_saved[""""x""""][4:6]\n-\n-            self.attr_saved[""""P""""][:2, :2] = m @ self.attr_saved[""""P""""][:2, :2] @ m.T\n-            self.attr_saved[""""P""""][4:6, 4:6] = m @ self.attr_saved[""""P""""][4:6, 4:6] @ m.T\n-\n-            self.attr_saved[""""last_measurement""""][:2] = m @ self.attr_saved[""""last_measurement""""][:2] + t\n+        if new_kf:\n+            big_m = np.kron(np.eye(4, dtype=float), m)\n+            self.x = big_m @ self.x\n+            self.x[:2] += t\n+            self.P = big_m @ self.P @ big_m.T\n+\n+            # If frozen, also need to update the frozen state for OOS\n+            if not self.observed and self.attr_saved is not None:\n+                self.attr_saved[""""x""""] = big_m @ self.attr_saved[""""x""""]\n+                self.attr_saved[""""x""""][:2] += t\n+                self.attr_saved[""""P""""] = big_m @ self.attr_saved[""""P""""] @ big_m.T\n+                self.attr_saved[""""last_measurement""""][:2] = m @ self.attr_saved[""""last_measurement""""][:2] + t\n+                self.attr_saved[""""last_measurement""""][2:] = m @ self.attr_saved[""""last_measurement""""][2:]\n+        else:\n+      ",add,Added STORM - 374 to Changelog
c0b24a24badca6cefa5e24b3c8020b2c81d70df3,fix,boxmot/motion/kalman_filters/xysr_kf.py,"""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","@@ -107,15 +107,16 @@ class KalmanFilterXYSR(object):\n         self.attr_saved = None\n         self.observed = False\n         self.last_measurement = None\n+        self.new_kf = True\n \n \n-    def apply_affine_correction(self, m, t, new_kf):\n+    def apply_affine_correction(self, m, t):\n         """"""""""""\n         Apply to both last state and last observation for OOS smoothing.\n \n         Messy due to internal logic for kalman filter being messy.\n         """"""""""""\n-        if new_kf:\n+        if self.new_kf:\n             big_m = np.kron(np.eye(4, dtype=float), m)\n             self.x = big_m @ self.x\n             self.x[:2] += t\n@@ -248,6 +249,7 @@ class KalmanFilterXYSR(object):\n         self._log_likelihood = None\n         self._likelihood = None\n         self._mahalanobis = None\n+        self.new_kf = False\n \n         # append the observation\n         self.history_obs.append(z)\n",add,Add note about data volume to enable_metrics_collection
1d4f522e62315b054f42ede4f2e80d3a7b40aac3,fix OOS smoothing,boxmot/motion/kalman_filters/xysr_kf.py,"""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","@@ -107,7 +107,6 @@ class KalmanFilterXYSR(object):\n         self.attr_saved = None\n         self.observed = False\n         self.last_measurement = None\n-        self.new_kf = True\n \n \n     def apply_affine_correction(self, m, t):\n@@ -116,45 +115,23 @@ class KalmanFilterXYSR(object):\n \n         Messy due to internal logic for kalman filter being messy.\n         """"""""""""\n-        if self.new_kf:\n-            big_m = np.kron(np.eye(4, dtype=float), m)\n-            self.x = big_m @ self.x\n-            self.x[:2] += t\n-            self.P = big_m @ self.P @ big_m.T\n-\n-            # If frozen, also need to update the frozen state for OOS\n-            if not self.observed and self.attr_saved is not None:\n-                self.attr_saved[""""x""""] = big_m @ self.attr_saved[""""x""""]\n-                self.attr_saved[""""x""""][:2] += t\n-                self.attr_saved[""""P""""] = big_m @ self.attr_saved[""""P""""] @ big_m.T\n-                self.attr_saved[""""last_measurement""""][:2] = m @ self.attr_saved[""""last_measurement""""][:2] + t\n-                self.attr_saved[""""last_measurement""""][2:] = m @ self.attr_saved[""""last_measurement""""][2:]\n-        else:\n-            scale = np.linalg.norm(m[:, 0])\n-            self.x[:2] = m @ self.x[:2] + t\n-            self.x[4:6] = m @ self.x[4:6]\n-            # self.x[2] *= scale\n-            # self.x[6] *= scale\n \n-            self.P[:2, :2] = m @ self.P[:2, :2] @ m.T\n-            self.P[4:6, 4:6] = m @ self.P[4:6, 4:6] @ m.T\n-            # self.P[2, 2] *= 2 * scale\n-            # self.P[6, 6] *= 2 * scale\n+        scale = np.linalg.norm(m[:, 0])\n+        self.x[:2] = m @ self.x[:2] + t\n+        self.x[4:6] = m @ self.x[4:6]\n+\n+        self.P[:2, :2] = m @ self.P[:2, :2] @ m.T\n+        self.P[4:6, 4:6] = m @ self.P[4:6, 4:6] @ m.T\n \n-            # If frozen, also need to update the frozen state for OOS\n-            if not self.observed and self.attr_saved is not None:\n-                self.attr_saved[""""x""""][:2] ",add,Added STORM - 374 to Changelog
30b75ae7249da3820c50b5e25e9f7a35dcc05247,fix ss evol,tracking/evolve.py,"\nimport os\nimport yaml\nfrom pathlib import Path\n\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils import EXAMPLES\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt,\n    download_mot_eval_tools\n)\nfrom boxmot.utils import ROOT, NUM_THREADS\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n\n    def objective_function(self, config):\n        download_mot_eval_tools(self.opt.val_tools_path)\n        # generate new set of mot challenge compliant results with\n        # new set of generated tracker parameters\n        run_generate_mot_results(self.opt, config)\n        # get MOTA, HOTA, IDF1 results\n        results = run_trackeval(self.opt)\n ","\nimport os\nimport yaml\nfrom pathlib import Path\n\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils import EXAMPLES\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt,\n    download_mot_eval_tools\n)\nfrom boxmot.utils import ROOT, NUM_THREADS\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n\n    def objective_function(self, config):\n        download_mot_eval_tools(self.opt.val_tools_path)\n        # generate new set of mot challenge compliant results with\n        # new set of generated tracker parameters\n        run_generate_mot_results(self.opt, config)\n        # get MOTA, HOTA, IDF1 results\n        results = run_trackeval(self.opt)\n ","@@ -45,7 +45,7 @@ def get_search_space(tracking_method):\n             """"iou_thresh"""": tune.uniform(0.1, 0.4),\n             """"ecc"""": tune.choice([True, False]),\n             """"ema_alpha"""": tune.uniform(0.7, 0.95),\n-            """"max_dist"""": tune.uniform(0.1, 0.4),\n+            """"max_cos_dist"""": tune.uniform(0.1, 0.4),\n             """"max_iou_dist"""": tune.uniform(0.5, 0.95),\n             """"max_age"""": tune.randint(10, 151),  # The upper bound is exclusive in randint\n             """"n_init"""": tune.randint(1, 4),  # The upper bound is exclusive in randint\n",add,Added STORM - 1270 to Changelog
d646be9ae1c7d965c97899f11e54a9b702b6232b,fix classes,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.r","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup\nfrom boxmot.appearance.r","@@ -448,6 +448,8 @@ def parse_opt() -> argparse.Namespace:\n     generate_dets_embs_parser.add_argument('--yolo-model', nargs='+', type=Path, default=WEIGHTS / 'yolov8n.pt', help='yolo model path')\n     generate_dets_embs_parser.add_argument('--reid-model', nargs='+', type=Path, default=WEIGHTS / 'osnet_x0_25_msmt17.pt', help='reid model path')\n     generate_dets_embs_parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n+    generate_dets_embs_parser.add_argument('--classes', nargs='+', type=int, default=0, help='filter by class: --classes 0, or --classes 0 2 3')\n+\n     \n     # Subparser for generate_mot_results\n     generate_mot_results_parser = subparsers.add_parser('generate_mot_results', help='Generate MOT results')\n",add,Fix bytesPerPixel for SurfaceTexture
9ab139c75b612d4adcd253f99a10712edd49bbfa,fix botsort evolve,tracking/evolve.py,"\nimport os\nimport yaml\nfrom pathlib import Path\n\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils import EXAMPLES\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt,\n    download_mot_eval_tools\n)\nfrom boxmot.utils import ROOT, NUM_THREADS\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n\n    def objective_function(self, config):\n        download_mot_eval_tools(self.opt.val_tools_path)\n        # generate new set of mot challenge compliant results with\n        # new set of generated tracker parameters\n        run_generate_mot_results(self.opt, config)\n        # get MOTA, HOTA, IDF1 results\n        results = run_trackeval(self.opt)\n ","\nimport os\nimport yaml\nfrom pathlib import Path\n\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils import EXAMPLES\nfrom tracking.val import (\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n    parse_opt as parse_optt,\n    download_mot_eval_tools\n)\nfrom boxmot.utils import ROOT, NUM_THREADS\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ray[tune]',))  # install\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.air import RunConfig\n\n\nclass Tracker:\n    def __init__(self, opt, parameters):\n        self.opt = opt\n\n    def objective_function(self, config):\n        download_mot_eval_tools(self.opt.val_tools_path)\n        # generate new set of mot challenge compliant results with\n        # new set of generated tracker parameters\n        run_generate_mot_results(self.opt, config)\n        # get MOTA, HOTA, IDF1 results\n        results = run_trackeval(self.opt)\n ","@@ -70,7 +70,7 @@ def get_search_space(tracking_method):\n             """"track_high_thresh"""": tune.uniform(0.3, 0.7),\n             """"track_low_thresh"""": tune.uniform(0.1, 0.3),\n             """"new_track_thresh"""": tune.uniform(0.1, 0.8),\n-            """"track_buffer"""": tune.randint(20, 81, 10),  # The upper bound is exclusive in randint\n+            """"track_buffer"""": tune.randint(20, 81),  # The upper bound is exclusive in randint\n             """"match_thresh"""": tune.uniform(0.1, 0.9),\n             """"proximity_thresh"""": tune.uniform(0.25, 0.75),\n             """"appearance_thresh"""": tune.uniform(0.1, 0.8),\n",add,Added STORM - 1270 to Changelog
379e4b241c456e4985eb0c4725dfe4a753894121,fix test,tests/unit/test_trackers.py,"import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, ImprAssocTrack, get_tracker_config, create_tracker,\n)\n\nfrom boxmot.trackers.ocsort.ocsort import KalmanBoxTracker as OCSortKalmanBoxTracker\nfrom boxmot.trackers.deepocsort.deep_ocsort import KalmanBoxTracker as DeepOCSortKalmanBoxTracker\n\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT, ImprAssocTrack]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort', 'imprassoc']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'imprassoc']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt","import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, ImprAssocTrack, get_tracker_config, create_tracker,\n)\n\nfrom boxmot.trackers.ocsort.ocsort import KalmanBoxTracker as OCSortKalmanBoxTracker\nfrom boxmot.trackers.deepocsort.deep_ocsort import KalmanBoxTracker as DeepOCSortKalmanBoxTracker\n\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT, ImprAssocTrack]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort', 'imprassoc']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'imprassoc']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        reid_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'","@@ -23,9 +23,9 @@ PER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'imprassoc']\n @pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\n def test_motion_n_appearance_trackers_instantiation(Tracker):\n     Tracker(\n-        model_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n+        reid_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n         device='cpu',\n-        fp16=True,\n+        half=True,\n     )\n \n \n@@ -92,9 +92,9 @@ TRACKER_CREATORS = {\n @pytest.mark.parametrize(""""Tracker, init_args"""", [\n     (OCSORT, {}),\n     (DeepOCSORT, {\n-        'model_weights': Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n+        'reid_weights': Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n         'device': 'cpu',\n-        'fp16': True\n+        'half': True\n     }),\n ])\n def test_Q_matrix_scaling(Tracker, init_args):\n",add,Added setup task
e1ccccfce6822e4a766aea584a560e2529aa0239,fix test,boxmot/tracker_zoo.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport yaml\nfrom boxmot.utils import BOXMOT\n\ndef get_tracker_config(tracker_type):\n    """"""""""""Returns the path to the tracker configuration file.""""""""""""\n    return BOXMOT / 'configs' / f'{tracker_type}.yaml'\n\ndef create_tracker(tracker_type, tracker_config=None, reid_weights=None, device=None, half=None, per_class=None, evolve_param_dict=None):\n    """"""""""""\n    Creates and returns an instance of the specified tracker type.\n    \n    Parameters:\n    - tracker_type: The type of the tracker (e.g., 'strongsort', 'ocsort').\n    - tracker_config: Path to the tracker configuration file.\n    - reid_weights: Weights for ReID (re-identification).\n    - device: Device to run the tracker on (e.g., 'cpu', 'cuda').\n    - half: Boolean indicating whether to use half-precision.\n    - per_class: Boolean for class-specific tracking (optional).\n    - evolve_param_dict: A dictionary of parameters for evolving the tracker.\n    \n    Retur","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport yaml\nfrom boxmot.utils import BOXMOT\n\ndef get_tracker_config(tracker_type):\n    """"""""""""Returns the path to the tracker configuration file.""""""""""""\n    return BOXMOT / 'configs' / f'{tracker_type}.yaml'\n\ndef create_tracker(tracker_type, tracker_config=None, reid_weights=None, device=None, half=None, per_class=None, evolve_param_dict=None):\n    """"""""""""\n    Creates and returns an instance of the specified tracker type.\n    \n    Parameters:\n    - tracker_type: The type of the tracker (e.g., 'strongsort', 'ocsort').\n    - tracker_config: Path to the tracker configuration file.\n    - reid_weights: Weights for ReID (re-identification).\n    - device: Device to run the tracker on (e.g., 'cpu', 'cuda').\n    - half: Boolean indicating whether to use half-precision.\n    - per_class: Boolean for class-specific tracking (optional).\n    - evolve_param_dict: A dictionary of parameters for evolving the tracker.\n    \n    Retur","@@ -36,7 +36,6 @@ def create_tracker(tracker_type, tracker_config=None, reid_weights=None, device=\n         'reid_weights': reid_weights,\n         'device': device,\n         'half': half,\n-        'per_class': per_class\n     }\n \n     # Map tracker types to their corresponding classes\n@@ -61,9 +60,12 @@ def create_tracker(tracker_type, tracker_config=None, reid_weights=None, device=\n     \n     # For specific trackers, update tracker arguments with ReID parameters\n     if tracker_type in ['strongsort', 'botsort', 'deepocsort', 'hybridsort', 'imprassoc']:\n+        tracker_args['per_class'] = per_class\n         tracker_args.update(reid_args)\n         if tracker_type == 'strongsort':\n-            tracker_args.pop('per_class')  # Remove per_class if not needed\n+            tracker_args.pop('per_class')  # per class not supported by\n+    else:\n+        tracker_args['per_class'] = per_class\n \n     # Return the instantiated tracker class with arguments\n     return tracker_class(**tracker_args)\n\ No newline at end of file\n",add,Add note about data volume to enable_metrics_collection
9c477c6bbec863298691d8e03fd5e073beddfe9c,fix,boxmot/trackers/botsort/botsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport torch\nimport numpy as np\nfrom pathlib import Path\n\nfrom boxmot.motion.kalman_filters.xywh_kf import KalmanFilterXYWH\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.botsort.botsort_utils import joint_stracks, sub_stracks, remove_duplicate_stracks \nfrom boxmot.trackers.botsort.botsort_track import STrack\n\n\nclass BoTSORT(BaseTracker):\n    """"""""""""\n    BoTSORT Tracker: A tracking algorithm that combines appearance and motion-based tracking.\n\n    Args:\n        reid_weights (str): Path to the model weights for ReID.\n        device (torch.device): Device to run the model on (e.g., 'cpu' o","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport torch\nimport numpy as np\nfrom pathlib import Path\n\nfrom boxmot.motion.kalman_filters.xywh_kf import KalmanFilterXYWH\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.botsort.botsort_utils import joint_stracks, sub_stracks, remove_duplicate_stracks \nfrom boxmot.trackers.botsort.botsort_track import STrack\n\n\nclass BoTSORT(BaseTracker):\n    """"""""""""\n    BoTSORT Tracker: A tracking algorithm that combines appearance and motion-based tracking.\n\n    Args:\n        reid_weights (str): Path to the model weights for ReID.\n        device (torch.device): Device to run the model on (e.g., 'cpu' o","@@ -96,7 +96,7 @@ class BoTSORT(BaseTracker):\n         if self.with_reid and embs is None:\n             features_high = self.model.get_features(dets_first[:, 0:4], img)\n         else:\n-            features_high = embs if embs is not None else []\n+            features_high = embs[first_mask] if embs is not None else []\n \n         # Create detections\n         detections = self._create_detections(dets_first, features_high)\n@@ -174,7 +174,18 @@ class BoTSORT(BaseTracker):\n             dists = ious_dists\n \n         matches, u_track, u_detection = linear_assignment(dists, thresh=self.match_thresh)\n-        self._update_tracks(matches, strack_pool, detections, activated_stracks, refind_stracks)\n+        \n+        #self._update_tracks(matches, strack_pool, detections, activated_stracks, refind_stracks)\n+        \n+        for itracked, idet in matches:\n+            track = strack_pool[itracked]\n+            det = detections[idet]\n+            if track.state == TrackState.Tracked:\n+                track.update(detections[idet], self.frame_count)\n+                activated_stracks.append(track)\n+            else:\n+                track.re_activate(det, self.frame_count, new_id=False)\n+                refind_stracks.append(track)\n \n     def _second_association(self, dets_second, activated_stracks, lost_stracks, refind_stracks):\n         if len(dets_second) > 0:\n@@ -188,7 +199,15 @@ class BoTSORT(BaseTracker):\n \n         dists = iou_distance(r_tracked_stracks, detections_second)\n         matches, u_track, u_detection_second = linear_assignment(dists, thresh=0.5)\n-        self._update_tracks(matches, r_tracked_stracks, detections_second, activated_stracks, refind_stracks)\n+        for itracked, idet in matches:\n+            track = r_tracked_stracks[itracked]\n+            det = detections_second[idet]\n+            if track.state == TrackState.Tracked:\n+                track.update(det, self.frame_count)\n+                activated_starcks.app",add,Added the UNSTARTED state to the YouTube PlayerState enum
2e58e898d08e540051a9803d73f5a3ea6be1428f,fixed refactor,boxmot/trackers/botsort/botsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport torch\nimport numpy as np\nfrom pathlib import Path\n\nfrom boxmot.motion.kalman_filters.xywh_kf import KalmanFilterXYWH\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.botsort.botsort_utils import joint_stracks, sub_stracks, remove_duplicate_stracks \nfrom boxmot.trackers.botsort.botsort_track import STrack\n\n\nclass BoTSORT(BaseTracker):\n    """"""""""""\n    BoTSORT Tracker: A tracking algorithm that combines appearance and motion-based tracking.\n\n    Args:\n        reid_weights (str): Path to the model weights for ReID.\n        device (torch.device): Device to run the model on (e.g., 'cpu' o","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport torch\nimport numpy as np\nfrom pathlib import Path\n\nfrom boxmot.motion.kalman_filters.xywh_kf import KalmanFilterXYWH\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.botsort.botsort_utils import joint_stracks, sub_stracks, remove_duplicate_stracks \nfrom boxmot.trackers.botsort.botsort_track import STrack\n\n\nclass BoTSORT(BaseTracker):\n    """"""""""""\n    BoTSORT Tracker: A tracking algorithm that combines appearance and motion-based tracking.\n\n    Args:\n        reid_weights (str): Path to the model weights for ReID.\n        device (torch.device): Device to run the model on (e.g., 'cpu' o","@@ -90,13 +90,13 @@ class BoTSORT(BaseTracker):\n         activated_stracks, refind_stracks, lost_stracks, removed_stracks = [], [], [], []\n \n         # Preprocess detections\n-        dets, dets_first, dets_second = self._split_detections(dets)\n+        dets, dets_first, embs_first, dets_second = self._split_detections(dets, embs)\n \n         # Extract appearance features\n         if self.with_reid and embs is None:\n             features_high = self.model.get_features(dets_first[:, 0:4], img)\n         else:\n-            features_high = embs[first_mask] if embs is not None else []\n+            features_high = embs_first if embs is not None else []\n \n         # Create detections\n         detections = self._create_detections(dets_first, features_high)\n@@ -105,13 +105,13 @@ class BoTSORT(BaseTracker):\n         unconfirmed, active_tracks = self._separate_tracks()\n \n         # First association\n-        self._first_association(dets_first, active_tracks, unconfirmed, img, detections, activated_stracks, refind_stracks)\n+        matches_first, u_track_first, u_detection_first, strack_pool = self._first_association(dets_first, active_tracks, unconfirmed, img, detections, activated_stracks, refind_stracks)\n \n         # Second association\n-        self._second_association(dets_second, activated_stracks, lost_stracks, refind_stracks)\n+        matches_second, u_track_second, u_detection_second = self._second_association(dets_second, activated_stracks, lost_stracks, refind_stracks, u_track_first, strack_pool)\n \n         # Handle unconfirmed tracks\n-        self._handle_unconfirmed_tracks(unconfirmed, detections, activated_stracks, removed_stracks)\n+        self._handle_unconfirmed_tracks(u_detection_first, detections, activated_stracks, removed_stracks, unconfirmed)\n \n         # Initialize new tracks\n         self._initialize_new_tracks(detections, activated_stracks)\n@@ -122,14 +122,15 @@ class BoTSORT(BaseTracker):\n         # Merge and prepare outp",add,Added net . kano . joustsim . oscar . os
c9a36fc868b4babe97c4c583eb89bd60860ac7d3,fix,boxmot/trackers/botsort/botsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport torch\nimport numpy as np\nfrom pathlib import Path\n\nfrom boxmot.motion.kalman_filters.xywh_kf import KalmanFilterXYWH\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.botsort.botsort_utils import joint_stracks, sub_stracks, remove_duplicate_stracks \nfrom boxmot.trackers.botsort.botsort_track import STrack\n\n\nclass BoTSORT(BaseTracker):\n    """"""""""""\n    BoTSORT Tracker: A tracking algorithm that combines appearance and motion-based tracking.\n\n    Args:\n        reid_weights (str): Path to the model weights for ReID.\n        device (torch.device): Device to run the model on (e.g., 'cpu' o","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport torch\nimport numpy as np\nfrom pathlib import Path\n\nfrom boxmot.motion.kalman_filters.xywh_kf import KalmanFilterXYWH\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.botsort.botsort_utils import joint_stracks, sub_stracks, remove_duplicate_stracks \nfrom boxmot.trackers.botsort.botsort_track import STrack\n\n\nclass BoTSORT(BaseTracker):\n    """"""""""""\n    BoTSORT Tracker: A tracking algorithm that combines appearance and motion-based tracking.\n\n    Args:\n        reid_weights (str): Path to the model weights for ReID.\n        device (torch.device): Device to run the model on (e.g., 'cpu' o","@@ -103,18 +103,20 @@ class BoTSORT(BaseTracker):\n \n         # Separate unconfirmed and active tracks\n         unconfirmed, active_tracks = self._separate_tracks()\n+        \n+        strack_pool = joint_stracks(active_tracks, self.lost_stracks)\n \n         # First association\n-        matches_first, u_track_first, u_detection_first, strack_pool = self._first_association(dets_first, active_tracks, unconfirmed, img, detections, activated_stracks, refind_stracks)\n+        matches_first, u_track_first, u_detection_first = self._first_association(dets_first, active_tracks, unconfirmed, img, detections, activated_stracks, refind_stracks, strack_pool)\n \n         # Second association\n         matches_second, u_track_second, u_detection_second = self._second_association(dets_second, activated_stracks, lost_stracks, refind_stracks, u_track_first, strack_pool)\n \n         # Handle unconfirmed tracks\n-        self._handle_unconfirmed_tracks(u_detection_first, detections, activated_stracks, removed_stracks, unconfirmed)\n+        matches_unc, u_track_unc, u_detection_unc = self._handle_unconfirmed_tracks(u_detection_first, detections, activated_stracks, removed_stracks, unconfirmed)\n \n         # Initialize new tracks\n-        self._initialize_new_tracks(detections, activated_stracks)\n+        self._initialize_new_tracks(u_detection_unc, activated_stracks, [detections[i] for i in u_detection_first])\n \n         # Update lost and removed tracks\n         self._update_track_states(lost_stracks, removed_stracks)\n@@ -151,8 +153,8 @@ class BoTSORT(BaseTracker):\n                 active_tracks.append(track)\n         return unconfirmed, active_tracks\n \n-    def _first_association(self, dets_first, active_tracks, unconfirmed, img, detections, activated_stracks, refind_stracks):\n-        strack_pool = joint_stracks(active_tracks, self.lost_stracks)\n+    def _first_association(self, dets_first, active_tracks, unconfirmed, img, detections, activated_stracks, refind_s",add,Add note about data volume to enable_metrics_collection
fce4a465cb375539bd956fe2b61ff5f4aac272a9,fix when embeddings are fed externally,boxmot/trackers/botsort/botsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport torch\nimport numpy as np\nfrom pathlib import Path\n\nfrom boxmot.motion.kalman_filters.xywh_kf import KalmanFilterXYWH\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.botsort.botsort_utils import joint_stracks, sub_stracks, remove_duplicate_stracks \nfrom boxmot.trackers.botsort.botsort_track import STrack\n\n\nclass BoTSORT(BaseTracker):\n    """"""""""""\n    BoTSORT Tracker: A tracking algorithm that combines appearance and motion-based tracking.\n\n    Args:\n        reid_weights (str): Path to the model weights for ReID.\n        device (torch.device): Device to run the model on (e.g., 'cpu' o","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport torch\nimport numpy as np\nfrom pathlib import Path\n\nfrom boxmot.motion.kalman_filters.xywh_kf import KalmanFilterXYWH\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import (embedding_distance, fuse_score,\n                                   iou_distance, linear_assignment)\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.botsort.botsort_utils import joint_stracks, sub_stracks, remove_duplicate_stracks \nfrom boxmot.trackers.botsort.botsort_track import STrack\n\n\nclass BoTSORT(BaseTracker):\n    """"""""""""\n    BoTSORT Tracker: A tracking algorithm that combines appearance and motion-based tracking.\n\n    Args:\n        reid_weights (str): Path to the model weights for ReID.\n        device (torch.device): Device to run the model on (e.g., 'cpu' o","@@ -96,7 +96,7 @@ class BoTSORT(BaseTracker):\n         if self.with_reid and embs is None:\n             features_high = self.model.get_features(dets_first[:, 0:4], img)\n         else:\n-            features_high = embs_first if embs is not None else []\n+            features_high = embs_first if embs_first is not None else []\n \n         # Create detections\n         detections = self._create_detections(dets_first, features_high)\n@@ -131,7 +131,7 @@ class BoTSORT(BaseTracker):\n         dets_second = dets[second_mask]\n         first_mask = confs > self.track_high_thresh\n         dets_first = dets[first_mask]\n-        embs_first = embs[first_mask]\n+        embs_first = embs[first_mask] if embs is not None else None\n         return dets, dets_first, embs_first, dets_second\n \n     def _create_detections(self, dets_first, features_high):\n",add,Added Type name for DFI ( # 3 )
a60d92523af20af6ae0da70a203102105e00389b,handle None and empty np inputs,boxmot/trackers/basetracker.py,"import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.iou import AssociationFunction\n\n\nclass BaseTracker(ABC):\n    def __init__(\n        self, \n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        max_obs: int = 50,\n        nr_classes: int = 80,\n        per_class: bool = False,\n        asso_func: str = 'iou'\n    ):\n        """"""""""""\n        Initialize the BaseTracker object with detection threshold, maximum age, minimum hits, \n        and Intersection Over Union (IOU) threshold for tracking objects in video frames.\n\n        Parameters:\n        - det_thresh (float): Detection threshold for considering detections.\n        - max_age (int): Maximum age of a track before it is considered lost.\n        - min_hits (int): Minimum number of detection hits before a track is considered co","import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.iou import AssociationFunction\n\n\nclass BaseTracker(ABC):\n    def __init__(\n        self, \n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        max_obs: int = 50,\n        nr_classes: int = 80,\n        per_class: bool = False,\n        asso_func: str = 'iou'\n    ):\n        """"""""""""\n        Initialize the BaseTracker object with detection threshold, maximum age, minimum hits, \n        and Intersection Over Union (IOU) threshold for tracking objects in video frames.\n\n        Parameters:\n        - det_thresh (float): Detection threshold for considering detections.\n        - max_age (int): Maximum age of a track before it is considered lost.\n        - min_hits (int): Minimum number of detection hits before a track is considered co","@@ -125,6 +125,11 @@ class BaseTracker(ABC):\n         Decorator for the update method to handle per-class processing.\n         """"""""""""\n         def wrapper(self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None):\n+            \n+            #handle different types of inputs\n+            if dets is None or len(dets) == 0:\n+                dets = np.empty((0, 6))\n+            \n             if self.per_class:\n                 # Initialize an array to store the tracks for each class\n                 per_class_tracks = []\n",add,Added STORM - 370 to Changelog
a60d92523af20af6ae0da70a203102105e00389b,handle None and empty np inputs,tests/unit/test_trackers.py,"import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, ImprAssocTrack, get_tracker_config, create_tracker,\n)\n\nfrom boxmot.trackers.ocsort.ocsort import KalmanBoxTracker as OCSortKalmanBoxTracker\nfrom boxmot.trackers.deepocsort.deep_ocsort import KalmanBoxTracker as DeepOCSortKalmanBoxTracker\n\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT, ImprAssocTrack]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort', 'imprassoc']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'imprassoc']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        reid_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'","import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSORT, BoTSORT, DeepOCSORT, OCSORT, BYTETracker, ImprAssocTrack, get_tracker_config, create_tracker,\n)\n\nfrom boxmot.trackers.ocsort.ocsort import KalmanBoxTracker as OCSortKalmanBoxTracker\nfrom boxmot.trackers.deepocsort.deep_ocsort import KalmanBoxTracker as DeepOCSortKalmanBoxTracker\n\n\n\nMOTION_ONLY_TRACKING_METHODS=[OCSORT, BYTETracker]\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSORT, BoTSORT, DeepOCSORT, ImprAssocTrack]\nALL_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort', 'imprassoc']\nPER_CLASS_TRACKERS=['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'imprassoc']\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        reid_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'","@@ -165,3 +165,22 @@ def test_per_class_tracker_active_tracks(tracker_type):\n     assert tracker.per_class_active_tracks[0], """"No active tracks for class 0""""\n     assert tracker.per_class_active_tracks[65], """"No active tracks for class 65""""\n \n+\n+@pytest.mark.parametrize(""""tracker_type"""", ALL_TRACKERS)\n+@pytest.mark.parametrize(""""dets"""", [None, np.array([])])\n+def test_tracker_with_no_detections(tracker_type, dets):\n+    tracker_conf = get_tracker_config(tracker_type)\n+    tracker = create_tracker(\n+        tracker_type=tracker_type,\n+        tracker_config=tracker_conf,\n+        reid_weights=WEIGHTS / 'mobilenetv2_x1_4_dukemtmcreid.pt',\n+        device='cpu',\n+        half=False,\n+        per_class=False\n+    )\n+\n+    rgb = np.random.randint(255, size=(640, 640, 3), dtype=np.uint8)\n+    embs = np.random.random(size=(2, 512))\n+    \n+    output = tracker.update(dets, rgb, embs)\n+    assert output.size == 0, """"Output should be empty when no detections are provided""""\n\ No newline at end of file\n",add,Fix bytesPerPixel for SurfaceTexture
35c02931bdcea982ba906f8871d86227a0f87972,fix import,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loaders import LoadImagesAndVideos\n\nfrom ultralytics import YOLO\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup, split_dataset\nfrom boxmot.ap","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\n\nfrom ultralytics import YOLO\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup, split_dataset\nfrom boxmot.app","@@ -21,7 +21,7 @@ from boxmot.tracker_zoo import create_tracker\n from boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\n from boxmot.utils.checks import RequirementsChecker\n from boxmot.utils.torch_utils import select_device\n-from boxmot.data.loaders import LoadImagesAndVideos\n+from boxmot.data.loader import LoadImagesAndVideos\n \n from ultralytics import YOLO\n from ultralytics.utils.files import increment_path\n",add,Add TraceV2 deletion to script
7ee8c511bccbd234183906cfc5d121c78ab05af3,fix import,boxmot/data/loader.py,"import os\nimport cv2\nimport glob\nimport math\nimport numpy as np\nfrom pathlib import Path\nfrom PIL import Image\n\n\nclass LoadImagesAndVideos:\n    """"""""""""\n    A data loader for handling both images and videos, providing batches of frames or images for processing.\n    Supports various image formats, including HEIC, and handles text files with paths to images/videos.\n    """"""""""""\n\n    def __init__(self, path, batch_size=1, vid_stride=1):\n        self.batch_size = batch_size\n        self.vid_stride = vid_stride\n        self.files = self._load_files(path)\n        self.video_flag = [self._is_video(f) for f in self.files]\n        self.nf = len(self.files)\n        self.ni = sum(not is_video for is_video in self.video_flag)\n        self.mode = """"image""""\n        \n        self.cap = None\n        if any(self.video_flag):\n            self._start_video(self.files[self.video_flag.index(True)])\n        \n        if not self.files:\n            raise FileNotFoundError(f""""No images","import os\nimport cv2\nimport glob\nimport math\nimport numpy as np\nfrom pathlib import Path\nfrom PIL import Image\n\n\nVID_FORMATS = """"asf"""", """"avi"""", """"gif"""", """"m4v"""", """"mkv"""", """"mov"""", """"mp4"""", """"mpeg"""", """"mpg"""", """"ts"""", """"wmv""""  # include video suffixes\n\n\nclass LoadImagesAndVideos:\n    """"""""""""\n    A data loader for handling both images and videos, providing batches of frames or images for processing.\n    Supports various image formats, including HEIC, and handles text files with paths to images/videos.\n    """"""""""""\n\n    def __init__(self, path, batch_size=1, vid_stride=1):\n        self.batch_size = batch_size\n        self.vid_stride = vid_stride\n        self.files = self._load_files(path)\n        self.video_flag = [self._is_video(f) for f in self.files]\n        self.nf = len(self.files)\n        self.ni = sum(not is_video for is_video in self.video_flag)\n        self.mode = """"image""""\n        \n        self.cap = None\n        if any(self.video_flag):\n            sel","@@ -7,6 +7,9 @@ from pathlib import Path\n from PIL import Image\n \n \n+VID_FORMATS = """"asf"""", """"avi"""", """"gif"""", """"m4v"""", """"mkv"""", """"mov"""", """"mp4"""", """"mpeg"""", """"mpg"""", """"ts"""", """"wmv""""  # include video suffixes\n+\n+\n class LoadImagesAndVideos:\n     """"""""""""\n     A data loader for handling both images and videos, providing batches of frames or images for processing.\n",add,Add note about data volume to enable EGL
7ee8c511bccbd234183906cfc5d121c78ab05af3,fix import,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\n\nfrom ultralytics import YOLO\nfrom ultralytics.utils.files import increment_path\nfrom ultralytics.data.utils import VID_FORMATS\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup, split_dataset\nfrom boxmot.app","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\n\nfrom ultralytics import YOLO\nfrom ultralytics.utils.files import increment_path\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup, split_dataset\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend","@@ -25,7 +25,6 @@ from boxmot.data.loader import LoadImagesAndVideos\n \n from ultralytics import YOLO\n from ultralytics.utils.files import increment_path\n-from ultralytics.data.utils import VID_FORMATS\n \n from tracking.detectors import get_yolo_inferer\n from tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup, split_dataset\n",add,added meteocons cheatsheet
e57f5af9afacdca179c95dc83cdadbca595145c7,Fix source reassignment,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup, split_dataset\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nch","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup, split_dataset\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nch","@@ -130,6 +130,7 @@ def generate_dets_embs(args: argparse.Namespace, y: Path) -> None:\n         args (Namespace): Parsed command line arguments.\n         y (Path): Path to the YOLO model file.\n     """"""""""""\n+    source = Path(args.source) / """"img1""""\n     WEIGHTS.mkdir(parents=True, exist_ok=True)\n     \n     ul_models = ['yolov8', 'yolov9', 'yolov10', 'yolo11', 'rtdetr', 'sam']\n@@ -137,7 +138,7 @@ def generate_dets_embs(args: argparse.Namespace, y: Path) -> None:\n     yolo = YOLO(y if any(yolo in str(args.yolo_model) for yolo in ul_models) else 'yolov8n.pt')\n     \n     results = yolo(\n-        source=args.source,\n+        source=source,\n         conf=args.conf,\n         iou=args.iou,\n         agnostic_nms=args.agnostic_nms,\n@@ -161,7 +162,7 @@ def generate_dets_embs(args: argparse.Namespace, y: Path) -> None:\n     for r in args.reid_model:\n         model = ReidAutoBackend(weights=args.reid_model, device=yolo.predictor.device, half=args.half).model\n         reids.append(model)\n-        embs_path = args.project / 'dets_n_embs' / y.stem / 'embs' / r.stem / (Path(args.source).parent.name + '.txt')\n+        embs_path = args.project / 'dets_n_embs' / y.stem / 'embs' / r.stem / (source.parent.name + '.txt')\n         embs_path.parent.mkdir(parents=True, exist_ok=True)\n         embs_path.touch(exist_ok=True)\n \n@@ -170,7 +171,7 @@ def generate_dets_embs(args: argparse.Namespace, y: Path) -> None:\n \n     yolo.predictor.custom_args = args\n \n-    dets_path = args.project / 'dets_n_embs' / y.stem / 'dets' / (Path(args.source).parent.name + '.txt')\n+    dets_path = args.project / 'dets_n_embs' / y.stem / 'dets' / (source.parent.name + '.txt')\n     dets_path.parent.mkdir(parents=True, exist_ok=True)\n     dets_path.touch(exist_ok=True)\n \n@@ -178,7 +179,7 @@ def generate_dets_embs(args: argparse.Namespace, y: Path) -> None:\n         open(dets_path, 'w').close()\n \n     with open(str(dets_path), 'ab+') as f:\n-        np.savetxt(f, [], fmt='%f', heade",add,Add custom timeout to landscape layout
9b5c53ef9f2bdaeb0dbf49f64bdcf1d942041b0b,Fix of source option for generating dets and embs,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup, split_dataset\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nch","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\n\nfrom tracking.detectors import get_yolo_inferer\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup, split_dataset\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\nch","@@ -122,15 +122,16 @@ def prompt_overwrite(path_type: str, path: str, ci: bool = True) -> bool:\n     return input_with_timeout(f""""{path_type} {path} already exists. Overwrite? [y/N]: """")\n \n \n-def generate_dets_embs(args: argparse.Namespace, y: Path) -> None:\n+def generate_dets_embs(args: argparse.Namespace, y: Path, source: Path) -> None:\n     """"""""""""\n-    Generates detections and embeddings for the specified YOLO model and arguments.\n+    Generates detections and embeddings for the specified \n+    arguments, YOLO model and source.\n \n     Args:\n         args (Namespace): Parsed command line arguments.\n         y (Path): Path to the YOLO model file.\n+        source (Path): Path to the source directory.\n     """"""""""""\n-    source = Path(args.source) / """"img1""""\n     WEIGHTS.mkdir(parents=True, exist_ok=True)\n     \n     ul_models = ['yolov8', 'yolov9', 'yolov10', 'yolo11', 'rtdetr', 'sam']\n@@ -349,8 +350,7 @@ def run_generate_dets_embs(opt: argparse.Namespace) -> None:\n                     LOGGER.info(f'Skipping generation for {mot_folder_path} as they already exist.')\n                     continue\n             LOGGER.info(f'Generating detections and embeddings for data under {mot_folder_path} [{i + 1}/{len(mot_folder_paths)} seqs]')\n-            # opt.source = mot_folder_path / 'img1'\n-            generate_dets_embs(opt, y)\n+            generate_dets_embs(opt, y, source=mot_folder_path / 'img1')\n \n \n def run_generate_mot_results(opt: argparse.Namespace, evolve_config: dict = None) -> None:\n",add,Add note about data volume to enable_metrics_collection
1e1c4fe41ea91a622fd57001cdb42ff0f0e9f188,Small fixes for model loading and preprocessing,tracking/detectors/yolox.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.ops import bytetrack_preprocess\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.ops import bytetrack_preprocess\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\","@@ -130,6 +130,8 @@ class YoloXStrategy(YoloInterface):\n             imgs_preprocessed.append(img_pre)\n             self._preproc_data.append(ratio)\n \n+        imgs_preprocessed = torch.vstack(imgs_preprocessed)\n+\n         return imgs_preprocessed\n \n     def postprocess(self, preds, im, im0s):\n",add,Added TypeSpec . h for layers .
1e1c4fe41ea91a622fd57001cdb42ff0f0e9f188,Small fixes for model loading and preprocessing,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model)\nfrom tracking.detectors.yolox import YoloXStrategy\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model)\nfrom tracking.detectors.yolox import YoloXStrategy\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_","@@ -141,7 +141,7 @@ def generate_dets_embs(args: argparse.Namespace, y: Path, source: Path) -> None:\n         args.imgsz = default_imgsz(args.yolo_model)\n \n     yolo = YOLO(\n-        args.yolo_model if is_ultralytics_model(args.yolo_model)\n+        y if is_ultralytics_model(args.yolo_model)\n         else 'yolov8n.pt',\n     )\n \n",add,Add note about data volume to enable_metrics_collection
f89ca72ec55881946637cf80ea5107b1ad01bb71,Fix for YOLOX inference,tracking/detectors/yolox.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.ops import bytetrack_preprocess\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.ops import bytetrack_preprocess\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\","@@ -101,8 +101,10 @@ class YoloXStrategy(YoloInterface):\n             else:\n                 im = torch.vstack(im)\n \n-        assert len(im.shape) == 4, f""""Expected 4D tensor as input, got {\n-            im.shape}""""\n+        if len(im.shape) == 3:\n+            im = im.unsqueeze(0)\n+\n+        assert len(im.shape) == 4, f""""Expected 4D tensor as input, got {im.shape}""""\n \n         preds = self.model(im)\n         return preds\n",add,Added string type to outerHTML .
3b0fa06d7f4e4c18332b2968c7111923823a9cc1,Fix for non-existing tracks files,tracking/utils.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nimport time\nimport pandas as pd\nfrom ultralytics.utils import ops\nfrom ultralytics.engine.results import Results\nfrom typing import Union\nfrom pathlib import Path\nimport os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\nfrom boxmot.utils import EXAMPLES, ROOT\n\n\ndef split_dataset(src_fldr: Path, percent_to_delete: float = 0.5) -> None:\n    """"""""""""\n    Copies the dataset to a new location and removes a specified percentage of images and annotations,\n    adjusting the frame index to start at 1.\n\n    Args:\n        src_fldr (Path): Source folder containing the dataset.\n        percent_to_delete (float): Percentage of images and annotations to remove.\n    """"""""""""\n    # Ensure source path is a Path object\n    src_fldr = Path(src_fldr)\n\n    # Generate the destination","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nimport time\nimport pandas as pd\nfrom ultralytics.utils import ops\nfrom ultralytics.engine.results import Results\nfrom typing import Union\nfrom pathlib import Path\nimport os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\nfrom boxmot.utils import EXAMPLES, ROOT\n\n\ndef split_dataset(src_fldr: Path, percent_to_delete: float = 0.5) -> None:\n    """"""""""""\n    Copies the dataset to a new location and removes a specified percentage of images and annotations,\n    adjusting the frame index to start at 1.\n\n    Args:\n        src_fldr (Path): Source folder containing the dataset.\n        percent_to_delete (float): Percentage of images and annotations to remove.\n    """"""""""""\n    # Ensure source path is a Path object\n    src_fldr = Path(src_fldr)\n\n    # Generate the destination","@@ -371,13 +371,13 @@ def write_mot_results(txt_path: Path, mot_results: np.ndarray) -> None:\n     path to the file will be created as well if necessary.\n     """"""""""""\n     if mot_results is not None:\n-        if mot_results.size != 0:\n-            # Ensure the parent directory of the txt_path exists\n-            txt_path.parent.mkdir(parents=True, exist_ok=True)\n+        # Ensure the parent directory of the txt_path exists\n+        txt_path.parent.mkdir(parents=True, exist_ok=True)\n \n-            # Ensure the file exists before opening\n-            txt_path.touch(exist_ok=True)\n+        # Ensure the file exists before opening\n+        txt_path.touch(exist_ok=True)\n \n+        if mot_results.size != 0:\n             # Open the file in append mode and save the MOT results\n             with open(str(txt_path), 'a') as file:\n                 np.savetxt(file, mot_results, fmt='%d,%d,%d,%d,%d,%d,%d,%d,%.6f')\n",add,Added STORM - 1270 to Changelog
3b0fa06d7f4e4c18332b2968c7111923823a9cc1,Fix for non-existing tracks files,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model)\nfrom tracking.detectors.yolox import YoloXStrategy\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model)\nfrom tracking.detectors.yolox import YoloXStrategy\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_","@@ -284,7 +284,10 @@ def generate_mot_results(args: argparse.Namespace, config_dict: dict = None) ->\n \n     if all_mot_results:\n         all_mot_results = np.vstack(all_mot_results)\n-        write_mot_results(txt_path, all_mot_results)\n+    else:\n+        all_mot_results = np.empty((0, 0))\n+\n+    write_mot_results(txt_path, all_mot_results)\n \n \n def parse_mot_results(results: str) -> dict:\n",add,Add note about data volume to enable_metrics_collection
b49911f08fccd0535b4629584b31ff14c3d4bc33,Fixes for correct yolox installing before importing,tracking/detectors/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\nUL_MODELS = ['yolov8', 'yolov9', 'yolov10', 'yolo11', 'rtdetr', 'sam']\n\n\ndef is_ultralytics_model(yolo_name):\n    return any(yolo in str(yolo_name) for yolo in UL_MODELS)\n\n\ndef is_yolox_model(yolo_name):\n    return 'yolox' in str(yolo_name)\n\n\ndef default_imgsz(yolo_name):\n    if is_ultralytics_model(yolo_name):\n        return [640, 640]\n    elif is_yolox_model(yolo_name):\n        return [800, 1440]\n    else:\n        return [640, 640]\n\n\ndef get_yolo_inferer(yolo_model):\n\n    if 'yolox' in str(yolo_model):\n        try:\n            import yolox  # for linear_assignment\n            assert yolox.__version__\n        except (ImportError, AssertionError, AttributeError):\n            checker.check_packages(('yolox==0.3.0',), cmds='--no-dependencies')\n            checker.check_packag","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\nUL_MODELS = ['yolov8', 'yolov9', 'yolov10', 'yolo11', 'rtdetr', 'sam']\n\n\ndef is_ultralytics_model(yolo_name):\n    return any(yolo in str(yolo_name) for yolo in UL_MODELS)\n\n\ndef is_yolox_model(yolo_name):\n    return 'yolox' in str(yolo_name)\n\n\ndef default_imgsz(yolo_name):\n    if is_ultralytics_model(yolo_name):\n        return [640, 640]\n    elif is_yolox_model(yolo_name):\n        return [800, 1440]\n    else:\n        return [640, 640]\n\n\ndef get_yolo_inferer(yolo_model):\n\n    if is_yolox_model(yolo_model):\n        try:\n            import yolox  # for linear_assignment\n            assert yolox.__version__\n        except (ImportError, AssertionError, AttributeError):\n            checker.check_packages(('yolox==0.3.0',), cmds='--no-dependencies')\n            checker.check_packag","@@ -27,7 +27,7 @@ def default_imgsz(yolo_name):\n \n def get_yolo_inferer(yolo_model):\n \n-    if 'yolox' in str(yolo_model):\n+    if is_yolox_model(yolo_model):\n         try:\n             import yolox  # for linear_assignment\n             assert yolox.__version__\n",add,Add note about data volume to enable_metrics_collection
b49911f08fccd0535b4629584b31ff14c3d4bc33,Fixes for correct yolox installing before importing,tracking/detectors/yolox.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.ops import bytetrack_preprocess\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.ops import bytetrack_preprocess\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\","@@ -121,20 +121,20 @@ class YoloXStrategy(YoloInterface):\n                 """"Only ultralytics predictors are supported"""")\n         self.im_paths = predictor.batch[0]\n \n-    def preprocess(self, imgs) -> torch.Tensor:\n-        assert isinstance(imgs, list)\n-        imgs_preprocessed = []\n+    def preprocess(self, im) -> torch.Tensor:\n+        assert isinstance(im, list)\n+        im_preprocessed = []\n         self._preproc_data = []\n-        for i, img in enumerate(imgs):\n+        for i, img in enumerate(im):\n             img_pre, ratio = bytetrack_preprocess(img, input_size=self.imgsz)\n             img_pre = torch.Tensor(img_pre).unsqueeze(0).to(self.device)\n \n-            imgs_preprocessed.append(img_pre)\n+            im_preprocessed.append(img_pre)\n             self._preproc_data.append(ratio)\n \n-        imgs_preprocessed = torch.vstack(imgs_preprocessed)\n+        im_preprocessed = torch.vstack(im_preprocessed)\n \n-        return imgs_preprocessed\n+        return im_preprocessed\n \n     def postprocess(self, preds, im, im0s):\n \n",add,Add note about data volume to enable_metrics_collection
b49911f08fccd0535b4629584b31ff14c3d4bc33,Fixes for correct yolox installing before importing,tracking/track.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom functools import partial\nfrom pathlib import Path\n\nimport torch\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.detectors import (is_ultralytics_model, get_yolo_inferer,\n                                default_imgsz)\nfrom tracking.detectors.yolox import YoloXStrategy\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.utils.plotting import Annotator, colors\nfrom ultralytics.data.utils import VID_FORMATS\nfrom ultralytics.utils.plotting import save_one_box\n\n\ndef on_predict_start(predictor, persist=False):\n    """"""""""""\n    Initialize trackers for object tracking during prediction","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport cv2\nimport numpy as np\nfrom functools import partial\nfrom pathlib import Path\n\nimport torch\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.utils.plotting import Annotator, colors\nfrom ultralytics.data.utils import VID_FORMATS\nfrom ultralytics.utils.plotting import save_one_box\n\n\ndef on_predict_start(predictor, persist=False):\n    """"""""""""\n    Initialize trackers for object tracking during prediction.\n\n    Args:\n        predictor (o","@@ -12,9 +12,8 @@ from boxmot import TRACKERS\n from boxmot.tracker_zoo import create_tracker\n from boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS\n from boxmot.utils.checks import RequirementsChecker\n-from tracking.detectors import (is_ultralytics_model, get_yolo_inferer,\n-                                default_imgsz)\n-from tracking.detectors.yolox import YoloXStrategy\n+from tracking.detectors import (get_yolo_inferer, default_imgsz,\n+                                is_ultralytics_model, is_yolox_model)\n \n checker = RequirementsChecker()\n checker.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n@@ -99,12 +98,12 @@ def run(args):\n         yolo.predictor.model = yolo_model\n \n         # If current model is YOLOX, change the preprocess and postprocess\n-        if isinstance(yolo_model, YoloXStrategy):\n+        if is_yolox_model(args.yolo_model):\n             # add callback to save image paths for further processing\n             yolo.add_callback(""""on_predict_batch_start"""",\n                               lambda p: yolo_model.update_im_paths(p))\n             yolo.predictor.preprocess = (\n-                lambda imgs: yolo_model.preprocess(imgs=imgs))\n+                lambda imgs: yolo_model.preprocess(im=imgs))\n             yolo.predictor.postprocess = (\n                 lambda preds, im, im0s:\n                 yolo_model.postprocess(preds=preds, im=im, im0s=im0s))\n",add,Added net . kano . joustsim . oscar . os
b49911f08fccd0535b4629584b31ff14c3d4bc33,Fixes for correct yolox installing before importing,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model)\nfrom tracking.detectors.yolox import YoloXStrategy\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_set","@@ -27,8 +27,7 @@ from boxmot.utils.misc import increment_path\n from ultralytics import YOLO\n \n from tracking.detectors import (get_yolo_inferer, default_imgsz,\n-                                is_ultralytics_model)\n-from tracking.detectors.yolox import YoloXStrategy\n+                                is_ultralytics_model, is_yolox_model)\n from tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup, split_dataset\n from boxmot.appearance.reid_auto_backend import ReidAutoBackend\n \n@@ -138,10 +137,10 @@ def generate_dets_embs(args: argparse.Namespace, y: Path, source: Path) -> None:\n     WEIGHTS.mkdir(parents=True, exist_ok=True)\n \n     if args.imgsz is None:\n-        args.imgsz = default_imgsz(args.yolo_model)\n+        args.imgsz = default_imgsz(y)\n \n     yolo = YOLO(\n-        y if is_ultralytics_model(args.yolo_model)\n+        y if is_ultralytics_model(y)\n         else 'yolov8n.pt',\n     )\n \n@@ -161,14 +160,14 @@ def generate_dets_embs(args: argparse.Namespace, y: Path, source: Path) -> None:\n         vid_stride=args.vid_stride,\n     )\n \n-    if not is_ultralytics_model(args.yolo_model):\n+    if not is_ultralytics_model(y):\n         m = get_yolo_inferer(y)\n         yolo_model = m(model=y, device=yolo.predictor.device,\n                        args=yolo.predictor.args)\n         yolo.predictor.model = yolo_model\n \n         # If current model is YOLOX, change the preprocess and postprocess\n-        if isinstance(yolo_model, YoloXStrategy):\n+        if is_yolox_model(y):\n             # add callback to save image paths for further processing\n             yolo.add_callback(""""on_predict_batch_start"""",\n                               lambda p: yolo_model.update_im_paths(p))\n",add,Add note about data volume to enable_metrics_collection
118a1d56b1c01419b30cebf1a0800564d11d0edf,fix when no model selected,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_set","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_set","@@ -445,8 +445,8 @@ def parse_opt() -> argparse.Namespace:\n     parser = argparse.ArgumentParser()\n \n     # Global arguments\n-    parser.add_argument('--yolo-model', nargs='+', type=Path, default=WEIGHTS / 'yolov8n.pt', help='yolo model path')\n-    parser.add_argument('--reid-model', nargs='+', type=Path, default=WEIGHTS / 'osnet_x0_25_msmt17.pt', help='reid model path')\n+    parser.add_argument('--yolo-model', nargs='+', type=Path, default=[WEIGHTS / 'yolov8n.pt'], help='yolo model path')\n+    parser.add_argument('--reid-model', nargs='+', type=Path, default=[WEIGHTS / 'osnet_x0_25_msmt17.pt'], help='reid model path')\n     parser.add_argument('--source', type=str, help='file/dir/URL/glob, 0 for webcam')\n     parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=None, help='inference size h,w')\n     parser.add_argument('--conf', type=float, default=0.5, help='confidence threshold')\n",add,Fix bytesPerPixel for SurfaceTexture
c22ddbad05e6bcde0a6f5631cf4cc2af7fde1018,Fix invalid escape sequence warning by using raw string in val.py,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_set","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.data.loader import LoadImagesAndVideos\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_set","@@ -300,7 +300,7 @@ def parse_mot_results(results: str) -> dict:\n         dict: A dictionary containing HOTA, MOTA, and IDF1 scores.\n     """"""""""""\n     combined_results = results.split('COMBINED')[2:-1]\n-    combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0])\n+    combined_results = [float(re.findall(r""""[-+]?(?:\d*\.*\d+)"""", f)[0])\n                         for f in combined_results]\n \n     results_dict = {}\n",add,Added Type name for DFI ( # 210 )
821e18eddc6950ede0e3add54bddd3489a31fdfc,fix,boxmot/trackers/basetracker.py,"import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.iou import AssociationFunction\n\n\nclass BaseTracker(ABC):\n    def __init__(\n        self, \n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        max_obs: int = 50,\n        nr_classes: int = 80,\n        per_class: bool = False,\n        asso_func: str = 'iou',\n        is_obb: bool = False\n    ):\n        """"""""""""\n        Initialize the BaseTracker object with detection threshold, maximum age, minimum hits, \n        and Intersection Over Union (IOU) threshold for tracking objects in video frames.\n\n        Parameters:\n        - det_thresh (float): Detection threshold for considering detections.\n        - max_age (int): Maximum age of a track before it is considered lost.\n        - min_hits (int): Minimum number of detection hits ","import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.iou import AssociationFunction\n\n\nclass BaseTracker(ABC):\n    def __init__(\n        self, \n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        max_obs: int = 50,\n        nr_classes: int = 80,\n        per_class: bool = False,\n        asso_func: str = 'iou',\n        is_obb: bool = False\n    ):\n        """"""""""""\n        Initialize the BaseTracker object with detection threshold, maximum age, minimum hits, \n        and Intersection Over Union (IOU) threshold for tracking objects in video frames.\n\n        Parameters:\n        - det_thresh (float): Detection threshold for considering detections.\n        - max_age (int): Maximum age of a track before it is considered lost.\n        - min_hits (int): Minimum number of detection hits ","@@ -111,12 +111,13 @@ class BaseTracker(ABC):\n             # Even if dets is empty (e.g., shape (0, 7)), this check will still pass if it's Nx7\n             if not self._first_dets_processed:\n                 dets = args[0]\n-                if dets.ndim == 2 and dets.shape[1] == 6:\n-                    self.is_obb = False\n-                    self._first_dets_processed = True\n-                elif dets.ndim == 2 and dets.shape[1] == 7:\n-                    self.is_obb = True\n-                    self._first_dets_processed = True\n+                if dets is not None:\n+                    if dets.ndim == 2 and dets.shape[1] == 6:\n+                        self.is_obb = False\n+                        self._first_dets_processed = True\n+                    elif dets.ndim == 2 and dets.shape[1] == 7:\n+                        self.is_obb = True\n+                        self._first_dets_processed = True\n \n             if not self._first_frame_processed:\n                 img = args[1]\n@@ -191,12 +192,10 @@ class BaseTracker(ABC):\n         assert (\n             len(dets.shape) == 2\n         ), """"Unsupported 'dets' dimensions, valid number of dimensions is two""""\n-        if self.is_obb :\n-\n+        if self.is_obb:\n             assert (\n                 dets.shape[1] == 7\n             ), """"Unsupported 'dets' 2nd dimension lenght, valid lenghts is 6 (cx,cy,w,h,angle,conf,cls)""""\n-            \n         else :\n             assert (\n                 dets.shape[1] == 6\n",add,Add note about data volume to enable_metrics_collection
648d8ff71403b8d8e9d4f29a9a812c1b1d98d29f,fix,boxmot/utils/selective_reid.py,"    def aiou(self, bbox, candidates):\n        """"""""""""\n        IoU - Aspect Ratio\n        """"""""""""\n        candidates = np.array(candidates)\n        bbox_tl, bbox_br = bbox[:2], bbox[:2] + bbox[2:]\n        candidates_tl = candidates[:, :2]\n        candidates_br = candidates[:, :2] + candidates[:, 2:]\n\n        tl = np.c_[np.maximum(bbox_tl[0], candidates_tl[:, 0])[:, np.newaxis],\n                np.maximum(bbox_tl[1], candidates_tl[:, 1])[:, np.newaxis]]\n        br = np.c_[np.minimum(bbox_br[0], candidates_br[:, 0])[:, np.newaxis],\n                np.minimum(bbox_br[1], candidates_br[:, 1])[:, np.newaxis]]\n        wh = np.maximum(0., br - tl)\n\n        area_intersection = wh.prod(axis=1)\n        area_bbox = bbox[2:].prod()\n        area_candidates = candidates[:, 2:].prod(axis=1)\n\n        iou = area_intersection / (area_bbox + area_candidates - area_intersection)\n\n        aspect_ratio = bbox[2] / bbox[3]\n        candidates_aspect_ratio = candidates[:, 2] / candidates[:, ",,"@@ -1,99 +0,0 @@\n-    def aiou(self, bbox, candidates):\n-        """"""""""""\n-        IoU - Aspect Ratio\n-        """"""""""""\n-        candidates = np.array(candidates)\n-        bbox_tl, bbox_br = bbox[:2], bbox[:2] + bbox[2:]\n-        candidates_tl = candidates[:, :2]\n-        candidates_br = candidates[:, :2] + candidates[:, 2:]\n-\n-        tl = np.c_[np.maximum(bbox_tl[0], candidates_tl[:, 0])[:, np.newaxis],\n-                np.maximum(bbox_tl[1], candidates_tl[:, 1])[:, np.newaxis]]\n-        br = np.c_[np.minimum(bbox_br[0], candidates_br[:, 0])[:, np.newaxis],\n-                np.minimum(bbox_br[1], candidates_br[:, 1])[:, np.newaxis]]\n-        wh = np.maximum(0., br - tl)\n-\n-        area_intersection = wh.prod(axis=1)\n-        area_bbox = bbox[2:].prod()\n-        area_candidates = candidates[:, 2:].prod(axis=1)\n-\n-        iou = area_intersection / (area_bbox + area_candidates - area_intersection)\n-\n-        aspect_ratio = bbox[2] / bbox[3]\n-        candidates_aspect_ratio = candidates[:, 2] / candidates[:, 3]\n-        arctan = np.arctan(aspect_ratio) - np.arctan(candidates_aspect_ratio)\n-        v = 1 - ((4 / np.pi ** 2) * arctan ** 2)\n-        alpha = v / (1 - iou + v)\n-\n-        return iou, alpha\n-\n-    def aiou_vectorized(self, bboxes1, bboxes2):\n-        """"""""""""\n-        Vectorized implementation of AIOU (IoU with aspect ratio consideration)\n-        \n-        Args:\n-        bboxes1: numpy array of shape (N, 4) in format (x, y, w, h)\n-        bboxes2: numpy array of shape (M, 4) in format (x, y, w, h)\n-        \n-        Returns:\n-        ious: numpy array of shape (N, M) containing IoU values\n-        alphas: numpy array of shape (N, M) containing alpha values\n-        """"""""""""\n-        # Convert (x, y, w, h) to (x1, y1, x2, y2)\n-        bboxes1_x1y1 = bboxes1[:, :2]\n-        bboxes1_x2y2 = bboxes1[:, :2] + bboxes1[:, 2:]\n-        bboxes2_x1y1 = bboxes2[:, :2]\n-        bboxes2_x2y2 = bboxes2[:, :2] + bboxes2[:, 2:]\n-\n-   ",add,Added STORM - 295 to Changelog
d12506a7e1b38559d7dd81f75946ab052470b569,Fix: Handle dynamic batch sizes in TensorRT ReID model,boxmot/appearance/backends/tensorrt_backend.py,"import torch\nimport numpy as np\nfrom pathlib import Path\nfrom collections import OrderedDict, namedtuple\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\n\nclass TensorRTBackend(BaseModelBackend):\n    def __init__(self, weights, device, half):\n        self.is_trt10 = False\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.device = device\n        self.weights = weights\n        self.fp16 = False  # Will be updated in load_model\n        self.load_model(self.weights)\n\n    def load_model(self, w):\n        LOGGER.info(f""""Loading {w} for TensorRT inference..."""")\n        self.checker.check_packages((""""nvidia-tensorrt"""",))\n        try:\n            import tensorrt as trt  # TensorRT library\n        except ImportError:\n            raise ImportError(""""Please install tensorrt to use this backend."""")\n\n        if self.device.type == """"cpu"""":\n           ","import torch\nimport numpy as np\nfrom pathlib import Path\nfrom collections import OrderedDict, namedtuple\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\n\nclass TensorRTBackend(BaseModelBackend):\n    def __init__(self, weights, device, half):\n        self.is_trt10 = False\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.device = device\n        self.weights = weights\n        self.fp16 = False  # Will be updated in load_model\n        self.load_model(self.weights)\n\n    def load_model(self, w):\n        LOGGER.info(f""""Loading {w} for TensorRT inference..."""")\n        self.checker.check_packages((""""nvidia-tensorrt"""",))\n        try:\n            import tensorrt as trt  # TensorRT library\n        except ImportError:\n            raise ImportError(""""Please install tensorrt to use this backend."""")\n\n        if self.device.type == """"cpu"""":\n           ","@@ -78,27 +78,49 @@ class TensorRTBackend(BaseModelBackend):\n         self.binding_addrs = OrderedDict((n, d.ptr) for n, d in self.bindings.items())\n \n     def forward(self, im_batch):\n-        # Adjust for dynamic shapes\n-        if im_batch.shape != self.bindings[""""images""""].shape:\n-            if self.is_trt10:\n-                self.context.set_input_shape(""""images"""", im_batch.shape)\n-                self.bindings[""""images""""] = self.bindings[""""images""""]._replace(shape=im_batch.shape)\n-                self.bindings[""""output""""].data.resize_(tuple(self.context.get_tensor_shape(""""output"""")))\n-            else:\n-                i_in = self.model_.get_binding_index(""""images"""")\n-                i_out = self.model_.get_binding_index(""""output"""")\n-                self.context.set_binding_shape(i_in, im_batch.shape)\n-                self.bindings[""""images""""] = self.bindings[""""images""""]._replace(shape=im_batch.shape)\n-                output_shape = tuple(self.context.get_binding_shape(i_out))\n-                self.bindings[""""output""""].data.resize_(output_shape)\n-\n-        s = self.bindings[""""images""""].shape\n-        assert im_batch.shape == s, f""""Input size {im_batch.shape} does not match model size {s}""""\n-\n-        # Set input buffer\n-        self.binding_addrs[""""images""""] = int(im_batch.data_ptr())\n-\n-        # Execute inference\n-        self.context.execute_v2(list(self.binding_addrs.values()))\n-        features = self.bindings[""""output""""].data\n-        return features\n+        temp_im_batch = im_batch.clone()\n+        batch_array = []\n+        inp_batch = im_batch.shape[0]\n+        out_batch = self.bindings[""""output""""].shape[0]\n+        resultant_features = []\n+\n+        # Divide batch to sub batches\n+        while inp_batch > out_batch:\n+            batch_array.append(temp_im_batch[:out_batch])\n+            temp_im_batch = temp_im_batch[out_batch:]\n+            inp_batch = temp_im_batch.shape[0]\n+        if temp_im_batch.shape[0] >",add,Add note about data volume to enable_metrics_collection
569b96c64d4e8ba15c88abc0b5c045fc53fa0a27,Fix: Mistake in handling the output features of ReID model in TensorRT,boxmot/appearance/backends/tensorrt_backend.py,"import torch\nimport numpy as np\nfrom pathlib import Path\nfrom collections import OrderedDict, namedtuple\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\n\nclass TensorRTBackend(BaseModelBackend):\n    def __init__(self, weights, device, half):\n        self.is_trt10 = False\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.device = device\n        self.weights = weights\n        self.fp16 = False  # Will be updated in load_model\n        self.load_model(self.weights)\n\n    def load_model(self, w):\n        LOGGER.info(f""""Loading {w} for TensorRT inference..."""")\n        self.checker.check_packages((""""nvidia-tensorrt"""",))\n        try:\n            import tensorrt as trt  # TensorRT library\n        except ImportError:\n            raise ImportError(""""Please install tensorrt to use this backend."""")\n\n        if self.device.type == """"cpu"""":\n           ","import torch\nimport numpy as np\nfrom pathlib import Path\nfrom collections import OrderedDict, namedtuple\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\n\nclass TensorRTBackend(BaseModelBackend):\n    def __init__(self, weights, device, half):\n        self.is_trt10 = False\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.device = device\n        self.weights = weights\n        self.fp16 = False  # Will be updated in load_model\n        self.load_model(self.weights)\n\n    def load_model(self, w):\n        LOGGER.info(f""""Loading {w} for TensorRT inference..."""")\n        self.checker.check_packages((""""nvidia-tensorrt"""",))\n        try:\n            import tensorrt as trt  # TensorRT library\n        except ImportError:\n            raise ImportError(""""Please install tensorrt to use this backend."""")\n\n        if self.device.type == """"cpu"""":\n           ","@@ -116,7 +116,7 @@ class TensorRTBackend(BaseModelBackend):\n             # Execute inference\n             self.context.execute_v2(list(self.binding_addrs.values()))\n             features = self.bindings[""""output""""].data\n-            resultant_features.append(features)\n+            resultant_features.append(features.clone())\n \n         if len(resultant_features)== 1:\n             return resultant_features[0]\n",add,Add note about data volume to enable_metrics_collection
7f3eee4f139c0f09123178b3d62ada58a9cfb6d8,FIX: RuntimeError: expected scalar type Float but found Half error on CLIP inference,boxmot/appearance/backbones/clip/clip/model.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import OrderedDict\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = None\n        s","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import OrderedDict\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = None\n        s","@@ -157,8 +157,11 @@ class LayerNorm(nn.LayerNorm):\n \n     def forward(self, x: torch.Tensor):\n         orig_type = x.dtype\n-        ret = super().forward(x.type(torch.float32))\n-        return ret.type(orig_type)\n+        for param in self.parameters():\n+            if param.dtype == torch.float16:\n+                param.data = param.data.to(torch.float32)\n+        ret = super().forward(x.to(torch.float32))\n+        return ret.to(orig_type)\n \n \n class QuickGELU(nn.Module):\n",add,Add note about data volume to enable_metrics_collection
54191fd2049ddfa81581d9baae1e1eb1dd929cef,Fix no response problem,tracking/utils.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nimport time\nimport pandas as pd\nfrom ultralytics.utils import ops\nfrom ultralytics.engine.results import Results\nfrom typing import Union\nfrom pathlib import Path\nimport os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\nfrom boxmot.utils import EXAMPLES, ROOT\n\n\ndef split_dataset(src_fldr: Path, percent_to_delete: float = 0.5) -> None:\n    """"""""""""\n    Copies the dataset to a new location and removes a specified percentage of images and annotations,\n    adjusting the frame index to start at 1.\n\n    Args:\n        src_fldr (Path): Source folder containing the dataset.\n        percent_to_delete (float): Percentage of images and annotations to remove.\n    """"""""""""\n    # Ensure source path is a Path object\n    src_fldr = Path(src_fldr)\n\n    # Generate the destination","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nimport time\nimport pandas as pd\nfrom ultralytics.utils import ops\nfrom ultralytics.engine.results import Results\nfrom typing import Union\nfrom pathlib import Path\nimport os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\nfrom boxmot.utils import EXAMPLES, ROOT\n\n\ndef split_dataset(src_fldr: Path, percent_to_delete: float = 0.5) -> None:\n    """"""""""""\n    Copies the dataset to a new location and removes a specified percentage of images and annotations,\n    adjusting the frame index to start at 1.\n\n    Args:\n        src_fldr (Path): Source folder containing the dataset.\n        percent_to_delete (float): Percentage of images and annotations to remove.\n    """"""""""""\n    # Ensure source path is a Path object\n    src_fldr = Path(src_fldr)\n\n    # Generate the destination","@@ -153,6 +153,7 @@ def download_mot_dataset(val_tools_path, benchmark, max_retries=5, backoff_facto\n \n     retries = 0  # Initialize retry counter\n \n+    response = None\n     while retries <= max_retries:\n         try:\n             response = requests.head(url, allow_redirects=True)\n@@ -201,7 +202,7 @@ def download_mot_dataset(val_tools_path, benchmark, max_retries=5, backoff_facto\n                 return None\n \n         except (requests.HTTPError, requests.ConnectionError) as e:\n-            if response.status_code == 416:  # Handle """"Requested Range Not Satisfiable"""" error\n+            if response and response.status_code == 416:  # Handle """"Requested Range Not Satisfiable"""" error\n                 LOGGER.info(f""""{benchmark}.zip is already fully downloaded."""")\n                 return zip_dst\n             LOGGER.error(f'Error occurred while downloading {benchmark}.zip: {e}')\n",add,Added Type name for DFI ( # 571 )
a7637639e4c29da0c7ab8a14cd5e71d7408133f2,Fix download in case of data already exists,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, ev","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, ev","@@ -503,8 +503,10 @@ if __name__ == """"__main__"""":\n     \n     # download MOT benchmark\n     download_mot_eval_tools(opt.val_tools_path)\n-    zip_path = download_mot_dataset(opt.val_tools_path, opt.benchmark)\n-    unzip_mot_dataset(zip_path, opt.val_tools_path, opt.benchmark)\n+\n+    if not Path(opt.source).exists():\n+        zip_path = download_mot_dataset(opt.val_tools_path, opt.benchmark)\n+        unzip_mot_dataset(zip_path, opt.val_tools_path, opt.benchmark)\n \n     if opt.benchmark == 'MOT17':\n         cleanup_mot17(opt.source)\n",fix,Add note about data volume to enable_metrics_collection
72a647c6b5f9598c4d8c7e21fc729bf2c33322be,Fix num classes for YOLOX,tracking/detectors/yolox.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.ops import yolox_preprocess\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.ops import yolox_preprocess\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n}\n","@@ -75,7 +75,7 @@ class YoloXStrategy(YoloInterface):\n             # needed for bytetrack yolox people models\n             # update with your custom model needs\n             exp.num_classes = 1\n-        elif model.stem == model_type:\n+        elif model.stem.startswith(model_type):\n             exp.num_classes = 1\n \n         ckpt = torch.load(\n",add,Add note about data volume to enable_metrics_collection
a261f125a28b7d58dc85aec8134c24d243851cc4,Fix with default values,boxmot/trackers/boosttrack/boosttrack.py,"import numpy as np\nfrom copy import deepcopy\nfrom typing import Optional, List\nimport cv2\n\nfrom boxmot.trackers.boosttrack.assoc import (\n    associate,\n    iou_batch,\n    MhDist_similarity,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\nfrom boxmot.trackers.boosttrack.ecc import ECC\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    if score is None:\n        return np.array([x","import numpy as np\nfrom copy import deepcopy\nfrom typing import Optional, List\nimport cv2\n\nfrom boxmot.trackers.boosttrack.assoc import (\n    associate,\n    iou_batch,\n    MhDist_similarity,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\nfrom boxmot.trackers.boosttrack.ecc import ECC\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    if score is None:\n        return np.array([x","@@ -107,29 +107,29 @@ class BoostTrack:\n         device,\n         half: bool,\n \n-        max_age: int,\n-        min_hits: int,\n-        det_thresh: float,\n-        iou_threshold: float,\n-        use_ecc: bool,\n-        min_box_area: int,\n-        aspect_ratio_thresh: 1.6,\n+        max_age: int = 60,\n+        min_hits: int = 3,\n+        det_thresh: float = 0.6,\n+        iou_threshold: float = 0.3,\n+        use_ecc: bool = True,\n+        min_box_area: int = 10,\n+        aspect_ratio_thresh: bool = 1.6,\n \n         # BoostTrack parameters\n-        lambda_iou: float,\n-        lambda_mhd: float,\n-        lambda_shape: float,\n-        use_dlo_boost: bool,\n-        use_duo_boost: bool,\n-        dlo_boost_coef: float,\n-        s_sim_corr: bool,\n+        lambda_iou: float = 0.5,\n+        lambda_mhd: float = 0.25,\n+        lambda_shape: float = 0.25,\n+        use_dlo_boost: bool = True,\n+        use_duo_boost: bool = True,\n+        dlo_boost_coef: float = 0.65,\n+        s_sim_corr: bool = False,\n     \n         # BoostTrack++ parameters\n-        use_rich_s: bool,\n-        use_sb: bool,\n-        use_vt: bool,\n+        use_rich_s: bool = False,\n+        use_sb: bool = False,\n+        use_vt: bool = False,\n \n-        with_reid: bool = True,\n+        with_reid: bool = False,\n     ):\n         self.frame_count = 0\n         self.trackers: List[KalmanBoxTracker] = []\n",add,Added net . kano . joustsim . oscar . os
027217b0c1919cb539d28a4e632368aa7b218192,Fix with no detections,boxmot/trackers/boosttrack/boosttrack.py,"import numpy as np\nfrom copy import deepcopy\nfrom typing import Optional, List\nimport cv2\n\nfrom boxmot.trackers.boosttrack.assoc import (\n    associate,\n    iou_batch,\n    MhDist_similarity,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\nfrom boxmot.trackers.boosttrack.ecc import ECC\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    if score is None:\n        return np.array([x","import numpy as np\nfrom copy import deepcopy\nfrom typing import Optional, List\nimport cv2\n\nfrom boxmot.trackers.boosttrack.assoc import (\n    associate,\n    iou_batch,\n    MhDist_similarity,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\nfrom boxmot.trackers.boosttrack.ecc import ECC\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    if score is None:\n        return np.array([x","@@ -208,9 +208,12 @@ class BoostTrack:\n         if self.use_duo_boost:\n             dets = self.duo_confidence_boost(dets)\n \n-        valid_inds = dets[:, 4] >= self.det_thresh\n-        dets = dets[valid_inds]\n-        scores = dets[:, 4]\n+        if dets.size > 0:\n+            valid_inds = dets[:, 4] >= self.det_thresh\n+            dets = dets[valid_inds]\n+            scores = dets[:, 4]\n+        else:\n+            scores = np.empty(0)\n \n         if self.with_reid and dets.shape[0] > 0:\n             dets_embs = self.reid_model.get_features(dets[:, :4], img)\n@@ -239,9 +242,12 @@ class BoostTrack:\n             s_sim_corr=self.s_sim_corr\n         )\n \n-        trust = (dets[:, 4] - self.det_thresh) / (1 - self.det_thresh)\n-        af = 0.95\n-        dets_alpha = af + (1 - af) * (1 - trust)\n+        if dets.size > 0:   \n+            trust = (dets[:, 4] - self.det_thresh) / (1 - self.det_thresh)\n+            af = 0.95\n+            dets_alpha = af + (1 - af) * (1 - trust)\n+        else:\n+            dets_alpha = np.empty(0)\n \n         for m in matched:\n             self.trackers[m[1]].update(dets[m[0], :], scores[m[0]])\n@@ -304,6 +310,9 @@ class BoostTrack:\n                 sigma_inv.reshape((1, -1, n_dims))).sum(axis=2)\n \n     def duo_confidence_boost(self, detections: np.ndarray) -> np.ndarray:\n+        if len(detections) == 0:\n+            return detections\n+\n         n_dims = 4\n         limit = 13.2767\n         mh_dist = self.get_mh_dist_matrix(detections, n_dims)\n@@ -330,6 +339,9 @@ class BoostTrack:\n         return detections\n \n     def dlo_confidence_boost(self, detections: np.ndarray) -> np.ndarray:\n+        if len(detections) == 0:\n+            return detections\n+        \n         sbiou_matrix = self.get_iou_matrix(detections, True)\n         if sbiou_matrix.size == 0:\n             return detections\n",add,Added STORM - 1270 to Changelog
33efc695c088b4b52e96483e0d51f3e52cfff0ea,Fixed box for boosttrack test,tests/unit/test_trackers.py,"import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSort, BotSort, DeepOcSort, OcSort, ByteTrack, ImprAssocTrack, get_tracker_config, create_tracker,\n)\n\nfrom boxmot.trackers.ocsort.ocsort import KalmanBoxTracker as OCSortKalmanBoxTracker\nfrom boxmot.trackers.deepocsort.deepocsort import KalmanBoxTracker as DeepOCSortKalmanBoxTracker\nfrom tests.test_config import MOTION_ONLY_TRACKING_METHODS, MOTION_N_APPEARANCE_TRACKING_METHODS, ALL_TRACKERS, PER_CLASS_TRACKERS\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        reid_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        half=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tra","import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom boxmot.utils import WEIGHTS\n\n\nfrom numpy.testing import assert_allclose\nfrom boxmot import (\n    StrongSort, BotSort, DeepOcSort, OcSort, ByteTrack, ImprAssocTrack, get_tracker_config, create_tracker,\n)\n\nfrom boxmot.trackers.ocsort.ocsort import KalmanBoxTracker as OCSortKalmanBoxTracker\nfrom boxmot.trackers.deepocsort.deepocsort import KalmanBoxTracker as DeepOCSortKalmanBoxTracker\nfrom tests.test_config import MOTION_ONLY_TRACKING_METHODS, MOTION_N_APPEARANCE_TRACKING_METHODS, ALL_TRACKERS, PER_CLASS_TRACKERS\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        reid_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        half=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instantiation(Tracker):\n    Tra","@@ -41,7 +41,7 @@ def test_tracker_output_size(tracker_type):\n     )\n \n     rgb = np.random.randint(255, size=(640, 640, 3), dtype=np.uint8)\n-    det = np.array([[144, 212, 578, 480, 0.82, 0],\n+    det = np.array([[144, 212, 400, 480, 0.82, 0],\n                     [425, 281, 576, 472, 0.72, 65]])\n \n     output = tracker.update(det, rgb)\n",add,Added STORM - 1564 to Changelog
79df3f754a1d04ec080d9190612bea46806b0e58,Fix embedding normalization,boxmot/appearance/backends/base_backend.py,"import cv2\nimport torch\nimport gdown\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.reid_model_factory import (\n    get_model_name,\n    get_model_url,\n    build_model,\n    get_nr_classes,\n    show_downloadable_models\n)\nfrom boxmot.utils.checks import RequirementsChecker\n\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights[0] if isinstance(weights, list) else weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n\n        self.download_model(self.weights)\n        self.model_name = get_model_name(self.weights)\n\n        self.model = build_model(\n            self.model_name,\n            num_classes=get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n   ","import cv2\nimport torch\nimport gdown\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.reid_model_factory import (\n    get_model_name,\n    get_model_url,\n    build_model,\n    get_nr_classes,\n    show_downloadable_models\n)\nfrom boxmot.utils.checks import RequirementsChecker\n\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights[0] if isinstance(weights, list) else weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n\n        self.download_model(self.weights)\n        self.model_name = get_model_name(self.weights)\n\n        self.model = build_model(\n            self.model_name,\n            num_classes=get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n   ","@@ -78,7 +78,7 @@ class BaseModelBackend:\n             features = self.inference_postprocess(features)\n         else:\n             features = np.array([])\n-        features = features / np.linalg.norm(features)\n+        features = features / np.linalg.norm(features, axis=-1, keepdims=True)\n         return features\n \n     def warmup(self, imgsz=[(256, 128, 3)]):\n",fix,Added Type name for wobly yum
d29eb245869f466deb19f9f748b56d9423d1ed23,Fix of invalid escape sequence,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, ev","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, ev","@@ -300,7 +300,7 @@ def parse_mot_results(results: str) -> dict:\n         dict: A dictionary containing HOTA, MOTA, and IDF1 scores.\n     """"""""""""\n     combined_results = results.split('COMBINED')[2:-1]\n-    combined_results = [float(re.findall(""""[-+]?(?:\d*\.*\d+)"""", f)[0])\n+    combined_results = [float(re.findall(r""""[-+]?(?:\d*\.*\d+)"""", f)[0])\n                         for f in combined_results]\n \n     results_dict = {}\n@@ -362,7 +362,7 @@ def run_generate_dets_embs(opt: argparse.Namespace) -> None:\n     Args:\n         opt (Namespace): Parsed command line arguments.\n     """"""""""""\n-    mot_folder_paths = [item for item in Path(opt.source).iterdir()]\n+    mot_folder_paths = sorted([item for item in Path(opt.source).iterdir()])\n     for y in opt.yolo_model:\n         for i, mot_folder_path in enumerate(mot_folder_paths):\n             dets_path = Path(opt.project) / 'dets_n_embs' / y.stem / 'dets' / (mot_folder_path.name + '.txt')\n@@ -391,12 +391,12 @@ def run_generate_mot_results(opt: argparse.Namespace, evolve_config: dict = None\n         opt.exp_folder_path = exp_folder_path\n \n         mot_folder_names = [item.stem for item in Path(opt.source).iterdir()]\n-        dets_file_paths = [item for item in (opt.project / """"dets_n_embs"""" / y.stem / 'dets').glob('*.txt')\n+        dets_file_paths = sorted([item for item in (opt.project / """"dets_n_embs"""" / y.stem / 'dets').glob('*.txt')\n                            if not item.name.startswith('.')\n-                           and item.stem in mot_folder_names]\n-        embs_file_paths = [item for item in (opt.project / """"dets_n_embs"""" / y.stem / 'embs' / opt.reid_model[0].stem).glob('*.txt')\n+                           and item.stem in mot_folder_names])\n+        embs_file_paths = sorted([item for item in (opt.project / """"dets_n_embs"""" / y.stem / 'embs' / opt.reid_model[0].stem).glob('*.txt')\n                            if not item.name.startswith('.')\n-                           and item.stem in mot_fo",add,Add warning about data volume to enable_metrics_collection
5db90516154a23c9e01af9500887bfe5b5fc4bfa,Fix of embedding extraction rounding and clipping,boxmot/appearance/backends/base_backend.py,"import cv2\nimport torch\nimport gdown\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.reid_model_factory import (\n    get_model_name,\n    get_model_url,\n    build_model,\n    get_nr_classes,\n    show_downloadable_models\n)\nfrom boxmot.utils.checks import RequirementsChecker\n\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights[0] if isinstance(weights, list) else weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n\n        self.download_model(self.weights)\n        self.model_name = get_model_name(self.weights)\n\n        self.model = build_model(\n            self.model_name,\n            num_classes=get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n   ","import cv2\nimport torch\nimport gdown\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.reid_model_factory import (\n    get_model_name,\n    get_model_url,\n    build_model,\n    get_nr_classes,\n    show_downloadable_models\n)\nfrom boxmot.utils.checks import RequirementsChecker\n\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights[0] if isinstance(weights, list) else weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n\n        self.download_model(self.weights)\n        self.model_name = get_model_name(self.weights)\n\n        self.model = build_model(\n            self.model_name,\n            num_classes=get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n   ","@@ -48,8 +48,8 @@ class BaseModelBackend:\n                             dtype=torch.half if self.half else torch.float, device=self.device)\n         \n         for i, box in enumerate(xyxys):\n-            x1, y1, x2, y2 = box.astype('int')\n-            x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w - 1, x2), min(h - 1, y2)\n+            x1, y1, x2, y2 = box.round().astype('int')\n+            x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w, x2), min(h, y2)\n             crop = img[y1:y2, x1:x2]\n             \n             # Resize and convert color in one step\n",fix,Added STORM - 860 to Changelog
20ffe4f99158931ce981449bbff23127a86e2d8e,Fixes of bugs with no dets or embs,boxmot/trackers/boosttrack/boosttrack.py,"import numpy as np\nfrom copy import deepcopy\nfrom typing import Optional, List\nimport cv2\n\nfrom boxmot.trackers.boosttrack.assoc import (\n    associate,\n    iou_batch,\n    MhDist_similarity,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\nfrom boxmot.trackers.boosttrack.ecc import ECC\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    if score is None:\n        return np.array([x","import numpy as np\nfrom copy import deepcopy\nfrom typing import Optional, List\nimport cv2\n\nfrom boxmot.trackers.boosttrack.assoc import (\n    associate,\n    iou_batch,\n    MhDist_similarity,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\nfrom boxmot.trackers.boosttrack.ecc import ECC\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    if score is None:\n        return np.array([x","@@ -208,6 +208,7 @@ class BoostTrack:\n         if self.use_duo_boost:\n             dets = self.duo_confidence_boost(dets)\n \n+        dets_embs = np.ones((dets.shape[0], 1))\n         if dets.size > 0:\n             remain_inds = dets[:, 4] >= self.det_thresh\n             dets = dets[remain_inds]\n@@ -220,10 +221,8 @@ class BoostTrack:\n                     dets_embs = self.reid_model.get_features(dets[:, :4], img)\n         else:\n             scores = np.empty(0)\n-            dets_embs = np.ones((dets.shape[0], 1))\n-            \n \n-        if self.with_reid and len(self.trackers) > 0:\n+        if self.with_reid and len(self.trackers) > 0 and len(dets_embs) > 0:\n             tracker_embs = np.array([trk.get_emb() for trk in self.trackers])\n             emb_cost = dets_embs.reshape(dets_embs.shape[0], -1) @ tracker_embs.reshape((tracker_embs.shape[0], -1)).T\n         else:\n",add,Added STORM - 146 to Changelog
531343d6fdf797fef805993c41c7be48419e217d,Changed --conf default value,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, ev","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, ev","@@ -449,7 +449,7 @@ def parse_opt() -> argparse.Namespace:\n     parser.add_argument('--reid-model', nargs='+', type=Path, default=[WEIGHTS / 'osnet_x0_25_msmt17.pt'], help='reid model path')\n     parser.add_argument('--source', type=str, help='file/dir/URL/glob, 0 for webcam')\n     parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=None, help='inference size h,w')\n-    parser.add_argument('--conf', type=float, default=0.5, help='confidence threshold')\n+    parser.add_argument('--conf', type=float, default=0.01, help='min confidence threshold')\n     parser.add_argument('--iou', type=float, default=0.7, help='intersection over union (IoU) threshold for NMS')\n     parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n     parser.add_argument('--classes', nargs='+', type=int, default=0, help='filter by class: --classes 0, or --classes 0 2 3')\n",add,Added STORM - 236 to Changelog
f61a8180a07fcf3350acaddcebc5ef2f22084ee2,Fix with visualization,boxmot/trackers/boosttrack/boosttrack.py,"import numpy as np\nfrom copy import deepcopy\nfrom typing import Optional, List\nimport cv2\n\nfrom boxmot.trackers.boosttrack.assoc import (\n    associate,\n    iou_batch,\n    MhDist_similarity,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\nfrom boxmot.trackers.boosttrack.ecc import ECC\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    if score is None:\n        return np.array([x","import numpy as np\nfrom typing import Optional, List\nfrom collections import deque\n\nfrom boxmot.trackers.boosttrack.assoc import (\n    associate,\n    iou_batch,\n    MhDist_similarity,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\nfrom boxmot.trackers.boosttrack.ecc import ECC\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    ","@@ -1,7 +1,6 @@\n import numpy as np\n-from copy import deepcopy\n from typing import Optional, List\n-import cv2\n+from collections import deque\n \n from boxmot.trackers.boosttrack.assoc import (\n     associate,\n@@ -10,9 +9,10 @@ from boxmot.trackers.boosttrack.assoc import (\n     shape_similarity,\n     soft_biou_batch,\n )\n+from boxmot.appearance.reid_auto_backend import ReidAutoBackend\n from boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\n from boxmot.trackers.boosttrack.ecc import ECC\n-from boxmot.appearance.reid_auto_backend import ReidAutoBackend\n+from boxmot.trackers.basetracker import BaseTracker\n \n \n def convert_bbox_to_z(bbox):\n@@ -48,18 +48,21 @@ class KalmanBoxTracker:\n     """"""""""""\n     count = 0\n \n-    def __init__(self, bbox, emb: Optional[np.ndarray] = None):\n+    def __init__(self, det, max_obs, emb: Optional[np.ndarray] = None):\n         self.bbox_to_z_func = convert_bbox_to_z\n         self.x_to_bbox_func = convert_x_to_bbox\n-\n-        self.time_since_update = 0\n-        self.id = KalmanBoxTracker.count\n         KalmanBoxTracker.count += 1\n \n-        self.kf = KalmanFilter(self.bbox_to_z_func(bbox))\n+        self.time_since_update = 0\n+        self.id = KalmanBoxTracker.count \n+        self.kf = KalmanFilter(self.bbox_to_z_func(det[:4]))\n+        self.conf = det[4]\n+        self.cls = det[5]\n+        self.det_ind = det[6]\n         self.emb = emb\n         self.hit_streak = 0\n         self.age = 0\n+        self.history_observations = deque([], maxlen=max_obs)\n \n     def get_confidence(self, coef: float = 0.9) -> float:\n         n = 7\n@@ -67,10 +70,14 @@ class KalmanBoxTracker:\n             return coef ** (n - self.age)\n         return coef ** (self.time_since_update - 1)\n \n-    def update(self, bbox: np.ndarray, score: float = 0):\n+    def update(self, det: np.ndarray, score: float = 0):\n         self.time_since_update = 0\n         self.hit_streak += 1\n-        self.kf.update(self.bbox_to_z_f",add,Added STORM - 1404 to the CHANGELOG .
338aef68d01b3ac76652b040d79ba669f90e01b8,Fix empty detections,boxmot/trackers/boosttrack/boosttrack.py,"import numpy as np\nfrom typing import Optional, List\nfrom collections import deque\n\nfrom boxmot.trackers.boosttrack.assoc import (\n    associate,\n    iou_batch,\n    MhDist_similarity,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\nfrom boxmot.trackers.boosttrack.ecc import ECC\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    ","import numpy as np\nfrom typing import Optional, List\nfrom collections import deque\n\nfrom boxmot.trackers.boosttrack.assoc import (\n    associate,\n    iou_batch,\n    MhDist_similarity,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\nfrom boxmot.trackers.boosttrack.ecc import ECC\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    ","@@ -189,11 +189,9 @@ class BoostTrack(BaseTracker):\n                       [x1, y1, x2, y2, id, confidence, cls, det_ind]\n                       (with cls and det_ind set to -1 if unused)\n         """"""""""""\n-        if dets is None:\n-            return np.empty((0, 5))\n-        if not isinstance(dets, np.ndarray):\n-            dets = dets.cpu().detach().numpy()\n-        \n+        if dets is None or dets.size == 0:\n+            dets = np.empty((0, 6))\n+\n         dets = np.hstack([dets, np.arange(len(dets)).reshape(-1, 1)])\n \n         self.frame_count += 1\n@@ -274,7 +272,7 @@ class BoostTrack(BaseTracker):\n         for trk in self.trackers:\n             d = trk.get_state()[0]\n             if (trk.time_since_update < 1) and (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits):\n-                # Format to match BotSort output: [x1, y1, x2, y2, id, confidence, cls, det_ind]\n+                # Format: [x1, y1, x2, y2, id, confidence, cls, det_ind]\n                 outputs.append(np.array([d[0], d[1], d[2], d[3], trk.id + 1, trk.conf, trk.cls, trk.det_ind]))\n                 self.active_tracks.append(trk)\n             \n",add,Added STORM - 146 to Changelog
e4016529f11910869866033bc991ae5f808cba61,fix no dets emb bug,boxmot/trackers/boosttrack/boosttrack.py,"import numpy as np\nfrom typing import Optional, List\nfrom collections import deque\n\nfrom boxmot.trackers.boosttrack.assoc import (\n    associate,\n    iou_batch,\n    MhDist_similarity,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\nfrom boxmot.trackers.boosttrack.ecc import ECC\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    ","import numpy as np\nfrom typing import Optional, List\nfrom collections import deque\n\nfrom boxmot.trackers.boosttrack.assoc import (\n    associate,\n    iou_batch,\n    MhDist_similarity,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.appearance.reid_auto_backend import ReidAutoBackend\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\nfrom boxmot.trackers.boosttrack.ecc import ECC\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    ","@@ -234,7 +234,10 @@ class BoostTrack(BaseTracker):\n \n         if self.with_reid and len(self.trackers) > 0:\n             tracker_embs = np.array([trk.get_emb() for trk in self.trackers])\n-            emb_cost = dets_embs.reshape(dets_embs.shape[0], -1) @ tracker_embs.reshape((tracker_embs.shape[0], -1)).T\n+            if dets_embs.shape[0] == 0:\n+                emb_cost = np.empty((0, tracker_embs.shape[0]))\n+            else:\n+                emb_cost = dets_embs.reshape(dets_embs.shape[0], -1) @ tracker_embs.reshape((tracker_embs.shape[0], -1)).T\n         else:\n             emb_cost = None\n \n",add,Added Type name for DFI ( # 3 )
46cb08542f1fafc37d9cd5d0e85a3c55aa4f5869,fix,tracking/detectors/yolox.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n    'yolox_x_ablation.pt': 'https://drive.google.c","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n    'yolox_x_ablation.pt': 'https://drive.google.c","@@ -161,7 +161,7 @@ class YoloXStrategy(YoloInterface):\n         im_preprocessed = []\n         self._preproc_data = []\n         for i, img in enumerate(im):\n-            img_pre, ratio = yolox_preprocess(img, input_size=self.imgsz)\n+            img_pre, ratio = self.yolox_preprocess(img, input_size=self.imgsz)\n             img_pre = torch.Tensor(img_pre).unsqueeze(0).to(self.device)\n \n             im_preprocessed.append(img_pre)\n",add,Added Type name for DFI ( # 57480 )
555fd011fee664517a6330e14d9c2dba1ce89342,fix,tracking/detectors/yolox.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n    'yolox_x_ablation.pt': 'https://drive.google.c","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nimport numpy as np\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n    'yolox_x_ablation.pt': 'ht","@@ -2,6 +2,7 @@\n \n import gdown\n import torch\n+import numpy as np\n from ultralytics.engine.results import Results\n from ultralytics.utils import ops\n from ultralytics.models.yolo.detect import DetectionPredictor\n",add,Add Rossen
00a88850df5609daba3cdbb36828c843552e103d,fix,tracking/detectors/yolox.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nimport numpy as np\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n    'yolox_x_ablation.pt': 'ht","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gdown\nimport torch\nimport numpy as np\nimport cv2\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n    'yolox_x_ablat","@@ -3,6 +3,7 @@\n import gdown\n import torch\n import numpy as np\n+import cv2\n from ultralytics.engine.results import Results\n from ultralytics.utils import ops\n from ultralytics.models.yolo.detect import DetectionPredictor\n",add,Add Rossen
a43055fb28b0654958cf7fa5b3e216b1f254331b,fix,tracking/detectors/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\nUL_MODELS = ['yolov8', 'yolov9', 'yolov10', 'yolo11', 'yolo12', 'rtdetr', 'sam']\n\n\ndef is_ultralytics_model(yolo_name):\n    return any(yolo in str(yolo_name) for yolo in UL_MODELS)\n\n\ndef is_yolox_model(yolo_name):\n    return 'yolox' in str(yolo_name)\n\n\ndef default_imgsz(yolo_name):\n    if is_ultralytics_model(yolo_name):\n        return [640, 640]\n    elif is_yolox_model(yolo_name):\n        return [800, 1440]\n    else:\n        return [640, 640]\n\n\ndef get_yolo_inferer(yolo_model):\n\n    if is_yolox_model(yolo_model):\n        try:\n            import yolox  # for linear_assignment\n            assert yolox.__version__\n        except (ImportError, AssertionError, AttributeError):\n            checker.check_packages(('yolox==0.3.0',), cmds='--no-deps')\n            checker.check_pack","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\nUL_MODELS = ['yolov8', 'yolov9', 'yolov10', 'yolo11', 'yolo12', 'rtdetr', 'sam']\n\n\ndef is_ultralytics_model(yolo_name):\n    return any(yolo in str(yolo_name) for yolo in UL_MODELS)\n\n\ndef is_yolox_model(yolo_name):\n    return 'yolox' in str(yolo_name)\n\n\ndef default_imgsz(yolo_name):\n    if is_ultralytics_model(yolo_name):\n        return [640, 640]\n    elif is_yolox_model(yolo_name):\n        return [800, 1440]\n    else:\n        return [640, 640]\n\n\ndef get_yolo_inferer(yolo_model):\n\n    if is_yolox_model(yolo_model):\n        try:\n            import yolox  # for linear_assignment\n            assert yolox.__version__\n        except (ImportError, AssertionError, AttributeError):\n            checker.check_packages(('yolox',), cmds='--no-deps')\n            checker.check_packages(('","@@ -32,7 +32,7 @@ def get_yolo_inferer(yolo_model):\n             import yolox  # for linear_assignment\n             assert yolox.__version__\n         except (ImportError, AssertionError, AttributeError):\n-            checker.check_packages(('yolox==0.3.0',), cmds='--no-deps')\n+            checker.check_packages(('yolox',), cmds='--no-deps')\n             checker.check_packages(('tabulate',))  # needed dependency\n             checker.check_packages(('thop',))  # needed dependency\n         from .yolox import YoloXStrategy\n",add,Added limits . h to headers to ijar
2a221e149031f97beff4b1dd693e2fcc27cbe8e7,fix,tracking/detectors/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\nUL_MODELS = ['yolov8', 'yolov9', 'yolov10', 'yolo11', 'yolo12', 'rtdetr', 'sam']\n\n\ndef is_ultralytics_model(yolo_name):\n    return any(yolo in str(yolo_name) for yolo in UL_MODELS)\n\n\ndef is_yolox_model(yolo_name):\n    return 'yolox' in str(yolo_name)\n\n\ndef default_imgsz(yolo_name):\n    if is_ultralytics_model(yolo_name):\n        return [640, 640]\n    elif is_yolox_model(yolo_name):\n        return [800, 1440]\n    else:\n        return [640, 640]\n\n\ndef get_yolo_inferer(yolo_model):\n\n    if is_yolox_model(yolo_model):\n        try:\n            import yolox  # for linear_assignment\n            assert yolox.__version__\n        except (ImportError, AssertionError, AttributeError):\n            checker.check_packages(('yolox',), cmds='--no-deps')\n            checker.check_packages(('","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\nUL_MODELS = ['yolov8', 'yolov9', 'yolov10', 'yolo11', 'yolo12', 'rtdetr', 'sam']\n\n\ndef is_ultralytics_model(yolo_name):\n    return any(yolo in str(yolo_name) for yolo in UL_MODELS)\n\n\ndef is_yolox_model(yolo_name):\n    return 'yolox' in str(yolo_name)\n\n\ndef default_imgsz(yolo_name):\n    if is_ultralytics_model(yolo_name):\n        return [640, 640]\n    elif is_yolox_model(yolo_name):\n        return [800, 1440]\n    else:\n        return [640, 640]\n\n\ndef get_yolo_inferer(yolo_model):\n\n    if is_yolox_model(yolo_model):\n        try:\n            import yolox  # for linear_assignment\n            assert yolox.__version__\n        except (ImportError, AssertionError, AttributeError):\n            checker.check_packages(('yolox',))\n            checker.check_packages(('tabulate',))  # ne","@@ -32,7 +32,7 @@ def get_yolo_inferer(yolo_model):\n             import yolox  # for linear_assignment\n             assert yolox.__version__\n         except (ImportError, AssertionError, AttributeError):\n-            checker.check_packages(('yolox',), cmds='--no-deps')\n+            checker.check_packages(('yolox',))\n             checker.check_packages(('tabulate',))  # needed dependency\n             checker.check_packages(('thop',))  # needed dependency\n         from .yolox import YoloXStrategy\n",add,Add warning about data volume to enable_metrics_collection
89d63b972ef0d449e0acff7cc524c05fa798e02b,info to debug,tracking/utils.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nimport time\nimport pandas as pd\nfrom ultralytics.utils import ops\nfrom ultralytics.engine.results import Results\nfrom typing import Union\nfrom pathlib import Path\nimport os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\nfrom boxmot.utils import EXAMPLES, ROOT\n\n\ndef split_dataset(src_fldr: Path, percent_to_delete: float = 0.5) -> None:\n    """"""""""""\n    Copies the dataset to a new location and removes a specified percentage of images and annotations,\n    adjusting the frame index to start at 1.\n\n    Args:\n        src_fldr (Path): Source folder containing the dataset.\n        percent_to_delete (float): Percentage of images and annotations to remove.\n    """"""""""""\n    # Ensure source path is a Path object\n    src_fldr = Path(src_fldr)\n\n    # Generate the destination","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nimport time\nimport pandas as pd\nfrom ultralytics.utils import ops\nfrom ultralytics.engine.results import Results\nfrom typing import Union\nfrom pathlib import Path\nimport os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\nfrom boxmot.utils import EXAMPLES, ROOT\n\n\ndef split_dataset(src_fldr: Path, percent_to_delete: float = 0.5) -> None:\n    """"""""""""\n    Copies the dataset to a new location and removes a specified percentage of images and annotations,\n    adjusting the frame index to start at 1.\n\n    Args:\n        src_fldr (Path): Source folder containing the dataset.\n        percent_to_delete (float): Percentage of images and annotations to remove.\n    """"""""""""\n    # Ensure source path is a Path object\n    src_fldr = Path(src_fldr)\n\n    # Generate the destination","@@ -113,9 +113,9 @@ def download_mot_eval_tools(val_tools_path):\n     try:\n         # Clone the repository\n         Repo.clone_from(val_tools_url, val_tools_path)\n-        LOGGER.info('Official MOT evaluation repo downloaded successfully.')\n+        LOGGER.debug('Official MOT evaluation repo downloaded successfully.')\n     except exc.GitError as err:\n-        LOGGER.info(f'Evaluation repo already downloaded or an error occurred: {err}')\n+        LOGGER.debug(f'Evaluation repo already downloaded or an error occurred: {err}')\n \n     # Fix deprecated np.float, np.int & np.bool by replacing them with native Python types\n     deprecated_types = {'np.float': 'float', 'np.int': 'int', 'np.bool': 'bool'}\n",add,Add note about data volume to enable_metrics_collection
a8d057e36bb7ff7c097525e8cfe1f96013119811,more dubug logging,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\nimport copy\nimport concurrent.futures\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path\nfrom boxmot.postprocessing.gsi import gsi\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, writ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\nimport copy\nimport concurrent.futures\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path\nfrom boxmot.postprocessing.gsi import gsi\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, writ","@@ -99,7 +99,7 @@ def prompt_overwrite(path_type: str, path: str, ci: bool = True) -> bool:\n         bool: True if user confirms to overwrite, False otherwise.\n     """"""""""""\n     if ci:\n-        print(f""""{path_type} {path} already exists. Use existing due to no UI mode."""")\n+        LOGGER.debug(f""""{path_type} {path} already exists. Use existing due to no UI mode."""")\n         return False\n \n     def input_with_timeout(prompt, timeout=3.0):\n@@ -267,7 +267,7 @@ def generate_mot_results(args: argparse.Namespace, config_dict: dict = None) ->\n     txt_path = args.exp_folder_path / (source.parent.name + '.txt')\n     all_mot_results = []\n \n-    for frame_idx, d in enumerate(tqdm(dataset, desc=source.parent.name)):\n+    for frame_idx, d in enumerate(tqdm(dataset, desc=source.parent.name, leave=False)):\n         if frame_idx == len(dataset):\n             break\n \n@@ -371,11 +371,11 @@ def run_generate_dets_embs(opt: argparse.Namespace) -> None:\n             embs_path = Path(opt.project) / 'dets_n_embs' / y.stem / 'embs' / (opt.reid_model[0].stem) / (mot_folder_path.name + '.txt')\n             if dets_path.exists() and embs_path.exists():\n                 if prompt_overwrite('Detections and Embeddings', dets_path, opt.ci):\n-                    LOGGER.info(f'Overwriting detections and embeddings for {mot_folder_path}...')\n+                    LOGGER.debug(f'Overwriting detections and embeddings for {mot_folder_path}...')\n                 else:\n-                    LOGGER.info(f'Skipping generation for {mot_folder_path} as they already exist.')\n+                    LOGGER.debug(f'Skipping generation for {mot_folder_path} as they already exist.')\n                     continue\n-            LOGGER.info(f'Generating detections and embeddings for data under {mot_folder_path} [{i + 1}/{len(mot_folder_paths)} seqs]')\n+            LOGGER.debug(f'Generating detections and embeddings for data under {mot_folder_path} [{i + 1}/{len(mot_folder_paths)} seqs]')\n     ",add,Add KHR_gl_texture_2D_image extension string .
dcea454dce132214596f7a4f22c12eec5718908c,fix performance drop,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport time\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport configparser\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\nimport copy\nimport concurrent.futures\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path\nfrom boxmot.postprocessing.gsi import gsi\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport time\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport configparser\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\nimport copy\nimport concurrent.futures\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path\nfrom boxmot.postprocessing.gsi import gsi\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils ","@@ -233,7 +233,7 @@ def generate_mot_results(args: argparse.Namespace, config_dict: dict = None) ->\n \n     seq_frame_nums = {source.parent.name: frame_nums.copy()}\n \n-    for frame_num, d in enumerate(tqdm(dataset, desc=source.parent.name), 1):\n+    for frame_num, d in enumerate(tqdm(dataset, desc=source.parent.name, leave=False), 1):\n         # Filter using list with needed numbers\n         if len(frame_nums) > 0:\n             if frame_num < frame_nums[0]:\n@@ -257,10 +257,6 @@ def generate_mot_results(args: argparse.Namespace, config_dict: dict = None) ->\n         if tracks.size > 0:\n             mot_results = convert_to_mot_format(tracks, frame_num)\n             all_mot_results.append(mot_results)\n-    \n-    end_time = time.time()\n-    total_frames = frame_idx + 1\n-    fps = total_frames / (end_time - start_time) if (end_time - start_time) > 0 else 0\n \n     if all_mot_results:\n         all_mot_results = np.vstack(all_mot_results)\n@@ -268,34 +264,43 @@ def generate_mot_results(args: argparse.Namespace, config_dict: dict = None) ->\n         all_mot_results = np.empty((0, 0))\n         \n     write_mot_results(txt_path, all_mot_results)\n-    return fps\n+\n \n def generate_fps_results(args: argparse.Namespace, config_dict: dict = None) -> float:\n     """"""""""""\n-    Computes FPS by processing frames without writing full MOT results.\n+    Computes FPS by processing frames and timing only the tracker.update() calls.\n     \n     Args:\n         args (argparse.Namespace): Command-line arguments.\n         config_dict (dict, optional): Additional configuration.\n     \n     Returns:\n-        float: Computed FPS.\n+        float: Computed FPS based solely on the tracker.update() process.\n     """"""""""""\n     tracker, source, dets_n_embs, dataset = _setup_tracker_and_data(args, config_dict)\n     \n-    start_time = time.time()\n+    total_update_time = 0.0\n+    total_frames = 0\n+    \n     for frame_idx, data in enumerate(tqdm(dataset, desc=source.pa",add,Don ' t need the application title to the library
94e60779bc600998da7ba227ef1edb2ecfe84126,fix,.github/workflows/ci.yml,"# name of the workflow, what it is doing (optional)\nname: BoxMOT CI\n\n# events that trigger the workflow (required)\non:\n  push:\n    # pushes to the following branches\n    branches:\n      - master\n  pull_request:\n    # pull request where master is target\n    branches:\n      - master\n\n\njobs:\n  tracking-methods:\n    runs-on: ${{ matrix.os }}\n    outputs:\n      status: ${{ job.status }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, macos-14]   # skip windows-latest for\n        python-version: ['3.12']\n        # leads to too many workflow which ends up queued\n        # tracking-method: [hybridsort, botsort, ocsort, bytetrack] \n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      - uses: actions/checkout@v4  # Check out the repository\n      - name: Set up Python\n        uses: actions/setup-python@v5  # Prepare environment with python 3.9\n        with:\n          python-version: ","# name of the workflow, what it is doing (optional)\nname: BoxMOT CI\n\n# events that trigger the workflow (required)\non:\n  push:\n    # pushes to the following branches\n    branches:\n      - master\n  pull_request:\n    # pull request where master is target\n    branches:\n      - master\n\n\njobs:\n  tracking-methods:\n    runs-on: ${{ matrix.os }}\n    outputs:\n      status: ${{ job.status }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, macos-14]   # skip windows-latest for\n        python-version: ['3.12']\n        # leads to too many workflow which ends up queued\n        # tracking-method: [hybridsort, botsort, ocsort, bytetrack] \n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      - uses: actions/checkout@v4  # Check out the repository\n      - name: Set up Python\n        uses: actions/setup-python@v5  # Prepare environment with python 3.9\n        with:\n          python-version: ","@@ -195,7 +195,7 @@ jobs:\n       - name: Set up Python\n         uses: actions/setup-python@v5\n         with:\n-          python-version: '3.12'\n+          python-version: '3.11'\n       - run: |\n           if [[ """"$OSTYPE"""" == """"darwin""""* ]]; then\n             # macOS\n@@ -260,7 +260,7 @@ jobs:\n       - name: Set up Python\n         uses: actions/setup-python@v5\n         with:\n-          python-version: '3.12'\n+          python-version: '3.11'\n       - name: Install dependencies\n         run: |\n           if [[ """"$OSTYPE"""" == """"darwin""""* ]]; then\n",add,Add note about data volume to enable
94e60779bc600998da7ba227ef1edb2ecfe84126,fix,tracking/utils.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nimport time\nimport pandas as pd\nfrom ultralytics.utils import ops\nfrom ultralytics.engine.results import Results\nfrom typing import Union\nfrom pathlib import Path\nimport json\nimport shutil\nimport os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\nfrom boxmot.utils import EXAMPLES, ROOT\n\n\ndef prompt_overwrite(path_type: str, path: str, ci: bool = True) -> bool:\n    """"""""""""\n    Prompts the user to confirm overwriting an existing file.\n\n    Args:\n        path_type (str): Type of the path (e.g., 'Detections and Embeddings', 'MOT Result').\n        path (str): The path to check.\n        ci (bool): If True, automatically reuse existing file without prompting (for CI environments).\n\n    Returns:\n        bool: True if user confirms to overwrite, False otherwise.\n    ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nimport time\nimport pandas as pd\nfrom ultralytics.utils import ops\nfrom ultralytics.engine.results import Results\nfrom typing import Union\nfrom pathlib import Path\nimport json\nimport shutil\nimport os\nimport sys\nimport git\nimport requests\nimport zipfile\nimport subprocess\nfrom git import Repo, exc\nfrom boxmot.utils import logger as LOGGER\nfrom tqdm import tqdm\nfrom boxmot.utils import EXAMPLES, ROOT\n\n\ndef split_dataset(src_fldr: Path, percent_to_delete: float = 0.5) -> None:\n    """"""""""""\n    Copies the dataset to a new location and removes a specified percentage of images and annotations,\n    adjusting the frame index to start at 1.\n\n    Args:\n        src_fldr (Path): Source folder containing the dataset.\n        percent_to_delete (float): Percentage of images and annotations to remove.\n    """"""""""""\n    # Ensure source path is a Path object\n    src_fldr = Path(src_fldr)\n\n  ","@@ -22,94 +22,6 @@ from tqdm import tqdm\n from boxmot.utils import EXAMPLES, ROOT\n \n \n-def prompt_overwrite(path_type: str, path: str, ci: bool = True) -> bool:\n-    """"""""""""\n-    Prompts the user to confirm overwriting an existing file.\n-\n-    Args:\n-        path_type (str): Type of the path (e.g., 'Detections and Embeddings', 'MOT Result').\n-        path (str): The path to check.\n-        ci (bool): If True, automatically reuse existing file without prompting (for CI environments).\n-\n-    Returns:\n-        bool: True if user confirms to overwrite, False otherwise.\n-    """"""""""""\n-    if ci:\n-        LOGGER.debug(f""""{path_type} {path} already exists. Use existing due to no UI mode."""")\n-        return False\n-\n-    def input_with_timeout(prompt, timeout=3.0):\n-        print(prompt, end='', flush=True)\n-\n-        result = []\n-        input_received = threading.Event()\n-\n-        def get_input():\n-            user_input = sys.stdin.readline().strip().lower()\n-            result.append(user_input)\n-            input_received.set()\n-\n-        input_thread = threading.Thread(target=get_input)\n-        input_thread.daemon = True  # Ensure thread does not prevent program exit\n-        input_thread.start()\n-        input_thread.join(timeout)\n-\n-        if input_received.is_set():\n-            return result[0] in ['y', 'yes']\n-        else:\n-            print(""""\nNo response, not proceeding with overwrite..."""")\n-            return False\n-\n-    return input_with_timeout(f""""{path_type} {path} already exists. Overwrite? [y/N]: """")\n-\n-\n-def cleanup_mot17(data_dir, keep_detection='FRCNN'):\n-    """"""""""""\n-    Cleans up the MOT17 dataset to resemble the MOT16 format by keeping only one detection folder per sequence.\n-    Skips sequences that have already been cleaned.\n-\n-    Args:\n-    - data_dir (str): Path to the MOT17 train directory.\n-    - keep_detection (str): Detection type to keep (options: 'DPM', 'FRCNN', 'SDP'). Default is 'DPM'",add,Add _binomial_double_trees by default
94e60779bc600998da7ba227ef1edb2ecfe84126,fix,tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport time\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport configparser\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\nimport copy\nimport concurrent.futures\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path\nfrom boxmot.postprocessing.gsi import gsi\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport configparser\nimport shutil\nimport json\nimport queue\nimport select\nimport re\nimport os\nimport torch\nfrom functools import partial\nimport threading\nimport sys\nimport copy\nimport concurrent.futures\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES, DATA\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path\nfrom boxmot.postprocessing.gsi import gsi\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.loaders import LoadImagesAndVideos\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import conver","@@ -1,6 +1,5 @@\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n-import time\n import argparse\n import subprocess\n from pathlib import Path\n@@ -33,13 +32,102 @@ from ultralytics.data.loaders import LoadImagesAndVideos\n \n from tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                 is_ultralytics_model, is_yolox_model)\n-from tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup, split_dataset, cleanup_mot17, prompt_overwrite\n+from tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup, split_dataset\n from boxmot.appearance.reid.auto_backend import ReidAutoBackend\n \n checker = RequirementsChecker()\n checker.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install\n \n \n+def cleanup_mot17(data_dir, keep_detection='FRCNN'):\n+    """"""""""""\n+    Cleans up the MOT17 dataset to resemble the MOT16 format by keeping only one detection folder per sequence.\n+    Skips sequences that have already been cleaned.\n+\n+    Args:\n+    - data_dir (str): Path to the MOT17 train directory.\n+    - keep_detection (str): Detection type to keep (options: 'DPM', 'FRCNN', 'SDP'). Default is 'DPM'.\n+    """"""""""""\n+\n+    # Get all folders in the train directory\n+    all_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n+\n+    # Identify unique sequences by removing detection suffixes\n+    unique_sequences = set(seq.split('-')[0] + '-' + seq.split('-')[1] for seq in all_dirs)\n+\n+    for seq in unique_sequences:\n+        # Directory path to the cleaned sequence\n+        cleaned_seq_dir = os.path.join(data_dir, seq)\n+\n+        # Skip if the sequence is already cleaned\n+        if os.path.exists(cleaned_seq_dir):\n+            print(f""""Sequence {seq} is already cleaned. Skipping.",fix,Don ' t define the right button twice .
b126e922bf5178e55bb600b4d1cd6df8a5fdde9b,fix,.github/workflows/ci.yml,"# name of the workflow, what it is doing (optional)\nname: BoxMOT CI\n\n# events that trigger the workflow (required)\non:\n  push:\n    # pushes to the following branches\n    branches:\n      - master\n  pull_request:\n    # pull request where master is target\n    branches:\n      - master\n\n\njobs:\n  tracking-methods:\n    runs-on: ${{ matrix.os }}\n    outputs:\n      status: ${{ job.status }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, macos-14]   # skip windows-latest for\n        python-version: ['3.12']\n        # leads to too many workflow which ends up queued\n        # tracking-method: [hybridsort, botsort, ocsort, bytetrack] \n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      - uses: actions/checkout@v4  # Check out the repository\n      - name: Set up Python\n        uses: actions/setup-python@v5  # Prepare environment with python 3.9\n        with:\n          python-version: ","# name of the workflow, what it is doing (optional)\nname: BoxMOT CI\n\n# events that trigger the workflow (required)\non:\n  push:\n    # pushes to the following branches\n    branches:\n      - master\n  pull_request:\n    # pull request where master is target\n    branches:\n      - master\n\n\njobs:\n  tracking-methods:\n    runs-on: ${{ matrix.os }}\n    outputs:\n      status: ${{ job.status }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, macos-14]   # skip windows-latest for\n        python-version: ['3.12']\n        # leads to too many workflow which ends up queued\n        # tracking-method: [hybridsort, botsort, ocsort, bytetrack] \n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      - uses: actions/checkout@v4  # Check out the repository\n      - name: Set up Python\n        uses: actions/setup-python@v5  # Prepare environment with python 3.9\n        with:\n          python-version: ","@@ -271,7 +271,7 @@ jobs:\n             sed -i 's/source=""""torch_cuda121""""/source=""""torchcpu""""/g' pyproject.toml\n           fi\n           python -m pip install --upgrade pip setuptools wheel uv\n-          uv sync --group export\n+          uv sync\n           # needed for TFLite export\n           sudo apt-get install flatbuffers-compiler\n           wget https://github.com/PINTO0309/onnx2tf/releases/download/1.16.31/flatc.tar.gz\n@@ -314,7 +314,7 @@ jobs:\n             sed -i 's/source=""""torch_cuda121""""/source=""""torchcpu""""/g' pyproject.toml\n           fi\n           python -m pip install --upgrade pip setuptools wheel uv\n-          uv sync --group test --group export\n+          uv sync --group test\n       - name: Test export models\n         run: |\n           source .venv/bin/activate\n",add,Add note about data volume to enable
b126e922bf5178e55bb600b4d1cd6df8a5fdde9b,fix,boxmot/appearance/exporters/tflite_exporter.py,"import os\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass TFLiteExporter(BaseExporter):\n    required_packages = (\n        """"onnx2tf>=1.18.0"""",\n        """"onnx>=1.16.1"""", \n        """"tensorflow==2.17.0"""",\n        """"tf_keras"""",  # required by 'onnx2tf' package\n        """"sng4onnx>=1.0.1"""",  # required by 'onnx2tf' package\n        """"onnx_graphsurgeon>=0.3.26"""",  # required by 'onnx2tf' package\n        """"onnxslim>=0.1.31"""",\n        """"onnxruntime"""",\n        """"flatbuffers>=23.5.26"""",\n        """"psutil==5.9.5"""",\n        """"ml_dtypes==0.3.2"""",\n        """"ai_edge_litert>=1.2.0""""\n    )\n    cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n    \n    def export(self):\n\n        import onnx2tf\n        input_onnx_file_path = str(self.file.with_suffix('.onnx'))\n        output_folder_path = input_onnx_file_path.replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n        onnx2tf.convert(\n            input_onnx_file_","import os\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass TFLiteExporter(BaseExporter):\n    required_packages = (\n        """"onnx2tf>=1.18.0"""",\n        """"onnx>=1.16.1"""", \n        """"tensorflow==2.17.0"""",\n        """"tf_keras"""",  # required by 'onnx2tf' package\n        """"sng4onnx>=1.0.1"""",  # required by 'onnx2tf' package\n        """"onnx_graphsurgeon>=0.3.26"""",  # required by 'onnx2tf' package\n        """"onnxslim>=0.1.31"""",\n        """"onnxruntime"""",\n        """"flatbuffers>=23.5.26"""",\n        """"onnxsim==0.4.33"""",\n        """"psutil==5.9.5"""",\n        """"ml_dtypes==0.3.2"""",\n        """"ai_edge_litert>=1.2.0""""\n    )\n    cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n    \n    def export(self):\n\n        import onnx2tf\n        input_onnx_file_path = str(self.file.with_suffix('.onnx'))\n        output_folder_path = input_onnx_file_path.replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n        onnx2tf.convert(","@@ -14,6 +14,7 @@ class TFLiteExporter(BaseExporter):\n         """"onnxslim>=0.1.31"""",\n         """"onnxruntime"""",\n         """"flatbuffers>=23.5.26"""",\n+        """"onnxsim==0.4.33"""",\n         """"psutil==5.9.5"""",\n         """"ml_dtypes==0.3.2"""",\n         """"ai_edge_litert>=1.2.0""""\n",add,Add note about data volume to enable_metrics_collection
b126e922bf5178e55bb600b4d1cd6df8a5fdde9b,fix,pyproject.toml,"[[tool.uv.index]]\nname = """"testpypi""""\nurl = """"https://test.pypi.org/simple/""""\npublish-url = """"https://test.pypi.org/legacy/""""\nexplicit = true\n\n[[tool.pdm.source]]\nname = """"torchcpu""""\nurl = """"https://download.pytorch.org/whl/cpu""""\nverify_ssl = true\n\n[[tool.pdm.source]]\nname = """"torch_cuda121""""\nurl = """"https://download.pytorch.org/whl/cu121""""\nverify_ssl = true\n\n[build-system]\nrequires = [""""pdm-backend""""]\nbuild-backend = """"pdm.backend""""\n\n[[tool.uv.dependency-metadata]]\nname = """"yolox""""\nversion = """"0.3.0""""\nrequires-dist = [""""onnx>=1.17.0"""", """"onnxsim<1.0.0,>=0.4.36""""]\n\n[tool.uv]\nno-build-isolation-package = [""""yolox""""]\n\n[tool.flake8]\nmax-line-length = 120\nexclude = ["""".tox"""", """"*.egg"""", """"build"""", """"temp""""]\nselect = [""""E"""", """"W"""", """"F""""]\ndoctests = true\nverbose = 2\nformat = """"pylint""""\nignore = [""""E731"""", """"F405"""", """"E402"""", """"W504"""", """"W605"""", """"E741""""]\n[project]\nauthors = [\n    {name = """"Mikel BrostrÃ¶m""""},\n]\nlicense = {text = """"AGPL-3.0""""}\nrequires","[[tool.uv.index]]\nname = """"testpypi""""\nurl = """"https://test.pypi.org/simple/""""\npublish-url = """"https://test.pypi.org/legacy/""""\nexplicit = true\n\n[[tool.pdm.source]]\nname = """"torchcpu""""\nurl = """"https://download.pytorch.org/whl/cpu""""\nverify_ssl = true\n\n[[tool.pdm.source]]\nname = """"torch_cuda121""""\nurl = """"https://download.pytorch.org/whl/cu121""""\nverify_ssl = true\n\n[build-system]\nrequires = [""""pdm-backend""""]\nbuild-backend = """"pdm.backend""""\n\n[[tool.uv.dependency-metadata]]\nname = """"yolox""""\nversion = """"0.3.0""""\nrequires-dist = [""""onnx>=1.17.0"""", """"onnxsim<1.0.0,>=0.4.36""""]\n\n[tool.uv]\nno-build-isolation-package = [""""yolox""""]\n\n[tool.flake8]\nmax-line-length = 120\nexclude = ["""".tox"""", """"*.egg"""", """"build"""", """"temp""""]\nselect = [""""E"""", """"W"""", """"F""""]\ndoctests = true\nverbose = 2\nformat = """"pylint""""\nignore = [""""E731"""", """"F405"""", """"E402"""", """"W504"""", """"W605"""", """"E741""""]\n[project]\nauthors = [\n    {name = """"Mikel BrostrÃ¶m""""},\n]\nlicense = {text = """"AGPL-3.0""""}\nrequires","@@ -102,11 +102,6 @@ test = [\n yolo = [\n     """"ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git"""",\n ]\n-export = [\n-    """"onnx<2.0.0,>=1.17.0"""",\n-    """"onnxsim==0.4.33"""",\n-    """"openvino-dev<2025.0,>=2023.3"""",\n-]\n evolve = [\n     """"ray<3.0.0,>=2.35.0"""",\n     """"plotly<6.0.0,>=5.19.0"""",\n",add,Added hypest who apparently actually wrote the C # port
ecbfd1b4d4621aeccc6d8262260d7f9c41f226e9,fix,boxmot/appearance/exporters/onnx_exporter.py,"import torch\nimport onnx\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass ONNXExporter(BaseExporter):\n    required_packages = (""""onnx>=1.16.1"""",)\n    \n    def export(self):\n\n        f = self.file.with_suffix("""".onnx"""")\n\n        dynamic = {""""images"""": {0: """"batch""""}, """"output"""": {0: """"batch""""}} if self.dynamic else None\n\n        torch.onnx.export(\n            self.model.cpu() if self.dynamic else self.model,\n            self.im.cpu() if self.dynamic else self.im,\n            f,\n            verbose=False,\n            opset_version=12,\n            do_constant_folding=True,\n            input_names=[""""images""""],\n            output_names=[""""output""""],\n            dynamic_axes=dynamic,\n        )\n\n        model_onnx = onnx.load(f)\n        onnx.checker.check_model(model_onnx)\n        onnx.save(model_onnx, f)\n\n        if self.simplify:\n            self.simplify_model(model_onnx, f)\n            \n","import torch\n\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass ONNXExporter(BaseExporter):\n    required_packages = (""""onnx>=1.16.1"""",)\n    \n    def export(self):\n        import onnx\n\n        f = self.file.with_suffix("""".onnx"""")\n\n        dynamic = {""""images"""": {0: """"batch""""}, """"output"""": {0: """"batch""""}} if self.dynamic else None\n\n        torch.onnx.export(\n            self.model.cpu() if self.dynamic else self.model,\n            self.im.cpu() if self.dynamic else self.im,\n            f,\n            verbose=False,\n            opset_version=12,\n            do_constant_folding=True,\n            input_names=[""""images""""],\n            output_names=[""""output""""],\n            dynamic_axes=dynamic,\n        )\n\n        model_onnx = onnx.load(f)\n        onnx.checker.check_model(model_onnx)\n        onnx.save(model_onnx, f)\n\n        if self.simplify:\n            self.simplify_model(model_onnx, f)\n    ","@@ -1,5 +1,5 @@\n import torch\n-import onnx\n+\n from boxmot.appearance.exporters.base_exporter import BaseExporter\n from boxmot.utils import logger as LOGGER\n \n@@ -8,6 +8,7 @@ class ONNXExporter(BaseExporter):\n     required_packages = (""""onnx>=1.16.1"""",)\n     \n     def export(self):\n+        import onnx\n \n         f = self.file.with_suffix("""".onnx"""")\n \n@@ -36,6 +37,7 @@ class ONNXExporter(BaseExporter):\n \n \n     def simplify_model(self, model_onnx, f):\n+        import onnx\n         try:\n             cuda = torch.cuda.is_available()\n             self.checker.check_packages(\n",add,Add note about data volume to enable
cc1b4a33d40fe69ca952d64fa564db8c022bce30,fix,boxmot/appearance/exporters/openvino_exporter.py,"import os\nfrom pathlib import Path\nimport openvino.runtime as ov\nfrom openvino.tools import mo\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass OpenVINOExporter(BaseExporter):\n    required_packages = (""""openvino-dev>=2023.0"""",)\n    \n    def export(self):\n\n        f = str(self.file).replace(self.file.suffix, f""""_openvino_model{os.sep}"""")\n        f_onnx = self.file.with_suffix("""".onnx"""")\n        f_ov = str(Path(f) / self.file.with_suffix("""".xml"""").name)\n\n        ov_model = mo.convert_model(\n            f_onnx,\n            model_name=self.file.with_suffix("""".xml""""),\n            framework=""""onnx"""",\n            compress_to_fp16=self.half,\n        )\n        ov.serialize(ov_model, f_ov)\n        \n        return f","import os\nfrom pathlib import Path\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass OpenVINOExporter(BaseExporter):\n    required_packages = (""""openvino-dev>=2023.0"""",)\n    \n    def export(self):\n        \n        import openvino.runtime as ov\n        from openvino.tools import mo\n\n        f = str(self.file).replace(self.file.suffix, f""""_openvino_model{os.sep}"""")\n        f_onnx = self.file.with_suffix("""".onnx"""")\n        f_ov = str(Path(f) / self.file.with_suffix("""".xml"""").name)\n\n        ov_model = mo.convert_model(\n            f_onnx,\n            model_name=self.file.with_suffix("""".xml""""),\n            framework=""""onnx"""",\n            compress_to_fp16=self.half,\n        )\n        ov.serialize(ov_model, f_ov)\n        \n        return f","@@ -1,7 +1,5 @@\n import os\n from pathlib import Path\n-import openvino.runtime as ov\n-from openvino.tools import mo\n from boxmot.appearance.exporters.base_exporter import BaseExporter\n from boxmot.utils import logger as LOGGER\n \n@@ -10,6 +8,9 @@ class OpenVINOExporter(BaseExporter):\n     required_packages = (""""openvino-dev>=2023.0"""",)\n     \n     def export(self):\n+        \n+        import openvino.runtime as ov\n+        from openvino.tools import mo\n \n         f = str(self.file).replace(self.file.suffix, f""""_openvino_model{os.sep}"""")\n         f_onnx = self.file.with_suffix("""".onnx"""")\n",add,Add note about data volume to enable_metrics_collection
bd791f0def7efc6499de1615c55e3c22c860da97,fix,boxmot/appearance/exporters/openvino_exporter.py,"import os\nfrom pathlib import Path\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass OpenVINOExporter(BaseExporter):\n    required_packages = (""""openvino-dev>=2023.0"""",)\n    \n    def export(self):\n        \n        import openvino.runtime as ov\n        from openvino.tools import mo\n\n        f = str(self.file).replace(self.file.suffix, f""""_openvino_model{os.sep}"""")\n        f_onnx = self.file.with_suffix("""".onnx"""")\n        f_ov = str(Path(f) / self.file.with_suffix("""".xml"""").name)\n\n        ov_model = mo.convert_model(\n            f_onnx,\n            model_name=self.file.with_suffix("""".xml""""),\n            framework=""""onnx"""",\n            compress_to_fp16=self.half,\n        )\n        ov.serialize(ov_model, f_ov)\n        \n        return f","import os\nfrom pathlib import Path\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass OpenVINOExporter(BaseExporter):\n    required_packages = (""""openvino-dev>=2023.3"""",)\n    \n    def export(self):\n        \n        import openvino.runtime as ov\n        from openvino.tools import mo\n\n        f = str(self.file).replace(self.file.suffix, f""""_openvino_model{os.sep}"""")\n        f_onnx = self.file.with_suffix("""".onnx"""")\n        f_ov = str(Path(f) / self.file.with_suffix("""".xml"""").name)\n\n        ov_model = mo.convert_model(\n            f_onnx,\n            model_name=self.file.with_suffix("""".xml""""),\n            framework=""""onnx"""",\n            compress_to_fp16=self.half,\n        )\n        ov.serialize(ov_model, f_ov)\n        \n        return f","@@ -5,7 +5,7 @@ from boxmot.utils import logger as LOGGER\n \n \n class OpenVINOExporter(BaseExporter):\n-    required_packages = (""""openvino-dev>=2023.0"""",)\n+    required_packages = (""""openvino-dev>=2023.3"""",)\n     \n     def export(self):\n         \n",add,Add forced default forptr Tablet
a417accf676e9926e0fe58763df6776593c48d71,Fix input shape for LMBN arch,boxmot/appearance/backbones/lmbn/lmbn_n.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport copy\n\nimport torch\nfrom torch import nn\n\nfrom boxmot.appearance.backbones.lmbn.attention import BatchFeatureErase_Top\nfrom boxmot.appearance.backbones.lmbn.bnneck import BNNeck, BNNeck3\nfrom boxmot.appearance.backbones.osnet import OSBlock, osnet_x1_0\n\n\nclass LMBN_n(nn.Module):\n    def __init__(self, num_classes, loss, pretrained, use_gpu):\n        super(LMBN_n, self).__init__()\n\n        self.n_ch = 2\n        self.chs = 512 // self.n_ch\n        self.training = False\n\n        osnet = osnet_x1_0(pretrained=True)\n\n        self.backone = nn.Sequential(\n            osnet.conv1, osnet.maxpool, osnet.conv2, osnet.conv3[0]\n        )\n\n        conv3 = osnet.conv3[1:]\n\n        self.global_branch = nn.Sequential(\n            copy.deepcopy(conv3), copy.deepcopy(osnet.conv4), copy.deepcopy(osnet.conv5)\n        )\n\n        self.partial_branch = nn.Sequential(\n            copy.deepcopy(conv3), copy.deepcopy(osn","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport copy\n\nimport torch\nfrom torch import nn\n\nfrom boxmot.appearance.backbones.lmbn.attention import BatchFeatureErase_Top\nfrom boxmot.appearance.backbones.lmbn.bnneck import BNNeck, BNNeck3\nfrom boxmot.appearance.backbones.osnet import OSBlock, osnet_x1_0\n\n\nclass LMBN_n(nn.Module):\n    def __init__(self, num_classes, loss, pretrained, use_gpu):\n        super(LMBN_n, self).__init__()\n\n        self.n_ch = 2\n        self.chs = 512 // self.n_ch\n        self.training = False\n\n        osnet = osnet_x1_0(pretrained=False)\n\n        self.backone = nn.Sequential(\n            osnet.conv1, osnet.maxpool, osnet.conv2, osnet.conv3[0]\n        )\n\n        conv3 = osnet.conv3[1:]\n\n        self.global_branch = nn.Sequential(\n            copy.deepcopy(conv3), copy.deepcopy(osnet.conv4), copy.deepcopy(osnet.conv5)\n        )\n\n        self.partial_branch = nn.Sequential(\n            copy.deepcopy(conv3), copy.deepcopy(os","@@ -18,7 +18,7 @@ class LMBN_n(nn.Module):\n         self.chs = 512 // self.n_ch\n         self.training = False\n \n-        osnet = osnet_x1_0(pretrained=True)\n+        osnet = osnet_x1_0(pretrained=False)\n \n         self.backone = nn.Sequential(\n             osnet.conv1, osnet.maxpool, osnet.conv2, osnet.conv3[0]\n",add,Add note about data volume to enable_metrics_collection
a417accf676e9926e0fe58763df6776593c48d71,Fix input shape for LMBN arch,boxmot/appearance/backends/base_backend.py,"import cv2\nimport torch\nimport gdown\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.reid.registry import ReIDModelRegistry\nfrom boxmot.utils.checks import RequirementsChecker\n\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights[0] if isinstance(weights, list) else weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n\n        self.download_model(self.weights)\n        self.model_name = ReIDModelRegistry.get_model_name(self.weights)\n\n        self.model = ReIDModelRegistry.build_model(\n            self.model_name,\n            num_classes=ReIDModelRegistry.get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n        )\n        self.checker = RequirementsCheck","import cv2\nimport torch\nimport gdown\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.appearance.reid.registry import ReIDModelRegistry\nfrom boxmot.utils.checks import RequirementsChecker\n\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights[0] if isinstance(weights, list) else weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n\n        self.download_model(self.weights)\n        self.model_name = ReIDModelRegistry.get_model_name(self.weights)\n\n        self.model = ReIDModelRegistry.build_model(\n            self.model_name,\n            num_classes=ReIDModelRegistry.get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n        )\n        self.checker = RequirementsCheck","@@ -27,18 +27,18 @@ class BaseModelBackend:\n         )\n         self.checker = RequirementsChecker()\n         self.load_model(self.weights)\n+        self.input_shape = (384, 128) if """"lmbn"""" in self.model_name else (256, 128)\n \n \n     def get_crops(self, xyxys, img):\n         h, w = img.shape[:2]\n-        resize_dims = (128, 256)\n         interpolation_method = cv2.INTER_LINEAR\n         mean_array = torch.tensor([0.485, 0.456, 0.406], device=self.device).view(1, 3, 1, 1)\n         std_array = torch.tensor([0.229, 0.224, 0.225], device=self.device).view(1, 3, 1, 1)\n         \n         # Preallocate tensor for crops\n         num_crops = len(xyxys)\n-        crops = torch.empty((num_crops, 3, resize_dims[1], resize_dims[0]), \n+        crops = torch.empty((num_crops, 3, *self.input_shape), \n                             dtype=torch.half if self.half else torch.float, device=self.device)\n         \n         for i, box in enumerate(xyxys):\n@@ -47,7 +47,8 @@ class BaseModelBackend:\n             crop = img[y1:y2, x1:x2]\n             \n             # Resize and convert color in one step\n-            crop = cv2.resize(crop, resize_dims, interpolation=interpolation_method)\n+            crop = cv2.resize(crop, (self.input_shape[1], self.input_shape[0]), \n+                              interpolation=interpolation_method)\n             crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n             \n             # Convert to tensor and normalize (convert to [0, 1] by dividing by 255 in batch later)\n",add,Add note about data volume to enable EGL
668782907d9cebd8707a950427abaeb54dfff5e6,"wrong im used and code refactor (#1949)

* BUG - wrong im used

* rfdetr internally supports nparray so we dont need to_pil

* double append for pred is None

* device tag needs to be fixed everywhere.

* minor refactor

* per_class_decorator refactor

* too much redundancy

* bring the indents down",boxmot/trackers/basetracker.py,"import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.iou import AssociationFunction\n\n\nclass BaseTracker(ABC):\n    def __init__(\n        self, \n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        max_obs: int = 50,\n        nr_classes: int = 80,\n        per_class: bool = False,\n        asso_func: str = 'iou',\n        is_obb: bool = False\n    ):\n        """"""""""""\n        Initialize the BaseTracker object with detection threshold, maximum age, minimum hits, \n        and Intersection Over Union (IOU) threshold for tracking objects in video frames.\n\n        Parameters:\n        - det_thresh (float): Detection threshold for considering detections.\n        - max_age (int): Maximum age of a track before it is considered lost.\n        - min_hits (int): Minimum number of detection hits ","import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.iou import AssociationFunction\n\n\nclass BaseTracker(ABC):\n    def __init__(\n        self, \n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        max_obs: int = 50,\n        nr_classes: int = 80,\n        per_class: bool = False,\n        asso_func: str = 'iou',\n        is_obb: bool = False\n    ):\n        """"""""""""\n        Initialize the BaseTracker object with detection threshold, maximum age, minimum hits, \n        and Intersection Over Union (IOU) threshold for tracking objects in video frames.\n\n        Parameters:\n        - det_thresh (float): Detection threshold for considering detections.\n        - max_age (int): Maximum age of a track before it is considered lost.\n        - min_hits (int): Minimum number of detection hits ","@@ -84,19 +84,22 @@ class BaseTracker(ABC):\n         class_embs = np.empty((0, self.last_emb_size)) if self.last_emb_size is not None else None\n \n         # Check if there are detections\n-        if dets.size > 0:\n-            class_indices = np.where(dets[:, 5] == cls_id)[0]\n-            class_dets = dets[class_indices]\n-            \n-            if embs is not None:\n-                # Assert that if embeddings are provided, they have the same number of elements as detections\n-                assert dets.shape[0] == embs.shape[0], """"Detections and embeddings must have the same number of elements when both are provided""""\n-                \n-                if embs.size > 0:\n-                    class_embs = embs[class_indices]\n-                    self.last_emb_size = class_embs.shape[1]  # Update the last known embedding size\n-                else:\n-                    class_embs = None\n+        if dets.size == 0:\n+            return class_dets, class_embs\n+\n+        class_indices = np.where(dets[:, 5] == cls_id)[0]\n+        class_dets = dets[class_indices]\n+\n+        if embs is None:\n+            return class_dets, class_embs\n+\n+        # Assert that if embeddings are provided, they have the same number of elements as detections\n+        assert dets.shape[0] == embs.shape[0], (""""Detections and embeddings """"\n+                                                """"must have the same number of elements when both are provided"""")\n+        class_embs = None\n+        if embs.size > 0:\n+            class_embs = embs[class_indices]\n+            self.last_emb_size = class_embs.shape[1]  # Update the last known embedding size\n         return class_dets, class_embs\n     \n     @staticmethod\n@@ -139,48 +142,46 @@ class BaseTracker(ABC):\n         Decorator for the update method to handle per-class processing.\n         """"""""""""\n         def wrapper(self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None):\n-            \n-            #handl",fix,Add note about data volume to enable_metrics_collection
668782907d9cebd8707a950427abaeb54dfff5e6,"wrong im used and code refactor (#1949)

* BUG - wrong im used

* rfdetr internally supports nparray so we dont need to_pil

* double append for pred is None

* device tag needs to be fixed everywhere.

* minor refactor

* per_class_decorator refactor

* too much redundancy

* bring the indents down",boxmot/trackers/boosttrack/boosttrack.py,"import numpy as np\nfrom typing import Optional, List\nfrom collections import deque\n\nfrom boxmot.trackers.boosttrack.assoc import (\n    associate,\n    iou_batch,\n    MhDist_similarity,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\nfrom boxmot.trackers.boosttrack.ecc import ECC\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    ","import numpy as np\nfrom typing import Optional, List\nfrom collections import deque\n\nfrom boxmot.trackers.boosttrack.assoc import (\n    associate,\n    iou_batch,\n    MhDist_similarity,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\nfrom boxmot.trackers.boosttrack.ecc import ECC\nfrom boxmot.trackers.basetracker import BaseTracker\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    ","@@ -318,33 +318,35 @@ class BoostTrack(BaseTracker):\n     def duo_confidence_boost(self, detections: np.ndarray) -> np.ndarray:\n         if len(detections) == 0:\n             return detections\n-\n         n_dims = 4\n-\n         limit = 13.2767\n         mh_dist = self.get_mh_dist_matrix(detections, n_dims)\n-        if mh_dist.size > 0 and self.frame_count > 1:\n-            min_dists = mh_dist.min(1)\n-            mask = (min_dists > limit) & (detections[:, 4] < self.det_thresh)\n-            boost_inds = np.where(mask)[0]\n-            iou_limit = 0.3\n-            if len(boost_inds) > 0:\n-                bdiou = iou_batch(detections[boost_inds], detections[boost_inds]) - np.eye(len(boost_inds))\n-                bdiou_max = bdiou.max(axis=1)\n-                remaining = boost_inds[bdiou_max <= iou_limit]\n-                args = np.where(bdiou_max > iou_limit)[0]\n-                for i in range(len(args)):\n-                    bi = args[i]\n-                    tmp = np.where(bdiou[bi] > iou_limit)[0]\n-                    args_tmp = np.append(np.intersect1d(boost_inds[args], boost_inds[tmp]), boost_inds[bi])\n-                    conf_max = np.max(detections[args_tmp, 4])\n-                    if detections[boost_inds[bi], 4] == conf_max:\n-                        remaining = np.concatenate([remaining, [boost_inds[bi]]])\n-                mask_boost = np.zeros_like(detections[:, 4], dtype=bool)\n-                mask_boost[remaining] = True\n-                detections[:, 4] = np.where(mask_boost, self.det_thresh + 1e-4, detections[:, 4])\n+        if mh_dist.size == 0 and self.frame_count < 2:\n+            return detections\n+        min_dists = mh_dist.min(1)\n+        mask = (min_dists > limit) & (detections[:, 4] < self.det_thresh)\n+        boost_inds = np.where(mask)[0]\n+        iou_limit = 0.3\n+        if len(boost_inds) == 0:\n+            return detections\n+\n+        bdiou = iou_batch(detections[boost_inds], detections[boost_inds]) - np.ey",unknown,Added the UNSTARTED state to the YouTube PlayerState enum
668782907d9cebd8707a950427abaeb54dfff5e6,"wrong im used and code refactor (#1949)

* BUG - wrong im used

* rfdetr internally supports nparray so we dont need to_pil

* double append for pred is None

* device tag needs to be fixed everywhere.

* minor refactor

* per_class_decorator refactor

* too much redundancy

* bring the indents down",tracking/detectors/rfdetr.py,"# Mikel BrostrÃ¶m ð¥ RFDETR Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nimport cv2\nfrom PIL import Image\nfrom rfdetr import RFDETRBase\nfrom rfdetr.util.coco_classes import COCO_CLASSES\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.models.yolo.detect import DetectionPredictor\n\n\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n\nclass RFDETRStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = COCO_CLASSES\n\n    def __init__(self, model, device, args):\n        self.args = args\n        LOGGER.info(""""Loading RFDETR model"""")\n        self.model = RFDETRBase(device='cpu')\n\n    @torch.no_grad()\n    def __call__(self, im, augment, visualize, embed):\n\n        # Convert frame to PIL Image format for RFDETR\n        frame_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n        image = Image.fromarray(fr","# Mikel BrostrÃ¶m ð¥ RFDETR Tracking ð§¾ AGPL-3.0 license\n\nimport cv2\nimport numpy as np\nimport torch\nfrom rfdetr.util.coco_classes import COCO_CLASSES\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect import DetectionPredictor\n\nfrom boxmot.utils import logger as LOGGER\nfrom rfdetr import RFDETRBase\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n\nclass RFDETRStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = COCO_CLASSES\n\n    def __init__(self, model, device, args):\n        self.args = args\n        LOGGER.info(""""Loading RFDETR model"""")\n        self.model = RFDETRBase(device=""""cpu"""")\n\n    @torch.no_grad()\n    def __call__(self, im, augment, visualize, embed):\n        image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)  # Convert frame BGR2RGB for RFDETR\n        with torch.no_grad():\n            detections = self.model.predict(image, threshold=self.args.conf)\n\n        r","@@ -1,18 +1,14 @@\n # Mikel BrostrÃ¶m ð¥ RFDETR Tracking ð§¾ AGPL-3.0 license\n \n+import cv2\n import numpy as np\n import torch\n-import cv2\n-from PIL import Image\n-from rfdetr import RFDETRBase\n from rfdetr.util.coco_classes import COCO_CLASSES\n from ultralytics.engine.results import Results\n-from ultralytics.utils import ops\n from ultralytics.models.yolo.detect import DetectionPredictor\n \n-\n-\n from boxmot.utils import logger as LOGGER\n+from rfdetr import RFDETRBase\n from tracking.detectors.yolo_interface import YoloInterface\n \n \n@@ -26,27 +22,19 @@ class RFDETRStrategy(YoloInterface):\n     def __init__(self, model, device, args):\n         self.args = args\n         LOGGER.info(""""Loading RFDETR model"""")\n-        self.model = RFDETRBase(device='cpu')\n+        self.model = RFDETRBase(device=""""cpu"""")\n \n     @torch.no_grad()\n     def __call__(self, im, augment, visualize, embed):\n-\n-        # Convert frame to PIL Image format for RFDETR\n-        frame_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n-        image = Image.fromarray(frame_rgb)\n+        image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)  # Convert frame BGR2RGB for RFDETR\n         with torch.no_grad():\n-            detections = self.model.predict(im, threshold=self.args.conf)\n+            detections = self.model.predict(image, threshold=self.args.conf)\n \n-        preds = np.column_stack(\n-            [\n+        return torch.from_numpy(np.column_stack([\n                 detections.xyxy,\n                 detections.confidence[:, np.newaxis],\n                 detections.class_id[:, np.newaxis]\n-            ]\n-        )\n-\n-        preds = torch.from_numpy(preds).unsqueeze(0)\n-        return preds\n+            ])).unsqueeze(0)\n \n     def warmup(self, imgsz):\n         pass\n@@ -67,17 +55,10 @@ class RFDETRStrategy(YoloInterface):\n     def postprocess(self, preds, im, im0s):\n         results = []\n         for i, pred in enumerate(preds):\n-            im_path = self.im_paths[i] ",add,Added example for MAP type in documentation
668782907d9cebd8707a950427abaeb54dfff5e6,"wrong im used and code refactor (#1949)

* BUG - wrong im used

* rfdetr internally supports nparray so we dont need to_pil

* double append for pred is None

* device tag needs to be fixed everywhere.

* minor refactor

* per_class_decorator refactor

* too much redundancy

* bring the indents down",tracking/detectors/yolov9.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.utils.downloads import download\nfrom ultralytics.utils import ops\n\nfrom yolov9 import load\nfrom boxmot.utils import logger as LOGGER\nfrom examples.detectors.yolo_interface import YoloInterface\n\n\nYOLOv9_ZOO = {\n    'gelan-c.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-c.pt',\n    'gelan-e.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-e.pt',\n    'yolov9-c.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt',\n    'yolov9-e.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt',\n}\n\nclass Yolov9Strategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\nfrom ultralytics.utils.downloads import download\nfrom ultralytics.utils import ops\n\nfrom yolov9 import load\nfrom boxmot.utils import logger as LOGGER\nfrom examples.detectors.yolo_interface import YoloInterface\n\n\nYOLOv9_ZOO = {\n    'gelan-c.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-c.pt',\n    'gelan-e.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-e.pt',\n    'yolov9-c.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt',\n    'yolov9-e.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt',\n}\n\nclass Yolov9Strategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n ","@@ -105,7 +105,6 @@ class Yolov9Strategy(YoloInterface):\n                     orig_img=im0s[i],\n                     names=self.names\n                 )\n-                results.append(r)\n             else:\n                 pred = self.clip(pred, im0s[i])\n                 r = Results(\n",add,Add note about data volume to enable
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n__version__ = '13.0.0'\n\nfrom boxmot.postprocessing.gsi import gsi\nfrom boxmot.tracker_zoo import create_tracker, get_tracker_config\nfrom boxmot.trackers.boosttrack.boosttrack import BoostTrack\nfrom boxmot.trackers.botsort.botsort import BotSort\nfrom boxmot.trackers.bytetrack.bytetrack import ByteTrack\nfrom boxmot.trackers.deepocsort.deepocsort import DeepOcSort\nfrom boxmot.trackers.hybridsort.hybridsort import HybridSort\nfrom boxmot.trackers.ocsort.ocsort import OcSort\nfrom boxmot.trackers.strongsort.strongsort import StrongSort\n\nTRACKERS = ['bytetrack', 'botsort', 'strongsort', 'ocsort', 'deepocsort', 'hybridsort', 'boosttrack']\n\n__all__ = (""""__version__"""",\n           """"StrongSort"""", """"OcSort"""", """"ByteTrack"""", """"BotSort"""", """"DeepOcSort"""", """"HybridSort"""", """"BoostTrack"""",\n           """"create_tracker"""", """"get_tracker_config"""", """"gsi"""")\n","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n__version__ = '13.0.0'\n\nfrom boxmot.postprocessing.gsi import gsi\nfrom boxmot.tracker_zoo import create_tracker, get_tracker_config\nfrom boxmot.trackers.boosttrack.boosttrack import BoostTrack\nfrom boxmot.trackers.botsort.botsort import BotSort\nfrom boxmot.trackers.bytetrack.bytetrack import ByteTrack\nfrom boxmot.trackers.deepocsort.deepocsort import DeepOcSort\nfrom boxmot.trackers.hybridsort.hybridsort import HybridSort\nfrom boxmot.trackers.ocsort.ocsort import OcSort\nfrom boxmot.trackers.strongsort.strongsort import StrongSort\n\nTRACKERS = [\n    """"bytetrack"""",\n    """"botsort"""",\n    """"strongsort"""",\n    """"ocsort"""",\n    """"deepocsort"""",\n    """"hybridsort"""",\n    """"boosttrack"""",\n]\n\n__all__ = (\n    """"__version__"""",\n    """"StrongSort"""",\n    """"OcSort"""",\n    """"ByteTrack"""",\n    """"BotSort"""",\n    """"DeepOcSort"""",\n    """"HybridSort"""",\n    """"BoostTrack"""",\n    """"create_tracker"""",\n    """"get_tracker_config"""",\n    """"gsi""""","@@ -12,8 +12,26 @@ from boxmot.trackers.hybridsort.hybridsort import HybridSort\n from boxmot.trackers.ocsort.ocsort import OcSort\n from boxmot.trackers.strongsort.strongsort import StrongSort\n \n-TRACKERS = ['bytetrack', 'botsort', 'strongsort', 'ocsort', 'deepocsort', 'hybridsort', 'boosttrack']\n+TRACKERS = [\n+    """"bytetrack"""",\n+    """"botsort"""",\n+    """"strongsort"""",\n+    """"ocsort"""",\n+    """"deepocsort"""",\n+    """"hybridsort"""",\n+    """"boosttrack"""",\n+]\n \n-__all__ = (""""__version__"""",\n-           """"StrongSort"""", """"OcSort"""", """"ByteTrack"""", """"BotSort"""", """"DeepOcSort"""", """"HybridSort"""", """"BoostTrack"""",\n-           """"create_tracker"""", """"get_tracker_config"""", """"gsi"""")\n+__all__ = (\n+    """"__version__"""",\n+    """"StrongSort"""",\n+    """"OcSort"""",\n+    """"ByteTrack"""",\n+    """"BotSort"""",\n+    """"DeepOcSort"""",\n+    """"HybridSort"""",\n+    """"BoostTrack"""",\n+    """"create_tracker"""",\n+    """"get_tracker_config"""",\n+    """"gsi"""",\n+)\n",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/__init__.py,# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license,# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n,@@ -1 +1 @@\n-# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\ No newline at end of file\n+# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n,add,Added STORM - 370 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/clip/clip/clip.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport hashlib\nimport os\nimport urllib\nimport warnings\nfrom typing import List, Union\n\nimport torch\nfrom PIL import Image\nfrom torchvision.transforms import CenterCrop, Compose, Normalize, Resize, ToTensor\nfrom tqdm import tqdm\n\nfrom .model import build_model\nfrom .simple_tokenizer import SimpleTokenizer as _Tokenizer\n\ntry:\n    from torchvision.transforms import InterpolationMode\n    BICUBIC = InterpolationMode.BICUBIC\nexcept ImportError:\n    BICUBIC = Image.BICUBIC\n\n\n__all__ = [""""available_models"""", """"load"""", """"tokenize""""]\n_tokenizer = _Tokenizer()\n\n_MODELS = {\n    """"RN50"""": """"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt"""",  # noqa: E501\n    """"RN101"""": """"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt"""",  # noqa: E501\n    """"RN50x4"""": """"https://openaipublic.az","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport hashlib\nimport os\nimport urllib\nimport warnings\nfrom typing import List, Union\n\nimport torch\nfrom PIL import Image\nfrom torchvision.transforms import CenterCrop, Compose, Normalize, Resize, ToTensor\nfrom tqdm import tqdm\n\nfrom .model import build_model\nfrom .simple_tokenizer import SimpleTokenizer as _Tokenizer\n\ntry:\n    from torchvision.transforms import InterpolationMode\n\n    BICUBIC = InterpolationMode.BICUBIC\nexcept ImportError:\n    BICUBIC = Image.BICUBIC\n\n\n__all__ = [""""available_models"""", """"load"""", """"tokenize""""]\n_tokenizer = _Tokenizer()\n\n_MODELS = {\n    """"RN50"""": """"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt"""",  # noqa: E501\n    """"RN101"""": """"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt"""",  # noqa: E501\n    """"RN50x4"""": """"https://openaipublic.","@@ -16,6 +16,7 @@ from .simple_tokenizer import SimpleTokenizer as _Tokenizer\n \n try:\n     from torchvision.transforms import InterpolationMode\n+\n     BICUBIC = InterpolationMode.BICUBIC\n except ImportError:\n     BICUBIC = Image.BICUBIC\n@@ -45,13 +46,23 @@ def _download(url: str, root: str = os.path.expanduser(""""~/.cache/clip"""")):\n         raise RuntimeError(f""""{download_target} exists and is not a regular file"""")\n \n     if os.path.isfile(download_target):\n-        if hashlib.sha256(open(download_target, """"rb"""").read()).hexdigest() == expected_sha256:\n+        if (\n+            hashlib.sha256(open(download_target, """"rb"""").read()).hexdigest()\n+            == expected_sha256\n+        ):\n             return download_target\n         else:\n-            warnings.warn(f""""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file"""")\n+            warnings.warn(\n+                f""""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file""""\n+            )\n \n     with urllib.request.urlopen(url) as source, open(download_target, """"wb"""") as output:\n-        with tqdm(total=int(source.info().get(""""Content-Length"""")), ncols=80, unit='iB', unit_scale=True) as loop:\n+        with tqdm(\n+            total=int(source.info().get(""""Content-Length"""")),\n+            ncols=80,\n+            unit=""""iB"""",\n+            unit_scale=True,\n+        ) as loop:\n             while True:\n                 buffer = source.read(8192)\n                 if not buffer:\n@@ -60,20 +71,30 @@ def _download(url: str, root: str = os.path.expanduser(""""~/.cache/clip"""")):\n                 output.write(buffer)\n                 loop.update(len(buffer))\n \n-    if hashlib.sha256(open(download_target, """"rb"""").read()).hexdigest() != expected_sha256:\n-        raise RuntimeError(""""Model has been downloaded but the SHA256 checksum does not not match"""")\n+    if (\n+        hashlib.sha256(open(download_target, """"rb"""").read())",add,Add forced default for text type in oCC
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/clip/clip/model.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import OrderedDict\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = None\n        s","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import OrderedDict\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = None\n        s","@@ -58,7 +58,9 @@ class Bottleneck(nn.Module):\n class AttentionPool2d(nn.Module):\n     def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n         super().__init__()\n-        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim + 1, embed_dim) / embed_dim ** 0.5)\n+        self.positional_embedding = nn.Parameter(\n+            torch.randn(spacial_dim + 1, embed_dim) / embed_dim**0.5\n+        )\n         self.k_proj = nn.Linear(embed_dim, embed_dim)\n         self.q_proj = nn.Linear(embed_dim, embed_dim)\n         self.v_proj = nn.Linear(embed_dim, embed_dim)\n@@ -71,7 +73,9 @@ class AttentionPool2d(nn.Module):\n         x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC  50,32,2048\n         x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n         x, _ = F.multi_head_attention_forward(\n-            query=x, key=x, value=x,\n+            query=x,\n+            key=x,\n+            value=x,\n             embed_dim_to_check=x.shape[-1],\n             num_heads=self.num_heads,\n             q_proj_weight=self.q_proj.weight,\n@@ -87,7 +91,7 @@ class AttentionPool2d(nn.Module):\n             out_proj_bias=self.c_proj.bias,\n             use_separate_proj_weight=True,\n             training=self.training,\n-            need_weights=False\n+            need_weights=False,\n         )\n \n         return x\n@@ -107,9 +111,13 @@ class ModifiedResNet(nn.Module):\n         self.input_resolution = input_resolution\n \n         # the 3-layer stem\n-        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n+        self.conv1 = nn.Conv2d(\n+            3, width // 2, kernel_size=3, stride=2, padding=1, bias=False\n+        )\n         self.bn1 = nn.BatchNorm2d(width // 2)\n-        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n+        self.conv2 = nn.Conv2d(\n+            width // 2, width // 2, kernel_si",add,Add wizSymbolicLookup annotation
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/clip/clip/simple_tokenizer.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gzip\nimport html\nfrom functools import lru_cache\n\nimport ftfy\nimport regex as re\n\nfrom boxmot.utils import BOXMOT\n\n\n@lru_cache()\ndef default_bpe():\n    return BOXMOT / """"appearance/backbones/clip/clip/bpe_simple_vocab_16e6.txt.gz""""\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""""""""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""""""""\n    bs = list(range(ord(""""!""""), ord(""""~"""") + 1)) + list","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport gzip\nimport html\nfrom functools import lru_cache\n\nimport ftfy\nimport regex as re\n\nfrom boxmot.utils import BOXMOT\n\n\n@lru_cache()\ndef default_bpe():\n    return BOXMOT / """"appearance/backbones/clip/clip/bpe_simple_vocab_16e6.txt.gz""""\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""""""""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""""""""\n    bs = (\n        list(range(ord(""""!""""), ord(""""~"""") +","@@ -26,13 +26,17 @@ def bytes_to_unicode():\n     To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n     And avoids mapping to whitespace/control characters the bpe code barfs on.\n     """"""""""""\n-    bs = list(range(ord(""""!""""), ord(""""~"""") + 1)) + list(range(ord(""""Â¡""""), ord(""""Â¬"""") + 1)) + list(range(ord(""""Â®""""), ord(""""Ã¿"""") + 1))\n+    bs = (\n+        list(range(ord(""""!""""), ord(""""~"""") + 1))\n+        + list(range(ord(""""Â¡""""), ord(""""Â¬"""") + 1))\n+        + list(range(ord(""""Â®""""), ord(""""Ã¿"""") + 1))\n+    )\n     cs = bs[:]\n     n = 0\n     for b in range(2**8):\n         if b not in bs:\n             bs.append(b)\n-            cs.append(2 ** 8 + n)\n+            cs.append(2**8 + n)\n             n += 1\n     cs = [chr(n) for n in cs]\n     return dict(zip(bs, cs))\n@@ -57,7 +61,7 @@ def basic_clean(text):\n \n \n def whitespace_clean(text):\n-    text = re.sub(r'\s+', ' ', text)\n+    text = re.sub(r""""\s+"""", """" """", text)\n     text = text.strip()\n     return text\n \n@@ -66,31 +70,37 @@ class SimpleTokenizer(object):\n     def __init__(self, bpe_path: str = default_bpe()):\n         self.byte_encoder = bytes_to_unicode()\n         self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n-        merges = gzip.open(bpe_path).read().decode(""""utf-8"""").split('\n')\n-        merges = merges[1:49152 - 256 - 2 + 1]\n+        merges = gzip.open(bpe_path).read().decode(""""utf-8"""").split(""""\n"""")\n+        merges = merges[1 : 49152 - 256 - 2 + 1]\n         merges = [tuple(merge.split()) for merge in merges]\n         vocab = list(bytes_to_unicode().values())\n-        vocab = vocab + [v + '</w>' for v in vocab]\n+        vocab = vocab + [v + """"</w>"""" for v in vocab]\n         for merge in merges:\n-            vocab.append(''.join(merge))\n-        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n+            vocab.append("""""""".join(merge))\n+        vocab.extend([""""<|startoftext|>"""", """"<|endoftext|>""""])\n         self.encoder = dict(zip(vocab, ",add,Added the UNSTARTED state to the YouTube PlayerState enum
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/clip/config/defaults.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom yacs.config import CfgNode as CN\n\n# -----------------------------------------------------------------------------\n# Convention about Training / Test specific parameters\n# -----------------------------------------------------------------------------\n# Whenever an argument can be either used for training or for testing, the\n# corresponding name will be post-fixed by a _TRAIN for a training parameter,\n\n# -----------------------------------------------------------------------------\n# Config definition\n# -----------------------------------------------------------------------------\n\n_C = CN()\n# -----------------------------------------------------------------------------\n# MODEL\n# -----------------------------------------------------------------------------\n_C.MODEL = CN()\n# Using cuda or cpu for training\n_C.MODEL.DEVICE = """"cuda""""\n# ID number of GPU\n_C.MODEL.DEVICE_ID = '0'\n# Name of backbone\n_C.MODEL.NAME = '","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom yacs.config import CfgNode as CN\n\n# -----------------------------------------------------------------------------\n# Convention about Training / Test specific parameters\n# -----------------------------------------------------------------------------\n# Whenever an argument can be either used for training or for testing, the\n# corresponding name will be post-fixed by a _TRAIN for a training parameter,\n\n# -----------------------------------------------------------------------------\n# Config definition\n# -----------------------------------------------------------------------------\n\n_C = CN()\n# -----------------------------------------------------------------------------\n# MODEL\n# -----------------------------------------------------------------------------\n_C.MODEL = CN()\n# Using cuda or cpu for training\n_C.MODEL.DEVICE = """"cuda""""\n# ID number of GPU\n_C.MODEL.DEVICE_ID = """"0""""\n# Name of backbone\n_C.MODEL.NAME =","@@ -20,35 +20,35 @@ _C.MODEL = CN()\n # Using cuda or cpu for training\n _C.MODEL.DEVICE = """"cuda""""\n # ID number of GPU\n-_C.MODEL.DEVICE_ID = '0'\n+_C.MODEL.DEVICE_ID = """"0""""\n # Name of backbone\n-_C.MODEL.NAME = 'ViT-B-16'\n+_C.MODEL.NAME = """"ViT-B-16""""\n # Last stride of backbone\n _C.MODEL.LAST_STRIDE = 1\n # Path to pretrained model of backbone\n-_C.MODEL.PRETRAIN_PATH = '/home/mikel.brostrom/yolo_tracking/clip_market1501.pt'\n+_C.MODEL.PRETRAIN_PATH = """"/home/mikel.brostrom/yolo_tracking/clip_market1501.pt""""\n \n # Use ImageNet pretrained model to initialize backbone or use self trained model to initialize the whole model\n # Options: 'imagenet' , 'self' , 'finetune'\n-_C.MODEL.PRETRAIN_CHOICE = 'imagenet'\n+_C.MODEL.PRETRAIN_CHOICE = """"imagenet""""\n \n # If train with BNNeck, options: 'bnneck' or 'no'\n-_C.MODEL.NECK = 'bnneck'\n+_C.MODEL.NECK = """"bnneck""""\n # If train loss include center loss, options: 'yes' or 'no'. Loss with center loss has different optimizer configuration\n-_C.MODEL.IF_WITH_CENTER = 'no'\n+_C.MODEL.IF_WITH_CENTER = """"no""""\n \n-_C.MODEL.ID_LOSS_TYPE = 'softmax'\n+_C.MODEL.ID_LOSS_TYPE = """"softmax""""\n _C.MODEL.ID_LOSS_WEIGHT = 1.0\n _C.MODEL.TRIPLET_LOSS_WEIGHT = 1.0\n _C.MODEL.I2T_LOSS_WEIGHT = 1.0\n \n-_C.MODEL.METRIC_LOSS_TYPE = 'triplet'\n+_C.MODEL.METRIC_LOSS_TYPE = """"triplet""""\n # If train with multi-gpu ddp mode, options: 'True', 'False'\n _C.MODEL.DIST_TRAIN = False\n # If train with soft triplet loss, options: 'True', 'False'\n _C.MODEL.NO_MARGIN = False\n # If train with label smooth, options: 'on', 'off'\n-_C.MODEL.IF_LABELSMOOTH = 'on'\n+_C.MODEL.IF_LABELSMOOTH = """"on""""\n # If train with arcface loss, options: 'True', 'False'\n _C.MODEL.COS_LAYER = False\n \n@@ -56,7 +56,7 @@ _C.MODEL.COS_LAYER = False\n _C.MODEL.DROP_PATH = 0.1\n _C.MODEL.DROP_OUT = 0.0\n _C.MODEL.ATT_DROP_RATE = 0.0\n-_C.MODEL.TRANSFORMER_TYPE = 'None'\n+_C.MODEL.TRANSFORMER_TYPE = """"None""""\n _C.MODEL.STRIDE_SIZE = [16, 16]\n \n # SIE Parameter\n@@ -88,9 +88",fix,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/clip/config/defaults_base.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom yacs.config import CfgNode as CN\n\n# -----------------------------------------------------------------------------\n# Convention about Training / Test specific parameters\n# -----------------------------------------------------------------------------\n# Whenever an argument can be either used for training or for testing, the\n# corresponding name will be post-fixed by a _TRAIN for a training parameter,\n\n# -----------------------------------------------------------------------------\n# Config definition\n# -----------------------------------------------------------------------------\n\n_C = CN()\n# -----------------------------------------------------------------------------\n# MODEL\n# -----------------------------------------------------------------------------\n_C.MODEL = CN()\n# Using cuda or cpu for training\n_C.MODEL.DEVICE = """"cuda""""\n# ID number of GPU\n_C.MODEL.DEVICE_ID = '0'\n# Name of backbone\n_C.MODEL.NAME = '","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom yacs.config import CfgNode as CN\n\n# -----------------------------------------------------------------------------\n# Convention about Training / Test specific parameters\n# -----------------------------------------------------------------------------\n# Whenever an argument can be either used for training or for testing, the\n# corresponding name will be post-fixed by a _TRAIN for a training parameter,\n\n# -----------------------------------------------------------------------------\n# Config definition\n# -----------------------------------------------------------------------------\n\n_C = CN()\n# -----------------------------------------------------------------------------\n# MODEL\n# -----------------------------------------------------------------------------\n_C.MODEL = CN()\n# Using cuda or cpu for training\n_C.MODEL.DEVICE = """"cuda""""\n# ID number of GPU\n_C.MODEL.DEVICE_ID = """"0""""\n# Name of backbone\n_C.MODEL.NAME =","@@ -20,35 +20,35 @@ _C.MODEL = CN()\n # Using cuda or cpu for training\n _C.MODEL.DEVICE = """"cuda""""\n # ID number of GPU\n-_C.MODEL.DEVICE_ID = '0'\n+_C.MODEL.DEVICE_ID = """"0""""\n # Name of backbone\n-_C.MODEL.NAME = 'resnet50'\n+_C.MODEL.NAME = """"resnet50""""\n # Last stride of backbone\n _C.MODEL.LAST_STRIDE = 1\n # Path to pretrained model of backbone\n-_C.MODEL.PRETRAIN_PATH = ''\n+_C.MODEL.PRETRAIN_PATH = """"""""\n \n # Use ImageNet pretrained model to initialize backbone or use self trained model to initialize the whole model\n # Options: 'imagenet' , 'self' , 'finetune'\n-_C.MODEL.PRETRAIN_CHOICE = 'imagenet'\n+_C.MODEL.PRETRAIN_CHOICE = """"imagenet""""\n \n # If train with BNNeck, options: 'bnneck' or 'no'\n-_C.MODEL.NECK = 'bnneck'\n+_C.MODEL.NECK = """"bnneck""""\n # If train loss include center loss, options: 'yes' or 'no'. Loss with center loss has different optimizer configuration\n-_C.MODEL.IF_WITH_CENTER = 'no'\n+_C.MODEL.IF_WITH_CENTER = """"no""""\n \n-_C.MODEL.ID_LOSS_TYPE = 'softmax'\n+_C.MODEL.ID_LOSS_TYPE = """"softmax""""\n _C.MODEL.ID_LOSS_WEIGHT = 1.0\n _C.MODEL.TRIPLET_LOSS_WEIGHT = 1.0\n _C.MODEL.I2T_LOSS_WEIGHT = 1.0\n \n-_C.MODEL.METRIC_LOSS_TYPE = 'triplet'\n+_C.MODEL.METRIC_LOSS_TYPE = """"triplet""""\n # If train with multi-gpu ddp mode, options: 'True', 'False'\n _C.MODEL.DIST_TRAIN = False\n # If train with soft triplet loss, options: 'True', 'False'\n _C.MODEL.NO_MARGIN = False\n # If train with label smooth, options: 'on', 'off'\n-_C.MODEL.IF_LABELSMOOTH = 'on'\n+_C.MODEL.IF_LABELSMOOTH = """"on""""\n # If train with arcface loss, options: 'True', 'False'\n _C.MODEL.COS_LAYER = False\n \n@@ -56,7 +56,7 @@ _C.MODEL.COS_LAYER = False\n _C.MODEL.DROP_PATH = 0.1\n _C.MODEL.DROP_OUT = 0.0\n _C.MODEL.ATT_DROP_RATE = 0.0\n-_C.MODEL.TRANSFORMER_TYPE = 'None'\n+_C.MODEL.TRANSFORMER_TYPE = """"None""""\n _C.MODEL.STRIDE_SIZE = [16, 16]\n \n # SIE Parameter\n@@ -88,9 +88,9 @@ _C.INPUT.PADDING = 10\n # --------------------------------------------------------------------------",fix,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/clip/make_model.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport torch\nimport torch.nn as nn\n\nfrom .clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n\n_tokenizer = _Tokenizer()\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\n        nn.init.constant_(m.bias, 0.0)\n\n    elif classname.find('Conv') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n    elif classname.find('BatchNorm') != -1:\n        if m.affine:\n            nn.init.constant_(m.weight, 1.0)\n            nn.init.constant_(m.bias, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.normal_(m.weight, std=0.001)\n        if m.bias:\n            nn.init.constant_(m.bias, 0.0)\n\n\nclass build_transformer(nn.M","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport torch\nimport torch.nn as nn\n\nfrom .clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n\n_tokenizer = _Tokenizer()\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    if classname.find(""""Linear"""") != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode=""""fan_out"""")\n        nn.init.constant_(m.bias, 0.0)\n\n    elif classname.find(""""Conv"""") != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode=""""fan_in"""")\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n    elif classname.find(""""BatchNorm"""") != -1:\n        if m.affine:\n            nn.init.constant_(m.weight, 1.0)\n            nn.init.constant_(m.bias, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find(""""Linear"""") != -1:\n        nn.init.normal_(m.weight, std=0.001)\n        if m.bias:\n            nn.init.constant_(m.bias, 0.0)\n\n\nclass build_tran","@@ -10,15 +10,15 @@ _tokenizer = _Tokenizer()\n \n def weights_init_kaiming(m):\n     classname = m.__class__.__name__\n-    if classname.find('Linear') != -1:\n-        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\n+    if classname.find(""""Linear"""") != -1:\n+        nn.init.kaiming_normal_(m.weight, a=0, mode=""""fan_out"""")\n         nn.init.constant_(m.bias, 0.0)\n \n-    elif classname.find('Conv') != -1:\n-        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n+    elif classname.find(""""Conv"""") != -1:\n+        nn.init.kaiming_normal_(m.weight, a=0, mode=""""fan_in"""")\n         if m.bias is not None:\n             nn.init.constant_(m.bias, 0.0)\n-    elif classname.find('BatchNorm') != -1:\n+    elif classname.find(""""BatchNorm"""") != -1:\n         if m.affine:\n             nn.init.constant_(m.weight, 1.0)\n             nn.init.constant_(m.bias, 0.0)\n@@ -26,7 +26,7 @@ def weights_init_kaiming(m):\n \n def weights_init_classifier(m):\n     classname = m.__class__.__name__\n-    if classname.find('Linear') != -1:\n+    if classname.find(""""Linear"""") != -1:\n         nn.init.normal_(m.weight, std=0.001)\n         if m.bias:\n             nn.init.constant_(m.bias, 0.0)\n@@ -39,10 +39,10 @@ class build_transformer(nn.Module):\n         self.cos_layer = cfg.MODEL.COS_LAYER\n         self.neck = cfg.MODEL.NECK\n         self.neck_feat = cfg.TEST.NECK_FEAT\n-        if self.model_name == 'ViT-B-16':\n+        if self.model_name == """"ViT-B-16"""":\n             self.in_planes = 768\n             self.in_planes_proj = 512\n-        elif self.model_name == 'RN50':\n+        elif self.model_name == """"RN50"""":\n             self.in_planes = 2048\n             self.in_planes_proj = 1024\n         self.num_classes = num_classes\n@@ -65,7 +65,12 @@ class build_transformer(nn.Module):\n         self.h_resolution = int((cfg.INPUT.SIZE_TRAIN[0] - 16) // cfg.MODEL.STRIDE_SIZE[0] + 1)\n         self.w_resolution = int((cfg.INPUT.SIZE_TRAIN[1] - 16) // cfg.MODEL.STRIDE_SIZE",add,Add note about data volume to enable EGL
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/clip/make_model_clipreid.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport torch\nimport torch.nn as nn\n\nfrom .clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n\n_tokenizer = _Tokenizer()\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\n        nn.init.constant_(m.bias, 0.0)\n\n    elif classname.find('Conv') != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n    elif classname.find('BatchNorm') != -1:\n        if m.affine:\n            nn.init.constant_(m.weight, 1.0)\n            nn.init.constant_(m.bias, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.normal_(m.weight, std=0.001)\n        if m.bias:\n            nn.init.constant_(m.bias, 0.0)\n\n\nclass TextEncoder(nn.Module)","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport torch\nimport torch.nn as nn\n\nfrom .clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n\n_tokenizer = _Tokenizer()\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    if classname.find(""""Linear"""") != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode=""""fan_out"""")\n        nn.init.constant_(m.bias, 0.0)\n\n    elif classname.find(""""Conv"""") != -1:\n        nn.init.kaiming_normal_(m.weight, a=0, mode=""""fan_in"""")\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n    elif classname.find(""""BatchNorm"""") != -1:\n        if m.affine:\n            nn.init.constant_(m.weight, 1.0)\n            nn.init.constant_(m.bias, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find(""""Linear"""") != -1:\n        nn.init.normal_(m.weight, std=0.001)\n        if m.bias:\n            nn.init.constant_(m.bias, 0.0)\n\n\nclass TextEncode","@@ -10,15 +10,15 @@ _tokenizer = _Tokenizer()\n \n def weights_init_kaiming(m):\n     classname = m.__class__.__name__\n-    if classname.find('Linear') != -1:\n-        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\n+    if classname.find(""""Linear"""") != -1:\n+        nn.init.kaiming_normal_(m.weight, a=0, mode=""""fan_out"""")\n         nn.init.constant_(m.bias, 0.0)\n \n-    elif classname.find('Conv') != -1:\n-        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n+    elif classname.find(""""Conv"""") != -1:\n+        nn.init.kaiming_normal_(m.weight, a=0, mode=""""fan_in"""")\n         if m.bias is not None:\n             nn.init.constant_(m.bias, 0.0)\n-    elif classname.find('BatchNorm') != -1:\n+    elif classname.find(""""BatchNorm"""") != -1:\n         if m.affine:\n             nn.init.constant_(m.weight, 1.0)\n             nn.init.constant_(m.bias, 0.0)\n@@ -26,7 +26,7 @@ def weights_init_kaiming(m):\n \n def weights_init_classifier(m):\n     classname = m.__class__.__name__\n-    if classname.find('Linear') != -1:\n+    if classname.find(""""Linear"""") != -1:\n         nn.init.normal_(m.weight, std=0.001)\n         if m.bias:\n             nn.init.constant_(m.bias, 0.0)\n@@ -61,10 +61,10 @@ class build_transformer(nn.Module):\n         self.cos_layer = cfg.MODEL.COS_LAYER\n         self.neck = cfg.MODEL.NECK\n         self.neck_feat = cfg.TEST.NECK_FEAT\n-        if self.model_name == 'ViT-B-16':\n+        if self.model_name == """"ViT-B-16"""":\n             self.in_planes = 768\n             self.in_planes_proj = 512\n-        elif self.model_name == 'RN50':\n+        elif self.model_name == """"RN50"""":\n             self.in_planes = 2048\n             self.in_planes_proj = 1024\n         self.num_classes = num_classes\n@@ -74,7 +74,9 @@ class build_transformer(nn.Module):\n \n         self.classifier = nn.Linear(self.in_planes, self.num_classes, bias=False)\n         self.classifier.apply(weights_init_classifier)\n-        self.classifier_proj = nn.Linear(s",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/hacnn.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import, division\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = [""""HACNN""""]\n\n\nclass ConvBlock(nn.Module):\n    """"""""""""Basic convolutional block.\n\n    convolution + batch normalization + relu.\n\n    Args:\n        in_c (int): number of input channels.\n        out_c (int): number of output channels.\n        k (int or tuple): kernel size.\n        s (int or tuple): stride.\n        p (int or tuple): padding.\n    """"""""""""\n\n    def __init__(self, in_c, out_c, k, s=1, p=0):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Conv2d(in_c, out_c, k, stride=s, padding=p)\n        self.bn = nn.BatchNorm2d(out_c)\n\n    def forward(self, x):\n        return F.relu(self.bn(self.conv(x)))\n\n\nclass InceptionA(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(InceptionA, self).__init__()\n        mid_channels = out_chann","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import, division\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = [""""HACNN""""]\n\n\nclass ConvBlock(nn.Module):\n    """"""""""""Basic convolutional block.\n\n    convolution + batch normalization + relu.\n\n    Args:\n        in_c (int): number of input channels.\n        out_c (int): number of output channels.\n        k (int or tuple): kernel size.\n        s (int or tuple): stride.\n        p (int or tuple): padding.\n    """"""""""""\n\n    def __init__(self, in_c, out_c, k, s=1, p=0):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Conv2d(in_c, out_c, k, stride=s, padding=p)\n        self.bn = nn.BatchNorm2d(out_c)\n\n    def forward(self, x):\n        return F.relu(self.bn(self.conv(x)))\n\n\nclass InceptionA(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(InceptionA, self).__init__()\n        mid_channels = out_chann","@@ -212,7 +212,7 @@ class HACNN(nn.Module):\n         feat_dim=512,\n         learn_region=True,\n         use_gpu=True,\n-        **kwargs\n+        **kwargs,\n     ):\n         super(HACNN, self).__init__()\n         self.loss = loss\n",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/lmbn/attention.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport math\nimport random\n\nimport torch\nfrom torch import nn\nfrom torch.nn import Conv2d, Module, Parameter, ReLU, Sigmoid, Softmax\nfrom torch.nn import functional as F\n\ntorch_ver = torch.__version__[:3]\n\n__all__ = [\n    """"BatchDrop"""",\n    """"BatchFeatureErase_Top"""",\n    """"BatchRandomErasing"""",\n    """"PAM_Module"""",\n    """"CAM_Module"""",\n    """"Dual_Module"""",\n    """"SE_Module"""",\n]\n\n\nclass BatchRandomErasing(nn.Module):\n    def __init__(\n        self, probability=0.5, sl=0.02, sh=0.4, r1=0.3, mean=[0.4914, 0.4822, 0.4465]\n    ):\n        super(BatchRandomErasing, self).__init__()\n\n        self.probability = probability\n        self.mean = mean\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n\n    def forward(self, img):\n        if self.training:\n            if random.uniform(0, 1) > self.probability:\n                return img\n\n            for attempt in range(100):\n                area =","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport math\nimport random\n\nimport torch\nfrom torch import nn\nfrom torch.nn import Conv2d, Module, Parameter, ReLU, Sigmoid, Softmax\nfrom torch.nn import functional as F\n\ntorch_ver = torch.__version__[:3]\n\n__all__ = [\n    """"BatchDrop"""",\n    """"BatchFeatureErase_Top"""",\n    """"BatchRandomErasing"""",\n    """"PAM_Module"""",\n    """"CAM_Module"""",\n    """"Dual_Module"""",\n    """"SE_Module"""",\n]\n\n\nclass BatchRandomErasing(nn.Module):\n    def __init__(\n        self, probability=0.5, sl=0.02, sh=0.4, r1=0.3, mean=[0.4914, 0.4822, 0.4465]\n    ):\n        super(BatchRandomErasing, self).__init__()\n\n        self.probability = probability\n        self.mean = mean\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n\n    def forward(self, img):\n        if self.training:\n            if random.uniform(0, 1) > self.probability:\n                return img\n\n            for attempt in range(100):\n                area =","@@ -51,11 +51,11 @@ class BatchRandomErasing(nn.Module):\n                     x1 = random.randint(0, img.size()[2] - h)\n                     y1 = random.randint(0, img.size()[3] - w)\n                     if img.size()[1] == 3:\n-                        img[:, 0, x1: x1 + h, y1: y1 + w] = self.mean[0]\n-                        img[:, 1, x1: x1 + h, y1: y1 + w] = self.mean[1]\n-                        img[:, 2, x1: x1 + h, y1: y1 + w] = self.mean[2]\n+                        img[:, 0, x1 : x1 + h, y1 : y1 + w] = self.mean[0]\n+                        img[:, 1, x1 : x1 + h, y1 : y1 + w] = self.mean[1]\n+                        img[:, 2, x1 : x1 + h, y1 : y1 + w] = self.mean[2]\n                     else:\n-                        img[:, 0, x1: x1 + h, y1: y1 + w] = self.mean[0]\n+                        img[:, 0, x1 : x1 + h, y1 : y1 + w] = self.mean[0]\n                     return img\n \n         return img\n@@ -81,7 +81,7 @@ class BatchDrop(nn.Module):\n             sx = random.randint(0, h - rh)\n             sy = random.randint(0, w - rw)\n             mask = x.new_ones(x.size())\n-            mask[:, :, sx: sx + rh, sy: sy + rw] = 0\n+            mask[:, :, sx : sx + rh, sy : sy + rw] = 0\n             x = x * mask\n         return x\n \n",add,Added STORM - 370 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/lmbn/lmbn_n.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport copy\n\nimport torch\nfrom torch import nn\n\nfrom boxmot.appearance.backbones.lmbn.attention import BatchFeatureErase_Top\nfrom boxmot.appearance.backbones.lmbn.bnneck import BNNeck, BNNeck3\nfrom boxmot.appearance.backbones.osnet import OSBlock, osnet_x1_0\n\n\nclass LMBN_n(nn.Module):\n    def __init__(self, num_classes, loss, pretrained, use_gpu):\n        super(LMBN_n, self).__init__()\n\n        self.n_ch = 2\n        self.chs = 512 // self.n_ch\n        self.training = False\n\n        osnet = osnet_x1_0(pretrained=False)\n\n        self.backone = nn.Sequential(\n            osnet.conv1, osnet.maxpool, osnet.conv2, osnet.conv3[0]\n        )\n\n        conv3 = osnet.conv3[1:]\n\n        self.global_branch = nn.Sequential(\n            copy.deepcopy(conv3), copy.deepcopy(osnet.conv4), copy.deepcopy(osnet.conv5)\n        )\n\n        self.partial_branch = nn.Sequential(\n            copy.deepcopy(conv3), copy.deepcopy(os","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport copy\n\nimport torch\nfrom torch import nn\n\nfrom boxmot.appearance.backbones.lmbn.attention import BatchFeatureErase_Top\nfrom boxmot.appearance.backbones.lmbn.bnneck import BNNeck, BNNeck3\nfrom boxmot.appearance.backbones.osnet import OSBlock, osnet_x1_0\n\n\nclass LMBN_n(nn.Module):\n    def __init__(self, num_classes, loss, pretrained, use_gpu):\n        super(LMBN_n, self).__init__()\n\n        self.n_ch = 2\n        self.chs = 512 // self.n_ch\n        self.training = False\n\n        osnet = osnet_x1_0(pretrained=False)\n\n        self.backone = nn.Sequential(\n            osnet.conv1, osnet.maxpool, osnet.conv2, osnet.conv3[0]\n        )\n\n        conv3 = osnet.conv3[1:]\n\n        self.global_branch = nn.Sequential(\n            copy.deepcopy(conv3), copy.deepcopy(osnet.conv4), copy.deepcopy(osnet.conv5)\n        )\n\n        self.partial_branch = nn.Sequential(\n            copy.deepcopy(conv3), copy.deepcopy(os","@@ -87,10 +87,10 @@ class LMBN_n(nn.Module):\n         if self.activation_map:\n             _, _, h_par, _ = par.size()\n \n-            fmap_p0 = par[:, :, :h_par // 2, :]\n-            fmap_p1 = par[:, :, h_par // 2:, :]\n+            fmap_p0 = par[:, :, : h_par // 2, :]\n+            fmap_p1 = par[:, :, h_par // 2 :, :]\n             fmap_c0 = cha[:, : self.chs, :, :]\n-            fmap_c1 = cha[:, self.chs:, :, :]\n+            fmap_c1 = cha[:, self.chs :, :, :]\n             print(""""Generating activation maps..."""")\n \n             return glo, glo_, fmap_c0, fmap_c1, fmap_p0, fmap_p1\n@@ -113,7 +113,7 @@ class LMBN_n(nn.Module):\n         ################\n \n         c0 = cha[:, : self.chs, :, :]\n-        c1 = cha[:, self.chs:, :, :]\n+        c1 = cha[:, self.chs :, :, :]\n         c0 = self.shared(c0)\n         c1 = self.shared(c1)\n         f_c0 = self.reduction_ch_0(c0)\n",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/mlfn.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import, division\n\nimport torch\nimport torch.utils.model_zoo as model_zoo\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = [""""mlfn""""]\n\nmodel_urls = {\n    # training epoch = 5, top1 = 51.6\n    """"imagenet"""": """"https://mega.nz/#!YHxAhaxC!yu9E6zWl0x5zscSouTdbZu8gdFFytDdl-RAdD2DEfpk"""",\n}\n\n\nclass MLFNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, fsm_channels, groups=32):\n        super(MLFNBlock, self).__init__()\n        self.groups = groups\n        mid_channels = out_channels // 2\n\n        # Factor Modules\n        self.fm_conv1 = nn.Conv2d(in_channels, mid_channels, 1, bias=False)\n        self.fm_bn1 = nn.BatchNorm2d(mid_channels)\n        self.fm_conv2 = nn.Conv2d(\n            mid_channels,\n            mid_channels,\n            3,\n            stride=stride,\n            padding=1,\n            bias=False,\n            groups=self.","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import, division\n\nimport torch\nimport torch.utils.model_zoo as model_zoo\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = [""""mlfn""""]\n\nmodel_urls = {\n    # training epoch = 5, top1 = 51.6\n    """"imagenet"""": """"https://mega.nz/#!YHxAhaxC!yu9E6zWl0x5zscSouTdbZu8gdFFytDdl-RAdD2DEfpk"""",\n}\n\n\nclass MLFNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, fsm_channels, groups=32):\n        super(MLFNBlock, self).__init__()\n        self.groups = groups\n        mid_channels = out_channels // 2\n\n        # Factor Modules\n        self.fm_conv1 = nn.Conv2d(in_channels, mid_channels, 1, bias=False)\n        self.fm_bn1 = nn.BatchNorm2d(mid_channels)\n        self.fm_conv2 = nn.Conv2d(\n            mid_channels,\n            mid_channels,\n            3,\n            stride=stride,\n            padding=1,\n            bias=False,\n            groups=self.","@@ -110,7 +110,7 @@ class MLFN(nn.Module):\n         groups=32,\n         channels=[64, 256, 512, 1024, 2048],\n         embed_dim=1024,\n-        **kwargs\n+        **kwargs,\n     ):\n         super(MLFN, self).__init__()\n         self.loss = loss\n",add,Add forced default for HADOOP - 7983
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/mobilenetv2.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import, division\n\nimport torch.utils.model_zoo as model_zoo\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = [""""mobilenetv2_x1_0"""", """"mobilenetv2_x1_4""""]\n\nmodel_urls = {\n    # 1.0: top-1 71.3\n    """"mobilenetv2_x1_0"""": """"https://mega.nz/#!NKp2wAIA!1NH1pbNzY_M2hVk_hdsxNM1NUOWvvGPHhaNr-fASF6c"""",\n    # 1.4: top-1 73.9\n    """"mobilenetv2_x1_4"""": """"https://mega.nz/#!RGhgEIwS!xN2s2ZdyqI6vQ3EwgmRXLEW3khr9tpXg96G9SUJugGk"""",\n}\n\n\nclass ConvBlock(nn.Module):\n    """"""""""""Basic convolutional block.\n\n    convolution (bias discarded) + batch normalization + relu6.\n\n    Args:\n        in_c (int): number of input channels.\n        out_c (int): number of output channels.\n        k (int or tuple): kernel size.\n        s (int or tuple): stride.\n        p (int or tuple): padding.\n        g (int): number of blocked connections from input channels\n            to output channels (d","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import, division\n\nimport torch.utils.model_zoo as model_zoo\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = [""""mobilenetv2_x1_0"""", """"mobilenetv2_x1_4""""]\n\nmodel_urls = {\n    # 1.0: top-1 71.3\n    """"mobilenetv2_x1_0"""": """"https://mega.nz/#!NKp2wAIA!1NH1pbNzY_M2hVk_hdsxNM1NUOWvvGPHhaNr-fASF6c"""",\n    # 1.4: top-1 73.9\n    """"mobilenetv2_x1_4"""": """"https://mega.nz/#!RGhgEIwS!xN2s2ZdyqI6vQ3EwgmRXLEW3khr9tpXg96G9SUJugGk"""",\n}\n\n\nclass ConvBlock(nn.Module):\n    """"""""""""Basic convolutional block.\n\n    convolution (bias discarded) + batch normalization + relu6.\n\n    Args:\n        in_c (int): number of input channels.\n        out_c (int): number of output channels.\n        k (int or tuple): kernel size.\n        s (int or tuple): stride.\n        p (int or tuple): padding.\n        g (int): number of blocked connections from input channels\n            to output channels (d","@@ -83,7 +83,7 @@ class MobileNetV2(nn.Module):\n         loss=""""softmax"""",\n         fc_dims=None,\n         dropout_p=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super(MobileNetV2, self).__init__()\n         self.loss = loss\n",fix,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/osnet.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import, division\n\nimport warnings\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = [""""osnet_x1_0"""", """"osnet_x0_75"""", """"osnet_x0_5"""", """"osnet_x0_25"""", """"osnet_ibn_x1_0""""]\n\npretrained_urls = {\n    """"osnet_x1_0"""": """"https://drive.google.com/uc?id=1LaG1EJpHrxdAxKnSCJ_i0u-nbxSAeiFY"""",\n    """"osnet_x0_75"""": """"https://drive.google.com/uc?id=1uwA9fElHOk3ZogwbeY5GkLI6QPTX70Hq"""",\n    """"osnet_x0_5"""": """"https://drive.google.com/uc?id=16DGLbZukvVYgINws8u8deSaOqjybZ83i"""",\n    """"osnet_x0_25"""": """"https://drive.google.com/uc?id=1rb8UN5ZzPKRc_xvtHlyDh-cSz88YX9hs"""",\n    """"osnet_ibn_x1_0"""": """"https://drive.google.com/uc?id=1sr90V6irlYYDd4_4ISU2iruoRG8J__6l"""",\n}\n\n\n##########\n# Basic layers\n##########\nclass ConvLayer(nn.Module):\n    """"""""""""Convolution layer (conv + bn + relu).""""""""""""\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n    ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import, division\n\nimport warnings\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = [""""osnet_x1_0"""", """"osnet_x0_75"""", """"osnet_x0_5"""", """"osnet_x0_25"""", """"osnet_ibn_x1_0""""]\n\npretrained_urls = {\n    """"osnet_x1_0"""": """"https://drive.google.com/uc?id=1LaG1EJpHrxdAxKnSCJ_i0u-nbxSAeiFY"""",\n    """"osnet_x0_75"""": """"https://drive.google.com/uc?id=1uwA9fElHOk3ZogwbeY5GkLI6QPTX70Hq"""",\n    """"osnet_x0_5"""": """"https://drive.google.com/uc?id=16DGLbZukvVYgINws8u8deSaOqjybZ83i"""",\n    """"osnet_x0_25"""": """"https://drive.google.com/uc?id=1rb8UN5ZzPKRc_xvtHlyDh-cSz88YX9hs"""",\n    """"osnet_ibn_x1_0"""": """"https://drive.google.com/uc?id=1sr90V6irlYYDd4_4ISU2iruoRG8J__6l"""",\n}\n\n\n##########\n# Basic layers\n##########\nclass ConvLayer(nn.Module):\n    """"""""""""Convolution layer (conv + bn + relu).""""""""""""\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n    ","@@ -279,7 +279,7 @@ class OSNet(nn.Module):\n         feature_dim=512,\n         loss=""""softmax"""",\n         IN=False,\n-        **kwargs\n+        **kwargs,\n     ):\n         super(OSNet, self).__init__()\n         num_blocks = len(blocks)\n@@ -491,7 +491,7 @@ def osnet_x1_0(num_classes=1000, pretrained=True, loss=""""softmax"""", **kwargs):\n         layers=[2, 2, 2],\n         channels=[64, 256, 384, 512],\n         loss=loss,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrained:\n         init_pretrained_weights(model, key=""""osnet_x1_0"""")\n@@ -506,7 +506,7 @@ def osnet_x0_75(num_classes=1000, pretrained=True, loss=""""softmax"""", **kwargs):\n         layers=[2, 2, 2],\n         channels=[48, 192, 288, 384],\n         loss=loss,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrained:\n         init_pretrained_weights(model, key=""""osnet_x0_75"""")\n@@ -521,7 +521,7 @@ def osnet_x0_5(num_classes=1000, pretrained=True, loss=""""softmax"""", **kwargs):\n         layers=[2, 2, 2],\n         channels=[32, 128, 192, 256],\n         loss=loss,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrained:\n         init_pretrained_weights(model, key=""""osnet_x0_5"""")\n@@ -536,7 +536,7 @@ def osnet_x0_25(num_classes=1000, pretrained=True, loss=""""softmax"""", **kwargs):\n         layers=[2, 2, 2],\n         channels=[16, 64, 96, 128],\n         loss=loss,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrained:\n         init_pretrained_weights(model, key=""""osnet_x0_25"""")\n@@ -553,7 +553,7 @@ def osnet_ibn_x1_0(num_classes=1000, pretrained=True, loss=""""softmax"""", **kwargs):\n         channels=[64, 256, 384, 512],\n         loss=loss,\n         IN=True,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrained:\n         init_pretrained_weights(model, key=""""osnet_ibn_x1_0"""")\n",add,Add forced default for newline
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/osnet_ain.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import, division\n\nimport warnings\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = [""""osnet_ain_x1_0"""", """"osnet_ain_x0_75"""", """"osnet_ain_x0_5"""", """"osnet_ain_x0_25""""]\n\npretrained_urls = {\n    """"osnet_ain_x1_0"""": """"https://drive.google.com/uc?id=1-CaioD9NaqbHK_kzSMW8VE4_3KcsRjEo"""",\n    """"osnet_ain_x0_75"""": """"https://drive.google.com/uc?id=1apy0hpsMypqstfencdH-jKIUEFOW4xoM"""",\n    """"osnet_ain_x0_5"""": """"https://drive.google.com/uc?id=1KusKvEYyKGDTUBVRxRiz55G31wkihB6l"""",\n    """"osnet_ain_x0_25"""": """"https://drive.google.com/uc?id=1SxQt2AvmEcgWNhaRb2xC4rP6ZwVDP0Wt"""",\n}\n\n\n##########\n# Basic layers\n##########\nclass ConvLayer(nn.Module):\n    """"""""""""Convolution layer (conv + bn + relu).""""""""""""\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        groups=1,\n       ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import, division\n\nimport warnings\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = [""""osnet_ain_x1_0"""", """"osnet_ain_x0_75"""", """"osnet_ain_x0_5"""", """"osnet_ain_x0_25""""]\n\npretrained_urls = {\n    """"osnet_ain_x1_0"""": """"https://drive.google.com/uc?id=1-CaioD9NaqbHK_kzSMW8VE4_3KcsRjEo"""",\n    """"osnet_ain_x0_75"""": """"https://drive.google.com/uc?id=1apy0hpsMypqstfencdH-jKIUEFOW4xoM"""",\n    """"osnet_ain_x0_5"""": """"https://drive.google.com/uc?id=1KusKvEYyKGDTUBVRxRiz55G31wkihB6l"""",\n    """"osnet_ain_x0_25"""": """"https://drive.google.com/uc?id=1SxQt2AvmEcgWNhaRb2xC4rP6ZwVDP0Wt"""",\n}\n\n\n##########\n# Basic layers\n##########\nclass ConvLayer(nn.Module):\n    """"""""""""Convolution layer (conv + bn + relu).""""""""""""\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        groups=1,\n       ","@@ -312,7 +312,7 @@ class OSNet(nn.Module):\n         feature_dim=512,\n         loss=""""softmax"""",\n         conv1_IN=False,\n-        **kwargs\n+        **kwargs,\n     ):\n         super(OSNet, self).__init__()\n         num_blocks = len(blocks)\n@@ -518,7 +518,7 @@ def osnet_ain_x1_0(num_classes=1000, pretrained=True, loss=""""softmax"""", **kwargs):\n         channels=[64, 256, 384, 512],\n         loss=loss,\n         conv1_IN=True,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrained:\n         init_pretrained_weights(model, key=""""osnet_ain_x1_0"""")\n@@ -537,7 +537,7 @@ def osnet_ain_x0_75(num_classes=1000, pretrained=True, loss=""""softmax"""", **kwargs)\n         channels=[48, 192, 288, 384],\n         loss=loss,\n         conv1_IN=True,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrained:\n         init_pretrained_weights(model, key=""""osnet_ain_x0_75"""")\n@@ -556,7 +556,7 @@ def osnet_ain_x0_5(num_classes=1000, pretrained=True, loss=""""softmax"""", **kwargs):\n         channels=[32, 128, 192, 256],\n         loss=loss,\n         conv1_IN=True,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrained:\n         init_pretrained_weights(model, key=""""osnet_ain_x0_5"""")\n@@ -575,7 +575,7 @@ def osnet_ain_x0_25(num_classes=1000, pretrained=True, loss=""""softmax"""", **kwargs)\n         channels=[16, 64, 96, 128],\n         loss=loss,\n         conv1_IN=True,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrained:\n         init_pretrained_weights(model, key=""""osnet_ain_x0_25"""")\n",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backbones/resnet.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nCode source: https://github.com/pytorch/vision\n""""""""""""\nfrom __future__ import absolute_import, division\n\nimport torch.utils.model_zoo as model_zoo\nfrom torch import nn\n\n__all__ = [\n    """"resnet18"""",\n    """"resnet34"""",\n    """"resnet50"""",\n    """"resnet101"""",\n    """"resnet152"""",\n    """"resnext50_32x4d"""",\n    """"resnext101_32x8d"""",\n    """"resnet50_fc512"""",\n]\n\nmodel_urls = {\n    """"resnet18"""": """"https://download.pytorch.org/models/resnet18-5c106cde.pth"""",\n    """"resnet34"""": """"https://download.pytorch.org/models/resnet34-333f7ec4.pth"""",\n    """"resnet50"""": """"https://download.pytorch.org/models/resnet50-19c8e357.pth"""",\n    """"resnet101"""": """"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth"""",\n    """"resnet152"""": """"https://download.pytorch.org/models/resnet152-b121ed2d.pth"""",\n    """"resnext50_32x4d"""": """"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth"""",\n    """"resnext101_32x8d"""": """"https://downloa","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nCode source: https://github.com/pytorch/vision\n""""""""""""\nfrom __future__ import absolute_import, division\n\nimport torch.utils.model_zoo as model_zoo\nfrom torch import nn\n\n__all__ = [\n    """"resnet18"""",\n    """"resnet34"""",\n    """"resnet50"""",\n    """"resnet101"""",\n    """"resnet152"""",\n    """"resnext50_32x4d"""",\n    """"resnext101_32x8d"""",\n    """"resnet50_fc512"""",\n]\n\nmodel_urls = {\n    """"resnet18"""": """"https://download.pytorch.org/models/resnet18-5c106cde.pth"""",\n    """"resnet34"""": """"https://download.pytorch.org/models/resnet34-333f7ec4.pth"""",\n    """"resnet50"""": """"https://download.pytorch.org/models/resnet50-19c8e357.pth"""",\n    """"resnet101"""": """"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth"""",\n    """"resnet152"""": """"https://download.pytorch.org/models/resnet152-b121ed2d.pth"""",\n    """"resnext50_32x4d"""": """"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth"""",\n    """"resnext101_32x8d"""": """"https://downloa","@@ -182,7 +182,7 @@ class ResNet(nn.Module):\n         last_stride=2,\n         fc_dims=None,\n         dropout_p=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super(ResNet, self).__init__()\n         if norm_layer is None:\n@@ -386,7 +386,7 @@ def resnet18(num_classes, loss=""""softmax"""", pretrained=True, **kwargs):\n         last_stride=2,\n         fc_dims=None,\n         dropout_p=None,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrained:\n         init_pretrained_weights(model, model_urls[""""resnet18""""])\n@@ -402,7 +402,7 @@ def resnet34(num_classes, loss=""""softmax"""", pretrained=True, **kwargs):\n         last_stride=2,\n         fc_dims=None,\n         dropout_p=None,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrained:\n         init_pretrained_weights(model, model_urls[""""resnet34""""])\n@@ -418,7 +418,7 @@ def resnet50(num_classes, loss=""""softmax"""", pretrained=True, **kwargs):\n         last_stride=2,\n         fc_dims=None,\n         dropout_p=None,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrained:\n         init_pretrained_weights(model, model_urls[""""resnet50""""])\n@@ -434,7 +434,7 @@ def resnet101(num_classes, loss=""""softmax"""", pretrained=True, **kwargs):\n         last_stride=2,\n         fc_dims=None,\n         dropout_p=None,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrained:\n         init_pretrained_weights(model, model_urls[""""resnet101""""])\n@@ -450,7 +450,7 @@ def resnet152(num_classes, loss=""""softmax"""", pretrained=True, **kwargs):\n         last_stride=2,\n         fc_dims=None,\n         dropout_p=None,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrained:\n         init_pretrained_weights(model, model_urls[""""resnet152""""])\n@@ -471,7 +471,7 @@ def resnext50_32x4d(num_classes, loss=""""softmax"""", pretrained=True, **kwargs):\n         dropout_p=None,\n         groups=32,\n         width_per_group=4,\n-        **kwargs\n+        **kwargs,\n     )\n     if pretrai",add,Add debug flag to enable up notification for optimization .
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backends/base_backend.py,"from abc import abstractmethod\n\nimport cv2\nimport gdown\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid.registry import ReIDModelRegistry\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import RequirementsChecker\n\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights[0] if isinstance(weights, list) else weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n\n        self.download_model(self.weights)\n        self.model_name = ReIDModelRegistry.get_model_name(self.weights)\n\n        self.model = ReIDModelRegistry.build_model(\n            self.model_name,\n            num_classes=ReIDModelRegistry.get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n        )\n        self.checker = RequirementsChecke","from abc import abstractmethod\n\nimport cv2\nimport gdown\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid.registry import ReIDModelRegistry\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import RequirementsChecker\n\n\nclass BaseModelBackend:\n    def __init__(self, weights, device, half):\n        self.weights = weights[0] if isinstance(weights, list) else weights\n        self.device = device\n        self.half = half\n        self.model = None\n        self.cuda = torch.cuda.is_available() and self.device.type != """"cpu""""\n\n        self.download_model(self.weights)\n        self.model_name = ReIDModelRegistry.get_model_name(self.weights)\n\n        self.model = ReIDModelRegistry.build_model(\n            self.model_name,\n            num_classes=ReIDModelRegistry.get_nr_classes(self.weights),\n            pretrained=not (self.weights and self.weights.is_file()),\n            use_gpu=device,\n        )\n        self.checker = RequirementsChecke","@@ -31,7 +31,6 @@ class BaseModelBackend:\n         self.load_model(self.weights)\n         self.input_shape = (384, 128) if """"lmbn"""" in self.model_name else (256, 128)\n \n-\n     def get_crops(self, xyxys, img):\n         h, w = img.shape[:2]\n         interpolation_method = cv2.INTER_LINEAR\n@@ -40,31 +39,38 @@ class BaseModelBackend:\n         \n         # Preallocate tensor for crops\n         num_crops = len(xyxys)\n-        crops = torch.empty((num_crops, 3, *self.input_shape), \n-                            dtype=torch.half if self.half else torch.float, device=self.device)\n-        \n+        crops = torch.empty(\n+            (num_crops, 3, *self.input_shape),\n+            dtype=torch.half if self.half else torch.float,\n+            device=self.device,\n+        )\n+\n         for i, box in enumerate(xyxys):\n-            x1, y1, x2, y2 = box.round().astype('int')\n+            x1, y1, x2, y2 = box.round().astype(""""int"""")\n             x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w, x2), min(h, y2)\n             crop = img[y1:y2, x1:x2]\n-            \n+\n             # Resize and convert color in one step\n-            crop = cv2.resize(crop, (self.input_shape[1], self.input_shape[0]), \n-                              interpolation=interpolation_method)\n+            crop = cv2.resize(\n+                crop,\n+                (self.input_shape[1], self.input_shape[0]),\n+                interpolation=interpolation_method,\n+            )\n             crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n-            \n+\n             # Convert to tensor and normalize (convert to [0, 1] by dividing by 255 in batch later)\n-            crop = torch.from_numpy(crop).to(self.device, dtype=torch.half if self.half else torch.float)\n+            crop = torch.from_numpy(crop).to(\n+                self.device, dtype=torch.half if self.half else torch.float\n+            )\n             crops[i] = torch.permute(crop, (2, 0, 1))  # Change to (C, H, W)\n-        \n+\n",add,Added the UNSTARTED state to the YouTube PlayerState enum
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backends/onnx_backend.py,"from boxmot.appearance.backends.base_backend import BaseModelBackend\n\n\nclass ONNXBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n\n    def load_model(self, w):\n\n            # ONNXRuntime will attempt to use the first provider, and if it fails or is not\n            # available for some reason, it will fall back to the next provider in the list\n            if self.device == """"mps"""":\n                self.checker.check_packages((""""onnxruntime-silicon==1.20.0"""",))\n                providers = [""""MPSExecutionProvider"""", """"CPUExecutionProvider""""]\n            elif self.device == """"cuda"""":\n                self.checker.check_packages((""""onnxruntime-gpu==1.20.0"""",))\n                providers = [""""CUDAExecutionProvider"""", """"CPUExecutionProvider""""]\n            else:\n                self.checker.check_packages((""""onnxruntime==1.20.0"""",))\n                pr","from boxmot.appearance.backends.base_backend import BaseModelBackend\n\n\nclass ONNXBackend(BaseModelBackend):\n\n    def __init__(self, weights, device, half):\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n\n    def load_model(self, w):\n\n        # ONNXRuntime will attempt to use the first provider, and if it fails or is not\n        # available for some reason, it will fall back to the next provider in the list\n        if self.device == """"mps"""":\n            self.checker.check_packages((""""onnxruntime-silicon==1.20.0"""",))\n            providers = [""""MPSExecutionProvider"""", """"CPUExecutionProvider""""]\n        elif self.device == """"cuda"""":\n            self.checker.check_packages((""""onnxruntime-gpu==1.20.0"""",))\n            providers = [""""CUDAExecutionProvider"""", """"CPUExecutionProvider""""]\n        else:\n            self.checker.check_packages((""""onnxruntime==1.20.0"""",))\n            providers = [""""CPUExecutionProvider""""]\n\n    ","@@ -10,21 +10,22 @@ class ONNXBackend(BaseModelBackend):\n \n     def load_model(self, w):\n \n-            # ONNXRuntime will attempt to use the first provider, and if it fails or is not\n-            # available for some reason, it will fall back to the next provider in the list\n-            if self.device == """"mps"""":\n-                self.checker.check_packages((""""onnxruntime-silicon==1.20.0"""",))\n-                providers = [""""MPSExecutionProvider"""", """"CPUExecutionProvider""""]\n-            elif self.device == """"cuda"""":\n-                self.checker.check_packages((""""onnxruntime-gpu==1.20.0"""",))\n-                providers = [""""CUDAExecutionProvider"""", """"CPUExecutionProvider""""]\n-            else:\n-                self.checker.check_packages((""""onnxruntime==1.20.0"""",))\n-                providers = [""""CPUExecutionProvider""""]\n-\n-            # Load the ONNX model using onnxruntime\n-            import onnxruntime\n-            self.session = onnxruntime.InferenceSession(str(w), providers=providers)\n+        # ONNXRuntime will attempt to use the first provider, and if it fails or is not\n+        # available for some reason, it will fall back to the next provider in the list\n+        if self.device == """"mps"""":\n+            self.checker.check_packages((""""onnxruntime-silicon==1.20.0"""",))\n+            providers = [""""MPSExecutionProvider"""", """"CPUExecutionProvider""""]\n+        elif self.device == """"cuda"""":\n+            self.checker.check_packages((""""onnxruntime-gpu==1.20.0"""",))\n+            providers = [""""CUDAExecutionProvider"""", """"CPUExecutionProvider""""]\n+        else:\n+            self.checker.check_packages((""""onnxruntime==1.20.0"""",))\n+            providers = [""""CPUExecutionProvider""""]\n+\n+        # Load the ONNX model using onnxruntime\n+        import onnxruntime\n+\n+        self.session = onnxruntime.InferenceSession(str(w), providers=providers)\n \n     def forward(self, im_batch):\n         # Convert torch tensor to numpy (onnxruntime expects nu",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backends/tensorrt_backend.py,"from collections import OrderedDict, namedtuple\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils import logger as LOGGER\n\n\nclass TensorRTBackend(BaseModelBackend):\n    def __init__(self, weights, device, half):\n        self.is_trt10 = False\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.device = device\n        self.weights = weights\n        self.fp16 = False  # Will be updated in load_model\n        self.load_model(self.weights)\n\n    def load_model(self, w):\n        LOGGER.info(f""""Loading {w} for TensorRT inference..."""")\n        self.checker.check_packages((""""nvidia-tensorrt"""",))\n        try:\n            import tensorrt as trt  # TensorRT library\n        except ImportError:\n            raise ImportError(""""Please install tensorrt to use this backend."""")\n\n        if self.device.type == """"cpu"""":\n            if torch.cuda.is_av","from collections import OrderedDict, namedtuple\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils import logger as LOGGER\n\n\nclass TensorRTBackend(BaseModelBackend):\n    def __init__(self, weights, device, half):\n        self.is_trt10 = False\n        super().__init__(weights, device, half)\n        self.nhwc = False\n        self.half = half\n        self.device = device\n        self.weights = weights\n        self.fp16 = False  # Will be updated in load_model\n        self.load_model(self.weights)\n\n    def load_model(self, w):\n        LOGGER.info(f""""Loading {w} for TensorRT inference..."""")\n        self.checker.check_packages((""""nvidia-tensorrt"""",))\n        try:\n            import tensorrt as trt  # TensorRT library\n        except ImportError:\n            raise ImportError(""""Please install tensorrt to use this backend."""")\n\n        if self.device.type == """"cpu"""":\n            if torch.cuda.is_av","@@ -38,7 +38,7 @@ class TensorRTBackend(BaseModelBackend):\n         # Deserialize the engine\n         with open(w, """"rb"""") as f, trt.Runtime(logger) as runtime:\n             self.model_ = runtime.deserialize_cuda_engine(f.read())\n-        \n+\n         # Execution context\n         self.context = self.model_.create_execution_context()\n         self.bindings = OrderedDict()\n@@ -93,12 +93,12 @@ class TensorRTBackend(BaseModelBackend):\n             inp_batch = temp_im_batch.shape[0]\n         if temp_im_batch.shape[0] > 0:\n             batch_array.append(temp_im_batch)\n-        \n+\n         for temp_batch in batch_array:\n             # Adjust for dynamic shapes\n             if temp_batch.shape != self.bindings[""""images""""].shape:\n                 if self.is_trt10:\n-                    \n+\n                     self.context.set_input_shape(""""images"""", temp_batch.shape)\n                     self.bindings[""""images""""] = self.bindings[""""images""""]._replace(shape=temp_batch.shape)\n                     self.bindings[""""output""""].data.resize_(tuple(self.context.get_tensor_shape(""""output"""")))\n@@ -120,9 +120,9 @@ class TensorRTBackend(BaseModelBackend):\n             features = self.bindings[""""output""""].data\n             resultant_features.append(features.clone())\n \n-        if len(resultant_features)== 1:\n+        if len(resultant_features) == 1:\n             return resultant_features[0]\n         else:\n-            rslt_features = torch.cat(resultant_features,dim=0)\n-            rslt_features= rslt_features[:im_batch.shape[0]]\n+            rslt_features = torch.cat(resultant_features, dim=0)\n+            rslt_features = rslt_features[: im_batch.shape[0]]\n             return rslt_features\n",add,Added STORM - 132 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/backends/tflite_backend.py,"from pathlib import Path\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils import logger as LOGGER\n\n\nclass TFLiteBackend(BaseModelBackend):\n    """"""""""""\n    A class to handle TensorFlow Lite model inference with dynamic batch size support.\n\n    Attributes:\n        nhwc (bool): A flag indicating the order of dimensions.\n        half (bool): A flag to indicate if half precision is used.\n        interpreter (tf.lite.Interpreter): The TensorFlow Lite interpreter.\n        current_allocated_batch_size (int): The current batch size allocated in the interpreter.\n    """"""""""""\n\n    def __init__(self, weights: Path, device: str, half: bool):\n        """"""""""""\n        Initializes the TFLiteBackend with given weights, device, and precision flag.\n\n        Args:\n            weights (Path): Path to the TFLite model file.\n            device (str): Device type (e.g., 'cpu', 'gpu').\n            half (bool): Flag to","from pathlib import Path\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.backends.base_backend import BaseModelBackend\nfrom boxmot.utils import logger as LOGGER\n\n\nclass TFLiteBackend(BaseModelBackend):\n    """"""""""""\n    A class to handle TensorFlow Lite model inference with dynamic batch size support.\n\n    Attributes:\n        nhwc (bool): A flag indicating the order of dimensions.\n        half (bool): A flag to indicate if half precision is used.\n        interpreter (tf.lite.Interpreter): The TensorFlow Lite interpreter.\n        current_allocated_batch_size (int): The current batch size allocated in the interpreter.\n    """"""""""""\n\n    def __init__(self, weights: Path, device: str, half: bool):\n        """"""""""""\n        Initializes the TFLiteBackend with given weights, device, and precision flag.\n\n        Args:\n            weights (Path): Path to the TFLite model file.\n            device (str): Device type (e.g., 'cpu', 'gpu').\n            half (bool): Flag to","@@ -45,13 +45,13 @@ class TFLiteBackend(BaseModelBackend):\n         LOGGER.info(f""""Loading {str(w)} for TensorFlow Lite inference..."""")\n \n         import tensorflow as tf\n-        self.interpreter = tf.lite.Interpreter(model_path=str(w))\n \n+        self.interpreter = tf.lite.Interpreter(model_path=str(w))\n \n         self.interpreter.allocate_tensors()  # allocate\n         self.input_details = self.interpreter.get_input_details()  # inputs\n         self.output_details = self.interpreter.get_output_details()  # outputs\n-        self.current_allocated_batch_size = self.input_details[0]['shape'][0]\n+        self.current_allocated_batch_size = self.input_details[0][""""shape""""][0]\n \n     def forward(self, im_batch: torch.Tensor) -> np.ndarray:\n         """"""""""""\n@@ -71,17 +71,19 @@ class TFLiteBackend(BaseModelBackend):\n         # Resize tensors if the new batch size is different from the current allocated batch size\n         if batch_size != self.current_allocated_batch_size:\n             # print(f""""Resizing tensor input to batch size {batch_size}"""")\n-            self.interpreter.resize_tensor_input(self.input_details[0]['index'], [batch_size, 256, 128, 3])\n+            self.interpreter.resize_tensor_input(\n+                self.input_details[0][""""index""""], [batch_size, 256, 128, 3]\n+            )\n             self.interpreter.allocate_tensors()\n             self.current_allocated_batch_size = batch_size\n \n         # Set the tensor to point to the input data\n-        self.interpreter.set_tensor(self.input_details[0]['index'], im_batch)\n+        self.interpreter.set_tensor(self.input_details[0][""""index""""], im_batch)\n \n         # Run inference\n         self.interpreter.invoke()\n \n         # Get the output data\n-        features = self.interpreter.get_tensor(self.output_details[0]['index'])\n+        features = self.interpreter.get_tensor(self.output_details[0][""""index""""])\n \n-        return features\n\ No newline at end of file\n+        ret",add,Add cdma_rmnet4 to list of monitored interfaces
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/exporters/base_exporter.py,"from pathlib import Path\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import RequirementsChecker\n\n\ndef export_decorator(export_func):\n    def wrapper(self, *args, **kwargs):\n        try:\n            if hasattr(self, 'group'):\n                if hasattr(self, 'cmd'):\n                    self.checker.sync_group_or_extra(self.group, cmd=self.cmd)\n                else:\n                    self.checker.sync_group_or_extra(self.group)\n                \n            LOGGER.info(f""""\nStarting {self.file} export with {self.__class__.__name__}..."""")\n            result = export_func(self, *args, **kwargs)\n            if result:\n                LOGGER.info(f""""Export success, saved as {result} ({self.file_size(result):.1f} MB)"""")\n            return result\n        except Exception as e:\n            LOGGER.error(f""""Export failure: {e}"""")\n            return None\n    return wrapper\n\n\nclass BaseExporter:\n    def __init__(self, model, im, file, optimize=Fal","from pathlib import Path\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import RequirementsChecker\n\n\ndef export_decorator(export_func):\n    def wrapper(self, *args, **kwargs):\n        try:\n            if hasattr(self, """"group""""):\n                if hasattr(self, """"cmd""""):\n                    self.checker.sync_group_or_extra(self.group, cmd=self.cmd)\n                else:\n                    self.checker.sync_group_or_extra(self.group)\n\n            LOGGER.info(\n                f""""\nStarting {self.file} export with {self.__class__.__name__}...""""\n            )\n            result = export_func(self, *args, **kwargs)\n            if result:\n                LOGGER.info(\n                    f""""Export success, saved as {result} ({self.file_size(result):.1f} MB)""""\n                )\n            return result\n        except Exception as e:\n            LOGGER.error(f""""Export failure: {e}"""")\n            return None\n\n    return wrapper\n\n\nclass BaseE","@@ -7,25 +7,32 @@ from boxmot.utils.checks import RequirementsChecker\n def export_decorator(export_func):\n     def wrapper(self, *args, **kwargs):\n         try:\n-            if hasattr(self, 'group'):\n-                if hasattr(self, 'cmd'):\n+            if hasattr(self, """"group""""):\n+                if hasattr(self, """"cmd""""):\n                     self.checker.sync_group_or_extra(self.group, cmd=self.cmd)\n                 else:\n                     self.checker.sync_group_or_extra(self.group)\n-                \n-            LOGGER.info(f""""\nStarting {self.file} export with {self.__class__.__name__}..."""")\n+\n+            LOGGER.info(\n+                f""""\nStarting {self.file} export with {self.__class__.__name__}...""""\n+            )\n             result = export_func(self, *args, **kwargs)\n             if result:\n-                LOGGER.info(f""""Export success, saved as {result} ({self.file_size(result):.1f} MB)"""")\n+                LOGGER.info(\n+                    f""""Export success, saved as {result} ({self.file_size(result):.1f} MB)""""\n+                )\n             return result\n         except Exception as e:\n             LOGGER.error(f""""Export failure: {e}"""")\n             return None\n+\n     return wrapper\n \n \n class BaseExporter:\n-    def __init__(self, model, im, file, optimize=False, dynamic=False, half=False, simplify=False):\n+    def __init__(\n+        self, model, im, file, optimize=False, dynamic=False, half=False, simplify=False\n+    ):\n         self.model = model\n         self.im = im\n         self.file = Path(file)\n@@ -48,8 +55,8 @@ class BaseExporter:\n \n     def export(self):\n         raise NotImplementedError(""""Export method must be implemented in subclasses."""")\n-    \n+\n     def __init_subclass__(cls, **kwargs):\n         super().__init_subclass__(**kwargs)\n-        if 'export' in cls.__dict__:\n-            cls.export = export_decorator(cls.export)\n\ No newline at end of file\n+        if """"export"""" in cls._",add,Don ' t define the dependency on real_commad twice
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/exporters/onnx_exporter.py,"import torch\n\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass ONNXExporter(BaseExporter):\n    group = """"onnx""""\n    \n    def export(self):\n        import onnx\n\n        f = self.file.with_suffix("""".onnx"""")\n\n        dynamic = {""""images"""": {0: """"batch""""}, """"output"""": {0: """"batch""""}} if self.dynamic else None\n\n        torch.onnx.export(\n            self.model.cpu() if self.dynamic else self.model,\n            self.im.cpu() if self.dynamic else self.im,\n            f,\n            verbose=False,\n            opset_version=12,\n            do_constant_folding=True,\n            input_names=[""""images""""],\n            output_names=[""""output""""],\n            dynamic_axes=dynamic,\n        )\n\n        model_onnx = onnx.load(f)\n        onnx.checker.check_model(model_onnx)\n        onnx.save(model_onnx, f)\n\n        if self.simplify:\n            self.simplify_model(model_onnx, f)\n            \n        retur","import torch\n\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass ONNXExporter(BaseExporter):\n    group = """"onnx""""\n\n    def export(self):\n        import onnx\n\n        f = self.file.with_suffix("""".onnx"""")\n\n        dynamic = {""""images"""": {0: """"batch""""}, """"output"""": {0: """"batch""""}} if self.dynamic else None\n\n        torch.onnx.export(\n            self.model.cpu() if self.dynamic else self.model,\n            self.im.cpu() if self.dynamic else self.im,\n            f,\n            verbose=False,\n            opset_version=12,\n            do_constant_folding=True,\n            input_names=[""""images""""],\n            output_names=[""""output""""],\n            dynamic_axes=dynamic,\n        )\n\n        model_onnx = onnx.load(f)\n        onnx.checker.check_model(model_onnx)\n        onnx.save(model_onnx, f)\n\n        if self.simplify:\n            self.simplify_model(model_onnx, f)\n\n        return f\n\n    def s","@@ -6,7 +6,7 @@ from boxmot.utils import logger as LOGGER\n \n class ONNXExporter(BaseExporter):\n     group = """"onnx""""\n-    \n+\n     def export(self):\n         import onnx\n \n@@ -32,12 +32,12 @@ class ONNXExporter(BaseExporter):\n \n         if self.simplify:\n             self.simplify_model(model_onnx, f)\n-            \n-        return f\n \n+        return f\n \n     def simplify_model(self, model_onnx, f):\n         import onnx\n+\n         try:\n             cuda = torch.cuda.is_available()\n             self.checker.check_packages(\n@@ -48,11 +48,9 @@ class ONNXExporter(BaseExporter):\n             )\n             import onnxsim\n \n-            LOGGER.info(\n-                f""""Simplifying with onnx-simplifier {onnxsim.__version__}...""""\n-            )\n+            LOGGER.info(f""""Simplifying with onnx-simplifier {onnxsim.__version__}..."""")\n             model_onnx, check = onnxsim.simplify(model_onnx)\n             assert check, """"assert check failed""""\n             onnx.save(model_onnx, f)\n         except Exception as e:\n-            LOGGER.error(f""""Simplifier failure: {e}"""")\n\ No newline at end of file\n+            LOGGER.error(f""""Simplifier failure: {e}"""")\n",add,Don ' t define the main thread
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/exporters/openvino_exporter.py,"from pathlib import Path\nimport openvino as ov\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\n\nclass OpenVINOExporter(BaseExporter):\n    group = """"openvino""""\n\n    def export(self) -> str:\n        # 1. Paths\n        #    assume self.file is e.g. """"model.onnx""""\n        onnx_path = self.file.with_suffix("""".onnx"""")\n        export_dir = self.file.parent / f""""{self.file.stem}_openvino_model""""\n        export_dir.mkdir(parents=True, exist_ok=True)\n\n        # 2. Convert ONNX â ov.Model\n        ov_model = ov.convert_model(input_model=onnx_path)\n\n        # 3. Save to IR (XML + BIN)\n        xml_name = self.file.with_suffix("""".xml"""").name\n        xml_path = export_dir / xml_name\n        ov.save_model(ov_model, xml_path, compress_to_fp16=self.half)\n\n        # Return the actual XML file so that BaseExporter.with_suffix() works correctly\n        return str(xml_path)","import openvino as ov\n\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\n\n\nclass OpenVINOExporter(BaseExporter):\n    group = """"openvino""""\n\n    def export(self):\n\n        from openvino.tools import mo\n\n    def export(self) -> str:\n        # 1. Paths\n        #    assume self.file is e.g. """"model.onnx""""\n        onnx_path = self.file.with_suffix("""".onnx"""")\n        export_dir = self.file.parent / f""""{self.file.stem}_openvino_model""""\n        export_dir.mkdir(parents=True, exist_ok=True)\n\n        # 2. Convert ONNX â ov.Model\n        ov_model = ov.convert_model(input_model=onnx_path)\n\n        # 3. Save to IR (XML + BIN)\n        xml_name = self.file.with_suffix("""".xml"""").name\n        xml_path = export_dir / xml_name\n        ov.save_model(ov_model, xml_path, compress_to_fp16=self.half)\n\n        # Return the actual XML file so that BaseExporter.with_suffix() works correctly\n        return str(xml_path)","@@ -1,10 +1,15 @@\n-from pathlib import Path\n import openvino as ov\n+\n from boxmot.appearance.exporters.base_exporter import BaseExporter\n \n+\n class OpenVINOExporter(BaseExporter):\n     group = """"openvino""""\n \n+    def export(self):\n+\n+        from openvino.tools import mo\n+\n     def export(self) -> str:\n         # 1. Paths\n         #    assume self.file is e.g. """"model.onnx""""\n",add,Add note about data volume to enable ELASTIC note about missing additional fields
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/exporters/tensorrt_exporter.py,"from boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.appearance.exporters.onnx_exporter import ONNXExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass EngineExporter(BaseExporter):\n    required_packages = (""""nvidia-tensorrt"""",)\n    cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n    \n    def export(self):\n\n        assert self.im.device.type != """"cpu"""", """"export running on CPU but must be on GPU, i.e. `python export.py --device 0`""""\n        try:\n            import tensorrt as trt\n        except ImportError:\n            import tensorrt as trt\n\n        onnx_file = self.export_onnx()\n        LOGGER.info(f""""\nStarting export with TensorRT {trt.__version__}..."""")\n        is_trt10 = int(trt.__version__.split(""""."""")[0]) >= 10  # is TensorRT >= 10\n        assert onnx_file.exists(), f""""Failed to export ONNX file: {onnx_file}""""\n        f = self.file.with_suffix("""".engine"""")\n        logger = trt.Logger(trt.Logger.INFO)\n        if True:","from boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.appearance.exporters.onnx_exporter import ONNXExporter\nfrom boxmot.utils import logger as LOGGER\n\n\nclass EngineExporter(BaseExporter):\n    required_packages = (""""nvidia-tensorrt"""",)\n    cmds = """"--extra-index-url https://pypi.ngc.nvidia.com""""\n\n    def export(self):\n\n        assert (\n            self.im.device.type != """"cpu""""\n        ), """"export running on CPU but must be on GPU, i.e. `python export.py --device 0`""""\n        try:\n            import tensorrt as trt\n        except ImportError:\n            import tensorrt as trt\n\n        onnx_file = self.export_onnx()\n        LOGGER.info(f""""\nStarting export with TensorRT {trt.__version__}..."""")\n        is_trt10 = int(trt.__version__.split(""""."""")[0]) >= 10  # is TensorRT >= 10\n        assert onnx_file.exists(), f""""Failed to export ONNX file: {onnx_file}""""\n        f = self.file.with_suffix("""".engine"""")\n        logger = trt.Logger(trt.Logger","@@ -5,11 +5,13 @@ from boxmot.utils import logger as LOGGER\n \n class EngineExporter(BaseExporter):\n     required_packages = (""""nvidia-tensorrt"""",)\n-    cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n-    \n+    cmds = """"--extra-index-url https://pypi.ngc.nvidia.com""""\n+\n     def export(self):\n \n-        assert self.im.device.type != """"cpu"""", """"export running on CPU but must be on GPU, i.e. `python export.py --device 0`""""\n+        assert (\n+            self.im.device.type != """"cpu""""\n+        ), """"export running on CPU but must be on GPU, i.e. `python export.py --device 0`""""\n         try:\n             import tensorrt as trt\n         except ImportError:\n@@ -61,7 +63,9 @@ class EngineExporter(BaseExporter):\n                 )\n             config.add_optimization_profile(profile)\n \n-        LOGGER.info(f""""Building FP{16 if builder.platform_has_fast_fp16 and self.half else 32} engine in {f}"""")\n+        LOGGER.info(\n+            f""""Building FP{16 if builder.platform_has_fast_fp16 and self.half else 32} engine in {f}""""\n+        )\n         if builder.platform_has_fast_fp16 and self.half:\n             config.set_flag(trt.BuilderFlag.FP16)\n             config.default_device_type = trt.DeviceType.GPU\n@@ -72,7 +76,14 @@ class EngineExporter(BaseExporter):\n \n         return f\n \n-\n     def export_onnx(self):\n-        onnx_exporter = ONNXExporter(self.model, self.im, self.file, self.optimize, self.dynamic, self.half, self.simplify)\n+        onnx_exporter = ONNXExporter(\n+            self.model,\n+            self.im,\n+            self.file,\n+            self.optimize,\n+            self.dynamic,\n+            self.half,\n+            self.simplify,\n+        )\n         return onnx_exporter.export()\n",add,Added STORM - 1270 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/exporters/tflite_exporter.py,"import os\n\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\n\n\nclass TFLiteExporter(BaseExporter):\n    group = """"tflite""""\n    cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n    \n    def export(self):\n\n        import onnx2tf\n        input_onnx_file_path = str(self.file.with_suffix('.onnx'))\n        output_folder_path = input_onnx_file_path.replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n        onnx2tf.convert(\n            input_onnx_file_path=input_onnx_file_path,\n            output_folder_path=output_folder_path,\n            not_use_onnxsim=True,\n            verbosity=True,\n            # output_integer_quantized_tflite=self.args.int8,\n            # quant_type=""""per-tensor"""",  # """"per-tensor"""" (faster) or """"per-channel"""" (slower but more accurate)\n            # custom_input_op_name_np_data_path=np_data,\n        )\n        return output_folder_path\n","import os\n\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\n\n\nclass TFLiteExporter(BaseExporter):\n    group = """"tflite""""\n    cmds = """"--extra-index-url https://pypi.ngc.nvidia.com""""\n\n    def export(self):\n\n        import onnx2tf\n\n        input_onnx_file_path = str(self.file.with_suffix("""".onnx""""))\n        output_folder_path = input_onnx_file_path.replace(\n            """".onnx"""", f""""_saved_model{os.sep}""""\n        )\n        onnx2tf.convert(\n            input_onnx_file_path=input_onnx_file_path,\n            output_folder_path=output_folder_path,\n            not_use_onnxsim=True,\n            verbosity=True,\n            # output_integer_quantized_tflite=self.args.int8,\n            # quant_type=""""per-tensor"""",  # """"per-tensor"""" (faster) or """"per-channel"""" (slower but more accurate)\n            # custom_input_op_name_np_data_path=np_data,\n        )\n        return output_folder_path\n","@@ -5,13 +5,16 @@ from boxmot.appearance.exporters.base_exporter import BaseExporter\n \n class TFLiteExporter(BaseExporter):\n     group = """"tflite""""\n-    cmds = '--extra-index-url https://pypi.ngc.nvidia.com'\n-    \n+    cmds = """"--extra-index-url https://pypi.ngc.nvidia.com""""\n+\n     def export(self):\n \n         import onnx2tf\n-        input_onnx_file_path = str(self.file.with_suffix('.onnx'))\n-        output_folder_path = input_onnx_file_path.replace("""".onnx"""", f""""_saved_model{os.sep}"""")\n+\n+        input_onnx_file_path = str(self.file.with_suffix("""".onnx""""))\n+        output_folder_path = input_onnx_file_path.replace(\n+            """".onnx"""", f""""_saved_model{os.sep}""""\n+        )\n         onnx2tf.convert(\n             input_onnx_file_path=input_onnx_file_path,\n             output_folder_path=output_folder_path,\n",add,Add warning about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/exporters/torchscript_exporter.py,"import torch\n\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\n\n\nclass TorchScriptExporter(BaseExporter):\n    def export(self):\n        f = self.file.with_suffix("""".torchscript"""")\n        ts = torch.jit.trace(self.model, self.im, strict=False)\n        if self.optimize:\n            torch.utils.mobile_optimizer.optimize_for_mobile(ts)._save_for_lite_interpreter(str(f))\n        else:\n            ts.save(str(f))\n\n        return f\n","import torch\n\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\n\n\nclass TorchScriptExporter(BaseExporter):\n    def export(self):\n        f = self.file.with_suffix("""".torchscript"""")\n        ts = torch.jit.trace(self.model, self.im, strict=False)\n        if self.optimize:\n            torch.utils.mobile_optimizer.optimize_for_mobile(\n                ts\n            )._save_for_lite_interpreter(str(f))\n        else:\n            ts.save(str(f))\n\n        return f\n","@@ -8,7 +8,9 @@ class TorchScriptExporter(BaseExporter):\n         f = self.file.with_suffix("""".torchscript"""")\n         ts = torch.jit.trace(self.model, self.im, strict=False)\n         if self.optimize:\n-            torch.utils.mobile_optimizer.optimize_for_mobile(ts)._save_for_lite_interpreter(str(f))\n+            torch.utils.mobile_optimizer.optimize_for_mobile(\n+                ts\n+            )._save_for_lite_interpreter(str(f))\n         else:\n             ts.save(str(f))\n \n",add,Fix the bug of type error when calling set_to_default_values
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/reid/auto_backend.py,"from pathlib import Path\nfrom typing import Tuple, Union\n\nimport torch\n\nfrom boxmot.appearance.backends.onnx_backend import ONNXBackend\nfrom boxmot.appearance.backends.openvino_backend import OpenVinoBackend\nfrom boxmot.appearance.backends.pytorch_backend import PyTorchBackend\nfrom boxmot.appearance.backends.tensorrt_backend import TensorRTBackend\nfrom boxmot.appearance.backends.tflite_backend import TFLiteBackend\nfrom boxmot.appearance.backends.torchscript_backend import TorchscriptBackend\nfrom boxmot.appearance.reid import export_formats\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.torch_utils import select_device\n\n\nclass ReidAutoBackend():\n    def __init__(\n        self,\n        weights: Path = WEIGHTS / """"osnet_x0_25_msmt17.pt"""",\n        device: torch.device = torch.device(""""cpu""""),\n        half: bool = False) -> None:\n        """"""""""""\n        Initializes the ReidAutoBackend instance with specified weights, devic","from pathlib import Path\nfrom typing import Tuple, Union\n\nimport torch\n\nfrom boxmot.appearance.backends.onnx_backend import ONNXBackend\nfrom boxmot.appearance.backends.openvino_backend import OpenVinoBackend\nfrom boxmot.appearance.backends.pytorch_backend import PyTorchBackend\nfrom boxmot.appearance.backends.tensorrt_backend import TensorRTBackend\nfrom boxmot.appearance.backends.tflite_backend import TFLiteBackend\nfrom boxmot.appearance.backends.torchscript_backend import TorchscriptBackend\nfrom boxmot.appearance.reid import export_formats\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.torch_utils import select_device\n\n\nclass ReidAutoBackend:\n    def __init__(\n        self,\n        weights: Path = WEIGHTS / """"osnet_x0_25_msmt17.pt"""",\n        device: torch.device = torch.device(""""cpu""""),\n        half: bool = False,\n    ) -> None:\n        """"""""""""\n        Initializes the ReidAutoBackend instance with specified weights, ","@@ -15,12 +15,13 @@ from boxmot.utils import logger as LOGGER\n from boxmot.utils.torch_utils import select_device\n \n \n-class ReidAutoBackend():\n+class ReidAutoBackend:\n     def __init__(\n         self,\n         weights: Path = WEIGHTS / """"osnet_x0_25_msmt17.pt"""",\n         device: torch.device = torch.device(""""cpu""""),\n-        half: bool = False) -> None:\n+        half: bool = False,\n+    ) -> None:\n         """"""""""""\n         Initializes the ReidAutoBackend instance with specified weights, device, and precision mode.\n \n@@ -45,14 +46,22 @@ class ReidAutoBackend():\n         self.half = half\n         self.model = self.get_backend()\n \n-\n-    def get_backend(self) -> Union['PyTorchBackend', 'TorchscriptBackend', 'ONNXBackend', 'TensorRTBackend', 'OpenVinoBackend', 'TFLiteBackend']:\n+    def get_backend(\n+        self,\n+    ) -> Union[\n+        """"PyTorchBackend"""",\n+        """"TorchscriptBackend"""",\n+        """"ONNXBackend"""",\n+        """"TensorRTBackend"""",\n+        """"OpenVinoBackend"""",\n+        """"TFLiteBackend"""",\n+    ]:\n         """"""""""""\n         Returns an instance of the appropriate backend based on the model type.\n \n         Returns:\n             An instance of a backend class corresponding to the detected model type.\n-        \n+\n         Raises:\n             SystemExit: If no supported model framework is detected.\n         """"""""""""\n@@ -64,7 +73,7 @@ class ReidAutoBackend():\n             self.onnx: ONNXBackend,\n             self.engine: TensorRTBackend,\n             self.xml: OpenVinoBackend,\n-            self.tflite: TFLiteBackend\n+            self.tflite: TFLiteBackend,\n         }\n \n         # Iterate through the mapping and return the first matching backend\n@@ -76,7 +85,6 @@ class ReidAutoBackend():\n         LOGGER.error(""""This model framework is not supported yet!"""")\n         exit()\n \n-\n     def forward(self, im_batch: torch.Tensor) -> torch.Tensor:\n         """"""""""""\n         Processes an image batch through the selected",add,Don ' t define the dependency on real_commad twice
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/reid/config.py,"MODEL_TYPES = [\n    """"resnet50"""",\n    """"resnet101"""",\n    """"mlfn"""",\n    """"hacnn"""",\n    """"mobilenetv2_x1_0"""",\n    """"mobilenetv2_x1_4"""",\n    """"osnet_x1_0"""",\n    """"osnet_x0_75"""",\n    """"osnet_x0_5"""",\n    """"osnet_x0_25"""",\n    """"osnet_ibn_x1_0"""",\n    """"osnet_ain_x1_0"""",\n    """"lmbn_n"""",\n    """"clip"""",\n]\n\nTRAINED_URLS = {\n    # resnet50\n    """"resnet50_market1501.pt"""": """"https://drive.google.com/uc?id=1dUUZ4rHDWohmsQXCRe2C_HbYkzz94iBV"""",\n    """"resnet50_dukemtmcreid.pt"""": """"https://drive.google.com/uc?id=17ymnLglnc64NRvGOitY3BqMRS9UWd1wg"""",\n    """"resnet50_msmt17.pt"""": """"https://drive.google.com/uc?id=1ep7RypVDOthCRIAqDnn4_N-UhkkFHJsj"""",\n    """"resnet50_fc512_market1501.pt"""": """"https://drive.google.com/uc?id=1kv8l5laX_YCdIGVCetjlNdzKIA3NvsSt"""",\n    """"resnet50_fc512_dukemtmcreid.pt"""": """"https://drive.google.com/uc?id=13QN8Mp3XH81GK4BPGXobKHKyTGH50Rtx"""",\n    """"resnet50_fc512_msmt17.pt"""": """"https://drive.google.com/uc?id=1fDJLcz4O5wxNSUvImIIjoaIF9u1Rwaud"""",\n    # mlfn\n    """"ml","MODEL_TYPES = [\n    """"resnet50"""",\n    """"resnet101"""",\n    """"mlfn"""",\n    """"hacnn"""",\n    """"mobilenetv2_x1_0"""",\n    """"mobilenetv2_x1_4"""",\n    """"osnet_x1_0"""",\n    """"osnet_x0_75"""",\n    """"osnet_x0_5"""",\n    """"osnet_x0_25"""",\n    """"osnet_ibn_x1_0"""",\n    """"osnet_ain_x1_0"""",\n    """"lmbn_n"""",\n    """"clip"""",\n]\n\nTRAINED_URLS = {\n    # resnet50\n    """"resnet50_market1501.pt"""": """"https://drive.google.com/uc?id=1dUUZ4rHDWohmsQXCRe2C_HbYkzz94iBV"""",\n    """"resnet50_dukemtmcreid.pt"""": """"https://drive.google.com/uc?id=17ymnLglnc64NRvGOitY3BqMRS9UWd1wg"""",\n    """"resnet50_msmt17.pt"""": """"https://drive.google.com/uc?id=1ep7RypVDOthCRIAqDnn4_N-UhkkFHJsj"""",\n    """"resnet50_fc512_market1501.pt"""": """"https://drive.google.com/uc?id=1kv8l5laX_YCdIGVCetjlNdzKIA3NvsSt"""",\n    """"resnet50_fc512_dukemtmcreid.pt"""": """"https://drive.google.com/uc?id=13QN8Mp3XH81GK4BPGXobKHKyTGH50Rtx"""",\n    """"resnet50_fc512_msmt17.pt"""": """"https://drive.google.com/uc?id=1fDJLcz4O5wxNSUvImIIjoaIF9u1Rwaud"""",\n    # mlfn\n    """"ml","@@ -62,7 +62,7 @@ TRAINED_URLS = {\n     """"clip_market1501.pt"""": """"https://drive.google.com/uc?id=1GnyAVeNOg3Yug1KBBWMKKbT2x43O5Ch7"""",\n     """"clip_duke.pt"""": """"https://drive.google.com/uc?id=1ldjSkj-7pXAWmx8on5x0EftlCaolU4dY"""",\n     """"clip_veri.pt"""": """"https://drive.google.com/uc?id=1RyfHdOBI2pan_wIGSim5-l6cM4S2WN8e"""",\n-    """"clip_vehicleid.pt"""": """"https://drive.google.com/uc?id=168BLegHHxNqatW5wx1YyL2REaThWoof5""""\n+    """"clip_vehicleid.pt"""": """"https://drive.google.com/uc?id=168BLegHHxNqatW5wx1YyL2REaThWoof5"""",\n }\n \n NR_CLASSES_DICT = {\n@@ -70,4 +70,4 @@ NR_CLASSES_DICT = {\n     """"duke"""": 702,\n     """"veri"""": 576,\n     """"vehicleid"""": 576,\n-}\n\ No newline at end of file\n+}\n",add,Added format51l
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/reid/export.py,"#!/usr/bin/env python3\nimport argparse\nimport time\nfrom pathlib import Path\n\nimport torch\n\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.appearance.exporters.onnx_exporter import ONNXExporter\nfrom boxmot.appearance.exporters.openvino_exporter import OpenVINOExporter\nfrom boxmot.appearance.exporters.tensorrt_exporter import EngineExporter\nfrom boxmot.appearance.exporters.tflite_exporter import TFLiteExporter\nfrom boxmot.appearance.exporters.torchscript_exporter import TorchScriptExporter\nfrom boxmot.appearance.reid import export_formats\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.appearance.reid.registry import ReIDModelRegistry\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.torch_utils import select_device\n\n\ndef parse_args():\n    """"""""""""\n    Parse command-line arguments for the ReID export script.\n    """"""""""""\n    parser = argparse.ArgumentParser(descript","#!/usr/bin/env python3\nimport argparse\nimport time\nfrom pathlib import Path\n\nimport torch\n\nfrom boxmot.appearance.exporters.base_exporter import BaseExporter\nfrom boxmot.appearance.exporters.onnx_exporter import ONNXExporter\nfrom boxmot.appearance.exporters.openvino_exporter import OpenVINOExporter\nfrom boxmot.appearance.exporters.tensorrt_exporter import EngineExporter\nfrom boxmot.appearance.exporters.tflite_exporter import TFLiteExporter\nfrom boxmot.appearance.exporters.torchscript_exporter import TorchScriptExporter\nfrom boxmot.appearance.reid import export_formats\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.appearance.reid.registry import ReIDModelRegistry\nfrom boxmot.utils import WEIGHTS\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.torch_utils import select_device\n\n\ndef parse_args():\n    """"""""""""\n    Parse command-line arguments for the ReID export script.\n    """"""""""""\n    parser = argparse.ArgumentParser(descript","@@ -145,7 +145,7 @@ def create_export_tasks(args, model, dummy_input):\n         """"torchscript"""": (\n             torchscript_flag,\n             TorchScriptExporter,\n-            (model, dummy_input, args.weights, args.optimize)\n+            (model, dummy_input, args.weights, args.optimize),\n         ),\n         """"engine"""": (\n             engine_flag,\n@@ -160,13 +160,13 @@ def create_export_tasks(args, model, dummy_input):\n         """"tflite"""": (\n             tflite_flag,\n             TFLiteExporter,\n-            (model, dummy_input, args.weights)\n+            (model, dummy_input, args.weights),\n         ),\n         """"openvino"""": (\n             openvino_flag,\n             OpenVINOExporter,\n-            (model, dummy_input, args.weights, args.half)\n-        )\n+            (model, dummy_input, args.weights, args.half),\n+        ),\n     }\n \n \n",add,Added auto permission to the route_logger_list_read
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/reid/factory.py,"from boxmot.appearance.backbones.clip.make_model import make_model\nfrom boxmot.appearance.backbones.hacnn import HACNN\nfrom boxmot.appearance.backbones.lmbn.lmbn_n import LMBN_n\nfrom boxmot.appearance.backbones.mlfn import mlfn\nfrom boxmot.appearance.backbones.mobilenetv2 import mobilenetv2_x1_0, mobilenetv2_x1_4\nfrom boxmot.appearance.backbones.osnet import (\n    osnet_ibn_x1_0,\n    osnet_x0_5,\n    osnet_x0_25,\n    osnet_x0_75,\n    osnet_x1_0,\n)\nfrom boxmot.appearance.backbones.osnet_ain import (\n    osnet_ain_x0_5,\n    osnet_ain_x0_25,\n    osnet_ain_x0_75,\n    osnet_ain_x1_0,\n)\nfrom boxmot.appearance.backbones.resnet import resnet50, resnet101\n\n# Map model names to their respective constructors\nMODEL_FACTORY = {\n    """"resnet50"""": resnet50,\n    """"resnet101"""": resnet101,\n    """"mobilenetv2_x1_0"""": mobilenetv2_x1_0,\n    """"mobilenetv2_x1_4"""": mobilenetv2_x1_4,\n    """"hacnn"""": HACNN,\n    """"mlfn"""": mlfn,\n    """"osnet_x1_0"""": osnet_x1_0,\n    """"osnet_x0_75"""": osnet_","from boxmot.appearance.backbones.clip.make_model import make_model\nfrom boxmot.appearance.backbones.hacnn import HACNN\nfrom boxmot.appearance.backbones.lmbn.lmbn_n import LMBN_n\nfrom boxmot.appearance.backbones.mlfn import mlfn\nfrom boxmot.appearance.backbones.mobilenetv2 import mobilenetv2_x1_0, mobilenetv2_x1_4\nfrom boxmot.appearance.backbones.osnet import (\n    osnet_ibn_x1_0,\n    osnet_x0_5,\n    osnet_x0_25,\n    osnet_x0_75,\n    osnet_x1_0,\n)\nfrom boxmot.appearance.backbones.osnet_ain import (\n    osnet_ain_x0_5,\n    osnet_ain_x0_25,\n    osnet_ain_x0_75,\n    osnet_ain_x1_0,\n)\nfrom boxmot.appearance.backbones.resnet import resnet50, resnet101\n\n# Map model names to their respective constructors\nMODEL_FACTORY = {\n    """"resnet50"""": resnet50,\n    """"resnet101"""": resnet101,\n    """"mobilenetv2_x1_0"""": mobilenetv2_x1_0,\n    """"mobilenetv2_x1_4"""": mobilenetv2_x1_4,\n    """"hacnn"""": HACNN,\n    """"mlfn"""": mlfn,\n    """"osnet_x1_0"""": osnet_x1_0,\n    """"osnet_x0_75"""": osnet_","@@ -37,4 +37,4 @@ MODEL_FACTORY = {\n     """"osnet_ain_x0_25"""": osnet_ain_x0_25,\n     """"lmbn_n"""": LMBN_n,\n     """"clip"""": make_model,\n-}\n\ No newline at end of file\n+}\n",add,Added net . kotlin . h to problem
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/appearance/reid/registry.py,"# model_registry.py\nfrom collections import OrderedDict\n\nimport torch\n\nfrom boxmot.appearance.reid.config import MODEL_TYPES, NR_CLASSES_DICT, TRAINED_URLS\nfrom boxmot.appearance.reid.factory import MODEL_FACTORY\nfrom boxmot.utils import logger as LOGGER\n\n\nclass ReIDModelRegistry:\n    """"""""""""Encapsulates model registration and related utilities.""""""""""""\n\n    @staticmethod\n    def show_downloadable_models():\n        LOGGER.info(""""Available .pt ReID models for automatic download"""")\n        LOGGER.info(list(TRAINED_URLS.keys()))\n\n    @staticmethod\n    def get_model_name(model):\n        for name in MODEL_TYPES:\n            if name in model.name:\n                return name\n        return None\n\n    @staticmethod\n    def get_model_url(model):\n        return TRAINED_URLS.get(model.name, None)\n\n    @staticmethod\n    def load_pretrained_weights(model, weight_path):\n        """"""""""""\n        Loads pretrained weights into a model.\n        Chooses the proper map_location","# model_registry.py\nfrom collections import OrderedDict\n\nimport torch\n\nfrom boxmot.appearance.reid.config import MODEL_TYPES, NR_CLASSES_DICT, TRAINED_URLS\nfrom boxmot.appearance.reid.factory import MODEL_FACTORY\nfrom boxmot.utils import logger as LOGGER\n\n\nclass ReIDModelRegistry:\n    """"""""""""Encapsulates model registration and related utilities.""""""""""""\n\n    @staticmethod\n    def show_downloadable_models():\n        LOGGER.info(""""Available .pt ReID models for automatic download"""")\n        LOGGER.info(list(TRAINED_URLS.keys()))\n\n    @staticmethod\n    def get_model_name(model):\n        for name in MODEL_TYPES:\n            if name in model.name:\n                return name\n        return None\n\n    @staticmethod\n    def get_model_url(model):\n        return TRAINED_URLS.get(model.name, None)\n\n    @staticmethod\n    def load_pretrained_weights(model, weight_path):\n        """"""""""""\n        Loads pretrained weights into a model.\n        Chooses the proper map_location","@@ -34,7 +34,9 @@ class ReIDModelRegistry:\n         Chooses the proper map_location based on CUDA availability.\n         """"""""""""\n         device = """"cpu"""" if not torch.cuda.is_available() else None\n-        checkpoint = torch.load(weight_path, map_location=torch.device(""""cpu"""") if device == """"cpu"""" else None)\n+        checkpoint = torch.load(\n+            weight_path, map_location=torch.device(""""cpu"""") if device == """"cpu"""" else None\n+        )\n         state_dict = checkpoint.get(""""state_dict"""", checkpoint)\n         model_dict = model.state_dict()\n \n@@ -53,14 +55,18 @@ class ReIDModelRegistry:\n                     discarded_layers.append(key)\n             model_dict.update(new_state_dict)\n             model.load_state_dict(model_dict)\n-            \n+\n             if not matched_layers:\n-                LOGGER.debug(f""""Pretrained weights from {weight_path} cannot be loaded. Check key names manually."""")\n+                LOGGER.debug(\n+                    f""""Pretrained weights from {weight_path} cannot be loaded. Check key names manually.""""\n+                )\n             else:\n                 LOGGER.success(f""""Loaded pretrained weights from {weight_path}"""")\n \n             if discarded_layers:\n-                LOGGER.debug(f""""Discarded layers due to unmatched keys or size: {discarded_layers}"""")\n+                LOGGER.debug(\n+                    f""""Discarded layers due to unmatched keys or size: {discarded_layers}""""\n+                )\n \n     @staticmethod\n     def show_available_models():\n@@ -70,7 +76,7 @@ class ReIDModelRegistry:\n     @staticmethod\n     def get_nr_classes(weights):\n         # Extract dataset name from weights name, then look up in the class dictionary\n-        dataset_key = weights.name.split('_')[1]\n+        dataset_key = weights.name.split(""""_"""")[1]\n         return NR_CLASSES_DICT.get(dataset_key, 1)\n \n     @staticmethod\n@@ -80,10 +86,13 @@ class ReIDModelRegistry:\n             raise KeyError(f""""Unknown mod",add,Add Application class .
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/data/loader.py,"import glob\nimport math\nimport os\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nVID_FORMATS = """"asf"""", """"avi"""", """"gif"""", """"m4v"""", """"mkv"""", """"mov"""", """"mp4"""", """"mpeg"""", """"mpg"""", """"ts"""", """"wmv""""  # include video suffixes\n\n\nclass LoadImagesAndVideos:\n    """"""""""""\n    A data loader for handling both images and videos, providing batches of frames or images for processing.\n    Supports various image formats, including HEIC, and handles text files with paths to images/videos.\n    """"""""""""\n\n    def __init__(self, path, batch_size=1, vid_stride=1):\n        self.batch_size = batch_size\n        self.vid_stride = vid_stride\n        self.files = self._load_files(path)\n        self.video_flag = [self._is_video(f) for f in self.files]\n        self.nf = len(self.files)\n        self.ni = sum(not is_video for is_video in self.video_flag)\n        self.mode = """"image""""\n        \n        self.cap = None\n        if any(self.video_flag):\n            sel","import glob\nimport math\nimport os\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nVID_FORMATS = (\n    """"asf"""",\n    """"avi"""",\n    """"gif"""",\n    """"m4v"""",\n    """"mkv"""",\n    """"mov"""",\n    """"mp4"""",\n    """"mpeg"""",\n    """"mpg"""",\n    """"ts"""",\n    """"wmv"""",\n)  # include video suffixes\n\n\nclass LoadImagesAndVideos:\n    """"""""""""\n    A data loader for handling both images and videos, providing batches of frames or images for processing.\n    Supports various image formats, including HEIC, and handles text files with paths to images/videos.\n    """"""""""""\n\n    def __init__(self, path, batch_size=1, vid_stride=1):\n        self.batch_size = batch_size\n        self.vid_stride = vid_stride\n        self.files = self._load_files(path)\n        self.video_flag = [self._is_video(f) for f in self.files]\n        self.nf = len(self.files)\n        self.ni = sum(not is_video for is_video in self.video_flag)\n        self.mode = """"image""""\n\n        self.cap = No","@@ -7,7 +7,19 @@ import cv2\n import numpy as np\n from PIL import Image\n \n-VID_FORMATS = """"asf"""", """"avi"""", """"gif"""", """"m4v"""", """"mkv"""", """"mov"""", """"mp4"""", """"mpeg"""", """"mpg"""", """"ts"""", """"wmv""""  # include video suffixes\n+VID_FORMATS = (\n+    """"asf"""",\n+    """"avi"""",\n+    """"gif"""",\n+    """"m4v"""",\n+    """"mkv"""",\n+    """"mov"""",\n+    """"mp4"""",\n+    """"mpeg"""",\n+    """"mpg"""",\n+    """"ts"""",\n+    """"wmv"""",\n+)  # include video suffixes\n \n \n class LoadImagesAndVideos:\n@@ -24,11 +36,11 @@ class LoadImagesAndVideos:\n         self.nf = len(self.files)\n         self.ni = sum(not is_video for is_video in self.video_flag)\n         self.mode = """"image""""\n-        \n+\n         self.cap = None\n         if any(self.video_flag):\n             self._start_video(self.files[self.video_flag.index(True)])\n-        \n+\n         if not self.files:\n             raise FileNotFoundError(f""""No images or videos found in {path}."""")\n \n@@ -36,7 +48,7 @@ class LoadImagesAndVideos:\n         """"""""""""Load files from a given path, which may be a directory, list, or text file.""""""""""""\n         if isinstance(path, str) and Path(path).suffix == """".txt"""":\n             path = Path(path).read_text().splitlines()\n-        \n+\n         files = []\n         for p in sorted(path) if isinstance(path, (list, tuple)) else [path]:\n             p = str(Path(p).absolute())\n@@ -52,7 +64,7 @@ class LoadImagesAndVideos:\n \n     def _is_video(self, file_path):\n         """"""""""""Check if a file is a video based on its extension.""""""""""""\n-        return file_path.split('.')[-1].lower() in VID_FORMATS\n+        return file_path.split(""""."""")[-1].lower() in VID_FORMATS\n \n     def __iter__(self):\n         self.count = 0\n@@ -66,7 +78,7 @@ class LoadImagesAndVideos:\n                     return paths, imgs, infos\n                 else:\n                     raise StopIteration\n-            \n+\n             path = self.files[self.count]\n             if self.video_flag[self.count]:\n                 self._process_v",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/motion/cmc/__init__.py,# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom boxmot.motion.cmc.ecc import ECC\nfrom boxmot.motion.cmc.orb import ORB\nfrom boxmot.motion.cmc.sift import SIFT\nfrom boxmot.motion.cmc.sof import SOF\n\n\ndef get_cmc_method(cmc_method):\n    if cmc_method == 'ecc':\n        return ECC\n    elif cmc_method == 'orb':\n        return ORB\n    elif cmc_method == 'sof':\n        return SOF\n    elif cmc_method == 'sift':\n        return SIFT\n    else:\n        return None\n,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom boxmot.motion.cmc.ecc import ECC\nfrom boxmot.motion.cmc.orb import ORB\nfrom boxmot.motion.cmc.sift import SIFT\nfrom boxmot.motion.cmc.sof import SOF\n\n\ndef get_cmc_method(cmc_method):\n    if cmc_method == """"ecc"""":\n        return ECC\n    elif cmc_method == """"orb"""":\n        return ORB\n    elif cmc_method == """"sof"""":\n        return SOF\n    elif cmc_method == """"sift"""":\n        return SIFT\n    else:\n        return None\n","@@ -7,13 +7,13 @@ from boxmot.motion.cmc.sof import SOF\n \n \n def get_cmc_method(cmc_method):\n-    if cmc_method == 'ecc':\n+    if cmc_method == """"ecc"""":\n         return ECC\n-    elif cmc_method == 'orb':\n+    elif cmc_method == """"orb"""":\n         return ORB\n-    elif cmc_method == 'sof':\n+    elif cmc_method == """"sof"""":\n         return SOF\n-    elif cmc_method == 'sift':\n+    elif cmc_method == """"sift"""":\n         return SIFT\n     else:\n         return None\n",add,Added TypeSpec . h for global .
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/motion/cmc/base_cmc.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom abc import ABC, abstractmethod\n\nimport cv2\nimport numpy as np\n\n\nclass BaseCMC(ABC):\n\n    @abstractmethod\n    def apply(self, im):\n        pass\n\n    def generate_mask(self, img, dets, scale):\n        h, w = img.shape\n        mask = np.zeros_like(img)\n\n        mask[int(0.02 * h): int(0.98 * h), int(0.02 * w): int(0.98 * w)] = 255\n        if dets is not None:\n            for det in dets:\n                tlbr = np.multiply(det, scale).astype(int)\n                mask[tlbr[1]:tlbr[3], tlbr[0]:tlbr[2]] = 0\n\n        return mask\n\n    def preprocess(self, img):\n\n        # bgr2gray\n        if self.grayscale:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n        # resize\n        if self.scale is not None:\n            img = cv2.resize(\n                img,\n                (0, 0),\n                fx=self.scale,\n                fy=self.scale,\n                interpolation=cv2.INTER_LINEAR\n   ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom abc import ABC, abstractmethod\n\nimport cv2\nimport numpy as np\n\n\nclass BaseCMC(ABC):\n\n    @abstractmethod\n    def apply(self, im):\n        pass\n\n    def generate_mask(self, img, dets, scale):\n        h, w = img.shape\n        mask = np.zeros_like(img)\n\n        mask[int(0.02 * h) : int(0.98 * h), int(0.02 * w) : int(0.98 * w)] = 255\n        if dets is not None:\n            for det in dets:\n                tlbr = np.multiply(det, scale).astype(int)\n                mask[tlbr[1] : tlbr[3], tlbr[0] : tlbr[2]] = 0\n\n        return mask\n\n    def preprocess(self, img):\n\n        # bgr2gray\n        if self.grayscale:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n        # resize\n        if self.scale is not None:\n            img = cv2.resize(\n                img,\n                (0, 0),\n                fx=self.scale,\n                fy=self.scale,\n                interpolation=cv2.INTER_LINEA","@@ -16,11 +16,11 @@ class BaseCMC(ABC):\n         h, w = img.shape\n         mask = np.zeros_like(img)\n \n-        mask[int(0.02 * h): int(0.98 * h), int(0.02 * w): int(0.98 * w)] = 255\n+        mask[int(0.02 * h) : int(0.98 * h), int(0.02 * w) : int(0.98 * w)] = 255\n         if dets is not None:\n             for det in dets:\n                 tlbr = np.multiply(det, scale).astype(int)\n-                mask[tlbr[1]:tlbr[3], tlbr[0]:tlbr[2]] = 0\n+                mask[tlbr[1] : tlbr[3], tlbr[0] : tlbr[2]] = 0\n \n         return mask\n \n@@ -37,7 +37,7 @@ class BaseCMC(ABC):\n                 (0, 0),\n                 fx=self.scale,\n                 fy=self.scale,\n-                interpolation=cv2.INTER_LINEAR\n+                interpolation=cv2.INTER_LINEAR,\n             )\n \n         return img\n",add,Added STORM - 1273 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/motion/cmc/ecc.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.base_cmc import BaseCMC\nfrom boxmot.utils import BOXMOT\nfrom boxmot.utils import logger as LOGGER\n\n\nclass ECC(BaseCMC):\n    def __init__(\n        self,\n        warp_mode: int = cv2.MOTION_TRANSLATION,\n        eps: float = 1e-5,\n        max_iter: int = 100,\n        scale: float = 0.15,\n        align: bool = False,\n        grayscale: bool = True\n    ) -> None:\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n           ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.base_cmc import BaseCMC\nfrom boxmot.utils import BOXMOT\nfrom boxmot.utils import logger as LOGGER\n\n\nclass ECC(BaseCMC):\n    def __init__(\n        self,\n        warp_mode: int = cv2.MOTION_TRANSLATION,\n        eps: float = 1e-5,\n        max_iter: int = 100,\n        scale: float = 0.15,\n        align: bool = False,\n        grayscale: bool = True,\n    ) -> None:\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n          ","@@ -18,7 +18,7 @@ class ECC(BaseCMC):\n         max_iter: int = 100,\n         scale: float = 0.15,\n         align: bool = False,\n-        grayscale: bool = True\n+        grayscale: bool = True,\n     ) -> None:\n         """"""""""""Compute the warp matrix from src to dst.\n \n@@ -53,7 +53,11 @@ class ECC(BaseCMC):\n         self.grayscale = grayscale\n         self.scale = scale\n         self.warp_mode = warp_mode\n-        self.termination_criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, max_iter, eps)\n+        self.termination_criteria = (\n+            cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT,\n+            max_iter,\n+            eps,\n+        )\n         self.prev_img = None\n \n     def apply(self, img: np.ndarray, dets: np.ndarray = None) -> np.ndarray:\n@@ -87,18 +91,18 @@ class ECC(BaseCMC):\n                 self.warp_mode,\n                 self.termination_criteria,\n                 None,\n-                1\n+                1,\n             )\n         except cv2.error as e:\n             # error 7 is StsNoConv, according to https://docs.opencv.org/3.4/d1/d0d/namespacecv_1_1Error.html\n             if e.code == cv2.Error.StsNoConv:\n-                LOGGER.warning(f'Affine matrix could not be generated: {e}. Returning identity')\n+                LOGGER.warning(\n+                    f""""Affine matrix could not be generated: {e}. Returning identity""""\n+                )\n                 return warp_matrix\n-            else: # other error codes\n+            else:  # other error codes\n                 raise\n \n-            \n-\n         # upscale warp matrix to original images size\n         if self.scale < 1:\n             warp_matrix[0, 2] /= self.scale\n@@ -108,10 +112,14 @@ class ECC(BaseCMC):\n             h, w = self.prev_img.shape\n             if self.warp_mode == cv2.MOTION_HOMOGRAPHY:\n                 # Use warpPerspective for Homography\n-                self.prev_img_aligned = cv2.warpPerspective(self.prev_img, warp",add,Added STORM - 132 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/motion/cmc/orb.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.base_cmc import BaseCMC\nfrom boxmot.utils import BOXMOT\n\n\nclass ORB(BaseCMC):\n\n    def __init__(\n        self,\n        feature_detector_threshold: int = 20,\n        matcher_norm_type: int = cv2.NORM_HAMMING,\n        scale: float = 0.15,\n        grayscale: bool = True,\n        draw_keypoint_matches: bool = False,\n        align: bool = False\n    ) -> None:\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        feature_detector_threshold: int, optional\n            The threshold for feature extraction. Defaults to 20.\n        matcher_norm_type: int, optional\n            The norm type of the matcher. Defaults to cv2.NORM_HAMMING.\n        scale: float, optional\n            Scale ratio. Defaults to 0.1.\n        grayscale: bool, optional\n            Whether to transform 3-channel RGB ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.base_cmc import BaseCMC\nfrom boxmot.utils import BOXMOT\n\n\nclass ORB(BaseCMC):\n\n    def __init__(\n        self,\n        feature_detector_threshold: int = 20,\n        matcher_norm_type: int = cv2.NORM_HAMMING,\n        scale: float = 0.15,\n        grayscale: bool = True,\n        draw_keypoint_matches: bool = False,\n        align: bool = False,\n    ) -> None:\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        feature_detector_threshold: int, optional\n            The threshold for feature extraction. Defaults to 20.\n        matcher_norm_type: int, optional\n            The norm type of the matcher. Defaults to cv2.NORM_HAMMING.\n        scale: float, optional\n            Scale ratio. Defaults to 0.1.\n        grayscale: bool, optional\n            Whether to transform 3-channel RGB","@@ -19,7 +19,7 @@ class ORB(BaseCMC):\n         scale: float = 0.15,\n         grayscale: bool = True,\n         draw_keypoint_matches: bool = False,\n-        align: bool = False\n+        align: bool = False,\n     ) -> None:\n         """"""""""""Compute the warp matrix from src to dst.\n \n@@ -42,7 +42,9 @@ class ORB(BaseCMC):\n         self.grayscale = grayscale\n         self.scale = scale\n \n-        self.detector = cv2.FastFeatureDetector_create(threshold=feature_detector_threshold)\n+        self.detector = cv2.FastFeatureDetector_create(\n+            threshold=feature_detector_threshold\n+        )\n         self.extractor = cv2.ORB_create()\n         self.matcher = cv2.BFMatcher(matcher_norm_type)\n \n@@ -113,18 +115,23 @@ class ORB(BaseCMC):\n                 prevKeyPointLocation = self.prev_keypoints[m.queryIdx].pt\n                 currKeyPointLocation = keypoints[m.trainIdx].pt\n \n-                spatial_distance = (prevKeyPointLocation[0] - currKeyPointLocation[0],\n-                                    prevKeyPointLocation[1] - currKeyPointLocation[1])\n+                spatial_distance = (\n+                    prevKeyPointLocation[0] - currKeyPointLocation[0],\n+                    prevKeyPointLocation[1] - currKeyPointLocation[1],\n+                )\n \n-                if (np.abs(spatial_distance[0]) < max_spatial_distance[0]) and \\n-                        (np.abs(spatial_distance[1]) < max_spatial_distance[1]):\n+                if (np.abs(spatial_distance[0]) < max_spatial_distance[0]) and (\n+                    np.abs(spatial_distance[1]) < max_spatial_distance[1]\n+                ):\n                     spatial_distances.append(spatial_distance)\n                     matches.append(m)\n \n         mean_spatial_distances = np.mean(spatial_distances, 0)\n         std_spatial_distances = np.std(spatial_distances, 0)\n \n-        inliesrs = (spatial_distances - mean_spatial_distances) < 2.5 * std_spatial_distances\n+        inliesrs = (\n+   ",add,Added auto permission to the image
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/motion/cmc/sift.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.base_cmc import BaseCMC\nfrom boxmot.utils import BOXMOT\n\n\nclass SIFT(BaseCMC):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.15,\n        grayscale=True,\n        draw_keypoint_matches=False,\n        align=False\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: floa","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport copy\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.base_cmc import BaseCMC\nfrom boxmot.utils import BOXMOT\n\n\nclass SIFT(BaseCMC):\n\n    def __init__(\n        self,\n        warp_mode=cv2.MOTION_EUCLIDEAN,\n        eps=1e-5,\n        max_iter=100,\n        scale=0.15,\n        grayscale=True,\n        draw_keypoint_matches=False,\n        align=False,\n    ):\n        """"""""""""Compute the warp matrix from src to dst.\n\n        Parameters\n        ----------\n        warp_mode: opencv flag\n            translation: cv2.MOTION_TRANSLATION\n            rotated and shifted: cv2.MOTION_EUCLIDEAN\n            affine(shift,rotated,shear): cv2.MOTION_AFFINE\n            homography(3d): cv2.MOTION_HOMOGRAPHY\n        eps: float\n            the threshold of the increment in the correlation coefficient between two iterations\n        max_iter: int\n            the number of iterations.\n        scale: flo","@@ -20,7 +20,7 @@ class SIFT(BaseCMC):\n         scale=0.15,\n         grayscale=True,\n         draw_keypoint_matches=False,\n-        align=False\n+        align=False,\n     ):\n         """"""""""""Compute the warp matrix from src to dst.\n \n@@ -133,11 +133,14 @@ class SIFT(BaseCMC):\n                 prevKeyPointLocation = self.prev_keypoints[m.queryIdx].pt\n                 currKeyPointLocation = keypoints[m.trainIdx].pt\n \n-                spatial_distance = (prevKeyPointLocation[0] - currKeyPointLocation[0],\n-                                    prevKeyPointLocation[1] - currKeyPointLocation[1])\n+                spatial_distance = (\n+                    prevKeyPointLocation[0] - currKeyPointLocation[0],\n+                    prevKeyPointLocation[1] - currKeyPointLocation[1],\n+                )\n \n-                if (np.abs(spatial_distance[0]) < max_spatial_distance[0]) and \\n-                        (np.abs(spatial_distance[1]) < max_spatial_distance[1]):\n+                if (np.abs(spatial_distance[0]) < max_spatial_distance[0]) and (\n+                    np.abs(spatial_distance[1]) < max_spatial_distance[1]\n+                ):\n                     spatial_distances.append(spatial_distance)\n                     matches.append(m)\n \n@@ -181,7 +184,9 @@ class SIFT(BaseCMC):\n             self.matches_img = None\n \n         # find rigid matrix\n-        if (np.size(prevPoints, 0) > 4) and (np.size(prevPoints, 0) == np.size(prevPoints, 0)):\n+        if (np.size(prevPoints, 0) > 4) and (\n+            np.size(prevPoints, 0) == np.size(prevPoints, 0)\n+        ):\n             H, inliers = cv2.estimateAffinePartial2D(prevPoints, currPoints, cv2.RANSAC)\n \n             # upscale warp matrix to original images size\n@@ -190,9 +195,11 @@ class SIFT(BaseCMC):\n                 H[1, 2] /= self.scale\n \n             if self.align:\n-                self.prev_img_aligned = cv2.warpAffine(self.prev_img, H, (w, h), flags=cv2.INTER_LINEAR)\n+                se",add,Fix bytesPerPixel for SurfaceTexture
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/motion/cmc/sof.py,"import cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.base_cmc import BaseCMC\n\n\nclass SOF(BaseCMC):\n    """"""""""""\n    Sparse Optical Flow (SOF) tracker for estimating a 2x3 warp (affine transformation)\n    between consecutive frames. This class is modeled after a GMC implementation using\n    the 'sparseOptFlow' method.\n    """"""""""""\n    def __init__(self, scale=0.15):\n        """"""""""""\n        Initialize the SOF object.\n\n        Parameters\n        ----------\n        downscale : int, optional\n            Factor by which to downscale the input frames. Defaults to 1 (no downscale).\n        feature_params : dict, optional\n            Parameters for cv2.goodFeaturesToTrack. Defaults to:\n                {\n                    maxCorners: 1000,\n                    qualityLevel: 0.01,\n                    minDistance: 1,\n                    blockSize: 3,\n                    useHarrisDetector: False,\n                    k: 0.04\n                }\n        lk_params : dict, opti","import cv2\nimport numpy as np\n\nfrom boxmot.motion.cmc.base_cmc import BaseCMC\n\n\nclass SOF(BaseCMC):\n    """"""""""""\n    Sparse Optical Flow (SOF) tracker for estimating a 2x3 warp (affine transformation)\n    between consecutive frames. This class is modeled after a GMC implementation using\n    the 'sparseOptFlow' method.\n    """"""""""""\n\n    def __init__(self, scale=0.15):\n        """"""""""""\n        Initialize the SOF object.\n\n        Parameters\n        ----------\n        downscale : int, optional\n            Factor by which to downscale the input frames. Defaults to 1 (no downscale).\n        feature_params : dict, optional\n            Parameters for cv2.goodFeaturesToTrack. Defaults to:\n                {\n                    maxCorners: 1000,\n                    qualityLevel: 0.01,\n                    minDistance: 1,\n                    blockSize: 3,\n                    useHarrisDetector: False,\n                    k: 0.04\n                }\n        lk_params : dict, op","@@ -10,6 +10,7 @@ class SOF(BaseCMC):\n     between consecutive frames. This class is modeled after a GMC implementation using\n     the 'sparseOptFlow' method.\n     """"""""""""\n+\n     def __init__(self, scale=0.15):\n         """"""""""""\n         Initialize the SOF object.\n@@ -38,7 +39,7 @@ class SOF(BaseCMC):\n         """"""""""""\n         self.scale = scale\n         self.grayscale = True\n-        \n+\n         # Set default feature detection parameters if not provided\n         self.feature_params = dict(\n             maxCorners=1000,\n@@ -46,14 +47,14 @@ class SOF(BaseCMC):\n             minDistance=1,\n             blockSize=3,\n             useHarrisDetector=False,\n-            k=0.04\n+            k=0.04,\n         )\n \n         # Set default Lucas-Kanade optical flow parameters if not provided.\n         self.lk_params = dict(\n             winSize=(21, 21),\n             maxLevel=3,\n-            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01)\n+            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01),\n         )\n \n         self.prevFrame = None\n@@ -86,12 +87,20 @@ class SOF(BaseCMC):\n \n         # On the first frame, detect keypoints and initialize internal state.\n         if not self.initializedFirstFrame:\n-            keypoints = cv2.goodFeaturesToTrack(frame_gray, mask=None, **self.feature_params)\n+            keypoints = cv2.goodFeaturesToTrack(\n+                frame_gray, mask=None, **self.feature_params\n+            )\n             if keypoints is None:\n                 return H\n             # Optional subpixel refinement.\n             term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01)\n-            cv2.cornerSubPix(frame_gray, keypoints, winSize=(5, 5), zeroZone=(-1, -1), criteria=term_crit)\n+            cv2.cornerSubPix(\n+                frame_gray,\n+                keypoints,\n+                winSize=(5, 5),\n+                zeroZone=(-1, -1),\n+                criter",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/motion/kalman_filters/aabb/base_kalman_filter.py,"from typing import Tuple\n\nimport numpy as np\nimport scipy.linalg\n\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919\n}\n\nclass BaseKalmanFilter:\n    """"""""""""\n    Base class for Kalman filters tracking bounding boxes in image space.\n    """"""""""""\n\n    def __init__(self, ndim: int):\n        self.ndim = ndim\n        self.dt = 1.\n\n        # Create Kalman filter model matrices.\n        self._motion_mat = np.eye(2 * ndim, 2 * ndim)  # State transition matrix\n        for i in range(ndim):\n            self._motion_mat[i, ndim + i] = self.dt\n        self._update_mat = np.eye(ndim, 2 * ndim)  # Observation matrix\n\n        # Motion and observation uncerta","from typing import Tuple\n\nimport numpy as np\nimport scipy.linalg\n\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919,\n}\n\n\nclass BaseKalmanFilter:\n    """"""""""""\n    Base class for Kalman filters tracking bounding boxes in image space.\n    """"""""""""\n\n    def __init__(self, ndim: int):\n        self.ndim = ndim\n        self.dt = 1.0\n\n        # Create Kalman filter model matrices.\n        self._motion_mat = np.eye(2 * ndim, 2 * ndim)  # State transition matrix\n        for i in range(ndim):\n            self._motion_mat[i, ndim + i] = self.dt\n        self._update_mat = np.eye(ndim, 2 * ndim)  # Observation matrix\n\n        # Motion and observation unc","@@ -17,9 +17,10 @@ chi2inv95 = {\n     6: 12.592,\n     7: 14.067,\n     8: 15.507,\n-    9: 16.919\n+    9: 16.919,\n }\n \n+\n class BaseKalmanFilter:\n     """"""""""""\n     Base class for Kalman filters tracking bounding boxes in image space.\n@@ -27,7 +28,7 @@ class BaseKalmanFilter:\n \n     def __init__(self, ndim: int):\n         self.ndim = ndim\n-        self.dt = 1.\n+        self.dt = 1.0\n \n         # Create Kalman filter model matrices.\n         self._motion_mat = np.eye(2 * ndim, 2 * ndim)  # State transition matrix\n@@ -36,8 +37,8 @@ class BaseKalmanFilter:\n         self._update_mat = np.eye(ndim, 2 * ndim)  # Observation matrix\n \n         # Motion and observation uncertainty weights.\n-        self._std_weight_position = 1. / 20\n-        self._std_weight_velocity = 1. / 160\n+        self._std_weight_position = 1.0 / 20\n+        self._std_weight_velocity = 1.0 / 160\n \n     def initiate(self, measurement: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n         """"""""""""\n@@ -58,7 +59,9 @@ class BaseKalmanFilter:\n         """"""""""""\n         raise NotImplementedError\n \n-    def predict(self, mean: np.ndarray, covariance: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n+    def predict(\n+        self, mean: np.ndarray, covariance: np.ndarray\n+    ) -> Tuple[np.ndarray, np.ndarray]:\n         """"""""""""\n         Run Kalman filter prediction step.\n         """"""""""""\n@@ -66,8 +69,10 @@ class BaseKalmanFilter:\n         motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\n \n         mean = np.dot(mean, self._motion_mat.T)\n-        covariance = np.linalg.multi_dot((\n-            self._motion_mat, covariance, self._motion_mat.T)) + motion_cov\n+        covariance = (\n+            np.linalg.multi_dot((self._motion_mat, covariance, self._motion_mat.T))\n+            + motion_cov\n+        )\n \n         return mean, covariance\n \n@@ -78,13 +83,15 @@ class BaseKalmanFilter:\n         """"""""""""\n         raise NotImplementedError\n \n-    def project(self, ",add,Add note about data volume to enable EGL
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/motion/kalman_filters/aabb/xyah_kf.py,"from typing import Tuple\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.aabb.base_kalman_filter import BaseKalmanFilter\n\n\nclass KalmanFilterXYAH(BaseKalmanFilter):\n    """"""""""""\n    A Kalman filter for tracking bounding boxes in image space with state space:\n        x, y, a, h, vx, vy, va, vh\n    """"""""""""\n\n    def __init__(self):\n        super().__init__(ndim=4)\n\n    def _get_initial_covariance_std(self, measurement: np.ndarray) -> np.ndarray:\n        # initial uncertainty in the aspect ratio is very low,\n        # suggesting that it is not expected to vary significantly.\n        return [\n            2 * self._std_weight_position * measurement[3],     # x\n            2 * self._std_weight_position * measurement[3],     # y\n            1e-2,                                               # a (aspect ratio)\n            2 * self._std_weight_position * measurement[3],     # H\n            10 * self._std_weight_velocity * measurement[3],    # vx\n            10 * sel","from typing import Tuple\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.aabb.base_kalman_filter import BaseKalmanFilter\n\n\nclass KalmanFilterXYAH(BaseKalmanFilter):\n    """"""""""""\n    A Kalman filter for tracking bounding boxes in image space with state space:\n        x, y, a, h, vx, vy, va, vh\n    """"""""""""\n\n    def __init__(self):\n        super().__init__(ndim=4)\n\n    def _get_initial_covariance_std(self, measurement: np.ndarray) -> np.ndarray:\n        # initial uncertainty in the aspect ratio is very low,\n        # suggesting that it is not expected to vary significantly.\n        return [\n            2 * self._std_weight_position * measurement[3],     # x\n            2 * self._std_weight_position * measurement[3],     # y\n            1e-2,                                               # a (aspect ratio)\n            2 * self._std_weight_position * measurement[3],     # H\n            10 * self._std_weight_velocity * measurement[3],    # vx\n            10 * sel","@@ -37,17 +37,19 @@ class KalmanFilterXYAH(BaseKalmanFilter):\n             self._std_weight_position * mean[3],\n             self._std_weight_position * mean[3],\n             1e-2,\n-            self._std_weight_position * mean[3]\n+            self._std_weight_position * mean[3],\n         ]\n         std_vel = [\n             self._std_weight_velocity * mean[3],\n             self._std_weight_velocity * mean[3],\n             1e-5,\n-            self._std_weight_velocity * mean[3]\n+            self._std_weight_velocity * mean[3],\n         ]\n         return std_pos, std_vel\n \n-    def _get_measurement_noise_std(self, mean: np.ndarray, confidence: float) -> np.ndarray:\n+    def _get_measurement_noise_std(\n+        self, mean: np.ndarray, confidence: float\n+    ) -> np.ndarray:\n         # small measurement noise standard deviation for\n         # aspect ratio state, indicating low expected measurement noise in\n         # the aspect ratio.\n@@ -55,21 +57,23 @@ class KalmanFilterXYAH(BaseKalmanFilter):\n             self._std_weight_position * mean[3],\n             self._std_weight_position * mean[3],\n             1e-1,\n-            self._std_weight_position * mean[3]\n+            self._std_weight_position * mean[3],\n         ]\n         return std_noise\n \n-    def _get_multi_process_noise_std(self, mean: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n+    def _get_multi_process_noise_std(\n+        self, mean: np.ndarray\n+    ) -> Tuple[np.ndarray, np.ndarray]:\n         std_pos = [\n             self._std_weight_position * mean[:, 3],\n             self._std_weight_position * mean[:, 3],\n             1e-2 * np.ones_like(mean[:, 3]),\n-            self._std_weight_position * mean[:, 3]\n+            self._std_weight_position * mean[:, 3],\n         ]\n         std_vel = [\n             self._std_weight_velocity * mean[:, 3],\n             self._std_weight_velocity * mean[:, 3],\n             1e-5 * np.ones_like(mean[:, 3]),\n-            self._st",add,Added format51l
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/motion/kalman_filters/aabb/xysr_kf.py,"""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","""""""""""""\nThis module implements the linear Kalman filter in both an object\noriented and procedural form. The KalmanFilter class implements\nthe filter by storing the various matrices in instance variables,\nminimizing the amount of bookkeeping you have to do.\nAll Kalman filters operate with a predict->update cycle. The\npredict step, implemented with the method or function predict(),\nuses the state transition matrix F to predict the state in the next\ntime period (epoch). The state is stored as a gaussian (x, P), where\nx is the state (column) vector, and P is its covariance. Covariance\nmatrix Q specifies the process covariance. In Bayesian terms, this\nprediction is called the *prior*, which you can think of colloquially\nas the estimate prior to incorporating the measurement.\nThe update step, implemented with the method or function `update()`,\nincorporates the measurement z with covariance R, into the state\nestimate (x, P). The class stores the system uncertainty in S,\nthe inn","@@ -46,18 +46,18 @@ from numpy import dot, eye, isscalar, zeros\n \n \n class KalmanFilterXYSR(object):\n-    """""""""""" Implements a Kalman filter. You are responsible for setting the\n+    """"""""""""Implements a Kalman filter. You are responsible for setting the\n     various state variables to reasonable values; the defaults will\n     not give you a functional filter.\n     """"""""""""\n \n     def __init__(self, dim_x, dim_z, dim_u=0, max_obs=50):\n         if dim_x < 1:\n-            raise ValueError('dim_x must be 1 or greater')\n+            raise ValueError(""""dim_x must be 1 or greater"""")\n         if dim_z < 1:\n-            raise ValueError('dim_z must be 1 or greater')\n+            raise ValueError(""""dim_z must be 1 or greater"""")\n         if dim_u < 0:\n-            raise ValueError('dim_u must be 0 or greater')\n+            raise ValueError(""""dim_u must be 0 or greater"""")\n \n         self.dim_x = dim_x\n         self.dim_z = dim_z\n@@ -77,10 +77,10 @@ class KalmanFilterXYSR(object):\n         # gain and residual are computed during the innovation step. We\n         # save them so that in case you want to inspect them for various\n         # purposes\n-        self.K = np.zeros((dim_x, dim_z)) # kalman gain\n+        self.K = np.zeros((dim_x, dim_z))  # kalman gain\n         self.y = zeros((dim_z, 1))\n-        self.S = np.zeros((dim_z, dim_z)) # system uncertainty\n-        self.SI = np.zeros((dim_z, dim_z)) # inverse system uncertainty\n+        self.S = np.zeros((dim_z, dim_z))  # system uncertainty\n+        self.SI = np.zeros((dim_z, dim_z))  # inverse system uncertainty\n \n         # identity matrix. Do not alter this.\n         self._I = np.eye(dim_x)\n@@ -90,7 +90,7 @@ class KalmanFilterXYSR(object):\n         self.P_prior = self.P.copy()\n \n         # these will always be a copy of x,P after update() is called\n-        self.x_post = self.x.copy()             \n+        self.x_post = self.x.copy()\n         self.P_post = self.P.copy()\n \n         # On",add,Added STORM - 146 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/motion/kalman_filters/aabb/xywh_kf.py,"from typing import Tuple\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.aabb.base_kalman_filter import BaseKalmanFilter\n\n\nclass KalmanFilterXYWH(BaseKalmanFilter):\n    """"""""""""\n    A Kalman filter for tracking bounding boxes in image space with state space:\n        x, y, w, h, vx, vy, vw, vh\n    """"""""""""\n\n    def __init__(self):\n        super().__init__(ndim=4)\n\n    def _get_initial_covariance_std(self, measurement: np.ndarray) -> np.ndarray:\n        return [\n            2 * self._std_weight_position * measurement[2],\n            2 * self._std_weight_position * measurement[3],\n            2 * self._std_weight_position * measurement[2],\n            2 * self._std_weight_position * measurement[3],\n            10 * self._std_weight_velocity * measurement[2],\n            10 * self._std_weight_velocity * measurement[3],\n            10 * self._std_weight_velocity * measurement[2],\n            10 * self._std_weight_velocity * measurement[3]\n        ]\n\n    def _g","from typing import Tuple\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.aabb.base_kalman_filter import BaseKalmanFilter\n\n\nclass KalmanFilterXYWH(BaseKalmanFilter):\n    """"""""""""\n    A Kalman filter for tracking bounding boxes in image space with state space:\n        x, y, w, h, vx, vy, vw, vh\n    """"""""""""\n\n    def __init__(self):\n        super().__init__(ndim=4)\n\n    def _get_initial_covariance_std(self, measurement: np.ndarray) -> np.ndarray:\n        return [\n            2 * self._std_weight_position * measurement[2],\n            2 * self._std_weight_position * measurement[3],\n            2 * self._std_weight_position * measurement[2],\n            2 * self._std_weight_position * measurement[3],\n            10 * self._std_weight_velocity * measurement[2],\n            10 * self._std_weight_velocity * measurement[3],\n            10 * self._std_weight_velocity * measurement[2],\n            10 * self._std_weight_velocity * measurement[3],\n        ]\n\n    def _","@@ -23,7 +23,7 @@ class KalmanFilterXYWH(BaseKalmanFilter):\n             10 * self._std_weight_velocity * measurement[2],\n             10 * self._std_weight_velocity * measurement[3],\n             10 * self._std_weight_velocity * measurement[2],\n-            10 * self._std_weight_velocity * measurement[3]\n+            10 * self._std_weight_velocity * measurement[3],\n         ]\n \n     def _get_process_noise_std(self, mean: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n@@ -31,13 +31,13 @@ class KalmanFilterXYWH(BaseKalmanFilter):\n             self._std_weight_position * mean[2],\n             self._std_weight_position * mean[3],\n             self._std_weight_position * mean[2],\n-            self._std_weight_position * mean[3]\n+            self._std_weight_position * mean[3],\n         ]\n         std_vel = [\n             self._std_weight_velocity * mean[2],\n             self._std_weight_velocity * mean[3],\n             self._std_weight_velocity * mean[2],\n-            self._std_weight_velocity * mean[3]\n+            self._std_weight_velocity * mean[3],\n         ]\n         return std_pos, std_vel\n \n@@ -46,7 +46,7 @@ class KalmanFilterXYWH(BaseKalmanFilter):\n             self._std_weight_position * mean[2],\n             self._std_weight_position * mean[3],\n             self._std_weight_position * mean[2],\n-            self._std_weight_position * mean[3]\n+            self._std_weight_position * mean[3],\n         ]\n         return std_noise\n     \n@@ -55,12 +55,12 @@ class KalmanFilterXYWH(BaseKalmanFilter):\n             self._std_weight_position * mean[:, 2],\n             self._std_weight_position * mean[:, 3],\n             self._std_weight_position * mean[:, 2],\n-            self._std_weight_position * mean[:, 3]\n+            self._std_weight_position * mean[:, 3],\n         ]\n         std_vel = [\n             self._std_weight_velocity * mean[:, 2],\n             self._std_weight_velocity * mean[:, 3],\n             self._std_weight_",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/motion/kalman_filters/obb/xywha_kf.py,"from __future__ import absolute_import, division\n\nimport sys\nfrom collections import deque\nfrom copy import deepcopy\nfrom math import exp, log, pi\n\nimport numpy as np\nimport numpy.linalg as linalg\nfrom filterpy.common import reshape_z\nfrom filterpy.stats import logpdf\nfrom numpy import dot, eye, isscalar, zeros\n\n\ndef speed_direction_obb(bbox1, bbox2):\n    cx1, cy1 = bbox1[0], bbox1[1]\n    cx2, cy2 = bbox2[0], bbox2[1]\n    speed = np.array([cy2 - cy1, cx2 - cx1])\n    norm = np.sqrt((cy2 - cy1) ** 2 + (cx2 - cx1) ** 2) + 1e-6\n    return speed / norm\n\n\nclass KalmanBoxTrackerOBB(object):\n    """"""""""""\n    This class represents the internal state of individual tracked objects observed as oriented bbox.\n    """"""""""""\n\n    count = 0\n\n    def __init__(self, bbox, cls, det_ind, delta_t=3, max_obs=50, Q_xy_scaling = 0.01, Q_a_scaling = 0.01):\n        """"""""""""\n        Initialises a tracker using initial bounding box.\n        """"""""""""\n        # define constant velocity model","from __future__ import absolute_import, division\n\nimport sys\nfrom collections import deque\nfrom copy import deepcopy\nfrom math import exp, log, pi\n\nimport numpy as np\nimport numpy.linalg as linalg\nfrom filterpy.common import reshape_z\nfrom filterpy.stats import logpdf\nfrom numpy import dot, eye, isscalar, zeros\n\n\ndef speed_direction_obb(bbox1, bbox2):\n    cx1, cy1 = bbox1[0], bbox1[1]\n    cx2, cy2 = bbox2[0], bbox2[1]\n    speed = np.array([cy2 - cy1, cx2 - cx1])\n    norm = np.sqrt((cy2 - cy1) ** 2 + (cx2 - cx1) ** 2) + 1e-6\n    return speed / norm\n\n\nclass KalmanBoxTrackerOBB(object):\n    """"""""""""\n    This class represents the internal state of individual tracked objects observed as oriented bbox.\n    """"""""""""\n\n    count = 0\n\n    def __init__(\n        self,\n        bbox,\n        cls,\n        det_ind,\n        delta_t=3,\n        max_obs=50,\n        Q_xy_scaling=0.01,\n        Q_a_scaling=0.01,\n    ):\n        """"""""""""\n        Initialises a tracker using ini","@@ -27,7 +27,16 @@ class KalmanBoxTrackerOBB(object):\n \n     count = 0\n \n-    def __init__(self, bbox, cls, det_ind, delta_t=3, max_obs=50, Q_xy_scaling = 0.01, Q_a_scaling = 0.01):\n+    def __init__(\n+        self,\n+        bbox,\n+        cls,\n+        det_ind,\n+        delta_t=3,\n+        max_obs=50,\n+        Q_xy_scaling=0.01,\n+        Q_a_scaling=0.01,\n+    ):\n         """"""""""""\n         Initialises a tracker using initial bounding box.\n         """"""""""""\n@@ -45,21 +54,21 @@ class KalmanBoxTrackerOBB(object):\n                 [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],  # w = w + vw\n                 [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],  # h = h + vh\n                 [0, 0, 0, 0, 1, 0, 0, 0, 0, 1],  # a = a + va\n-                [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], \n-                [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], \n-                [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], \n-                [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], \n-                [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n-        ] \n-    )\n+                [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n+                [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n+                [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n+                [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n+                [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n+            ]\n+        )\n         self.kf.H = np.array(\n             [\n-                [1, 0, 0, 0, 0, 0, 0, 0, 0 ,0],  # cx\n+                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # cx\n                 [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],  # cy\n                 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],  # w\n                 [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],  # h\n                 [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],  # angle\n-    ]\n+            ]\n         )\n \n         self.kf.R[2:, 2:] *= 10.0\n@@ -135,9 +144,9 @@ class KalmanBoxTrackerOBB(object):\n         """"""""""""\n         Advances the state vector and returns the predicted bounding box estimate.\n         """"""""""""\n-        if (self.kf.x[7] + self.kf.x[2]) <= 0: # Negative width\n+        if (self.kf.x[",add,Added STORM - 1404 to the contributors
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/postprocessing/gsi.py,"import argparse\nimport concurrent.futures\nfrom pathlib import Path\n\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor as GPR\nfrom sklearn.gaussian_process.kernels import RBF\nfrom tqdm import tqdm\n\nfrom boxmot.utils import logger as LOGGER\n\n\ndef linear_interpolation(data: np.ndarray, interval: int) -> np.ndarray:\n    """"""""""""\n    Apply linear interpolation between rows in the tracking results.\n    \n    The function assumes the first two columns of `data` represent frame number and object ID.\n    Interpolated rows are added when consecutive rows for the same ID have a gap of more than 1\n    frame but less than the specified interval.\n    \n    Parameters:\n        data (np.ndarray): Input tracking results.\n        interval (int): Maximum gap to perform interpolation.\n    \n    Returns:\n        np.ndarray: Tracking results with interpolated rows included.\n    """"""""""""\n    # Sort data by frame and then by ID\n    sorted_data = data[np.lexs","import argparse\nimport concurrent.futures\nfrom pathlib import Path\n\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor as GPR\nfrom sklearn.gaussian_process.kernels import RBF\nfrom tqdm import tqdm\n\nfrom boxmot.utils import logger as LOGGER\n\n\ndef linear_interpolation(data: np.ndarray, interval: int) -> np.ndarray:\n    """"""""""""\n    Apply linear interpolation between rows in the tracking results.\n\n    The function assumes the first two columns of `data` represent frame number and object ID.\n    Interpolated rows are added when consecutive rows for the same ID have a gap of more than 1\n    frame but less than the specified interval.\n\n    Parameters:\n        data (np.ndarray): Input tracking results.\n        interval (int): Maximum gap to perform interpolation.\n\n    Returns:\n        np.ndarray: Tracking results with interpolated rows included.\n    """"""""""""\n    # Sort data by frame and then by ID\n    sorted_data = data[np.lexsort((data[:,","@@ -13,15 +13,15 @@ from boxmot.utils import logger as LOGGER\n def linear_interpolation(data: np.ndarray, interval: int) -> np.ndarray:\n     """"""""""""\n     Apply linear interpolation between rows in the tracking results.\n-    \n+\n     The function assumes the first two columns of `data` represent frame number and object ID.\n     Interpolated rows are added when consecutive rows for the same ID have a gap of more than 1\n     frame but less than the specified interval.\n-    \n+\n     Parameters:\n         data (np.ndarray): Input tracking results.\n         interval (int): Maximum gap to perform interpolation.\n-    \n+\n     Returns:\n         np.ndarray: Tracking results with interpolated rows included.\n     """"""""""""\n@@ -34,11 +34,17 @@ def linear_interpolation(data: np.ndarray, interval: int) -> np.ndarray:\n \n     for row in sorted_data:\n         current_frame, current_id = int(row[0]), int(row[1])\n-        if previous_id is not None and current_id == previous_id and previous_frame + 1 < current_frame < previous_frame + interval:\n+        if (\n+            previous_id is not None\n+            and current_id == previous_id\n+            and previous_frame + 1 < current_frame < previous_frame + interval\n+        ):\n             gap = current_frame - previous_frame - 1\n             for i in range(1, gap + 1):\n                 # Linear interpolation for each missing frame\n-                new_row = previous_row + (row - previous_row) * (i / (current_frame - previous_frame))\n+                new_row = previous_row + (row - previous_row) * (\n+                    i / (current_frame - previous_frame)\n+                )\n                 result_rows.append(new_row)\n         result_rows.append(row)\n         previous_id, previous_frame, previous_row = current_id, current_frame, row\n@@ -51,15 +57,15 @@ def linear_interpolation(data: np.ndarray, interval: int) -> np.ndarray:\n def gaussian_smooth(data: np.ndarray, tau: float) -> np.ndarray:\n     """"""""""""\n",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/tracker_zoo.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport yaml\n\nfrom boxmot.utils import TRACKER_CONFIGS\n\n\ndef get_tracker_config(tracker_type):\n    """"""""""""Returns the path to the tracker configuration file.""""""""""""\n    return TRACKER_CONFIGS / f'{tracker_type}.yaml'\n\ndef create_tracker(tracker_type, tracker_config=None, reid_weights=None, device=None, half=None, per_class=None, evolve_param_dict=None):\n    """"""""""""\n    Creates and returns an instance of the specified tracker type.\n    \n    Parameters:\n    - tracker_type: The type of the tracker (e.g., 'strongsort', 'ocsort').\n    - tracker_config: Path to the tracker configuration file.\n    - reid_weights: Weights for ReID (re-identification).\n    - device: Device to run the tracker on (e.g., 'cpu', 'cuda').\n    - half: Boolean indicating whether to use half-precision.\n    - per_class: Boolean for class-specific tracking (optional).\n    - evolve_param_dict: A dictionary of parameters for evolving the tracker.\n    \","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport yaml\n\nfrom boxmot.utils import TRACKER_CONFIGS\n\n\ndef get_tracker_config(tracker_type):\n    """"""""""""Returns the path to the tracker configuration file.""""""""""""\n    return TRACKER_CONFIGS / f""""{tracker_type}.yaml""""\n\n\ndef create_tracker(\n    tracker_type,\n    tracker_config=None,\n    reid_weights=None,\n    device=None,\n    half=None,\n    per_class=None,\n    evolve_param_dict=None,\n):\n    """"""""""""\n    Creates and returns an instance of the specified tracker type.\n\n    Parameters:\n    - tracker_type: The type of the tracker (e.g., 'strongsort', 'ocsort').\n    - tracker_config: Path to the tracker configuration file.\n    - reid_weights: Weights for ReID (re-identification).\n    - device: Device to run the tracker on (e.g., 'cpu', 'cuda').\n    - half: Boolean indicating whether to use half-precision.\n    - per_class: Boolean for class-specific tracking (optional).\n    - evolve_param_dict: A dictionary of para","@@ -7,12 +7,21 @@ from boxmot.utils import TRACKER_CONFIGS\n \n def get_tracker_config(tracker_type):\n     """"""""""""Returns the path to the tracker configuration file.""""""""""""\n-    return TRACKER_CONFIGS / f'{tracker_type}.yaml'\n+    return TRACKER_CONFIGS / f""""{tracker_type}.yaml""""\n \n-def create_tracker(tracker_type, tracker_config=None, reid_weights=None, device=None, half=None, per_class=None, evolve_param_dict=None):\n+\n+def create_tracker(\n+    tracker_type,\n+    tracker_config=None,\n+    reid_weights=None,\n+    device=None,\n+    half=None,\n+    per_class=None,\n+    evolve_param_dict=None,\n+):\n     """"""""""""\n     Creates and returns an instance of the specified tracker type.\n-    \n+\n     Parameters:\n     - tracker_type: The type of the tracker (e.g., 'strongsort', 'ocsort').\n     - tracker_config: Path to the tracker configuration file.\n@@ -21,53 +30,61 @@ def create_tracker(tracker_type, tracker_config=None, reid_weights=None, device=\n     - half: Boolean indicating whether to use half-precision.\n     - per_class: Boolean for class-specific tracking (optional).\n     - evolve_param_dict: A dictionary of parameters for evolving the tracker.\n-    \n+\n     Returns:\n     - An instance of the selected tracker.\n     """"""""""""\n-    \n+\n     # Load configuration from file or use provided dictionary\n     if evolve_param_dict is None:\n         with open(tracker_config, """"r"""") as f:\n             yaml_config = yaml.load(f, Loader=yaml.FullLoader)\n-            tracker_args = {param: details['default'] for param, details in yaml_config.items()}\n+            tracker_args = {\n+                param: details[""""default""""] for param, details in yaml_config.items()\n+            }\n     else:\n         tracker_args = evolve_param_dict\n \n     # Arguments specific to ReID models\n     reid_args = {\n-        'reid_weights': reid_weights,\n-        'device': device,\n-        'half': half,\n+        """"reid_weights"""": reid_weights,\n+        """"device"""": dev",add,added KT - 1036 to list of EDU
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/basetracker.py,"import numpy as np\nimport cv2 as cv\nimport hashlib\nimport colorsys\nfrom abc import ABC, abstractmethod\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.iou import AssociationFunction\n\n\nclass BaseTracker(ABC):\n    def __init__(\n        self, \n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        max_obs: int = 50,\n        nr_classes: int = 80,\n        per_class: bool = False,\n        asso_func: str = 'iou',\n        is_obb: bool = False\n    ):\n        """"""""""""\n        Initialize the BaseTracker object with detection threshold, maximum age, minimum hits, \n        and Intersection Over Union (IOU) threshold for tracking objects in video frames.\n\n        Parameters:\n        - det_thresh (float): Detection threshold for considering detections.\n        - max_age (int): Maximum age of a track before it is considered lost.\n        - min_hits (int): Minimum number of detection hits ","import colorsys\nimport hashlib\nfrom abc import ABC, abstractmethod\n\nimport cv2 as cv\nimport numpy as np\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.iou import AssociationFunction\n\n\nclass BaseTracker(ABC):\n    def __init__(\n        self,\n        det_thresh: float = 0.3,\n        max_age: int = 30,\n        min_hits: int = 3,\n        iou_threshold: float = 0.3,\n        max_obs: int = 50,\n        nr_classes: int = 80,\n        per_class: bool = False,\n        asso_func: str = """"iou"""",\n        is_obb: bool = False,\n    ):\n        """"""""""""\n        Initialize the BaseTracker object with detection threshold, maximum age, minimum hits,\n        and Intersection Over Union (IOU) threshold for tracking objects in video frames.\n\n        Parameters:\n        - det_thresh (float): Detection threshold for considering detections.\n        - max_age (int): Maximum age of a track before it is considered lost.\n        - min_hits (int): Minimum number of detection ","@@ -1,15 +1,17 @@\n-import numpy as np\n-import cv2 as cv\n-import hashlib\n import colorsys\n+import hashlib\n from abc import ABC, abstractmethod\n+\n+import cv2 as cv\n+import numpy as np\n+\n from boxmot.utils import logger as LOGGER\n from boxmot.utils.iou import AssociationFunction\n \n \n class BaseTracker(ABC):\n     def __init__(\n-        self, \n+        self,\n         det_thresh: float = 0.3,\n         max_age: int = 30,\n         min_hits: int = 3,\n@@ -17,11 +19,11 @@ class BaseTracker(ABC):\n         max_obs: int = 50,\n         nr_classes: int = 80,\n         per_class: bool = False,\n-        asso_func: str = 'iou',\n-        is_obb: bool = False\n+        asso_func: str = """"iou"""",\n+        is_obb: bool = False,\n     ):\n         """"""""""""\n-        Initialize the BaseTracker object with detection threshold, maximum age, minimum hits, \n+        Initialize the BaseTracker object with detection threshold, maximum age, minimum hits,\n         and Intersection Over Union (IOU) threshold for tracking objects in video frames.\n \n         Parameters:\n@@ -42,21 +44,21 @@ class BaseTracker(ABC):\n         self.nr_classes = nr_classes\n         self.iou_threshold = iou_threshold\n         self.last_emb_size = None\n-        self.asso_func_name = asso_func+""""_obb"""" if is_obb else asso_func\n+        self.asso_func_name = asso_func + """"_obb"""" if is_obb else asso_func\n         self.is_obb = is_obb\n-        \n+\n         self.frame_count = 0\n         self.active_tracks = []  # This might be handled differently in derived classes\n         self.per_class_active_tracks = None\n         self._first_frame_processed = False  # Flag to track if the first frame has been processed\n         self._first_dets_processed = False\n-        \n+\n         # Initialize per-class active tracks\n         if self.per_class:\n             self.per_class_active_tracks = {}\n             for i in range(self.nr_classes):\n                 self.per_class_active_tracks[i] = []\n-  ",add,Added note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/boosttrack/assoc.py,"import warnings\nfrom copy import deepcopy\nfrom typing import Optional\n\nimport lap\nimport numpy as np\n\n\ndef shape_similarity(detects: np.ndarray, tracks: np.ndarray, s_sim_corr: bool) -> np.ndarray:\n    if not s_sim_corr:\n        return shape_similarity_v1(detects, tracks)\n    return shape_similarity_v2(detects, tracks)\n\n\ndef shape_similarity_v1(detects: np.ndarray, tracks: np.ndarray) -> np.ndarray:\n    if detects.size == 0 or tracks.size == 0:\n        return np.zeros((0, 0))\n\n    dw = (detects[:, 2] - detects[:, 0]).reshape((-1, 1))\n    dh = (detects[:, 3] - detects[:, 1]).reshape((-1, 1))\n    tw = (tracks[:, 2] - tracks[:, 0]).reshape((1, -1))\n    th = (tracks[:, 3] - tracks[:, 1]).reshape((1, -1))\n    return np.exp(-(np.abs(dw - tw) / np.maximum(dw, tw) + np.abs(dh - th) / np.maximum(dw, tw)))\n\n\ndef shape_similarity_v2(detects: np.ndarray, tracks: np.ndarray) -> np.ndarray:\n    if detects.size == 0 or tracks.size == 0:\n        return np.zeros((0, 0))\n\n  ","import warnings\nfrom copy import deepcopy\nfrom typing import Optional\n\nimport lap\nimport numpy as np\n\n\ndef shape_similarity(detects: np.ndarray, tracks: np.ndarray, s_sim_corr: bool) -> np.ndarray:\n    if not s_sim_corr:\n        return shape_similarity_v1(detects, tracks)\n    return shape_similarity_v2(detects, tracks)\n\n\ndef shape_similarity_v1(detects: np.ndarray, tracks: np.ndarray) -> np.ndarray:\n    if detects.size == 0 or tracks.size == 0:\n        return np.zeros((0, 0))\n\n    dw = (detects[:, 2] - detects[:, 0]).reshape((-1, 1))\n    dh = (detects[:, 3] - detects[:, 1]).reshape((-1, 1))\n    tw = (tracks[:, 2] - tracks[:, 0]).reshape((1, -1))\n    th = (tracks[:, 3] - tracks[:, 1]).reshape((1, -1))\n    return np.exp(-(np.abs(dw - tw) / np.maximum(dw, tw) + np.abs(dh - th) / np.maximum(dw, tw)))\n\n\ndef shape_similarity_v2(detects: np.ndarray, tracks: np.ndarray) -> np.ndarray:\n    if detects.size == 0 or tracks.size == 0:\n        return np.zeros((0, 0))\n\n  ","@@ -115,12 +115,13 @@ def match(cost_matrix: np.ndarray, threshold: float) -> np.ndarray:\n \n \n def linear_assignment(\n-        detections: np.ndarray,\n-        trackers: np.ndarray,\n-        iou_matrix: np.ndarray,\n-        cost_matrix: np.ndarray,\n-        threshold: float,\n-        emb_cost: Optional[np.ndarray] = None):\n+    detections: np.ndarray,\n+    trackers: np.ndarray,\n+    iou_matrix: np.ndarray,\n+    cost_matrix: np.ndarray,\n+    threshold: float,\n+    emb_cost: Optional[np.ndarray] = None,\n+):\n     if iou_matrix is None and cost_matrix is None:\n         raise Exception(""""Both iou_matrix and cost_matrix are None!"""")\n     if iou_matrix is None:\n@@ -170,7 +171,7 @@ def associate(\n             np.empty((0, 2), dtype=int),\n             np.arange(len(detections)),\n             np.empty((0, 5), dtype=int),\n-            np.empty((0, 0))\n+            np.empty((0, 0)),\n         )\n     iou_matrix = iou_batch(detections, trackers)\n \n",add,Added STORM - 236 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/boosttrack/boosttrack.py,"from collections import deque\nfrom typing import List, Optional\n\nimport numpy as np\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.boosttrack.assoc import (\n    MhDist_similarity,\n    associate,\n    iou_batch,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    ","from collections import deque\nfrom typing import List, Optional\n\nimport numpy as np\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.boosttrack.assoc import (\n    MhDist_similarity,\n    associate,\n    iou_batch,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    ","@@ -46,13 +46,14 @@ class KalmanBoxTracker:\n     """"""""""""\n     Single object tracker using a Kalman filter.\n     """"""""""""\n+\n     count = 0\n \n     def __init__(self, det, max_obs, emb: Optional[np.ndarray] = None):\n         KalmanBoxTracker.count += 1\n \n         self.time_since_update = 0\n-        self.id = KalmanBoxTracker.count \n+        self.id = KalmanBoxTracker.count\n         self.kf = KalmanFilter(convert_bbox_to_z(det[:4]))\n         self.conf = det[4]\n         self.cls = det[5]\n@@ -98,9 +99,8 @@ class KalmanBoxTracker:\n \n         # âââ rebuild Kalman state âââââ\n         w, h = x2_ - x1_, y2_ - y1_\n-        cx, cy = x1_ + w/2, y1_ + h/2\n-        self.kf.x[:4] = [cx, cy, h, w/h]\n-\n+        cx, cy = x1_ + w / 2, y1_ + h / 2\n+        self.kf.x[:4] = [cx, cy, h, w / h]\n \n     def predict(self):\n         self.kf.predict()\n@@ -128,7 +128,6 @@ class BoostTrack(BaseTracker):\n         reid_weights,\n         device,\n         half: bool,\n-\n         max_age: int = 60,\n         min_hits: int = 3,\n         det_thresh: float = 0.6,\n@@ -136,8 +135,7 @@ class BoostTrack(BaseTracker):\n         use_ecc: bool = True,\n         min_box_area: int = 10,\n         aspect_ratio_thresh: bool = 1.6,\n-        cmc_method: str = 'ecc',\n-\n+        cmc_method: str = """"ecc"""",\n         # BoostTrack parameters\n         lambda_iou: float = 0.5,\n         lambda_mhd: float = 0.25,\n@@ -146,12 +144,10 @@ class BoostTrack(BaseTracker):\n         use_duo_boost: bool = True,\n         dlo_boost_coef: float = 0.65,\n         s_sim_corr: bool = False,\n-    \n         # BoostTrack++ parameters\n         use_rich_s: bool = False,\n         use_sb: bool = False,\n         use_vt: bool = False,\n-\n         with_reid: bool = False,\n     ):\n         super().__init__()\n@@ -197,12 +193,12 @@ class BoostTrack(BaseTracker):\n     def update(self, dets: np.ndarray, img: np.ndarray, embs: Optional[np.ndarray] = None) -> np.ndarray:\n         """"""""""""\n         Update the tr",add,Added STORM - 236 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/boosttrack/kalmanfilter.py,"from copy import deepcopy\nfrom typing import Optional\n\nimport numpy as np\nimport scipy.linalg\n\n\nclass ConstantNoise:\n    def __init__(self, x_dim: int, z_dim: int):\n        self.x_dim = x_dim\n        self.z_dim = z_dim\n\n    def get_init_state_cov(self) -> np.ndarray:\n        p = np.eye(self.x_dim)\n        p[4:, 4:] *= 1000.0  # give high uncertainty to the unobservable initial velocities\n        p *= 10.0\n        return p\n\n    @staticmethod\n    def get_r() -> np.ndarray:\n        return np.diag([1, 1, 10, 0.01])\n\n    def get_q(self) -> np.ndarray:\n        q = np.eye(self.x_dim)\n        q[4:, 4:] *= 0.01\n        return q\n\n\nclass KalmanFilter:\n    """"""""""""\n    A simple Kalman filter for tracking bounding boxes in image space.\n\n    The 8-dimensional state space\n\n        x, y, h, a, vx, vy, vh, va\n\n    contains the bounding box center position (x, y), aspect ratio a, height h,\n    and their respective velocities.\n\n    Object motion follows a constant vel","from copy import deepcopy\nfrom typing import Optional\n\nimport numpy as np\nimport scipy.linalg\n\n\nclass ConstantNoise:\n    def __init__(self, x_dim: int, z_dim: int):\n        self.x_dim = x_dim\n        self.z_dim = z_dim\n\n    def get_init_state_cov(self) -> np.ndarray:\n        p = np.eye(self.x_dim)\n        p[4:, 4:] *= 1000.0  # give high uncertainty to the unobservable initial velocities\n        p *= 10.0\n        return p\n\n    @staticmethod\n    def get_r() -> np.ndarray:\n        return np.diag([1, 1, 10, 0.01])\n\n    def get_q(self) -> np.ndarray:\n        q = np.eye(self.x_dim)\n        q[4:, 4:] *= 0.01\n        return q\n\n\nclass KalmanFilter:\n    """"""""""""\n    A simple Kalman filter for tracking bounding boxes in image space.\n\n    The 8-dimensional state space\n\n        x, y, h, a, vx, vy, vh, va\n\n    contains the bounding box center position (x, y), aspect ratio a, height h,\n    and their respective velocities.\n\n    Object motion follows a constant vel","@@ -49,7 +49,7 @@ class KalmanFilter:\n             dt: int = 1,\n             id: int = -1):\n         if z.ndim == 2:\n-            z = deepcopy(z.reshape((-1, )))\n+            z = deepcopy(z.reshape((-1,)))\n \n         self.dt = dt\n         self.ndim = ndim\n@@ -67,8 +67,10 @@ class KalmanFilter:\n         self.covariance = self.cov_update_policy.get_init_state_cov()\n         self.id = id\n \n-    def predict(self, mean: Optional[np.ndarray] = None,\n-                covariance: Optional[np.ndarray] = None):\n+    def predict(self,\n+            mean: Optional[np.ndarray] = None,\n+            covariance: Optional[np.ndarray] = None\n+        ):\n         """"""""""""Run Kalman filter prediction step.\n \n         Parameters\n@@ -133,19 +135,23 @@ class KalmanFilter:\n         """"""""""""\n \n         if z.ndim == 2:\n-            z = deepcopy(z.reshape((-1, )))\n+            z = deepcopy(z.reshape((-1,)))\n         projected_mean, projected_cov = self.project()\n \n         chol_factor, lower = scipy.linalg.cho_factor(\n-            projected_cov, lower=True, check_finite=False)\n+            projected_cov, lower=True, check_finite=False\n+        )\n         kalman_gain = scipy.linalg.cho_solve(\n-            (chol_factor, lower), np.dot(self.covariance, self._update_mat.T).T,\n-            check_finite=False).T\n+            (chol_factor, lower),\n+            np.dot(self.covariance, self._update_mat.T).T,\n+            check_finite=False,\n+        ).T\n \n         innovation = z - projected_mean\n \n         self.x = self.x + np.dot(innovation, kalman_gain.T)\n-        self.covariance = self.covariance - np.linalg.multi_dot((\n-            kalman_gain, projected_cov, kalman_gain.T))\n+        self.covariance = self.covariance - np.linalg.multi_dot(\n+            (kalman_gain, projected_cov, kalman_gain.T)\n+        )\n \n         return self.x, self.covariance\n",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/botsort/basetrack.py,"from collections import OrderedDict\n\nimport numpy as np\n\n\nclass TrackState:\n    """"""""""""\n    Enum-like class for tracking states.\n\n    Attributes:\n        New (int): Represents a newly created track.\n        Tracked (int): Represents a currently tracked object.\n        Lost (int): Represents a temporarily lost track.\n        LongLost (int): Represents a track that has been lost for a long time.\n        Removed (int): Represents a track that has been removed.\n    """"""""""""\n    New = 0\n    Tracked = 1\n    Lost = 2\n    LongLost = 3\n    Removed = 4\n\n\nclass BaseTrack:\n    """"""""""""\n    Base class for managing the state of a track in multi-object tracking.\n\n    Attributes:\n        _count (int): Class variable to keep track of the number of tracks created.\n        track_id (int): The unique ID assigned to the track.\n        is_activated (bool): Whether the track has been activated.\n        state (TrackState): The current state of the track.\n        history (OrderedDict","from collections import OrderedDict\n\nimport numpy as np\n\n\nclass TrackState:\n    """"""""""""\n    Enum-like class for tracking states.\n\n    Attributes:\n        New (int): Represents a newly created track.\n        Tracked (int): Represents a currently tracked object.\n        Lost (int): Represents a temporarily lost track.\n        LongLost (int): Represents a track that has been lost for a long time.\n        Removed (int): Represents a track that has been removed.\n    """"""""""""\n\n    New = 0\n    Tracked = 1\n    Lost = 2\n    LongLost = 3\n    Removed = 4\n\n\nclass BaseTrack:\n    """"""""""""\n    Base class for managing the state of a track in multi-object tracking.\n\n    Attributes:\n        _count (int): Class variable to keep track of the number of tracks created.\n        track_id (int): The unique ID assigned to the track.\n        is_activated (bool): Whether the track has been activated.\n        state (TrackState): The current state of the track.\n        history (OrderedDi","@@ -14,6 +14,7 @@ class TrackState:\n         LongLost (int): Represents a track that has been lost for a long time.\n         Removed (int): Represents a track that has been removed.\n     """"""""""""\n+\n     New = 0\n     Tracked = 1\n     Lost = 2\n@@ -39,6 +40,7 @@ class BaseTrack:\n         time_since_update (int): The number of frames since the track was last updated.\n         location (tuple): The location of the object in multi-camera tracking (set to infinity by default).\n     """"""""""""\n+\n     _count = 0\n \n     track_id: int = 0\n",add,Added STORM - 146 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/botsort/botsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.aabb.xywh_kf import KalmanFilterXYWH\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.trackers.botsort.botsort_track import STrack\nfrom boxmot.trackers.botsort.botsort_utils import (\n    joint_stracks,\n    remove_duplicate_stracks,\n    sub_stracks,\n)\nfrom boxmot.utils.matching import (\n    embedding_distance,\n    fuse_score,\n    iou_distance,\n    linear_assignment,\n)\n\n\nclass BotSort(BaseTracker):\n    """"""""""""\n    BoTSORT Tracker: A tracking algorithm that combines appearance and motion-based tracking.\n\n    Args:\n        reid_weights (str): Path to the model weights for ReID.\n        device (torch.device): Device to run the m","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.aabb.xywh_kf import KalmanFilterXYWH\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.trackers.botsort.botsort_track import STrack\nfrom boxmot.trackers.botsort.botsort_utils import (\n    joint_stracks,\n    remove_duplicate_stracks,\n    sub_stracks,\n)\nfrom boxmot.utils.matching import (\n    embedding_distance,\n    fuse_score,\n    iou_distance,\n    linear_assignment,\n)\n\n\nclass BotSort(BaseTracker):\n    """"""""""""\n    BoTSORT Tracker: A tracking algorithm that combines appearance and motion-based tracking.\n\n    Args:\n        reid_weights (str): Path to the model weights for ReID.\n        device (torch.device): Device to run the m","@@ -93,7 +93,9 @@ class BotSort(BaseTracker):\n \n     @BaseTracker.setup_decorator\n     @BaseTracker.per_class_decorator\n-    def update(self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None) -> np.ndarray:\n+    def update(\n+        self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None\n+    ) -> np.ndarray:\n         self.check_inputs(dets, img, embs)\n         self.frame_count += 1\n \n@@ -113,31 +115,62 @@ class BotSort(BaseTracker):\n \n         # Separate unconfirmed and active tracks\n         unconfirmed, active_tracks = self._separate_tracks()\n-        \n+\n         strack_pool = joint_stracks(active_tracks, self.lost_stracks)\n \n         # First association\n-        matches_first, u_track_first, u_detection_first = self._first_association(dets, dets_first, active_tracks, unconfirmed, img, detections, activated_stracks, refind_stracks, strack_pool)\n+        matches_first, u_track_first, u_detection_first = self._first_association(\n+            dets,\n+            dets_first,\n+            active_tracks,\n+            unconfirmed,\n+            img,\n+            detections,\n+            activated_stracks,\n+            refind_stracks,\n+            strack_pool,\n+        )\n \n         # Second association\n-        matches_second, u_track_second, u_detection_second = self._second_association(dets_second, activated_stracks, lost_stracks, refind_stracks, u_track_first, strack_pool)\n+        matches_second, u_track_second, u_detection_second = self._second_association(\n+            dets_second,\n+            activated_stracks,\n+            lost_stracks,\n+            refind_stracks,\n+            u_track_first,\n+            strack_pool,\n+        )\n \n         # Handle unconfirmed tracks\n-        matches_unc, u_track_unc, u_detection_unc = self._handle_unconfirmed_tracks(u_detection_first, detections, activated_stracks, removed_stracks, unconfirmed)\n+        matches_unc, u_track_unc, u_detection_unc = self._handle_unconf",add,Added net . kano . joustsim . oscar . os
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/botsort/botsort_track.py,"from collections import deque\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.aabb.xywh_kf import KalmanFilterXYWH\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilterXYWH()\n\n    def __init__(self, det, feat=None, feat_history=50, max_obs=50):\n        # Initialize detection parameters\n        self.xywh = xyxy2xywh(det[:4])  # Convert to (xc, yc, w, h)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs = max_obs\n\n        # Kalman filter and tracking state\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        self.tracklet_len = 0\n\n        # Classification history and feature history\n        self.cls_hist = []\n        self.history_observations = deque(maxlen=self.max_obs)\n        self.features = deque(maxlen=","from collections import deque\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.aabb.xywh_kf import KalmanFilterXYWH\nfrom boxmot.trackers.botsort.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.ops import xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilterXYWH()\n\n    def __init__(self, det, feat=None, feat_history=50, max_obs=50):\n        # Initialize detection parameters\n        self.xywh = xyxy2xywh(det[:4])  # Convert to (xc, yc, w, h)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs = max_obs\n\n        # Kalman filter and tracking state\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n        self.tracklet_len = 0\n\n        # Classification history and feature history\n        self.cls_hist = []\n        self.history_observations = deque(maxlen=self.max_obs)\n        self.features = deque(maxlen=","@@ -68,7 +68,9 @@ class STrack(BaseTrack):\n         mean_state = self.mean.copy()\n         if self.state != TrackState.Tracked:\n             mean_state[6:8] = 0  # Reset velocities\n-        self.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\n+        self.mean, self.covariance = self.kalman_filter.predict(\n+            mean_state, self.covariance\n+        )\n \n     @staticmethod\n     def multi_predict(stracks):\n@@ -80,7 +82,9 @@ class STrack(BaseTrack):\n         for i, st in enumerate(stracks):\n             if st.state != TrackState.Tracked:\n                 multi_mean[i][6:8] = 0  # Reset velocities\n-        multi_mean, multi_covariance = STrack.shared_kalman.multi_predict(multi_mean, multi_covariance)\n+        multi_mean, multi_covariance = STrack.shared_kalman.multi_predict(\n+            multi_mean, multi_covariance\n+        )\n         for st, mean, cov in zip(stracks, multi_mean, multi_covariance):\n             st.mean, st.covariance = mean, cov\n \n@@ -113,7 +117,9 @@ class STrack(BaseTrack):\n \n     def re_activate(self, new_track, frame_id, new_id=False):\n         """"""""""""Re-activate a track with a new detection.""""""""""""\n-        self.mean, self.covariance = self.kalman_filter.update(self.mean, self.covariance, new_track.xywh)\n+        self.mean, self.covariance = self.kalman_filter.update(\n+            self.mean, self.covariance, new_track.xywh\n+        )\n         if new_track.curr_feat is not None:\n             self.update_features(new_track.curr_feat)\n         self.tracklet_len = 0\n@@ -133,7 +139,9 @@ class STrack(BaseTrack):\n         self.tracklet_len += 1\n         self.history_observations.append(self.xyxy)\n \n-        self.mean, self.covariance = self.kalman_filter.update(self.mean, self.covariance, new_track.xywh)\n+        self.mean, self.covariance = self.kalman_filter.update(\n+            self.mean, self.covariance, new_track.xywh\n+        )\n         if new_track.curr_feat is not None:\",add,Added the UNSTARTED state to the YouTube PlayerState enum
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/botsort/botsort_utils.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom typing import List, Tuple\n\nimport numpy as np\n\nfrom boxmot.utils.matching import iou_distance\n\n\ndef joint_stracks(tlista: List['STrack'], tlistb: List['STrack']) -> List['STrack']:\n    """"""""""""\n    Joins two lists of tracks, ensuring that there are no duplicates based on track IDs.\n\n    Args:\n        tlista (List[STrack]): The first list of tracks.\n        tlistb (List[STrack]): The second list of tracks.\n\n    Returns:\n        List[STrack]: A combined list of tracks from both input lists, without duplicates.\n    """"""""""""\n    exists = {}\n    res = []\n    for t in tlista:\n        exists[t.id] = 1\n        res.append(t)\n    for t in tlistb:\n        tid = t.id\n        if not exists.get(tid, 0):\n            exists[tid] = 1\n            res.append(t)\n    return res\n\n\ndef sub_stracks(tlista: List['STrack'], tlistb: List['STrack']) -> List['STrack']:\n    """"""""""""\n    Subtracts the tracks in tlistb from tlista ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom typing import List, Tuple\n\nimport numpy as np\n\nfrom boxmot.utils.matching import iou_distance\n\n\ndef joint_stracks(tlista: List[""""STrack""""], tlistb: List[""""STrack""""]) -> List[""""STrack""""]:\n    """"""""""""\n    Joins two lists of tracks, ensuring that there are no duplicates based on track IDs.\n\n    Args:\n        tlista (List[STrack]): The first list of tracks.\n        tlistb (List[STrack]): The second list of tracks.\n\n    Returns:\n        List[STrack]: A combined list of tracks from both input lists, without duplicates.\n    """"""""""""\n    exists = {}\n    res = []\n    for t in tlista:\n        exists[t.id] = 1\n        res.append(t)\n    for t in tlistb:\n        tid = t.id\n        if not exists.get(tid, 0):\n            exists[tid] = 1\n            res.append(t)\n    return res\n\n\ndef sub_stracks(tlista: List[""""STrack""""], tlistb: List[""""STrack""""]) -> List[""""STrack""""]:\n    """"""""""""\n    Subtracts the tracks in tlistb ","@@ -7,7 +7,7 @@ import numpy as np\n from boxmot.utils.matching import iou_distance\n \n \n-def joint_stracks(tlista: List['STrack'], tlistb: List['STrack']) -> List['STrack']:\n+def joint_stracks(tlista: List[""""STrack""""], tlistb: List[""""STrack""""]) -> List[""""STrack""""]:\n     """"""""""""\n     Joins two lists of tracks, ensuring that there are no duplicates based on track IDs.\n \n@@ -31,7 +31,7 @@ def joint_stracks(tlista: List['STrack'], tlistb: List['STrack']) -> List['STrac\n     return res\n \n \n-def sub_stracks(tlista: List['STrack'], tlistb: List['STrack']) -> List['STrack']:\n+def sub_stracks(tlista: List[""""STrack""""], tlistb: List[""""STrack""""]) -> List[""""STrack""""]:\n     """"""""""""\n     Subtracts the tracks in tlistb from tlista based on track IDs.\n \n@@ -50,7 +50,9 @@ def sub_stracks(tlista: List['STrack'], tlistb: List['STrack']) -> List['STrack'\n     return list(stracks.values())\n \n \n-def remove_duplicate_stracks(stracksa: List['STrack'], stracksb: List['STrack']) -> Tuple[List['STrack'], List['STrack']]:\n+def remove_duplicate_stracks(\n+    stracksa: List[""""STrack""""], stracksb: List[""""STrack""""]\n+) -> Tuple[List[""""STrack""""], List[""""STrack""""]]:\n     """"""""""""\n     Removes duplicate tracks between two lists based on their IoU distance and track duration.\n \n@@ -75,5 +77,5 @@ def remove_duplicate_stracks(stracksa: List['STrack'], stracksb: List['STrack'])\n \n     resa = [t for i, t in enumerate(stracksa) if i not in dupa]\n     resb = [t for i, t in enumerate(stracksb) if i not in dupb]\n-    \n+\n     return resa, resb\n",add,Fix typo
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/bytetrack/bytetrack.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.aabb.xyah_kf import KalmanFilterXYAH\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilterXYAH()\n\n    def __init__(self, det, max_obs):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n        self.xyah = tlwh2xyah(self.tlwh)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs=max_obs\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n       ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.aabb.xyah_kf import KalmanFilterXYAH\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.bytetrack.basetrack import BaseTrack, TrackState\nfrom boxmot.utils.matching import fuse_score, iou_distance, linear_assignment\nfrom boxmot.utils.ops import tlwh2xyah, xywh2tlwh, xywh2xyxy, xyxy2xywh\n\n\nclass STrack(BaseTrack):\n    shared_kalman = KalmanFilterXYAH()\n\n    def __init__(self, det, max_obs):\n        # wait activate\n        self.xywh = xyxy2xywh(det[0:4])  # (x1, y1, x2, y2) --> (xc, yc, w, h)\n        self.tlwh = xywh2tlwh(self.xywh)  # (xc, yc, w, h) --> (t, l, w, h)\n        self.xyah = tlwh2xyah(self.tlwh)\n        self.conf = det[4]\n        self.cls = det[5]\n        self.det_ind = det[6]\n        self.max_obs = max_obs\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n     ","@@ -22,7 +22,7 @@ class STrack(BaseTrack):\n         self.conf = det[4]\n         self.cls = det[5]\n         self.det_ind = det[6]\n-        self.max_obs=max_obs\n+        self.max_obs = max_obs\n         self.kalman_filter = None\n         self.mean, self.covariance = None, None\n         self.is_activated = False\n@@ -128,6 +128,7 @@ class ByteTrack(BaseTracker):\n         frame_rate (int, optional): Frame rate of the video being processed. Used to scale the track buffer size.\n         per_class (bool, optional): Whether to perform per-class tracking. If True, tracks are maintained separately for each object class.\n     """"""""""""\n+\n     def __init__(\n         self,\n         min_conf: float = 0.1,\n@@ -156,8 +157,10 @@ class ByteTrack(BaseTracker):\n \n     @BaseTracker.setup_decorator\n     @BaseTracker.per_class_decorator\n-    def update(self, dets: np.ndarray, img: np.ndarray = None, embs: np.ndarray = None) -> np.ndarray:\n-        \n+    def update(\n+        self, dets: np.ndarray, img: np.ndarray = None, embs: np.ndarray = None\n+    ) -> np.ndarray:\n+\n         self.check_inputs(dets, img)\n \n         dets = np.hstack([dets, np.arange(len(dets)).reshape(-1, 1)])\n@@ -179,9 +182,7 @@ class ByteTrack(BaseTracker):\n \n         if len(dets) > 0:\n             """"""""""""Detections""""""""""""\n-            detections = [\n-                STrack(det, max_obs=self.max_obs) for det in dets\n-            ]\n+            detections = [STrack(det, max_obs=self.max_obs) for det in dets]\n         else:\n             detections = []\n \n@@ -219,7 +220,9 @@ class ByteTrack(BaseTracker):\n         # association the untrack to the low conf detections\n         if len(dets_second) > 0:\n             """"""""""""Detections""""""""""""\n-            detections_second = [STrack(det_second, max_obs=self.max_obs) for det_second in dets_second]\n+            detections_second = [\n+                STrack(det_second, max_obs=self.max_obs) for det_second in dets_second\n+            ]\n        ",add,Added the UNSTARTED state to the YouTube PlayerState enum
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/deepocsort/deepocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.aabb.xysr_kf import KalmanFilterXYSR\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.ops import xyxy2xysr\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\n      [x1,y1,x2,y2] where x1,y1 is the top ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.aabb.xysr_kf import KalmanFilterXYSR\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.ops import xyxy2xysr\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\n      [x1,y1,x2,y2] where x1,y1 is the top ","@@ -33,9 +33,11 @@ def convert_x_to_bbox(x, score=None):\n     w = np.sqrt(x[2] * x[3])\n     h = x[2] / w\n     if score is None:\n-        return np.array([x[0] - w / 2.0, x[1] - h / 2.0, x[0] + w / 2.0, x[1] + h / 2.0]).reshape((1, 4))\n-    else:\n-        return np.array([x[0] - w / 2.0, x[1] - h / 2.0, x[0] + w / 2.0, x[1] + h / 2.0, score]).reshape((1, 5))\n+        return np.array([x[0] - w / 2.0, x[1] - h / 2.0,\n+                         x[0] + w / 2.0, x[1] + h / 2.0]).reshape((1, 4))\n+    return np.array([x[0] - w / 2.0, x[1] - h / 2.0,\n+                     x[0] + w / 2.0, x[1] + h / 2.0, score]\n+        ).reshape((1, 5))\n \n \n def speed_direction(bbox1, bbox2):\n@@ -46,20 +48,29 @@ def speed_direction(bbox1, bbox2):\n     return speed / norm\n \n \n-class KalmanBoxTracker(object):\n+class KalmanBoxTracker:\n     """"""""""""\n     This class represents the internal state of individual tracked objects observed as bbox.\n     """"""""""""\n \n     count = 0\n \n-    def __init__(self, det, delta_t=3, emb=None, alpha=0, max_obs=50, Q_xy_scaling = 0.01, Q_s_scaling = 0.0001):\n+    def __init__(\n+        self,\n+        det,\n+        delta_t=3,\n+        emb=None,\n+        alpha=0,\n+        max_obs=50,\n+        Q_xy_scaling=0.01,\n+        Q_s_scaling=0.0001,\n+    ):\n         """"""""""""\n         Initialises a tracker using initial bounding box.\n \n         """"""""""""\n         # define constant velocity model\n-        self.max_obs=max_obs\n+        self.max_obs = max_obs\n         bbox = det[0:5]\n         self.conf = det[4]\n         self.cls = det[5]\n@@ -90,7 +101,9 @@ class KalmanBoxTracker(object):\n             ]\n         )\n         self.kf.R[2:, 2:] *= 10.0\n-        self.kf.P[4:, 4:] *= 1000.0  # give high uncertainty to the unobservable initial velocities\n+        self.kf.P[\n+            4:, 4:\n+        ] *= 1000.0  # give high uncertainty to the unobservable initial velocities\n         self.kf.P *= 10.0\n         self.kf.Q[4:6, 4:6] *= self.Q_xy",add,Added limits . permission to groups
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/hybridsort/association.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\n\ndef intersection_batch(bboxes1, bboxes2):\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n    yy2 = np.minimum(bboxes1[..., 3], bboxes2[..., 3])\n    w = np.maximum(0., xx2 - xx1)\n    h = np.maximum(0., yy2 - yy1)\n    intersections = w * h\n    return intersections\n\n\ndef box_area(bbox):\n    area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n    return area\n\n\ndef iou_batch(bboxes1, bboxes2):\n    """"""""""""\n    From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n    """"""""""""\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.mi","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\n\ndef intersection_batch(bboxes1, bboxes2):\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n    yy2 = np.minimum(bboxes1[..., 3], bboxes2[..., 3])\n    w = np.maximum(0.0, xx2 - xx1)\n    h = np.maximum(0.0, yy2 - yy1)\n    intersections = w * h\n    return intersections\n\n\ndef box_area(bbox):\n    area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n    return area\n\n\ndef iou_batch(bboxes1, bboxes2):\n    """"""""""""\n    From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n    """"""""""""\n    bboxes2 = np.expand_dims(bboxes2, 0)\n    bboxes1 = np.expand_dims(bboxes1, 1)\n\n    xx1 = np.maximum(bboxes1[..., 0], bboxes2[..., 0])\n    yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n    xx2 = np.","@@ -11,8 +11,8 @@ def intersection_batch(bboxes1, bboxes2):\n     yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n     xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n     yy2 = np.minimum(bboxes1[..., 3], bboxes2[..., 3])\n-    w = np.maximum(0., xx2 - xx1)\n-    h = np.maximum(0., yy2 - yy1)\n+    w = np.maximum(0.0, xx2 - xx1)\n+    h = np.maximum(0.0, yy2 - yy1)\n     intersections = w * h\n     return intersections\n \n@@ -33,12 +33,15 @@ def iou_batch(bboxes1, bboxes2):\n     yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n     xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n     yy2 = np.minimum(bboxes1[..., 3], bboxes2[..., 3])\n-    w = np.maximum(0., xx2 - xx1)\n-    h = np.maximum(0., yy2 - yy1)\n+    w = np.maximum(0.0, xx2 - xx1)\n+    h = np.maximum(0.0, yy2 - yy1)\n     wh = w * h\n-    o = wh / ((bboxes1[..., 2] - bboxes1[..., 0]) * (bboxes1[..., 3] - bboxes1[..., 1]) +\n-              (bboxes2[..., 2] - bboxes2[..., 0]) * (bboxes2[..., 3] - bboxes2[..., 1]) - wh)\n-    return (o)\n+    o = wh / (\n+        (bboxes1[..., 2] - bboxes1[..., 0]) * (bboxes1[..., 3] - bboxes1[..., 1])\n+        + (bboxes2[..., 2] - bboxes2[..., 0]) * (bboxes2[..., 3] - bboxes2[..., 1])\n+        - wh\n+    )\n+    return o\n \n \n def cal_score_dif_batch(bboxes1, bboxes2):\n@@ -51,7 +54,7 @@ def cal_score_dif_batch(bboxes1, bboxes2):\n     score2 = bboxes2[..., 4]\n     score1 = bboxes1[..., 4]\n \n-    return (abs(score2 - score1))\n+    return abs(score2 - score1)\n \n \n def cal_score_dif_batch_two_score(bboxes1, bboxes2):\n@@ -64,7 +67,7 @@ def cal_score_dif_batch_two_score(bboxes1, bboxes2):\n     score2 = bboxes2[..., 5]\n     score1 = bboxes1[..., 4]\n \n-    return (abs(score2 - score1))\n+    return abs(score2 - score1)\n \n \n def hmiou(bboxes1, bboxes2):\n@@ -85,12 +88,15 @@ def hmiou(bboxes1, bboxes2):\n     yy1 = np.maximum(bboxes1[..., 1], bboxes2[..., 1])\n     xx2 = np.minimum(bboxes1[..., 2], bboxes2[..., 2])\n     yy2 = np.minimum(bboxes1",add,Added STORM - 295 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/hybridsort/hybridsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.hybridsort.association import (\n    associate_4_points_with_score,\n    associate_4_points_with_score_with_reid,\n    cal_score_dif_batch_two_score,\n    embedding_distance,\n    linear_assignment,\n)\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nThis script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\n\nfrom collections import deque  # [hgx0418] deque for reid feature\n\nimport numpy as np\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.hybridsort.association import (\n    associate_4_points_with_score,\n    associate_4_points_with_score_with_reid,\n    cal_score_dif_batch_two_score,\n    embedding_distance,\n    linear_assignment,\n)\n\nnp.random.seed(0)\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_bbox_to_z","@@ -1,7 +1,7 @@\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n """"""""""""\n-    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n+This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n """"""""""""\n \n from collections import deque  # [hgx0418] deque for reid feature\n@@ -41,8 +41,8 @@ def convert_bbox_to_z(bbox):\n     """"""""""""\n     w = bbox[2] - bbox[0]\n     h = bbox[3] - bbox[1]\n-    x = bbox[0] + w / 2.\n-    y = bbox[1] + h / 2.\n+    x = bbox[0] + w / 2.0\n+    y = bbox[1] + h / 2.0\n     s = w * h  # scale is just area\n     r = w / float(h + 1e-6)\n     score = bbox[4]\n@@ -61,9 +61,10 @@ def convert_x_to_bbox(x, score=None):\n     h = x[2] / w\n     score = x[3]\n     if score is None:\n-        return np.array([x[0] - w / 2., x[1] - h / 2., x[0] + w / 2., x[1] + h / 2.]).reshape((1, 4))\n-    else:\n-        return np.array([x[0] - w / 2., x[1] - h / 2., x[0] + w / 2., x[1] + h / 2., score]).reshape((1, 5))\n+        return np.array([x[0] - w / 2.0, x[1] - h / 2.0,\n+                        x[0] + w / 2.0, x[1] + h / 2.0]).reshape((1, 4))\n+    return np.array([x[0] - w / 2.0, x[1] - h / 2.0,\n+                     x[0] + w / 2.0, x[1] + h / 2.0, score]).reshape((1, 5))\n \n \n def speed_direction(bbox1, bbox2):\n@@ -86,7 +87,7 @@ def speed_direction_rt(bbox1, bbox2):\n     cx1, cy1 = bbox1[0], bbox1[3]\n     cx2, cy2 = bbox2[0], bbox2[3]\n     speed = np.array([cy2 - cy1, cx2 - cx1])\n-    norm = np.sqrt((cy2 - cy1)**2 + (cx2 - cx1) ** 2) + 1e-6\n+    norm = np.sqrt((cy2 - cy1) ** 2 + (cx2 - cx1) ** 2) + 1e-6\n     return speed / norm\n \n \n@@ -102,7 +103,7 @@ def speed_direction_rb(bbox1, bbox2):\n     cx1, cy1 = bbox1[2], bbox1[3]\n     cx2, cy2 = bbox2[2], bbox2[3]\n     speed = np.array([cy2 - cy1, cx2 - cx1])\n-    norm = np.sqrt((cy2 - cy1)**2 + (cx2 - cx1) ** 2) + 1e-6\n+    norm = np.sqrt((cy2 - cy1) ** 2 + (cx2 - cx1) ** 2) + 1e-6\n     return speed / norm\n \n \n@@ -110,6 +111,7 @@ class Kalman",add,Added Type name for DFI ( # 2 )
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/ocsort/ocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\n    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.aabb.xysr_kf import KalmanFilterXYSR\nfrom boxmot.motion.kalman_filters.obb.xywha_kf import KalmanBoxTrackerOBB\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.ops import xyxy2xysr\n\n\ndef k_previous_obs(observations, cur_age, k, is_obb=False):\n    if len(observations) == 0:\n        if is_obb:\n            return [-1, -1, -1, -1, -1, -1]\n        else :\n            return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Tak","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nThis script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.aabb.xysr_kf import KalmanFilterXYSR\nfrom boxmot.motion.kalman_filters.obb.xywha_kf import KalmanBoxTrackerOBB\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.ops import xyxy2xysr\n\n\ndef k_previous_obs(observations, cur_age, k, is_obb=False):\n    if len(observations) == 0:\n        if is_obb:\n            return [-1, -1, -1, -1, -1, -1]\n        else:\n            return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Takes a ","@@ -1,7 +1,7 @@\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n """"""""""""\n-    This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n+This script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n """"""""""""\n from collections import deque\n \n@@ -18,7 +18,7 @@ def k_previous_obs(observations, cur_age, k, is_obb=False):\n     if len(observations) == 0:\n         if is_obb:\n             return [-1, -1, -1, -1, -1, -1]\n-        else :\n+        else:\n             return [-1, -1, -1, -1, -1]\n     for i in range(k):\n         dt = k - i\n@@ -60,7 +60,16 @@ class KalmanBoxTracker(object):\n \n     count = 0\n \n-    def __init__(self, bbox, cls, det_ind, delta_t=3, max_obs=50, Q_xy_scaling = 0.01, Q_s_scaling = 0.0001):\n+    def __init__(\n+        self,\n+        bbox,\n+        cls,\n+        det_ind,\n+        delta_t=3,\n+        max_obs=50,\n+        Q_xy_scaling=0.01,\n+        Q_s_scaling=0.0001,\n+    ):\n         """"""""""""\n         Initialises a tracker using initial bounding box.\n \n@@ -200,6 +209,7 @@ class OcSort(BaseTracker):\n         Q_xy_scaling (float, optional): Scaling factor for the process noise covariance in the Kalman Filter for position coordinates.\n         Q_s_scaling (float, optional): Scaling factor for the process noise covariance in the Kalman Filter for scale coordinates.\n     """"""""""""\n+\n     def __init__(\n         self,\n         per_class: bool = False,\n@@ -235,7 +245,9 @@ class OcSort(BaseTracker):\n \n     @BaseTracker.setup_decorator\n     @BaseTracker.per_class_decorator\n-    def update(self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None) -> np.ndarray:\n+    def update(\n+        self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None\n+    ) -> np.ndarray:\n         """"""""""""\n         Params:\n           dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\n@@ -251,7 +263,7 @@ class OcSort(BaseTracker):\n         h, w = img.",add,Added STORM - 2056 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/strongsort/sort/detection.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nclass Detection(object):\n    """"""""""""\n    This class represents a bounding box detection in a single image.\n\n    Parameters\n    ----------\n    tlwh : array_like\n        Bounding box in format `(x, y, w, h)`.\n    confidence : float\n        Detector confidence score.\n    feature : array_like\n        A feature vector that describes the object contained in this image.\n\n    Attributes\n    ----------\n    tlwh : ndarray\n        Bounding box in format `(top left x, top left y, width, height)`.\n    confidence : ndarray\n        Detector confidence score.\n    feature : ndarray | NoneType\n        A feature vector that describes the object contained in this image.\n\n    """"""""""""\n\n    def __init__(self, tlwh, conf, cls, det_ind, feat):\n        self.tlwh = tlwh\n        self.conf = conf\n        self.cls = cls\n        self.det_ind = det_ind\n        self.feat = feat\n\n    def to_xyah(self):\n        """"""""""""Convert bounding bo","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n\nclass Detection(object):\n    """"""""""""\n    This class represents a bounding box detection in a single image.\n\n    Parameters\n    ----------\n    tlwh : array_like\n        Bounding box in format `(x, y, w, h)`.\n    confidence : float\n        Detector confidence score.\n    feature : array_like\n        A feature vector that describes the object contained in this image.\n\n    Attributes\n    ----------\n    tlwh : ndarray\n        Bounding box in format `(top left x, top left y, width, height)`.\n    confidence : ndarray\n        Detector confidence score.\n    feature : ndarray | NoneType\n        A feature vector that describes the object contained in this image.\n\n    """"""""""""\n\n    def __init__(self, tlwh, conf, cls, det_ind, feat):\n        self.tlwh = tlwh\n        self.conf = conf\n        self.cls = cls\n        self.det_ind = det_ind\n        self.feat = feat\n\n    def to_xyah(self):\n        """"""""""""Convert bounding ","@@ -1,5 +1,6 @@\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n+\n class Detection(object):\n     """"""""""""\n     This class represents a bounding box detection in a single image.\n",add,Added example for MAP type in documentation
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/strongsort/sort/linear_assignment.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import\n\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\nfrom boxmot.utils.matching import chi2inv95\n\nINFTY_COST = 1e5\n\n\ndef min_cost_matching(\n    distance_metric,\n    max_distance,\n    tracks,\n    detections,\n    track_indices=None,\n    detection_indices=None,\n):\n    """"""""""""Solve linear assignment problem.\n    Parameters\n    ----------\n    distance_metric : Callable[List[Track], List[Detection], List[int], List[int]) -> ndarray\n        The distance metric is given a list of tracks and detections as well as\n        a list of N track indices and M detection indices. The metric should\n        return the NxM dimensional cost matrix, where element (i, j) is the\n        association cost between the i-th track in the given track indices and\n        the j-th detection in the given detection_indices.\n    max_distance : float\n        Gating threshold. Associatio","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import\n\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\nfrom boxmot.utils.matching import chi2inv95\n\nINFTY_COST = 1e5\n\n\ndef min_cost_matching(\n    distance_metric,\n    max_distance,\n    tracks,\n    detections,\n    track_indices=None,\n    detection_indices=None,\n):\n    """"""""""""Solve linear assignment problem.\n    Parameters\n    ----------\n    distance_metric : Callable[List[Track], List[Detection], List[int], List[int]) -> ndarray\n        The distance metric is given a list of tracks and detections as well as\n        a list of N track indices and M detection indices. The metric should\n        return the NxM dimensional cost matrix, where element (i, j) is the\n        association cost between the i-th track in the given track indices and\n        the j-th detection in the given detection_indices.\n    max_distance : float\n        Gating threshold. Associatio","@@ -182,16 +182,13 @@ def gate_cost_matrix(\n     ndarray\n         Returns the modified cost matrix.\n     """"""""""""\n-    \n+\n     gating_threshold = chi2inv95[4]\n     measurements = np.asarray([detections[i].to_xyah() for i in detection_indices])\n     for row, track_idx in enumerate(track_indices):\n         track = tracks[track_idx]\n         gating_distance = track.kf.gating_distance(\n-            track.mean,\n-            track.covariance,\n-            measurements,\n-            only_position\n+            track.mean, track.covariance, measurements, only_position\n         )\n         cost_matrix[row, gating_distance > gating_threshold] = gated_cost\n         cost_matrix[row] = (\n",update,Added Type name for DFI ( # 3 )
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/strongsort/sort/track.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.aabb.xyah_kf import KalmanFilterXYAH\n\n\nclass TrackState:\n    """"""""""""\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n\n    """"""""""""\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    """"""""""""\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initi","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.aabb.xyah_kf import KalmanFilterXYAH\n\n\nclass TrackState:\n    """"""""""""\n    Enumeration type for the single target track state. Newly created tracks are\n    classified as `tentative` until enough evidence has been collected. Then,\n    the track state is changed to `confirmed`. Tracks that are no longer alive\n    are classified as `deleted` to mark them for removal from the set of active\n    tracks.\n\n    """"""""""""\n\n    Tentative = 1\n    Confirmed = 2\n    Deleted = 3\n\n\nclass Track:\n    """"""""""""\n    A single target track with state space `(x, y, a, h)` and associated\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n    aspect ratio and `h` is the height.\n\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector of the initial state distribution.\n    covariance : ndarray\n        Covariance matrix of the initi","@@ -88,7 +88,14 @@ class Track:\n         self.ema_alpha = ema_alpha\n \n         # start with confirmed in Ci as test expect equal amount of outputs as inputs\n-        self.state = TrackState.Confirmed if (os.getenv('GITHUB_ACTIONS') == 'true' and os.getenv('GITHUB_JOB') != 'mot-metrics-benchmark') else TrackState.Tentative\n+        self.state = (\n+            TrackState.Confirmed\n+            if (\n+                os.getenv(""""GITHUB_ACTIONS"""") == """"true""""\n+                and os.getenv(""""GITHUB_JOB"""") != """"mot-metrics-benchmark""""\n+            )\n+            else TrackState.Tentative\n+        )\n         self.features = []\n         if detection.feat is not None:\n             detection.feat /= np.linalg.norm(detection.feat)\n",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/strongsort/sort/tracker.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import\n\nimport numpy as np\n\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.strongsort.sort import iou_matching, linear_assignment\nfrom boxmot.trackers.strongsort.sort.track import Track\nfrom boxmot.utils.matching import chi2inv95\n\n\nclass Tracker:\n    """"""""""""\n    This is the multi-target tracker.\n    Parameters\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        A distance metric for measurement-to-track association.\n    max_age : int\n        Maximum number of missed misses before a track is deleted.\n    n_init : int\n        Number of consecutive detections before the track is confirmed. The\n        track state is set to `Deleted` if a miss occurs within the first\n        `n_init` frames.\n    Attributes\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        The distance metric used for measurement to track ass","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom __future__ import absolute_import\n\nimport numpy as np\n\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.strongsort.sort import iou_matching, linear_assignment\nfrom boxmot.trackers.strongsort.sort.track import Track\nfrom boxmot.utils.matching import chi2inv95\n\n\nclass Tracker:\n    """"""""""""\n    This is the multi-target tracker.\n    Parameters\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        A distance metric for measurement-to-track association.\n    max_age : int\n        Maximum number of missed misses before a track is deleted.\n    n_init : int\n        Number of consecutive detections before the track is confirmed. The\n        track state is set to `Deleted` if a miss occurs within the first\n        `n_init` frames.\n    Attributes\n    ----------\n    metric : nn_matching.NearestNeighborDistanceMetric\n        The distance metric used for measurement to track ass","@@ -57,7 +57,7 @@ class Tracker:\n \n         self.tracks = []\n         self._next_id = 1\n-        self.cmc = get_cmc_method('ecc')()\n+        self.cmc = get_cmc_method(""""ecc"""")()\n \n     def predict(self):\n         """"""""""""Propagate track state distributions one time step forward.\n",add,Added net . kotlin .
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/strongsort/strongsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom pathlib import Path\n\nimport numpy as np\nfrom torch import device\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.strongsort.sort.detection import Detection\nfrom boxmot.trackers.strongsort.sort.tracker import Tracker\nfrom boxmot.utils.matching import NearestNeighborDistanceMetric\nfrom boxmot.utils.ops import xyxy2tlwh\n\n\nclass StrongSort(object):\n    """"""""""""\n    StrongSORT Tracker: A tracking algorithm that utilizes a combination of appearance and motion-based tracking.\n\n    Args:\n        model_weights (str): Path to the model weights for ReID (Re-Identification).\n        device (str): Device on which to run the model (e.g., 'cpu' or 'cuda').\n        fp16 (bool): Whether to use half-precision (fp16) for faster inference on compatible devices.\n        per_class (bool, optional","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom pathlib import Path\n\nimport numpy as np\nfrom torch import device\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.strongsort.sort.detection import Detection\nfrom boxmot.trackers.strongsort.sort.tracker import Tracker\nfrom boxmot.utils.matching import NearestNeighborDistanceMetric\nfrom boxmot.utils.ops import xyxy2tlwh\n\n\nclass StrongSort(object):\n    """"""""""""\n    StrongSORT Tracker: A tracking algorithm that utilizes a combination of appearance and motion-based tracking.\n\n    Args:\n        model_weights (str): Path to the model weights for ReID (Re-Identification).\n        device (str): Device on which to run the model (e.g., 'cpu' or 'cuda').\n        fp16 (bool): Whether to use half-precision (fp16) for faster inference on compatible devices.\n        per_class (bool, optional","@@ -31,6 +31,7 @@ class StrongSort(object):\n         mc_lambda (float, optional): Weight for motion consistency in the track state estimation. Higher values give more weight to motion information.\n         ema_alpha (float, optional): Alpha value for exponential moving average (EMA) update of appearance features. Controls the contribution of new and old embeddings in the ReID model.\n     """"""""""""\n+\n     def __init__(\n         self,\n         reid_weights: Path,\n@@ -61,10 +62,12 @@ class StrongSort(object):\n             mc_lambda=mc_lambda,\n             ema_alpha=ema_alpha,\n         )\n-        self.cmc = get_cmc_method('ecc')()\n+        self.cmc = get_cmc_method(""""ecc"""")()\n \n     @BaseTracker.per_class_decorator\n-    def update(self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None) -> np.ndarray:\n+    def update(\n+        self, dets: np.ndarray, img: np.ndarray, embs: np.ndarray = None\n+    ) -> np.ndarray:\n         assert isinstance(\n             dets, np.ndarray\n         ), f""""Unsupported 'dets' input format '{type(dets)}', valid format is np.ndarray""""\n@@ -104,9 +107,10 @@ class StrongSort(object):\n \n         tlwh = xyxy2tlwh(xyxy)\n         detections = [\n-            Detection(box, conf, cls, det_ind, feat) for\n-            box, conf, cls, det_ind, feat in\n-            zip(tlwh, confs, clss, det_ind, features)\n+            Detection(box, conf, cls, det_ind, feat)\n+            for box, conf, cls, det_ind, feat in zip(\n+                tlwh, confs, clss, det_ind, features\n+            )\n         ]\n \n         # update tracker\n@@ -127,7 +131,9 @@ class StrongSort(object):\n             det_ind = track.det_ind\n \n             outputs.append(\n-                np.concatenate(([x1, y1, x2, y2], [id], [conf], [cls], [det_ind])).reshape(1, -1)\n+                np.concatenate(\n+                    ([x1, y1, x2, y2], [id], [conf], [cls], [det_ind])\n+                ).reshape(1, -1)\n             )\n         if len(outputs) > 0",add,Added format51l
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/trackers/strongsort/strongsort_kf.py,"# vim: expandtab:ts=4:sw=4\nimport numpy as np\nimport scipy.linalg\n\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919}\n\n\nclass KalmanFilter(object):\n    """"""""""""\n    A simple Kalman filter for tracking bounding boxes in image space.\n\n    The 8-dimensional state space\n\n        x, y, a, h, vx, vy, va, vh\n\n    contains the bounding box center position (x, y), aspect ratio a, height h,\n    and their respective velocities.\n\n    Object motion follows a constant velocity model. The bounding box location\n    (x, y, a, h) is taken as direct observation of the state space (linear\n    observation model).\n\n    """"""""""""\n\n    def __init__(self):\n        ","# vim: expandtab:ts=4:sw=4\nimport numpy as np\nimport scipy.linalg\n\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919,\n}\n\n\nclass KalmanFilter(object):\n    """"""""""""\n    A simple Kalman filter for tracking bounding boxes in image space.\n\n    The 8-dimensional state space\n\n        x, y, a, h, vx, vy, va, vh\n\n    contains the bounding box center position (x, y), aspect ratio a, height h,\n    and their respective velocities.\n\n    Object motion follows a constant velocity model. The bounding box location\n    (x, y, a, h) is taken as direct observation of the state space (linear\n    observation model).\n\n    """"""""""""\n\n    def __init__(self):\n     ","@@ -16,7 +16,8 @@ chi2inv95 = {\n     6: 12.592,\n     7: 14.067,\n     8: 15.507,\n-    9: 16.919}\n+    9: 16.919,\n+}\n \n \n class KalmanFilter(object):\n@@ -37,7 +38,7 @@ class KalmanFilter(object):\n     """"""""""""\n \n     def __init__(self):\n-        ndim, dt = 4, 1.\n+        ndim, dt = 4, 1.0\n \n         # Create Kalman filter model matrices.\n         self._motion_mat = np.eye(2 * ndim, 2 * ndim)\n@@ -49,8 +50,8 @@ class KalmanFilter(object):\n         # Motion and observation uncertainty are chosen relative to the current\n         # state estimate. These weights control the amount of uncertainty in\n         # the model. This is a bit hacky.\n-        self._std_weight_position = 1. / 20\n-        self._std_weight_velocity = 1. / 160\n+        self._std_weight_position = 1.0 / 20\n+        self._std_weight_velocity = 1.0 / 160\n \n     def initiate(self, measurement):\n         """"""""""""Create track from unassociated measurement.\n@@ -81,7 +82,8 @@ class KalmanFilter(object):\n             10 * self._std_weight_velocity * measurement[3],\n             10 * self._std_weight_velocity * measurement[3],\n             1e-5,\n-            10 * self._std_weight_velocity * measurement[3]]\n+            10 * self._std_weight_velocity * measurement[3],\n+        ]\n         covariance = np.diag(np.square(std))\n         return mean, covariance\n \n@@ -108,12 +110,14 @@ class KalmanFilter(object):\n             self._std_weight_position * mean[3],\n             self._std_weight_position * mean[3],\n             1e-2,\n-            self._std_weight_position * mean[3]]\n+            self._std_weight_position * mean[3],\n+        ]\n         std_vel = [\n             self._std_weight_velocity * mean[3],\n             self._std_weight_velocity * mean[3],\n             1e-5,\n-            self._std_weight_velocity * mean[3]]\n+            self._std_weight_velocity * mean[3],\n+        ]\n         motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\n \n         mean = np",add,Added the UNSTARTED state to the YouTube PlayerState enum
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nimport threading\nfrom pathlib import Path\n\nimport numpy as np\n\n# global logger\nfrom loguru import logger\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / 'data'\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of BoxMOT multiprocessing threads\n\n\ndef only_main_thread(record):\n    # Check if the current thread is the main thread\n    return threading.current_thread().name == """"MainThread""""\n\nlogger.remove()\nlogger.add(sys.stderr, filter=only_main_thread, colorize=True, level=""""INFO"""")","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nimport threading\nfrom pathlib import Path\n\nimport numpy as np\n\n# global logger\nfrom loguru import logger\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\nDATA = ROOT / """"data""""\nBOXMOT = ROOT / """"boxmot""""\nEXAMPLES = ROOT / """"tracking""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\nWEIGHTS = ROOT / """"tracking"""" / """"weights""""\nREQUIREMENTS = ROOT / """"requirements.txt""""\n\n# number of BoxMOT multiprocessing threads\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))\n\n\n\ndef only_main_thread(record):\n    # Check if the current thread is the main thread\n    return threading.current_thread().name == """"MainThread""""\n\n\nlogger.remove()\nlogger.add(sys.stderr, filter=only_main_thread, colorize=True, level=""""INFO"""")\n","@@ -12,19 +12,22 @@ from loguru import logger\n \n FILE = Path(__file__).resolve()\n ROOT = FILE.parents[2]  # root directory\n-DATA = ROOT / 'data'\n+DATA = ROOT / """"data""""\n BOXMOT = ROOT / """"boxmot""""\n EXAMPLES = ROOT / """"tracking""""\n TRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\n WEIGHTS = ROOT / """"tracking"""" / """"weights""""\n REQUIREMENTS = ROOT / """"requirements.txt""""\n \n-NUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of BoxMOT multiprocessing threads\n+# number of BoxMOT multiprocessing threads\n+NUM_THREADS = min(8, max(1, os.cpu_count() - 1))\n+\n \n \n def only_main_thread(record):\n     # Check if the current thread is the main thread\n     return threading.current_thread().name == """"MainThread""""\n \n+\n logger.remove()\n-logger.add(sys.stderr, filter=only_main_thread, colorize=True, level=""""INFO"""")\n\ No newline at end of file\n+logger.add(sys.stderr, filter=only_main_thread, colorize=True, level=""""INFO"""")\n",add,Add jni error string
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/utils/association.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.utils.iou import AssociationFunction\n\n\ndef speed_direction_batch(dets, tracks):\n    tracks = tracks[..., np.newaxis]\n    CX1, CY1 = (dets[:, 0] + dets[:, 2]) / 2.0, (dets[:, 1] + dets[:, 3]) / 2.0\n    CX2, CY2 = (tracks[:, 0] + tracks[:, 2]) / 2.0, (tracks[:, 1] + tracks[:, 3]) / 2.0\n    dx = CX1 - CX2\n    dy = CY1 - CY2\n    norm = np.sqrt(dx**2 + dy**2) + 1e-6\n    dx = dx / norm\n    dy = dy / norm\n    return dy, dx  # size: num_track x num_det\n\n\ndef linear_assignment(cost_matrix):\n    try:\n        import lap\n        _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n        return np.array([[y[i], i] for i in x if i >= 0])  #\n    except ImportError:\n        from scipy.optimize import linear_sum_assignment\n        x, y = linear_sum_assignment(cost_matrix)\n        return np.array([list(zip(x, y))])\n\n\ndef associate_detections_to_trackers(detections, trackers, iou_threshold=0","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\n\nfrom boxmot.utils.iou import AssociationFunction\n\n\ndef speed_direction_batch(dets, tracks):\n    tracks = tracks[..., np.newaxis]\n    CX1, CY1 = (dets[:, 0] + dets[:, 2]) / 2.0, (dets[:, 1] + dets[:, 3]) / 2.0\n    CX2, CY2 = (tracks[:, 0] + tracks[:, 2]) / 2.0, (tracks[:, 1] + tracks[:, 3]) / 2.0\n    dx = CX1 - CX2\n    dy = CY1 - CY2\n    norm = np.sqrt(dx**2 + dy**2) + 1e-6\n    dx = dx / norm\n    dy = dy / norm\n    return dy, dx  # size: num_track x num_det\n\n\ndef linear_assignment(cost_matrix):\n    try:\n        import lap\n\n        _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n        return np.array([[y[i], i] for i in x if i >= 0])  #\n    except ImportError:\n        from scipy.optimize import linear_sum_assignment\n\n        x, y = linear_sum_assignment(cost_matrix)\n        return np.array([list(zip(x, y))])\n\n\ndef associate_detections_to_trackers(detections, trackers, iou_thresho","@@ -20,10 +20,12 @@ def speed_direction_batch(dets, tracks):\n def linear_assignment(cost_matrix):\n     try:\n         import lap\n+\n         _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n         return np.array([[y[i], i] for i in x if i >= 0])  #\n     except ImportError:\n         from scipy.optimize import linear_sum_assignment\n+\n         x, y = linear_sum_assignment(cost_matrix)\n         return np.array([list(zip(x, y))])\n \n@@ -122,7 +124,6 @@ def associate(\n     w_assoc_emb=None,\n     aw_off=None,\n     aw_param=None,\n-    \n ):\n     if len(trackers) == 0:\n         return (\n@@ -144,7 +145,7 @@ def associate(\n     valid_mask[np.where(previous_obs[:, 4] < 0)] = 0\n \n     iou_matrix = asso_func(detections, trackers)\n-    #iou_matrix = iou_batch(detections, trackers)\n+    # iou_matrix = iou_batch(detections, trackers)\n     scores = np.repeat(detections[:, -1][:, np.newaxis], trackers.shape[0], axis=1)\n     # iou_matrix = iou_matrix * scores # a trick sometiems works, we don't encourage this\n     valid_mask = np.repeat(valid_mask[:, np.newaxis], X.shape[1], axis=1)\n@@ -164,7 +165,9 @@ def associate(\n                 emb_cost = emb_cost\n                 emb_cost[iou_matrix <= 0] = 0\n                 if not aw_off:\n-                    emb_cost = compute_aw_max_metric(emb_cost, w_assoc_emb, bottom=aw_param)\n+                    emb_cost = compute_aw_max_metric(\n+                        emb_cost, w_assoc_emb, bottom=aw_param\n+                    )\n                 else:\n                     emb_cost *= w_assoc_emb\n \n",add,Added STORM - 01 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/utils/checks.py,"import subprocess\nfrom pathlib import Path\nfrom typing import Optional\n\nimport pkg_resources\n\nfrom boxmot.utils import logger as LOGGER\n\nREQUIREMENTS_FILE = Path(""""requirements.txt"""")\n\nclass RequirementsChecker:\n    def __init__(self, group: str = None, requirements_file: Path = REQUIREMENTS_FILE):\n        """"""""""""\n        If `group` is provided, we'll sync that PDM/uv dependency-group.\n        Otherwise we'll read requirements_file and pip-install missing packages.\n        """"""""""""\n        self.group = group\n        self.requirements_file = requirements_file\n\n    def check_requirements(self):\n        if self.group:\n            self._sync_group(self.group)\n        else:\n            self._check_from_requirements()\n\n    def _check_from_requirements(self):\n        # parse requirements.txt\n        with self.requirements_file.open() as f:\n            reqs = pkg_resources.parse_requirements(f)\n        self._check_packages(reqs)\n\n    def check_packages(self, require","import subprocess\nfrom pathlib import Path\nfrom typing import Optional\n\nimport pkg_resources\n\nfrom boxmot.utils import logger as LOGGER\n\nREQUIREMENTS_FILE = Path(""""requirements.txt"""")\n\n\nclass RequirementsChecker:\n    def __init__(self, group: str = None, requirements_file: Path = REQUIREMENTS_FILE):\n        """"""""""""\n        If `group` is provided, we'll sync that PDM/uv dependency-group.\n        Otherwise we'll read requirements_file and pip-install missing packages.\n        """"""""""""\n        self.group = group\n        self.requirements_file = requirements_file\n\n    def check_requirements(self):\n        if self.group:\n            self._sync_group(self.group)\n        else:\n            self._check_from_requirements()\n\n    def _check_from_requirements(self):\n        # parse requirements.txt\n        with self.requirements_file.open() as f:\n            reqs = pkg_resources.parse_requirements(f)\n        self._check_packages(reqs)\n\n    def check_packages(self, requi","@@ -8,6 +8,7 @@ from boxmot.utils import logger as LOGGER\n \n REQUIREMENTS_FILE = Path(""""requirements.txt"""")\n \n+\n class RequirementsChecker:\n     def __init__(self, group: str = None, requirements_file: Path = REQUIREMENTS_FILE):\n         """"""""""""\n@@ -53,7 +54,9 @@ class RequirementsChecker:\n             LOGGER.error(f""""Failed to install packages: {e}"""")\n             raise RuntimeError(f""""Failed to install packages: {e}"""")\n \n-    def sync_group_or_extra(self, group: Optional[str] = None, extra: Optional[str] = None):\n+    def sync_group_or_extra(\n+        self, group: Optional[str] = None, extra: Optional[str] = None\n+    ):\n         """"""""""""\n         Sync a uv dependency-group or an extra.\n \n@@ -68,11 +71,7 @@ class RequirementsChecker:\n         kind = """"group"""" if group else """"extra""""\n         LOGGER.warning(f""""Syncing {kind} '{name}'..."""")\n \n-        cmd = [\n-            """"uv"""", """"sync"""",\n-            """"--no-default-groups"""",\n-            f""""--{kind}"""", name\n-        ]\n+        cmd = [""""uv"""", """"sync"""", """"--no-default-groups"""", f""""--{kind}"""", name]\n \n         try:\n             subprocess.check_call(cmd)\n@@ -88,4 +87,4 @@ if __name__ == """"__main__"""":\n     RequirementsChecker(group=""""tflite"""").check_requirements()\n \n     # 2) to fall back on a requirements.txt:\n-    # RequirementsChecker().check_requirements()\n\ No newline at end of file\n+    # RequirementsChecker().check_requirements()\n",add,Don ' t schedule script execution on the main thread
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/utils/iou.py,"import cv2 as cv\nimport numpy as np\n\n\ndef iou_obb_pair(i, j, bboxes1, bboxes2):\n    """"""""""""\n    Compute IoU for the rotated rectangles at index i and j in the batches `bboxes1`, `bboxes2` .\n    """"""""""""\n    rect1 = bboxes1[int(i)]\n    rect2 = bboxes2[int(j)]\n\n    (cx1, cy1, w1, h1, angle1) = rect1[0:5]\n    (cx2, cy2, w2, h2, angle2) = rect2[0:5]\n\n    r1 = ((cx1, cy1), (w1, h1), angle1)\n    r2 = ((cx2, cy2), (w2, h2), angle2)\n\n    # Compute intersection\n    ret, intersect = cv.rotatedRectangleIntersection(r1, r2)\n    if ret == 0 or intersect is None:\n        return 0.0  # No intersection\n\n    # Calculate intersection area\n    intersection_area = cv.contourArea(intersect)\n\n    # Calculate union area\n    area1 = w1 * h1\n    area2 = w2 * h2\n    union_area = area1 + area2 - intersection_area\n\n    # Compute IoU\n    return intersection_area / union_area if union_area > 0 else 0.0\n\n\nclass AssociationFunction:\n    def __init__(self, w, h, asso_mode=""""iou""""):\n   ","import cv2 as cv\nimport numpy as np\n\n\ndef iou_obb_pair(i, j, bboxes1, bboxes2):\n    """"""""""""\n    Compute IoU for the rotated rectangles at index i and j in the batches `bboxes1`, `bboxes2` .\n    """"""""""""\n    rect1 = bboxes1[int(i)]\n    rect2 = bboxes2[int(j)]\n\n    (cx1, cy1, w1, h1, angle1) = rect1[0:5]\n    (cx2, cy2, w2, h2, angle2) = rect2[0:5]\n\n    r1 = ((cx1, cy1), (w1, h1), angle1)\n    r2 = ((cx2, cy2), (w2, h2), angle2)\n\n    # Compute intersection\n    ret, intersect = cv.rotatedRectangleIntersection(r1, r2)\n    if ret == 0 or intersect is None:\n        return 0.0  # No intersection\n\n    # Calculate intersection area\n    intersection_area = cv.contourArea(intersect)\n\n    # Calculate union area\n    area1 = w1 * h1\n    area2 = w2 * h2\n    union_area = area1 + area2 - intersection_area\n\n    # Compute IoU\n    return intersection_area / union_area if union_area > 0 else 0.0\n\n\nclass AssociationFunction:\n    def __init__(self, w, h, asso_mode=""""iou""""):\n   ","@@ -60,9 +60,9 @@ class AssociationFunction:\n         h = np.maximum(0.0, yy2 - yy1)\n         wh = w * h\n         o = wh / (\n-                (bboxes1[..., 2] - bboxes1[..., 0]) * (bboxes1[..., 3] - bboxes1[..., 1]) +\n-                (bboxes2[..., 2] - bboxes2[..., 0]) * (bboxes2[..., 3] - bboxes2[..., 1]) -\n-                wh\n+            (bboxes1[..., 2] - bboxes1[..., 0]) * (bboxes1[..., 3] - bboxes1[..., 1])\n+            + (bboxes2[..., 2] - bboxes2[..., 0]) * (bboxes2[..., 3] - bboxes2[..., 1])\n+            - wh\n         )\n         return o\n \n@@ -178,7 +178,7 @@ class AssociationFunction:\n         centroids2 = np.expand_dims(centroids2, 0)\n \n         distances = np.sqrt(np.sum((centroids1 - centroids2) ** 2, axis=-1))\n-        norm_factor = np.sqrt(self.w ** 2 + self.h ** 2)\n+        norm_factor = np.sqrt(self.w**2 + self.h**2)\n         normalized_distances = distances / norm_factor\n \n         return 1 - normalized_distances\n@@ -191,7 +191,7 @@ class AssociationFunction:\n         centroids2 = np.expand_dims(centroids2, 0)\n \n         distances = np.sqrt(np.sum((centroids1 - centroids2) ** 2, axis=-1))\n-        norm_factor = np.sqrt(self.w ** 2 + self.h ** 2)\n+        norm_factor = np.sqrt(self.w**2 + self.h**2)\n         normalized_distances = distances / norm_factor\n \n         return 1 - normalized_distances\n@@ -251,7 +251,7 @@ class AssociationFunction:\n         h2 = h2 + epsilon\n         h1 = h1 + epsilon\n         arctan_diff = np.arctan(w2 / h2) - np.arctan(w1 / h1)\n-        v = (4 / (np.pi ** 2)) * (arctan_diff ** 2)\n+        v = (4 / (np.pi**2)) * (arctan_diff**2)\n \n         # Calculate alpha\n         S = 1 - iou\n@@ -284,9 +284,9 @@ class AssociationFunction:\n         h = np.maximum(0.0, yy2 - yy1)\n         wh = w * h\n         iou = wh / (\n-                (bboxes1[..., 2] - bboxes1[..., 0]) * (bboxes1[..., 3] - bboxes1[..., 1]) +\n-                (bboxes2[..., 2] - bboxes2[..., 0]) * (bboxes2[..., 3] - bboxes2",add,Added STORM - 146 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/utils/matching.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport lap\nimport numpy as np\nimport scipy\nimport torch\nfrom scipy.spatial.distance import cdist\n\nfrom boxmot.utils.iou import AssociationFunction\n\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919,\n}\n\n\ndef merge_matches(m1, m2, shape):\n    O, P, Q = shape\n    m1 = np.asarray(m1)\n    m2 = np.asarray(m2)\n\n    M1 = scipy.sparse.coo_matrix((np.ones(len(m1)), (m1[:, 0], m1[:, 1])), shape=(O, P))\n    M2 = scipy.sparse.coo_matrix((np.ones(len(m2)), (m2[:, 0], m2[:, 1])), shape=(P, Q))\n\n    mask = M1 * M2\n    match = mask.nonzero()\n    match = list(zip(match[0], match[1]))\n    unmatched_O =","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport lap\nimport numpy as np\nimport scipy\nimport torch\nfrom scipy.spatial.distance import cdist\n\nfrom boxmot.utils.iou import AssociationFunction\n\n""""""""""""\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\nfunction and used as Mahalanobis gating threshold.\n""""""""""""\nchi2inv95 = {\n    1: 3.8415,\n    2: 5.9915,\n    3: 7.8147,\n    4: 9.4877,\n    5: 11.070,\n    6: 12.592,\n    7: 14.067,\n    8: 15.507,\n    9: 16.919,\n}\n\n\ndef merge_matches(m1, m2, shape):\n    O, P, Q = shape\n    m1 = np.asarray(m1)\n    m2 = np.asarray(m2)\n\n    M1 = scipy.sparse.coo_matrix((np.ones(len(m1)), (m1[:, 0], m1[:, 1])), shape=(O, P))\n    M2 = scipy.sparse.coo_matrix((np.ones(len(m2)), (m2[:, 0], m2[:, 1])), shape=(P, Q))\n\n    mask = M1 * M2\n    match = mask.nonzero()\n    match = list(zip(match[0], match[1]))\n    unmatched_O =","@@ -91,6 +91,7 @@ def ious(atlbrs, btlbrs):\n \n     return ious\n \n+\n def d_iou_distance(atracks, btracks):\n     """"""""""""\n     Compute cost based on IoU\n@@ -118,6 +119,7 @@ def d_iou_distance(atracks, btracks):\n \n     return cost_matrix\n \n+\n def iou_distance(atracks, btracks):\n     """"""""""""\n     Compute cost based on IoU\n@@ -381,7 +383,7 @@ class NearestNeighborDistanceMetric(object):\n         for feature, target in zip(features, targets):\n             self.samples.setdefault(target, []).append(feature)\n             if self.budget is not None:\n-                self.samples[target] = self.samples[target][-self.budget:]\n+                self.samples[target] = self.samples[target][-self.budget :]\n         self.samples = {k: self.samples[k] for k in active_targets}\n \n     def distance(self, features, targets):\n",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/utils/misc.py,"from pathlib import Path\n\n\ndef increment_path(path, exist_ok=False, sep="""""""", mkdir=False):\n    """"""""""""\n    Generates an incremented file or directory path if it already exists, with an option to create the directory.\n\n    Args:\n        path (str or Path): Initial file or directory path.\n        exist_ok (bool): If True, returns the original path even if it exists.\n        sep (str): Separator to use between path stem and increment.\n        mkdir (bool): If True, creates the directory if it doesnât exist.\n\n    Returns:\n        Path: Incremented path, or original if exist_ok is True.\n        \n    Example:\n        runs/exp --> runs/exp2, runs/exp3, etc.\n    """"""""""""\n    path = Path(path)  # ensures OS compatibility\n    if path.exists() and not exist_ok:\n        base, suffix = (path.with_suffix(""""""""), path.suffix) if path.is_file() else (path, """""""")\n        \n        # Increment path until a non-existing one is found\n        for n in range(2, 9999):\n            new_pa","from pathlib import Path\n\n\ndef increment_path(path, exist_ok=False, sep="""""""", mkdir=False):\n    """"""""""""\n    Generates an incremented file or directory path if it already exists, with an option to create the directory.\n\n    Args:\n        path (str or Path): Initial file or directory path.\n        exist_ok (bool): If True, returns the original path even if it exists.\n        sep (str): Separator to use between path stem and increment.\n        mkdir (bool): If True, creates the directory if it doesnât exist.\n\n    Returns:\n        Path: Incremented path, or original if exist_ok is True.\n\n    Example:\n        runs/exp --> runs/exp2, runs/exp3, etc.\n    """"""""""""\n    path = Path(path)  # ensures OS compatibility\n    if path.exists() and not exist_ok:\n        base, suffix = (path.with_suffix(""""""""), path.suffix) if path.is_file() else (path, """""""")\n        \n        # Increment path until a non-existing one is found\n        for n in range(2, 9999):\n            new_path = f""""","@@ -13,7 +13,7 @@ def increment_path(path, exist_ok=False, sep="""""""", mkdir=False):\n \n     Returns:\n         Path: Incremented path, or original if exist_ok is True.\n-        \n+\n     Example:\n         runs/exp --> runs/exp2, runs/exp3, etc.\n     """"""""""""\n@@ -31,4 +31,4 @@ def increment_path(path, exist_ok=False, sep="""""""", mkdir=False):\n     if mkdir:\n         path.mkdir(parents=True, exist_ok=True)  # creates the directory if it doesnât exist\n \n-    return path\n\ No newline at end of file\n+    return path\n",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/utils/ops.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom typing import Tuple, Union\n\nimport cv2\nimport numpy as np\nimport torch\n\n\ndef xyxy2xywh(x):\n    """"""""""""\n    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format.\n\n    Args:\n        x (np.ndarray) or (torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n    Returns:\n       y (np.ndarray) or (torch.Tensor): The bounding box coordinates in (x, y, width, height) format.\n    """"""""""""\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n    y[..., 2] = x[..., 2] - x[..., 0]  # width\n    y[..., 3] = x[..., 3] - x[..., 1]  # height\n    return y\n\n\ndef xywh2xyxy(x):\n    """"""""""""\n    Convert bounding box coordinates from (x_c, y_c, width, height) format to\n    (x1, y1, x2, y2) format where (x1, y1) is the top-left corner and (x2, ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom typing import Tuple, Union\n\nimport cv2\nimport numpy as np\nimport torch\n\n\ndef xyxy2xywh(x):\n    """"""""""""\n    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format.\n\n    Args:\n        x (np.ndarray) or (torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n    Returns:\n       y (np.ndarray) or (torch.Tensor): The bounding box coordinates in (x, y, width, height) format.\n    """"""""""""\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n    y[..., 2] = x[..., 2] - x[..., 0]  # width\n    y[..., 3] = x[..., 3] - x[..., 1]  # height\n    return y\n\n\ndef xywh2xyxy(x):\n    """"""""""""\n    Convert bounding box coordinates from (x_c, y_c, width, height) format to\n    (x1, y1, x2, y2) format where (x1, y1) is the top-left corner and (x2, ","@@ -114,12 +114,12 @@ def xyxy2xysr(x):\n     """"""""""""\n     x = x[0:4]\n     y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n-    w = y[..., 2] - y[..., 0]  # width\n-    h = y[..., 3] - y[..., 1]  # height\n-    y[..., 0] = y[..., 0] + w / 2.0            # x center\n-    y[..., 1] = y[..., 1] + h / 2.0            # y center\n-    y[..., 2] = w * h                                  # scale (area)\n-    y[..., 3] = w / (h + 1e-6)                         # aspect ratio\n+    w = y[..., 2] - y[..., 0]        # width\n+    h = y[..., 3] - y[..., 1]        # height\n+    y[..., 0] = y[..., 0] + w / 2.0  # x center\n+    y[..., 1] = y[..., 1] + h / 2.0  # y center\n+    y[..., 2] = w * h                # scale (area)\n+    y[..., 3] = w / (h + 1e-6)       # aspect ratio\n     y = y.reshape((4, 1))\n     return y\n \n@@ -130,14 +130,14 @@ def letterbox(\n     color: Tuple[int, int, int] = (114, 114, 114),\n     auto: bool = True,\n     scaleFill: bool = False,\n-    scaleup: bool = True\n+    scaleup: bool = True,\n ) -> Tuple[np.ndarray, Tuple[float, float], Tuple[float, float]]:\n     """"""""""""\n     Resizes an image to a new shape while maintaining aspect ratio, padding with color if needed.\n \n     Args:\n         img (np.ndarray): The original image in BGR format.\n-        new_shape (Union[int, Tuple[int, int]], optional): Desired size as an integer (e.g., 640) \n+        new_shape (Union[int, Tuple[int, int]], optional): Desired size as an integer (e.g., 640)\n             or tuple (width, height). Default is (640, 640).\n         color (Tuple[int, int, int], optional): Padding color in BGR format. Default is (114, 114, 114).\n         auto (bool, optional): If True, adjusts padding to be a multiple of 32. Default is True.\n@@ -184,6 +184,8 @@ def letterbox(\n     # Add border to the image\n     top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n     left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n-    img = cv2.copyMakeBorder(img, top, bo",add,Added STORM - 149 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",boxmot/utils/torch_utils.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport platform\n\nimport torch\n\nfrom .. import __version__\nfrom . import logger as LOGGER\n\n\ndef get_system_info():\n    return f""""Yolo Tracking v{__version__} ð Python-{platform.python_version()} torch-{torch.__version__}""""\n\ndef parse_device(device):\n    device = str(device).lower().replace(""""cuda:"""", """""""").replace(""""none"""", """""""").replace(""""("""", """""""").replace("""")"""", """""""").replace(""""["""", """""""").replace(""""]"""", """""""").replace(""""'"""", """""""").replace("""" """", """""""")\n    return device\n\ndef assert_cuda_available(device):\n    if not (torch.cuda.is_available() and torch.cuda.device_count() >= len(device.replace("""","""", """"""""))):\n        install = (""""See https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n"""" if torch.cuda.device_count() == 0 else """""""")\n        raise ValueError(f""""Invalid CUDA 'device={device}' requested. Use 'device=cpu' or pass valid CUD","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport platform\n\nimport torch\n\nfrom .. import __version__\nfrom . import logger as LOGGER\n\n\ndef get_system_info():\n    return f""""Yolo Tracking v{__version__} ð Python-{platform.python_version()} torch-{torch.__version__}""""\n\n\ndef parse_device(device):\n    device = (\n        str(device)\n        .lower()\n        .replace(""""cuda:"""", """""""")\n        .replace(""""none"""", """""""")\n        .replace(""""("""", """""""")\n        .replace("""")"""", """""""")\n        .replace(""""["""", """""""")\n        .replace(""""]"""", """""""")\n        .replace(""""'"""", """""""")\n        .replace("""" """", """""""")\n    )\n    return device\n\n\ndef assert_cuda_available(device):\n    if not (\n        torch.cuda.is_available()\n        and torch.cuda.device_count() >= len(device.replace("""","""", """"""""))\n    ):\n        install = (\n            """"See https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n""""","@@ -12,17 +12,40 @@ from . import logger as LOGGER\n def get_system_info():\n     return f""""Yolo Tracking v{__version__} ð Python-{platform.python_version()} torch-{torch.__version__}""""\n \n+\n def parse_device(device):\n-    device = str(device).lower().replace(""""cuda:"""", """""""").replace(""""none"""", """""""").replace(""""("""", """""""").replace("""")"""", """""""").replace(""""["""", """""""").replace(""""]"""", """""""").replace(""""'"""", """""""").replace("""" """", """""""")\n+    device = (\n+        str(device)\n+        .lower()\n+        .replace(""""cuda:"""", """""""")\n+        .replace(""""none"""", """""""")\n+        .replace(""""("""", """""""")\n+        .replace("""")"""", """""""")\n+        .replace(""""["""", """""""")\n+        .replace(""""]"""", """""""")\n+        .replace(""""'"""", """""""")\n+        .replace("""" """", """""""")\n+    )\n     return device\n \n+\n def assert_cuda_available(device):\n-    if not (torch.cuda.is_available() and torch.cuda.device_count() >= len(device.replace("""","""", """"""""))):\n-        install = (""""See https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n"""" if torch.cuda.device_count() == 0 else """""""")\n-        raise ValueError(f""""Invalid CUDA 'device={device}' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n"""" +\n-                         f""""\ntorch.cuda.is_available(): {torch.cuda.is_available()}"""" +\n-                         f""""\ntorch.cuda.device_count(): {torch.cuda.device_count()}"""" +\n-                         f""""\nos.environ['CUDA_VISIBLE_DEVICES']: {os.environ.get('CUDA_VISIBLE_DEVICES', None)}\n{install}"""")\n+    if not (\n+        torch.cuda.is_available()\n+        and torch.cuda.device_count() >= len(device.replace("""","""", """"""""))\n+    ):\n+        install = (\n+            """"See https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n""""\n+            if torch.cuda.device_count() == 0\n+            else """"""",add,Add debug flag to enable pre - Table option
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tests/performance/test_cmcs_p.py,"import time\n\nimport cv2\nimport numpy as np\nimport pytest\n\nfrom boxmot.motion.cmc.ecc import ECC\nfrom boxmot.motion.cmc.orb import ORB\nfrom boxmot.motion.cmc.sift import SIFT\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.utils import ROOT\n\n\n# Fixture for creating CMC objects\n@pytest.fixture\ndef cmc_object(request):\n    cmc_class = request.param\n    return cmc_class()\n\n\n# Define the test function\n@pytest.mark.parametrize(""""cmc_object"""", [ECC, ORB, SIFT, SOF], indirect=True)\ndef test_cmc_apply(cmc_object):\n\n    # Create dummy images and detections\n    curr_img = cv2.imread(str(ROOT / 'assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000005.jpg'))\n    prev_img = cv2.imread(str(ROOT / 'assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000001.jpg'))\n    \n    print(curr_img.shape)\n    print(prev_img.shape)\n    \n    dets = np.array([[0, 0, 10, 10]])\n\n    n_runs = 100\n    start = time.process_time()\n    for i in range(0, n_runs):\n        warp_matrix = cmc_object.app","import time\n\nimport cv2\nimport numpy as np\nimport pytest\n\nfrom boxmot.motion.cmc.ecc import ECC\nfrom boxmot.motion.cmc.orb import ORB\nfrom boxmot.motion.cmc.sift import SIFT\nfrom boxmot.motion.cmc.sof import SOF\nfrom boxmot.utils import ROOT\n\n\n# Fixture for creating CMC objects\n@pytest.fixture\ndef cmc_object(request):\n    cmc_class = request.param\n    return cmc_class()\n\n\n# Define the test function\n@pytest.mark.parametrize(""""cmc_object"""", [ECC, ORB, SIFT, SOF], indirect=True)\ndef test_cmc_apply(cmc_object):\n\n    # Create dummy images and detections\n    curr_img = cv2.imread(\n        str(ROOT / """"assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000005.jpg"""")\n    )\n    prev_img = cv2.imread(\n        str(ROOT / """"assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000001.jpg"""")\n    )\n\n    print(curr_img.shape)\n    print(prev_img.shape)\n\n    dets = np.array([[0, 0, 10, 10]])\n\n    n_runs = 100\n    start = time.process_time()\n    for i in range(0, n_runs):\n        ","@@ -23,12 +23,16 @@ def cmc_object(request):\n def test_cmc_apply(cmc_object):\n \n     # Create dummy images and detections\n-    curr_img = cv2.imread(str(ROOT / 'assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000005.jpg'))\n-    prev_img = cv2.imread(str(ROOT / 'assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000001.jpg'))\n-    \n+    curr_img = cv2.imread(\n+        str(ROOT / """"assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000005.jpg"""")\n+    )\n+    prev_img = cv2.imread(\n+        str(ROOT / """"assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000001.jpg"""")\n+    )\n+\n     print(curr_img.shape)\n     print(prev_img.shape)\n-    \n+\n     dets = np.array([[0, 0, 10, 10]])\n \n     n_runs = 100\n@@ -43,4 +47,6 @@ def test_cmc_apply(cmc_object):\n     max_allowed_time = 0.1\n \n     # Assert that the elapsed time is within the allowed limit\n-    assert elapsed_time_per_interation < max_allowed_time, """"CMC algorithm processing time exceeds the allowed limit""""\n\ No newline at end of file\n+    assert (\n+        elapsed_time_per_interation < max_allowed_time\n+    ), """"CMC algorithm processing time exceeds the allowed limit""""\n",add,Added TypeSpec . set to Address BookView
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tests/performance/test_tracking_p.py,"import subprocess\nimport time\n\nimport numpy as np\nimport pytest\n\nfrom boxmot import create_tracker, get_tracker_config\nfrom boxmot.utils import WEIGHTS\nfrom tests.test_config import (\n    MOTION_N_APPEARANCE_TRACKING_NAMES,\n    MOTION_ONLY_TRACKING_NAMES,\n)\n\n\n@pytest.mark.parametrize(""""tracker_type"""", MOTION_ONLY_TRACKING_NAMES)\ndef test_motion_tracker_update_time(tracker_type):\n    tracker_conf = get_tracker_config(tracker_type)\n    tracker = create_tracker(\n        tracker_type=tracker_type,\n        tracker_config=tracker_conf,\n        reid_weights=WEIGHTS / 'mobilenetv2_x1_4_dukemtmcreid.pt',\n        device='cpu',\n        half=False,\n        per_class=False\n    )\n\n    rgb = np.random.randint(0, 255, size=(640, 640, 3), dtype=np.uint8)\n    det = np.array([[144, 212, 578, 480, 0.82, 0],\n                    [425, 281, 576, 472, 0.56, 65]])\n    \n    n_runs = 100\n\n    # Warm-up iteration to ensure initialization overhead is not measured\n    tracker.update","import subprocess\nimport time\n\nimport numpy as np\nimport pytest\n\nfrom boxmot import create_tracker, get_tracker_config\nfrom boxmot.utils import WEIGHTS\nfrom tests.test_config import (\n    MOTION_N_APPEARANCE_TRACKING_NAMES,\n    MOTION_ONLY_TRACKING_NAMES,\n)\n\n\n@pytest.mark.parametrize(""""tracker_type"""", MOTION_ONLY_TRACKING_NAMES)\ndef test_motion_tracker_update_time(tracker_type):\n    tracker_conf = get_tracker_config(tracker_type)\n    tracker = create_tracker(\n        tracker_type=tracker_type,\n        tracker_config=tracker_conf,\n        reid_weights=WEIGHTS / """"mobilenetv2_x1_4_dukemtmcreid.pt"""",\n        device=""""cpu"""",\n        half=False,\n        per_class=False,\n    )\n\n    rgb = np.random.randint(0, 255, size=(640, 640, 3), dtype=np.uint8)\n    det = np.array([[144, 212, 578, 480, 0.82, 0],\n                    [425, 281, 576, 472, 0.56, 65]])\n\n    n_runs = 100\n\n    # Warm-up iteration to ensure initialization overhead is not measured\n    tracker.updat","@@ -18,40 +18,40 @@ def test_motion_tracker_update_time(tracker_type):\n     tracker = create_tracker(\n         tracker_type=tracker_type,\n         tracker_config=tracker_conf,\n-        reid_weights=WEIGHTS / 'mobilenetv2_x1_4_dukemtmcreid.pt',\n-        device='cpu',\n+        reid_weights=WEIGHTS / """"mobilenetv2_x1_4_dukemtmcreid.pt"""",\n+        device=""""cpu"""",\n         half=False,\n-        per_class=False\n+        per_class=False,\n     )\n \n     rgb = np.random.randint(0, 255, size=(640, 640, 3), dtype=np.uint8)\n     det = np.array([[144, 212, 578, 480, 0.82, 0],\n                     [425, 281, 576, 472, 0.56, 65]])\n-    \n+\n     n_runs = 100\n \n     # Warm-up iteration to ensure initialization overhead is not measured\n     tracker.update(det, rgb)\n-    \n+\n     start = time.perf_counter()\n     for _ in range(n_runs):\n         tracker.update(det, rgb)\n     end = time.perf_counter()\n-    \n+\n     elapsed_time_per_iteration = (end - start) / n_runs\n     fps = 1.0 / elapsed_time_per_iteration\n-    \n+\n     # Print FPS for each tracker type\n     print(f""""Tracker type: {tracker_type} - FPS: {fps:.2f}"""")\n     result = subprocess.run(\n         """"cat /proc/cpuinfo | grep 'model name' | head -1"""",\n         shell=True,\n         capture_output=True,\n-        text=True\n+        text=True,\n     )\n     print(result.stdout.strip())\n     max_allowed_time = 0.005  # maximum allowed time per iteration in seconds\n-    \n+\n     assert elapsed_time_per_iteration < max_allowed_time, (\n         f""""Tracking algorithm's processing time per iteration ({elapsed_time_per_iteration:.6f}s) """"\n         f""""exceeds the allowed limit of {max_allowed_time}s.""""\n@@ -64,34 +64,34 @@ def test_motion_n_appearance_tracker_update_time(tracker_type):\n     tracker = create_tracker(\n         tracker_type=tracker_type,\n         tracker_config=tracker_conf,\n-        reid_weights=WEIGHTS / 'mobilenetv2_x1_4_dukemtmcreid.pt',\n-        device='cpu',\n+        reid_weig",add,Added STORM - 1271 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tests/test_config.py,"from boxmot import (\n    BoostTrack,\n    BotSort,\n    ByteTrack,\n    DeepOcSort,\n    OcSort,\n    StrongSort,\n)\n\nMOTION_N_APPEARANCE_TRACKING_NAMES = ['botsort', 'deepocsort', 'strongsort', 'boosttrack']\nMOTION_ONLY_TRACKING_NAMES = ['ocsort', 'bytetrack']\n\nMOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSort, BotSort, DeepOcSort, BoostTrack]\nMOTION_ONLY_TRACKING_METHODS=[OcSort, ByteTrack]\n\nALL_TRACKERS = ['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort', 'boosttrack']\nPER_CLASS_TRACKERS = ['botsort', 'deepocsort', 'ocsort', 'bytetrack']","from boxmot import (\n    BoostTrack,\n    BotSort,\n    ByteTrack,\n    DeepOcSort,\n    OcSort,\n    StrongSort,\n)\n\nMOTION_N_APPEARANCE_TRACKING_NAMES = [\n    """"botsort"""",\n    """"deepocsort"""",\n    """"strongsort"""",\n    """"boosttrack"""",\n]\nMOTION_ONLY_TRACKING_NAMES = [""""ocsort"""", """"bytetrack""""]\n\nMOTION_N_APPEARANCE_TRACKING_METHODS = [StrongSort, BotSort, DeepOcSort, BoostTrack]\nMOTION_ONLY_TRACKING_METHODS = [OcSort, ByteTrack]\n\nALL_TRACKERS = [\n    """"botsort"""",\n    """"deepocsort"""",\n    """"ocsort"""",\n    """"bytetrack"""",\n    """"strongsort"""",\n    """"boosttrack"""",\n]\nPER_CLASS_TRACKERS = [""""botsort"""", """"deepocsort"""", """"ocsort"""", """"bytetrack""""]\n","@@ -7,11 +7,23 @@ from boxmot import (\n     StrongSort,\n )\n \n-MOTION_N_APPEARANCE_TRACKING_NAMES = ['botsort', 'deepocsort', 'strongsort', 'boosttrack']\n-MOTION_ONLY_TRACKING_NAMES = ['ocsort', 'bytetrack']\n+MOTION_N_APPEARANCE_TRACKING_NAMES = [\n+    """"botsort"""",\n+    """"deepocsort"""",\n+    """"strongsort"""",\n+    """"boosttrack"""",\n+]\n+MOTION_ONLY_TRACKING_NAMES = [""""ocsort"""", """"bytetrack""""]\n \n-MOTION_N_APPEARANCE_TRACKING_METHODS=[StrongSort, BotSort, DeepOcSort, BoostTrack]\n-MOTION_ONLY_TRACKING_METHODS=[OcSort, ByteTrack]\n+MOTION_N_APPEARANCE_TRACKING_METHODS = [StrongSort, BotSort, DeepOcSort, BoostTrack]\n+MOTION_ONLY_TRACKING_METHODS = [OcSort, ByteTrack]\n \n-ALL_TRACKERS = ['botsort', 'deepocsort', 'ocsort', 'bytetrack', 'strongsort', 'boosttrack']\n-PER_CLASS_TRACKERS = ['botsort', 'deepocsort', 'ocsort', 'bytetrack']\n\ No newline at end of file\n+ALL_TRACKERS = [\n+    """"botsort"""",\n+    """"deepocsort"""",\n+    """"ocsort"""",\n+    """"bytetrack"""",\n+    """"strongsort"""",\n+    """"boosttrack"""",\n+]\n+PER_CLASS_TRACKERS = [""""botsort"""", """"deepocsort"""", """"ocsort"""", """"bytetrack""""]\n",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tests/unit/test_cuda.py,"from pathlib import Path\n\nimport pytest\nimport torch\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\n\nREID_MODELS = [\n    Path('mobilenetv2_x1_0_market1501.pt'),\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_device(reid_model):\n\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\n    rab = ReidAutoBackend(\n        weights=reid_model, device=device, half=False\n    )\n    r = rab.get_backend()\n\n    if torch.cuda.is_available():\n        assert next(r.model.parameters()).is_cuda\n    else:\n        assert next(r.model.parameters()).device.type == 'cpu'\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_half(reid_model):\n\n    half = True if torch.cuda.is_available() else False\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    rab = ReidAutoBackend(\n        weights=reid_model, device=device, half=False\n    )\n    r = rab.get_backend()\n\n    if device == 'cpu","from pathlib import Path\n\nimport pytest\nimport torch\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\n\nREID_MODELS = [\n    Path(""""mobilenetv2_x1_0_market1501.pt""""),\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_device(reid_model):\n\n    device = """"cuda:0"""" if torch.cuda.is_available() else """"cpu""""\n\n    rab = ReidAutoBackend(weights=reid_model, device=device, half=False)\n    r = rab.get_backend()\n\n    if torch.cuda.is_available():\n        assert next(r.model.parameters()).is_cuda\n    else:\n        assert next(r.model.parameters()).device.type == """"cpu""""\n\n\n@pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\ndef test_reidbackend_half(reid_model):\n\n    half = True if torch.cuda.is_available() else False\n    device = """"cuda:0"""" if torch.cuda.is_available() else """"cpu""""\n    rab = ReidAutoBackend(weights=reid_model, device=device, half=False)\n    r = rab.get_backend()\n\n    if device == """"cpu"""":\n        expect","@@ -6,37 +6,33 @@ import torch\n from boxmot.appearance.reid.auto_backend import ReidAutoBackend\n \n REID_MODELS = [\n-    Path('mobilenetv2_x1_0_market1501.pt'),\n+    Path(""""mobilenetv2_x1_0_market1501.pt""""),\n ]\n \n \n @pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\n def test_reidbackend_device(reid_model):\n \n-    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n+    device = """"cuda:0"""" if torch.cuda.is_available() else """"cpu""""\n \n-    rab = ReidAutoBackend(\n-        weights=reid_model, device=device, half=False\n-    )\n+    rab = ReidAutoBackend(weights=reid_model, device=device, half=False)\n     r = rab.get_backend()\n \n     if torch.cuda.is_available():\n         assert next(r.model.parameters()).is_cuda\n     else:\n-        assert next(r.model.parameters()).device.type == 'cpu'\n+        assert next(r.model.parameters()).device.type == """"cpu""""\n \n \n @pytest.mark.parametrize(""""reid_model"""", REID_MODELS)\n def test_reidbackend_half(reid_model):\n \n     half = True if torch.cuda.is_available() else False\n-    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n-    rab = ReidAutoBackend(\n-        weights=reid_model, device=device, half=False\n-    )\n+    device = """"cuda:0"""" if torch.cuda.is_available() else """"cpu""""\n+    rab = ReidAutoBackend(weights=reid_model, device=device, half=False)\n     r = rab.get_backend()\n \n-    if device == 'cpu':\n+    if device == """"cpu"""":\n         expected_dtype = torch.float32\n     else:\n         expected_dtype = torch.float16\n",fix,Add warning about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tests/unit/test_postprocessing.py,"import numpy as np\n\nfrom boxmot.postprocessing.gsi import gaussian_smooth, linear_interpolation\n\n\ndef test_gsi():\n    tracking_results = np.array([\n        [1, 1, 1475, 419, 75, 169, 0, 0, -1],\n        [2, 1, 1475, 419, 75, 169, 0, 0, -1],\n        [4, 1, 1475, 419, 75, 169, 0, 0, -1],\n        [6, 1, 1475, 419, 75, 169, 0, 0, -1]\n    ])\n    li = linear_interpolation(tracking_results, interval=20)\n    gsi = gaussian_smooth(li, tau=10)\n    assert len(gsi) == 6","import numpy as np\n\nfrom boxmot.postprocessing.gsi import gaussian_smooth, linear_interpolation\n\n\ndef test_gsi():\n    tracking_results = np.array(\n        [\n            [1, 1, 1475, 419, 75, 169, 0, 0, -1],\n            [2, 1, 1475, 419, 75, 169, 0, 0, -1],\n            [4, 1, 1475, 419, 75, 169, 0, 0, -1],\n            [6, 1, 1475, 419, 75, 169, 0, 0, -1],\n        ]\n    )\n    li = linear_interpolation(tracking_results, interval=20)\n    gsi = gaussian_smooth(li, tau=10)\n    assert len(gsi) == 6\n","@@ -4,12 +4,14 @@ from boxmot.postprocessing.gsi import gaussian_smooth, linear_interpolation\n \n \n def test_gsi():\n-    tracking_results = np.array([\n-        [1, 1, 1475, 419, 75, 169, 0, 0, -1],\n-        [2, 1, 1475, 419, 75, 169, 0, 0, -1],\n-        [4, 1, 1475, 419, 75, 169, 0, 0, -1],\n-        [6, 1, 1475, 419, 75, 169, 0, 0, -1]\n-    ])\n+    tracking_results = np.array(\n+        [\n+            [1, 1, 1475, 419, 75, 169, 0, 0, -1],\n+            [2, 1, 1475, 419, 75, 169, 0, 0, -1],\n+            [4, 1, 1475, 419, 75, 169, 0, 0, -1],\n+            [6, 1, 1475, 419, 75, 169, 0, 0, -1],\n+        ]\n+    )\n     li = linear_interpolation(tracking_results, interval=20)\n     gsi = gaussian_smooth(li, tau=10)\n-    assert len(gsi) == 6\n\ No newline at end of file\n+    assert len(gsi) == 6\n",add,Added STORM - 146 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tests/unit/test_reidbackend.py,"import cv2\nimport numpy as np\nimport pytest\n\nfrom boxmot.appearance.backends.onnx_backend import ONNXBackend\nfrom boxmot.appearance.backends.openvino_backend import OpenVinoBackend\nfrom boxmot.appearance.backends.pytorch_backend import PyTorchBackend\nfrom boxmot.appearance.backends.torchscript_backend import TorchscriptBackend\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.utils import ROOT, WEIGHTS\n\n# generated in previous job step\nEXPORTED_REID_MODELS = [\n    WEIGHTS / 'osnet_x0_25_msmt17.pt',\n    WEIGHTS / 'osnet_x0_25_msmt17.torchscript',\n    WEIGHTS / 'osnet_x0_25_msmt17.onnx',\n    WEIGHTS / 'osnet_x0_25_msmt17_openvino_model'\n]\n\nASSOCIATED_BACKEND = [\n    PyTorchBackend,\n    TorchscriptBackend,\n    ONNXBackend,\n    OpenVinoBackend\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", EXPORTED_REID_MODELS)\ndef test_reidbackend_output(reid_model):\n\n    rab = ReidAutoBackend(\n        weights=reid_model, device='cpu', half=False\n   ","import cv2\nimport numpy as np\nimport pytest\n\nfrom boxmot.appearance.backends.onnx_backend import ONNXBackend\nfrom boxmot.appearance.backends.openvino_backend import OpenVinoBackend\nfrom boxmot.appearance.backends.pytorch_backend import PyTorchBackend\nfrom boxmot.appearance.backends.torchscript_backend import TorchscriptBackend\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.utils import ROOT, WEIGHTS\n\n# generated in previous job step\nEXPORTED_REID_MODELS = [\n    WEIGHTS / """"osnet_x0_25_msmt17.pt"""",\n    WEIGHTS / """"osnet_x0_25_msmt17.torchscript"""",\n    WEIGHTS / """"osnet_x0_25_msmt17.onnx"""",\n    WEIGHTS / """"osnet_x0_25_msmt17_openvino_model"""",\n]\n\nASSOCIATED_BACKEND = [\n    PyTorchBackend,\n    TorchscriptBackend,\n    ONNXBackend,\n    OpenVinoBackend\n]\n\n\n@pytest.mark.parametrize(""""reid_model"""", EXPORTED_REID_MODELS)\ndef test_reidbackend_output(reid_model):\n\n    rab = ReidAutoBackend(weights=reid_model, device=""""cpu"""", half=False)\n ","@@ -11,10 +11,10 @@ from boxmot.utils import ROOT, WEIGHTS\n \n # generated in previous job step\n EXPORTED_REID_MODELS = [\n-    WEIGHTS / 'osnet_x0_25_msmt17.pt',\n-    WEIGHTS / 'osnet_x0_25_msmt17.torchscript',\n-    WEIGHTS / 'osnet_x0_25_msmt17.onnx',\n-    WEIGHTS / 'osnet_x0_25_msmt17_openvino_model'\n+    WEIGHTS / """"osnet_x0_25_msmt17.pt"""",\n+    WEIGHTS / """"osnet_x0_25_msmt17.torchscript"""",\n+    WEIGHTS / """"osnet_x0_25_msmt17.onnx"""",\n+    WEIGHTS / """"osnet_x0_25_msmt17_openvino_model"""",\n ]\n \n ASSOCIATED_BACKEND = [\n@@ -28,26 +28,26 @@ ASSOCIATED_BACKEND = [\n @pytest.mark.parametrize(""""reid_model"""", EXPORTED_REID_MODELS)\n def test_reidbackend_output(reid_model):\n \n-    rab = ReidAutoBackend(\n-        weights=reid_model, device='cpu', half=False\n-    )\n+    rab = ReidAutoBackend(weights=reid_model, device=""""cpu"""", half=False)\n     b = rab.get_backend()\n \n-    img = cv2.imread(str(ROOT / 'assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000001.jpg'))\n+    img = cv2.imread(\n+        str(ROOT / """"assets/MOT17-mini/train/MOT17-04-FRCNN/img1/000001.jpg"""")\n+    )\n     dets = np.array([[144, 212, 578, 480, 0.82, 0],\n-                    [425, 281, 576, 472, 0.56, 65]])\n+                     [425, 281, 576, 472, 0.56, 65]])\n \n     embs = b.get_features(dets[:, 0:4], img)\n-    assert embs.shape[0] == 2   # two crops should give two embeddings\n-    assert embs.shape[1] == 512 # osnet embeddings are of size 512\n+    assert embs.shape[0] == 2  # two crops should give two embeddings\n+    assert embs.shape[1] == 512  # osnet embeddings are of size 512\n \n \n-@pytest.mark.parametrize(""""exported_reid_model, backend"""", zip(EXPORTED_REID_MODELS, ASSOCIATED_BACKEND))\n+@pytest.mark.parametrize(\n+    """"exported_reid_model, backend"""", zip(EXPORTED_REID_MODELS, ASSOCIATED_BACKEND)\n+)\n def test_reidbackend_type(exported_reid_model, backend):\n \n-    rab = ReidAutoBackend(\n-        weights=exported_reid_model, device='cpu', half=False\n-    )\n+    rab",add,Added STORM - 236 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tests/unit/test_trackers.py,"from pathlib import Path\n\nimport numpy as np\nimport pytest\n\nfrom boxmot import (\n    DeepOcSort,\n    OcSort,\n    create_tracker,\n    get_tracker_config,\n)\nfrom boxmot.trackers.deepocsort.deepocsort import (\n    KalmanBoxTracker as DeepOCSortKalmanBoxTracker,\n)\nfrom boxmot.trackers.ocsort.ocsort import KalmanBoxTracker as OCSortKalmanBoxTracker\nfrom boxmot.utils import WEIGHTS\nfrom tests.test_config import (\n    ALL_TRACKERS,\n    MOTION_N_APPEARANCE_TRACKING_METHODS,\n    MOTION_N_APPEARANCE_TRACKING_NAMES,\n    MOTION_ONLY_TRACKING_METHODS,\n    PER_CLASS_TRACKERS,\n)\n\n# --- existing tests ---\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        reid_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n        device='cpu',\n        half=True,\n    )\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_instanti","from pathlib import Path\n\nimport numpy as np\nimport pytest\n\nfrom boxmot import (\n    DeepOcSort,\n    OcSort,\n    create_tracker,\n    get_tracker_config,\n)\nfrom boxmot.trackers.deepocsort.deepocsort import (\n    KalmanBoxTracker as DeepOCSortKalmanBoxTracker,\n)\nfrom boxmot.trackers.ocsort.ocsort import KalmanBoxTracker as OCSortKalmanBoxTracker\nfrom boxmot.utils import WEIGHTS\nfrom tests.test_config import (\n    ALL_TRACKERS,\n    MOTION_N_APPEARANCE_TRACKING_METHODS,\n    MOTION_N_APPEARANCE_TRACKING_NAMES,\n    MOTION_ONLY_TRACKING_METHODS,\n    PER_CLASS_TRACKERS,\n)\n\n# --- existing tests ---\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\ndef test_motion_n_appearance_trackers_instantiation(Tracker):\n    Tracker(\n        reid_weights=Path(WEIGHTS / """"osnet_x0_25_msmt17.pt""""),\n        device=""""cpu"""",\n        half=True,\n    )\n\n\n@pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\ndef test_motion_only_trackers_","@@ -24,72 +24,83 @@ from tests.test_config import (\n \n # --- existing tests ---\n \n+\n @pytest.mark.parametrize(""""Tracker"""", MOTION_N_APPEARANCE_TRACKING_METHODS)\n def test_motion_n_appearance_trackers_instantiation(Tracker):\n     Tracker(\n-        reid_weights=Path(WEIGHTS / 'osnet_x0_25_msmt17.pt'),\n-        device='cpu',\n+        reid_weights=Path(WEIGHTS / """"osnet_x0_25_msmt17.pt""""),\n+        device=""""cpu"""",\n         half=True,\n     )\n \n+\n @pytest.mark.parametrize(""""Tracker"""", MOTION_ONLY_TRACKING_METHODS)\n def test_motion_only_trackers_instantiation(Tracker):\n     Tracker()\n \n+\n @pytest.mark.parametrize(""""tracker_type"""", ALL_TRACKERS)\n def test_tracker_output_size(tracker_type):\n     tracker_conf = get_tracker_config(tracker_type)\n     tracker = create_tracker(\n         tracker_type=tracker_type,\n         tracker_config=tracker_conf,\n-        reid_weights=WEIGHTS / 'mobilenetv2_x1_4_dukemtmcreid.pt',\n-        device='cpu',\n+        reid_weights=WEIGHTS / """"mobilenetv2_x1_4_dukemtmcreid.pt"""",\n+        device=""""cpu"""",\n         half=False,\n-        per_class=False\n+        per_class=False,\n     )\n \n     rgb = np.random.randint(255, size=(640, 640, 3), dtype=np.uint8)\n-    det = np.array([[144, 212, 400, 480, 0.82, 0],\n-                    [425, 281, 576, 472, 0.72, 65]])\n+    det = np.array([[144, 212, 400, 480, 0.82, 0], [425, 281, 576, 472, 0.72, 65]])\n \n     output = tracker.update(det, rgb)\n     assert output.shape == (2, 8)\n \n+\n def test_dynamic_max_obs_based_on_max_age():\n     max_age = 400\n     ocsort = OcSort(max_age=max_age)\n     assert ocsort.max_obs == (max_age + 5)\n \n+\n def create_kalman_box_tracker_ocsort(bbox, cls, det_ind, tracker):\n     return OCSortKalmanBoxTracker(\n         bbox,\n         cls,\n         det_ind,\n         Q_xy_scaling=tracker.Q_xy_scaling,\n-        Q_s_scaling=tracker.Q_s_scaling\n+        Q_s_scaling=tracker.Q_s_scaling,\n     )\n \n+\n def create_kalman_box_tracker_deepocsort",add,Added STORM - 1270 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tracking/detectors/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\nUL_MODELS = ['yolov8', 'yolov9', 'yolov10', 'yolo11', 'yolo12', 'rtdetr', 'sam']\n\n\ndef is_ultralytics_model(yolo_name):\n    return any(yolo in str(yolo_name) for yolo in UL_MODELS)\n\n\ndef is_yolox_model(yolo_name):\n    return 'yolox' in str(yolo_name)\n\n\ndef default_imgsz(yolo_name):\n    if is_ultralytics_model(yolo_name):\n        return [640, 640]\n    elif is_yolox_model(yolo_name):\n        return [800, 1440]\n    else:\n        return [640, 640]\n\n\ndef get_yolo_inferer(yolo_model):\n\n    if is_yolox_model(yolo_model):\n        try:\n            import yolox  # for linear_assignment\n            assert yolox.__version__\n        except (ImportError, AssertionError, AttributeError):\n            checker.check_packages(('yolox',), cmds=[""""--no-deps""""])\n            checker.check_package","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.utils.checks import RequirementsChecker\n\nchecker = RequirementsChecker()\n\nUL_MODELS = [""""yolov8"""", """"yolov9"""", """"yolov10"""", """"yolo11"""", """"yolo12"""", """"rtdetr"""", """"sam""""]\n\n\ndef is_ultralytics_model(yolo_name):\n    return any(yolo in str(yolo_name) for yolo in UL_MODELS)\n\n\ndef is_yolox_model(yolo_name):\n    return """"yolox"""" in str(yolo_name)\n\n\ndef default_imgsz(yolo_name):\n    if is_ultralytics_model(yolo_name):\n        return [640, 640]\n    elif is_yolox_model(yolo_name):\n        return [800, 1440]\n    else:\n        return [640, 640]\n\n\ndef get_yolo_inferer(yolo_model):\n\n    if is_yolox_model(yolo_model):\n        try:\n            import yolox  # for linear_assignment\n\n            assert yolox.__version__\n        except (ImportError, AssertionError, AttributeError):\n            checker.check_packages((""""yolox"""",), cmds=[""""--no-deps""""])\n            c","@@ -5,7 +5,7 @@ from boxmot.utils.checks import RequirementsChecker\n \n checker = RequirementsChecker()\n \n-UL_MODELS = ['yolov8', 'yolov9', 'yolov10', 'yolo11', 'yolo12', 'rtdetr', 'sam']\n+UL_MODELS = [""""yolov8"""", """"yolov9"""", """"yolov10"""", """"yolo11"""", """"yolo12"""", """"rtdetr"""", """"sam""""]\n \n \n def is_ultralytics_model(yolo_name):\n@@ -13,7 +13,7 @@ def is_ultralytics_model(yolo_name):\n \n \n def is_yolox_model(yolo_name):\n-    return 'yolox' in str(yolo_name)\n+    return """"yolox"""" in str(yolo_name)\n \n \n def default_imgsz(yolo_name):\n@@ -30,34 +30,40 @@ def get_yolo_inferer(yolo_model):\n     if is_yolox_model(yolo_model):\n         try:\n             import yolox  # for linear_assignment\n+\n             assert yolox.__version__\n         except (ImportError, AssertionError, AttributeError):\n-            checker.check_packages(('yolox',), cmds=[""""--no-deps""""])\n-            checker.check_packages(('tabulate',))  # needed dependency\n-            checker.check_packages(('thop',))  # needed dependency\n+            checker.check_packages((""""yolox"""",), cmds=[""""--no-deps""""])\n+            checker.check_packages((""""tabulate"""",))  # needed dependency\n+            checker.check_packages((""""thop"""",))  # needed dependency\n         from .yolox import YoloXStrategy\n+\n         return YoloXStrategy\n     elif is_ultralytics_model(yolo_model):\n         # ultralytics already installed when running track.py\n         from .yolov8 import Yolov8Strategy\n+\n         return Yolov8Strategy\n-    elif 'rf-detr' in str(yolo_model):\n+    elif """"rf-detr"""" in str(yolo_model):\n         try:\n             import rfdetr\n         except (ImportError, AssertionError, AttributeError):\n-            checker.check_packages(('onnxruntime',))  # needed dependency\n-            checker.check_packages(('rfdetr',))  # needed dependency\n+            checker.check_packages((""""onnxruntime"""",))  # needed dependency\n+            checker.check_packages((""""rfdetr"""",))  # needed dependency\n ",add,Added limits . h to headers to iOS
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tracking/detectors/rfdetr.py,"# Mikel BrostrÃ¶m ð¥ RFDETR Tracking ð§¾ AGPL-3.0 license\n\nimport cv2\nimport numpy as np\nimport torch\nfrom rfdetr import RFDETRBase\nfrom rfdetr.util.coco_classes import COCO_CLASSES\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect import DetectionPredictor\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n\nclass RFDETRStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = COCO_CLASSES\n\n    def __init__(self, model, device, args):\n        self.args = args\n        LOGGER.info(""""Loading RFDETR model"""")\n        self.model = RFDETRBase(device=""""cpu"""")\n\n    @torch.no_grad()\n    def __call__(self, im, augment, visualize, embed):\n        image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)  # Convert frame BGR2RGB for RFDETR\n        with torch.no_grad():\n            detections = self.model.predict(image, threshold=self.args.conf)\n\n        r","# Mikel BrostrÃ¶m ð¥ RFDETR Tracking ð§¾ AGPL-3.0 license\n\nimport cv2\nimport numpy as np\nimport torch\nfrom rfdetr import RFDETRBase\nfrom rfdetr.util.coco_classes import COCO_CLASSES\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect import DetectionPredictor\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n\nclass RFDETRStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = COCO_CLASSES\n\n    def __init__(self, model, device, args):\n        self.args = args\n        LOGGER.info(""""Loading RFDETR model"""")\n        self.model = RFDETRBase(device=""""cpu"""")\n\n    @torch.no_grad()\n    def __call__(self, im, augment, visualize, embed):\n        image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)  # Convert frame BGR2RGB for RFDETR\n        with torch.no_grad():\n            detections = self.model.predict(image, threshold=self.args.conf)\n\n        r","@@ -38,16 +38,18 @@ class RFDETRStrategy(YoloInterface):\n \n     def warmup(self, imgsz):\n         pass\n-    \n+\n     def update_im_paths(self, predictor: DetectionPredictor):\n         """"""""""""\n         This function saves image paths for the current batch,\n         being passed as callback on_predict_batch_start\n         """"""""""""\n-        assert (isinstance(predictor, DetectionPredictor),\n-                """"Only ultralytics predictors are supported"""")\n+        assert (\n+            isinstance(predictor, DetectionPredictor),\n+            """"Only ultralytics predictors are supported"""",\n+        )\n         self.im_paths = predictor.batch[0]\n-    \n+\n     def preprocess(self, im) -> torch.Tensor:\n         assert isinstance(im, list)\n         return im[0]\n@@ -59,6 +61,10 @@ class RFDETRStrategy(YoloInterface):\n                 continue\n             im_path = self.im_paths[i] if len(self.im_paths) else """"""""\n             if self.args.classes:\n-                pred = pred[torch.isin(pred[:, 5].cpu(), torch.as_tensor(self.args.classes))]\n-            results.append(Results(path=im_path, boxes=pred, orig_img=im0s[i], names=COCO_CLASSES))\n+                pred = pred[\n+                    torch.isin(pred[:, 5].cpu(), torch.as_tensor(self.args.classes))\n+                ]\n+            results.append(\n+                Results(path=im_path, boxes=pred, orig_img=im0s[i], names=COCO_CLASSES)\n+            )\n         return results\n",add,Fix Snackbar theme on sw600dp
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tracking/detectors/yolo_interface.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\n\nimport torch\n\n\nclass YoloInterface(ABC):\n\n    @abstractmethod\n    def __call__(self, im):\n        pass\n\n    @abstractmethod\n    def preprocess(self, ims):\n        pass\n\n    @abstractmethod\n    def postprocess(self, preds):\n        pass\n\n    def get_scaling_factors(self, im, im0):\n\n        # im to im0 factor for predictions\n        im0_w = im0.shape[1]\n        im0_h = im0.shape[0]\n        im_w = im.shape[2]\n        im_h = im.shape[1]\n        w_r = im0_w / im_w\n        h_r = im0_h / im_h\n\n        return im_w, im_h, w_r, h_r\n\n    def scale_and_clip(self, preds, im_w, im_h, w_r, h_r):\n        # scale bboxes to original image\n        preds[:, [0, 2]] = preds[:, [0, 2]] * self.w_r\n        preds[:, [1, 3]] = preds[:, [1, 3]] * self.h_r\n\n        if not isinstance(preds, (torch.Tensor)):\n            preds = torch.from_numpy(preds)\n\n        ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\n\nimport torch\n\n\nclass YoloInterface(ABC):\n\n    @abstractmethod\n    def __call__(self, im):\n        pass\n\n    @abstractmethod\n    def preprocess(self, ims):\n        pass\n\n    @abstractmethod\n    def postprocess(self, preds):\n        pass\n\n    def get_scaling_factors(self, im, im0):\n\n        # im to im0 factor for predictions\n        im0_w = im0.shape[1]\n        im0_h = im0.shape[0]\n        im_w = im.shape[2]\n        im_h = im.shape[1]\n        w_r = im0_w / im_w\n        h_r = im0_h / im_h\n\n        return im_w, im_h, w_r, h_r\n\n    def scale_and_clip(self, preds, im_w, im_h, w_r, h_r):\n        # scale bboxes to original image\n        preds[:, [0, 2]] = preds[:, [0, 2]] * self.w_r\n        preds[:, [1, 3]] = preds[:, [1, 3]] * self.h_r\n\n        if not isinstance(preds, (torch.Tensor)):\n            preds = torch.from_numpy(preds)\n\n        ","@@ -49,6 +49,6 @@ class YoloInterface(ABC):\n         model_type = None\n         for key in l:\n             if Path(key).stem in str(model.name):\n-                model_type = str(Path(key).with_suffix(''))\n+                model_type = str(Path(key).with_suffix(""""""""))\n                 break\n         return model_type\n",add,Add note about data volume to enable_metrics_collection
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tracking/detectors/yolonas.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n\nclass YoloNASStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = {\n        0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus',\n        6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n        11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat',\n        16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant',\n        21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella',\n        26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis',\n     ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport numpy as np\nimport torch\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n\nclass YoloNASStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = {\n        0: """"person"""",\n        1: """"bicycle"""",\n        2: """"car"""",\n        3: """"motorcycle"""",\n        4: """"airplane"""",\n        5: """"bus"""",\n        6: """"train"""",\n        7: """"truck"""",\n        8: """"boat"""",\n        9: """"traffic light"""",\n        10: """"fire hydrant"""",\n        11: """"stop sign"""",\n        12: """"parking meter"""",\n        13: """"bench"""",\n        14: """"bird"""",\n        15: """"cat"""",\n        16: """"dog"""",\n        17: """"horse"""",\n        18: """"sheep"""",\n        19: """"cow""","@@ -17,22 +17,86 @@ class YoloNASStrategy(YoloInterface):\n     fp16 = False\n     triton = False\n     names = {\n-        0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus',\n-        6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n-        11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat',\n-        16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant',\n-        21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella',\n-        26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis',\n-        31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove',\n-        36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass',\n-        41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl',\n-        46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli',\n-        51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake',\n-        56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table',\n-        61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote',\n-        66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster',\n-        71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase',\n-        76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'\n+        0: """"person"""",\n+        1: """"bicycle"""",\n+        2: """"car"""",\n+        3: """"motorcycle"""",\n+        4: """"airplane"""",\n+        5: """"bus"""",\n+        6: """"train"""",\n+        7: """"truck"""",\n+        8: """"boat"""",\n+        9: """"traffic light"""",\n+        10: """"fire hydrant"""",\n+        11: """"stop sign"""",\n+        12: """"parking meter"""",\n+        13: """"bench"""",\n+        14: """"bird"""",\n+        15: """"cat"""",\n+        16: """"dog"""",\n+        17: """"horse"""",\n+        18: """"sheep"""",\n+        19: """"cow"""",\n+        20: """"elephant"""",\n+        21",add,Added STORM - 2056 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tracking/detectors/yolov9.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport torch\nfrom ultralytics.engine.results import Results\nfrom yolov9 import load\n\nfrom boxmot.utils import logger as LOGGER\nfrom examples.detectors.yolo_interface import YoloInterface\n\nYOLOv9_ZOO = {\n    'gelan-c.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-c.pt',\n    'gelan-e.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-e.pt',\n    'yolov9-c.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt',\n    'yolov9-e.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt',\n}\n\nclass Yolov9Strategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = {\n        0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus',\n        6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n        11: 'stop sign', 12: 'parking meter', 13: 'bench'","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport torch\nfrom ultralytics.engine.results import Results\nfrom yolov9 import load\n\nfrom boxmot.utils import logger as LOGGER\nfrom examples.detectors.yolo_interface import YoloInterface\n\nYOLOv9_ZOO = {\n    """"gelan-c.pt"""": """"https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-c.pt"""",\n    """"gelan-e.pt"""": """"https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-e.pt"""",\n    """"yolov9-c.pt"""": """"https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt"""",\n    """"yolov9-e.pt"""": """"https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt"""",\n}\n\n\nclass Yolov9Strategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = {\n        0: """"person"""",\n        1: """"bicycle"""",\n        2: """"car"""",\n        3: """"motorcycle"""",\n        4: """"airplane"""",\n        5: """"bus"""",\n        6: """"train"""",\n        7: """"truck"""",\n        8: """"boat"""",","@@ -8,34 +8,99 @@ from boxmot.utils import logger as LOGGER\n from examples.detectors.yolo_interface import YoloInterface\n \n YOLOv9_ZOO = {\n-    'gelan-c.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-c.pt',\n-    'gelan-e.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-e.pt',\n-    'yolov9-c.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt',\n-    'yolov9-e.pt': 'https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt',\n+    """"gelan-c.pt"""": """"https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-c.pt"""",\n+    """"gelan-e.pt"""": """"https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-e.pt"""",\n+    """"yolov9-c.pt"""": """"https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt"""",\n+    """"yolov9-e.pt"""": """"https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt"""",\n }\n \n+\n class Yolov9Strategy(YoloInterface):\n     pt = False\n     stride = 32\n     fp16 = False\n     triton = False\n     names = {\n-        0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus',\n-        6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n-        11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat',\n-        16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant',\n-        21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella',\n-        26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis',\n-        31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove',\n-        36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass',\n-        41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl',\n-        46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli',\n-        51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake',\n-        56: 'chair",add,Added STORM - 2056 to Changelog
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tracking/detectors/yolox.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport cv2\nimport gdown\nimport numpy as np\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n    'yolox_x_ablation.pt': 'https://drive.google.com/","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport cv2\nimport gdown\nimport numpy as np\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom tracking.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    """"yolox_n.pt"""": """"https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX"""",\n    """"yolox_s.pt"""": """"https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj"""",\n    """"yolox_m.pt"""": """"https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun"""",\n    """"yolox_l.pt"""": """"https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz"""",\n    """"yolox_x.pt"""": """"https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5"""",\n    """"yolox_x_ablation.pt"""": """"ht","@@ -15,12 +15,12 @@ from tracking.detectors.yolo_interface import YoloInterface\n \n # default model weigths for these model names\n YOLOX_ZOO = {\n-    'yolox_n.pt': 'https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX',\n-    'yolox_s.pt': 'https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj',\n-    'yolox_m.pt': 'https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun',\n-    'yolox_l.pt': 'https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz',\n-    'yolox_x.pt': 'https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5',\n-    'yolox_x_ablation.pt': 'https://drive.google.com/uc?id=1iqhM-6V_r1FpOlOzrdP_Ejshgk0DxOob',\n+    """"yolox_n.pt"""": """"https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX"""",\n+    """"yolox_s.pt"""": """"https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj"""",\n+    """"yolox_m.pt"""": """"https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun"""",\n+    """"yolox_l.pt"""": """"https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz"""",\n+    """"yolox_x.pt"""": """"https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5"""",\n+    """"yolox_x_ablation.pt"""": """"https://drive.google.com/uc?id=1iqhM-6V_r1FpOlOzrdP_Ejshgk0DxOob"""",\n }\n \n \n@@ -30,22 +30,86 @@ class YoloXStrategy(YoloInterface):\n     fp16 = False\n     triton = False\n     names = {\n-        0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus',\n-        6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n-        11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat',\n-        16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant',\n-        21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella',\n-        26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis',\n-        31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove',\n-        36: 'skateboard', 37: 'surfbo",add,Added tag 1 . 0 - RC1
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tracking/evolve.py,"#!/usr/bin/env python3\n""""""""""""\nThis script runs a hyperparameter tuning process for a multi-object tracking (MOT) tracker using Ray Tune.\nIt loads the tracker configuration from a YAML file, sets up the search space for hyperparameters, and evaluates\nthe tracker to optimize selected metrics (e.g., MOTA, HOTA, IDF1).\n""""""""""""\n\nimport os\nfrom pathlib import Path\n\nimport yaml\nfrom ray import tune\nfrom ray.air import RunConfig\n\nfrom boxmot.utils import EXAMPLES, NUM_THREADS, TRACKER_CONFIGS\nfrom tracking.val import (\n    download_mot_eval_tools,\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n)\n\n\nclass Tracker:\n    """"""""""""\n    Encapsulates the evaluation of a tracking configuration.\n    """"""""""""\n    def __init__(self, opt):\n        self.opt = opt\n\n    def objective_function(self, config: dict) -> dict:\n        """"""""""""\n        Evaluates a given tracker configuration.\n\n        Args:\n            config (dict): A dictionary of tracker hy","#!/usr/bin/env python3\n""""""""""""\nThis script runs a hyperparameter tuning process for a multi-object tracking (MOT) tracker using Ray Tune.\nIt loads the tracker configuration from a YAML file, sets up the search space for hyperparameters, and evaluates\nthe tracker to optimize selected metrics (e.g., MOTA, HOTA, IDF1).\n""""""""""""\n\nimport os\nfrom pathlib import Path\n\nimport yaml\nfrom ray import tune\nfrom ray.air import RunConfig\n\nfrom boxmot.utils import EXAMPLES, NUM_THREADS, TRACKER_CONFIGS\nfrom tracking.val import (\n    download_mot_eval_tools,\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n)\n\n\nclass Tracker:\n    """"""""""""\n    Encapsulates the evaluation of a tracking configuration.\n    """"""""""""\n\n    def __init__(self, opt):\n        self.opt = opt\n\n    def objective_function(self, config: dict) -> dict:\n        """"""""""""\n        Evaluates a given tracker configuration.\n\n        Args:\n            config (dict): A dictionary of tracker ","@@ -25,6 +25,7 @@ class Tracker:\n     """"""""""""\n     Encapsulates the evaluation of a tracking configuration.\n     """"""""""""\n+\n     def __init__(self, opt):\n         self.opt = opt\n \n@@ -60,7 +61,7 @@ def load_yaml_config(tracking_method: str) -> dict:\n         dict: Configuration parameters loaded from the YAML file.\n     """"""""""""\n     config_path = TRACKER_CONFIGS / f""""{tracking_method}.yaml""""\n-    with open(config_path, 'r') as file:\n+    with open(config_path, """"r"""") as file:\n         config = yaml.safe_load(file)\n     return config\n \n@@ -77,26 +78,25 @@ def yaml_to_search_space(config: dict) -> dict:\n     """"""""""""\n     search_space = {}\n     for param, details in config.items():\n-        search_type = details.get('type')\n-        if search_type == 'uniform':\n-            search_space[param] = tune.uniform(*details['range'])\n-        elif search_type == 'randint':\n-            search_space[param] = tune.randint(*details['range'])\n-        elif search_type == 'qrandint':\n-            search_space[param] = tune.qrandint(*details['range'])\n-        elif search_type == 'choice':\n-            search_space[param] = tune.choice(details['options'])\n-        elif search_type == 'grid_search':\n-            search_space[param] = tune.grid_search(details['values'])\n-        elif search_type == 'loguniform':\n-            search_space[param] = tune.loguniform(*details['range'])\n+        search_type = details.get(""""type"""")\n+        if search_type == """"uniform"""":\n+            search_space[param] = tune.uniform(*details[""""range""""])\n+        elif search_type == """"randint"""":\n+            search_space[param] = tune.randint(*details[""""range""""])\n+        elif search_type == """"qrandint"""":\n+            search_space[param] = tune.qrandint(*details[""""range""""])\n+        elif search_type == """"choice"""":\n+            search_space[param] = tune.choice(details[""""options""""])\n+        elif search_type == """"grid_search"""":\n+            search_space[param] = tune.g",add,Fix react word wrapping crash
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tracking/track.py,"\n# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom functools import partial\nfrom pathlib import Path\n\nimport cv2\nimport torch\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, TRACKER_CONFIGS, WEIGHTS\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.detectors import default_imgsz, get_yolo_inferer, is_ultralytics_model\n\nchecker = RequirementsChecker()\nchecker.check_packages(('ultralytics', ))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.utils.plotting import Annotator  # ultralytics.yolo.utils.plotting is deprecated\nfrom ultralytics.utils.plotting import colors\nfrom ultralytics.utils import plotting\n\n# Make every drawing call a no-op\nplotting.Annotator.box       = lambda *args, **kwargs: None\nplotting.Annotator.box_label = lambda *args, **kwargs: None\nplotting.Annotator.line      = lambda *args, **kwargs: None\n\n\ndef on_predict_start(predictor, pers","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nfrom functools import partial\nfrom pathlib import Path\n\nimport cv2\nimport torch\n\nfrom boxmot import TRACKERS\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, TRACKER_CONFIGS, WEIGHTS\nfrom boxmot.utils.checks import RequirementsChecker\nfrom tracking.detectors import default_imgsz, get_yolo_inferer, is_ultralytics_model\n\nchecker = RequirementsChecker()\nchecker.check_packages((""""ultralytics"""", ))  # install\n\nfrom ultralytics import YOLO\nfrom ultralytics.utils.plotting import Annotator  # ultralytics.yolo.utils.plotting is deprecated\nfrom ultralytics.utils.plotting import colors\nfrom ultralytics.utils import plotting\n\n# Make every drawing call a no-op\nplotting.Annotator.box       = lambda *args, **kwargs: None\nplotting.Annotator.box_label = lambda *args, **kwargs: None\nplotting.Annotator.line      = lambda *args, **kwargs: None\n\n\ndef on_predict_start(predictor, pers","@@ -1,4 +1,3 @@\n-\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n import argparse\n@@ -15,7 +14,7 @@ from boxmot.utils.checks import RequirementsChecker\n from tracking.detectors import default_imgsz, get_yolo_inferer, is_ultralytics_model\n \n checker = RequirementsChecker()\n-checker.check_packages(('ultralytics', ))  # install\n+checker.check_packages((""""ultralytics"""", ))  # install\n \n from ultralytics import YOLO\n from ultralytics.utils.plotting import Annotator  # ultralytics.yolo.utils.plotting is deprecated\n@@ -47,15 +46,15 @@ def on_predict_start(predictor, persist=False):\n             predictor.custom_args.reid_model,\n             predictor.device,\n             predictor.custom_args.half,\n-            predictor.custom_args.per_class\n+            predictor.custom_args.per_class,\n         )\n         # motion only models do not have\n-        if hasattr(tracker, 'model'):\n+        if hasattr(tracker, """"model""""):\n             tracker.model.warmup()\n         trackers.append(tracker)\n \n     predictor.trackers = trackers\n-    \n+\n # callback to plot trajectories on each frame\n def plot_trajectories(predictor):\n     # predictor.results is a list of Results, one per frame in the batch\n@@ -70,8 +69,7 @@ def main(args):\n     if args.imgsz is None:\n         args.imgsz = default_imgsz(args.yolo_model)\n     yolo = YOLO(\n-        args.yolo_model if is_ultralytics_model(args.yolo_model)\n-        else 'yolov8n.pt',\n+        args.yolo_model if is_ultralytics_model(args.yolo_model) else """"yolov8n.pt"""",\n     )\n \n     results = yolo.track(\n@@ -94,31 +92,32 @@ def main(args):\n         imgsz=args.imgsz,\n         vid_stride=args.vid_stride,\n         line_width=args.line_width,\n-        save_crop=args.save_crop\n+        save_crop=args.save_crop,\n     )\n \n-    yolo.add_callback('on_predict_start', partial(on_predict_start, persist=True))\n-    yolo.add_callback('on_predict_postprocess_end', plot_trajectories)\n+    yolo.add_callback",add,Fix format
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tracking/utils.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport json\nimport shutil\nimport time\nimport zipfile\nfrom pathlib import Path\nfrom typing import Union\n\nimport numpy as np\nimport pandas as pd\nimport requests\nimport torch\nfrom git import Repo, exc\nfrom tqdm import tqdm\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\n\nfrom boxmot.utils import ROOT\nfrom boxmot.utils import logger as LOGGER\n\n\ndef split_dataset(src_fldr: Path, percent_to_delete: float = 0.5) -> None:\n    """"""""""""\n    Copies the dataset to a new location and removes a specified percentage of images and annotations,\n    adjusting the frame index to start at 1.\n\n    Args:\n        src_fldr (Path): Source folder containing the dataset.\n        percent_to_delete (float): Percentage of images and annotations to remove.\n    """"""""""""\n    # Ensure source path is a Path object\n    src_fldr = Path(src_fldr)\n\n    # Generate the destination path by replacing """"MOT17"""" wit","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport json\nimport shutil\nimport time\nimport zipfile\nfrom pathlib import Path\nfrom typing import Union\n\nimport numpy as np\nimport pandas as pd\nimport requests\nimport torch\nfrom git import Repo, exc\nfrom tqdm import tqdm\nfrom ultralytics.engine.results import Results\nfrom ultralytics.utils import ops\n\nfrom boxmot.utils import ROOT\nfrom boxmot.utils import logger as LOGGER\n\n\ndef split_dataset(src_fldr: Path, percent_to_delete: float = 0.5) -> None:\n    """"""""""""\n    Copies the dataset to a new location and removes a specified percentage of images and annotations,\n    adjusting the frame index to start at 1.\n\n    Args:\n        src_fldr (Path): Source folder containing the dataset.\n        percent_to_delete (float): Percentage of images and annotations to remove.\n    """"""""""""\n    # Ensure source path is a Path object\n    src_fldr = Path(src_fldr)\n\n    # Generate the destination path by replacing """"MOT17"""" wit","@@ -33,15 +33,17 @@ def split_dataset(src_fldr: Path, percent_to_delete: float = 0.5) -> None:\n     src_fldr = Path(src_fldr)\n \n     # Generate the destination path by replacing """"MOT17"""" with """"MOT17-half"""" in the source path\n-    new_benchmark_name = f'MOT17-{int(percent_to_delete * 100)}'\n-    dst_fldr = Path(str(src_fldr).replace('MOT17', new_benchmark_name))\n+    new_benchmark_name = f""""MOT17-{int(percent_to_delete * 100)}""""\n+    dst_fldr = Path(str(src_fldr).replace(""""MOT17"""", new_benchmark_name))\n \n     # Copy the dataset to a new location manually using pathlib if it doesn't already exist\n     if not dst_fldr.exists():\n         dst_fldr.mkdir(parents=True)\n-        for item in src_fldr.rglob('*'):\n+        for item in src_fldr.rglob(""""*""""):\n             if item.is_dir():\n-                (dst_fldr / item.relative_to(src_fldr)).mkdir(parents=True, exist_ok=True)\n+                (dst_fldr / item.relative_to(src_fldr)).mkdir(\n+                    parents=True, exist_ok=True\n+                )\n             else:\n                 (dst_fldr / item.relative_to(src_fldr)).write_bytes(item.read_bytes())\n \n@@ -50,8 +52,8 @@ def split_dataset(src_fldr: Path, percent_to_delete: float = 0.5) -> None:\n \n     # Iterate over each sequence and remove a percentage of images and annotations\n     for seq_path in seq_paths:\n-        seq_gt_path = seq_path / 'gt' / 'gt.txt'\n-        \n+        seq_gt_path = seq_path / """"gt"""" / """"gt.txt""""\n+\n         # Check if the gt.txt file exists\n         if not seq_gt_path.exists():\n             print(f""""Ground truth file not found for {seq_path}. Skipping..."""")\n@@ -60,25 +62,25 @@ def split_dataset(src_fldr: Path, percent_to_delete: float = 0.5) -> None:\n         df = pd.read_csv(seq_gt_path, sep="""","""", header=None)\n         nr_seq_imgs = df[0].unique().max()\n         split = int(nr_seq_imgs * (1 - percent_to_delete))\n-        \n+\n         # Check if the sequence is already split\n         if nr_seq_imgs ",fix,Fix memory leak
0487d2d7ee926ab3241d4e752643f53902a7a40d,"code refac: single to double quotes + fixed whitespace + line length (#1987)

* sort imports

* single to double quotes + whitespace + line length

* remove parse_opt",tracking/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport configparser\nimport shutil\nimport json\nimport re\nimport os\nimport torch\nimport threading\nimport sys\nimport copy\nimport concurrent.futures\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path\nfrom boxmot.postprocessing.gsi import gsi\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.build import load_inference_source\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_da","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport concurrent.futures\nimport configparser\nimport copy\nimport json\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom ultralytics import YOLO\nfrom ultralytics.data.build import load_inference_source\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.postprocessing.gsi import gsi\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.misc import increment_path\nfrom boxmot.utils.torch_utils import select_device\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                is_ultralytics_model, is_yolox_model)\nfrom tracking.utils import convert_to_mot_format, write_mot_resul","@@ -1,55 +1,55 @@\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n import argparse\n-import subprocess\n-from pathlib import Path\n-import numpy as np\n-from tqdm import tqdm\n+import concurrent.futures\n import configparser\n-import shutil\n+import copy\n import json\n-import re\n import os\n-import torch\n-import threading\n+import re\n+import shutil\n+import subprocess\n import sys\n-import copy\n-import concurrent.futures\n+import threading\n+from pathlib import Path\n \n+import numpy as np\n+import torch\n+from tqdm import tqdm\n+from ultralytics import YOLO\n+from ultralytics.data.build import load_inference_source\n+\n+from boxmot.appearance.reid.auto_backend import ReidAutoBackend\n+from boxmot.postprocessing.gsi import gsi\n from boxmot.tracker_zoo import create_tracker\n from boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES\n from boxmot.utils.checks import RequirementsChecker\n-from boxmot.utils.torch_utils import select_device\n from boxmot.utils.misc import increment_path\n-from boxmot.postprocessing.gsi import gsi\n-\n-from ultralytics import YOLO\n-from ultralytics.data.build import load_inference_source\n-\n+from boxmot.utils.torch_utils import select_device\n from tracking.detectors import (get_yolo_inferer, default_imgsz,\n                                 is_ultralytics_model, is_yolox_model)\n-from tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, unzip_mot_dataset, eval_setup, split_dataset\n-from boxmot.appearance.reid.auto_backend import ReidAutoBackend\n+from tracking.utils import convert_to_mot_format, write_mot_results, download_mot_eval_tools, download_mot_dataset, \\n+    unzip_mot_dataset, eval_setup, split_dataset\n \n checker = RequirementsChecker()\n-checker.check_packages(('ultralytics', ))  # install\n+checker.check_packages((""""ultralytics"""", ))  # install\n \n \n-def cleanup_mot17(data_dir, keep_detection='FRCNN'):\n+def cleanup_mot1",add,Add note about data volume to enable_metrics_collection
8cd53ee74137205da64cf998dfb517e6d8709cce,Evolve config fix (#1997),tracking/val.py,"\n# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport configparser\nimport shutil\nimport json\nimport cv2\nimport re\nimport os\nimport torch\nimport threading\nimport sys\nimport copy\nimport concurrent.futures\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path, prompt_overwrite\nfrom boxmot.utils.clean import cleanup_mot17\nfrom typing import Optional, List, Dict, Generator, Union\n\nfrom boxmot.utils.dataloaders.MOT17 import MOT17DetEmbDataset\nfrom boxmot.postprocessing.gsi import gsi\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.build import load_inference_source\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz","\n# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport configparser\nimport shutil\nimport json\nimport cv2\nimport re\nimport os\nimport torch\nimport threading\nimport sys\nimport copy\nimport concurrent.futures\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.misc import increment_path, prompt_overwrite\nfrom boxmot.utils.clean import cleanup_mot17\nfrom typing import Optional, List, Dict, Generator, Union\n\nfrom boxmot.utils.dataloaders.MOT17 import MOT17DetEmbDataset\nfrom boxmot.postprocessing.gsi import gsi\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.build import load_inference_source\n\nfrom tracking.detectors import (get_yolo_inferer, default_imgsz","@@ -260,17 +260,18 @@ def process_sequence(seq_name: str,\n                      reid_name: str,\n                      tracking_method: str,\n                      exp_folder: str,\n-                     target_fps: Optional[int]):\n+                     target_fps: Optional[int],\n+                     cfg_dict: Optional[Dict] = None):\n \n     device = select_device('cpu')\n     tracker = create_tracker(\n-        tracking_method,\n-        TRACKER_CONFIGS / (tracking_method + """".yaml""""),\n-        Path(reid_name + '.pt'),\n-        device,\n-        False,\n-        False,\n-        None,\n+        tracker_type=tracking_method,\n+        tracker_config=TRACKER_CONFIGS / (tracking_method + """".yaml""""),\n+        reid_weights=Path(reid_name + '.pt'),\n+        device=device,\n+        half=False,\n+        per_class=False,\n+        evolve_param_dict=cfg_dict,\n     )\n \n     # load with the userâs FPS\n@@ -326,7 +327,8 @@ def run_generate_mot_results(opt: argparse.Namespace, evolve_config: dict = None\n             opt.reid_model[0].stem,\n             opt.tracking_method,\n             str(exp_dir),\n-            getattr(opt, 'fps', None)\n+            getattr(opt, 'fps', None),\n+            evolve_config\n         )\n         for seq in sequence_names\n     ]\n",add,Fix KT - 10 flag
cc466d12c2075f6a06dafe1e408be8eda11251e9,boosttrack bug fix (#2009),boxmot/trackers/boosttrack/boosttrack.py,"from collections import deque\nfrom typing import List, Optional\n\nimport numpy as np\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.boosttrack.assoc import (\n    MhDist_similarity,\n    associate,\n    iou_batch,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    ","from collections import deque\nfrom typing import List, Optional\n\nimport numpy as np\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.boosttrack.assoc import (\n    MhDist_similarity,\n    associate,\n    iou_batch,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    ","@@ -421,4 +421,4 @@ class BoostTrack(BaseTracker):\n             scores = detections[:, 4].copy()\n             scores[tmp] = np.maximum(scores[tmp], self.det_thresh + 1e-5)\n             detections[:, 4] = scores\n-        return detections\n+        return detections\n\ No newline at end of file\n",fix,Add Table about data volume to enable EGL
cc466d12c2075f6a06dafe1e408be8eda11251e9,boosttrack bug fix (#2009),boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nimport threading\nfrom pathlib import Path\n\nimport numpy as np\n\n# global logger\nfrom loguru import logger\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\n\nDATA = ROOT / """"data""""\nBOXMOT = ROOT / """"boxmot""""\nTOML = ROOT / """"pyproject.toml""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\n\nEXAMPLES = BOXMOT / """"tracking""""\nWEIGHTS = BOXMOT / """"engine"""" / """"weights""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))\n\n\ndef only_main_thread(record):\n    # Check if the current thread is the main thread\n    return threading.current_thread().name == """"MainThread""""\n","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nimport threading\nfrom pathlib import Path\n\nimport numpy as np\n\n# global logger\nfrom loguru import logger\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\n\nDATA = ROOT / """"data""""\nBOXMOT = ROOT / """"boxmot""""\nTOML = ROOT / """"pyproject.toml""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\n\nEXAMPLES = BOXMOT / """"engine""""\nWEIGHTS = BOXMOT / """"engine"""" / """"weights""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))\n\n\ndef only_main_thread(record):\n    # Check if the current thread is the main thread\n    return threading.current_thread().name == """"MainThread""""\n","@@ -18,7 +18,7 @@ BOXMOT = ROOT / """"boxmot""""\n TOML = ROOT / """"pyproject.toml""""\n TRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\n \n-EXAMPLES = BOXMOT / """"tracking""""\n+EXAMPLES = BOXMOT / """"engine""""\n WEIGHTS = BOXMOT / """"engine"""" / """"weights""""\n \n NUM_THREADS = min(8, max(1, os.cpu_count() - 1))\n",add,Added net . kano . mac to example
4b8c902d5cceaf063a0fc116a990d0bb3e7b1a26,fix bug,boxmot/trackers/boosttrack/boosttrack.py,"from collections import deque\nfrom typing import List, Optional\n\nimport numpy as np\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.boosttrack.assoc import (\n    MhDist_similarity,\n    associate,\n    iou_batch,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    ","from collections import deque\nfrom typing import List, Optional\n\nimport numpy as np\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.trackers.boosttrack.assoc import (\n    MhDist_similarity,\n    associate,\n    iou_batch,\n    shape_similarity,\n    soft_biou_batch,\n)\nfrom boxmot.trackers.boosttrack.kalmanfilter import KalmanFilter\n\n\ndef convert_bbox_to_z(bbox):\n    """"""""""""\n    Converts a bounding box [x1,y1,x2,y2] to state vector [x, y, h, r].\n    """"""""""""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.0\n    y = bbox[1] + h / 2.0\n    r = w / float(h + 1e-6)\n    return np.array([x, y, h, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Converts a state vector [x, y, h, r] back to bounding box [x1,y1,x2,y2].\n    """"""""""""\n    h = x[2]\n    r = x[3]\n    w = 0 if r <= 0 else r * h\n    ","@@ -342,14 +342,19 @@ class BoostTrack(BaseTracker):\n         return ((z.reshape((-1, 1, n_dims)) - x.reshape((1, -1, n_dims))) ** 2 *\n                 sigma_inv.reshape((1, -1, n_dims))).sum(axis=2)\n \n+\n     def duo_confidence_boost(self, detections: np.ndarray) -> np.ndarray:\n         if len(detections) == 0:\n             return detections\n+\n         n_dims = 4\n         limit = 13.2767\n         mh_dist = self.get_mh_dist_matrix(detections, n_dims)\n-        if mh_dist.size == 0 and self.frame_count < 2:\n+\n+        # If there are no existing trackers, bail out immediately\n+        if mh_dist.size == 0:\n             return detections\n+\n         min_dists = mh_dist.min(1)\n         mask = (min_dists > limit) & (detections[:, 4] < self.det_thresh)\n         boost_inds = np.where(mask)[0]\n@@ -372,6 +377,7 @@ class BoostTrack(BaseTracker):\n             conf_max = np.max(detections[args_tmp, 4])\n             if detections[boost_inds[bi], 4] == conf_max:\n                 remaining = np.concatenate([remaining, [boost_inds[bi]]])\n+\n         mask_boost = np.zeros_like(detections[:, 4], dtype=bool)\n         mask_boost[remaining] = True\n         detections[:, 4] = np.where(\n@@ -421,4 +427,4 @@ class BoostTrack(BaseTracker):\n             scores = detections[:, 4].copy()\n             scores[tmp] = np.maximum(scores[tmp], self.det_thresh + 1e-5)\n             detections[:, 4] = scores\n-        return detections\n\ No newline at end of file\n+        return detections\n",fix,Added STORM - 236 to Changelog
033c2470a2a841ddfa7f5c365456d50f283f7670,"gpu by default, cpu forced on macos (#2012)",.github/workflows/ci.yml,"# name of the workflow, what it is doing (optional)\nname: BoxMOT CI\n\n# events that trigger the workflow (required)\non:\n  push:\n    # pushes to the following branches\n    branches:\n      - master\n  pull_request:\n    # pull request where master is target\n    branches:\n      - master\n\n\njobs:\n  tracking-methods:\n    runs-on: ${{ matrix.os }}\n    outputs:\n      status: ${{ job.status }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, macos-14]   # skip windows-latest for\n        python-version: ['3.12']\n        # leads to too many workflow which ends up queued\n        # tracking-method: [hybridsort, botsort, ocsort, bytetrack] \n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      - uses: actions/checkout@v4  # Check out the repository\n      - name: Set up Python\n        uses: actions/setup-python@v5  # Prepare environment with python 3.9\n        with:\n          python-version: ","# name of the workflow, what it is doing (optional)\nname: BoxMOT CI\n\n# events that trigger the workflow (required)\non:\n  push:\n    # pushes to the following branches\n    branches:\n      - master\n  pull_request:\n    # pull request where master is target\n    branches:\n      - master\n\n\njobs:\n  tracking-methods:\n    runs-on: ${{ matrix.os }}\n    outputs:\n      status: ${{ job.status }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, macos-14]   # skip windows-latest for\n        python-version: ['3.12']\n        # leads to too many workflow which ends up queued\n        # tracking-method: [hybridsort, botsort, ocsort, bytetrack] \n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n      - uses: actions/checkout@v4  # Check out the repository\n      - name: Set up Python\n        uses: actions/setup-python@v5  # Prepare environment with python 3.9\n        with:\n          python-version: ","@@ -38,13 +38,7 @@ jobs:\n       - name: Install requirements\n         shell: bash  # for Windows compatibility\n         run: |\n-          if [[ """"$OSTYPE"""" == """"darwin""""* ]]; then\n-            # macOS\n-            sed -i '' 's/source=""""torch_cuda121""""/source=""""torchcpu""""/g' pyproject.toml\n-          elif [[ """"$OSTYPE"""" == """"linux-gnu""""* ]]; then\n-            # Linux\n-            sed -i 's/source=""""torch_cuda121""""/source=""""torchcpu""""/g' pyproject.toml\n-          fi\n+          sed -i'' -e 's/index = """"torch-gpu""""/index = """"torch-cpu""""/g' pyproject.toml\n           python -m pip install --upgrade pip setuptools wheel uv\n           uv sync --group yolo\n       - name: Generate detections and embeddings\n@@ -80,13 +74,7 @@ jobs:\n       - name: Install requirements\n         shell: bash  # for Windows compatibility\n         run: |\n-          if [[ """"$OSTYPE"""" == """"darwin""""* ]]; then\n-            # macOS\n-            sed -i '' 's/source=""""torch_cuda121""""/source=""""torchcpu""""/g' pyproject.toml\n-          elif [[ """"$OSTYPE"""" == """"linux-gnu""""* ]]; then\n-            # Linux\n-            sed -i 's/source=""""torch_cuda121""""/source=""""torchcpu""""/g' pyproject.toml\n-          fi\n+          sed -i'' -e 's/index = """"torch-gpu""""/index = """"torch-cpu""""/g' pyproject.toml\n           python -m pip install --upgrade pip setuptools wheel uv\n           uv sync --group yolo --group evolve\n       - name: Evolve set of parameters for selected tracking method\n@@ -119,13 +107,7 @@ jobs:\n       - name: Install requirements\n         shell: bash  # for Windows compatibility\n         run: |\n-          if [[ """"$OSTYPE"""" == """"darwin""""* ]]; then\n-            # macOS\n-            sed -i '' 's/source=""""torch_cuda121""""/source=""""torchcpu""""/g' pyproject.toml\n-          elif [[ """"$OSTYPE"""" == """"linux-gnu""""* ]]; then\n-            # Linux\n-            sed -i 's/source=""""torch_cuda121""""/source=""""torchcpu""""/g' pyproject.toml\n-          fi\n+          sed -i'' -e 's/index = """"torch-",add,Add note about data volume to enable pre - dom API
033c2470a2a841ddfa7f5c365456d50f283f7670,"gpu by default, cpu forced on macos (#2012)",boxmot/engine/detectors/rfdetr.py,"# Mikel BrostrÃ¶m ð¥ RFDETR Tracking ð§¾ AGPL-3.0 license\n\nimport cv2\nimport numpy as np\nimport torch\nfrom rfdetr import RFDETRBase\nfrom rfdetr.util.coco_classes import COCO_CLASSES\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect import DetectionPredictor\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.engine.detectors.yolo_interface import YoloInterface\n\n\nclass RFDETRStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = COCO_CLASSES\n\n    def __init__(self, model, device, args):\n        self.args = args\n        LOGGER.info(""""Loading RFDETR model"""")\n        self.model = RFDETRBase(device=""""cpu"""")\n\n    @torch.no_grad()\n    def __call__(self, im, augment, visualize, embed):\n        image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)  # Convert frame BGR2RGB for RFDETR\n        with torch.no_grad():\n            detections = self.model.predict(image, threshold=self.args.conf)\n\n    ","# Mikel BrostrÃ¶m ð¥ RFDETR Tracking ð§¾ AGPL-3.0 license\n\nimport cv2\nimport numpy as np\nimport torch\nfrom rfdetr import RFDETRBase\nfrom rfdetr.util.coco_classes import COCO_CLASSES\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect import DetectionPredictor\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.engine.detectors.yolo_interface import YoloInterface\n\n\nclass RFDETRStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = COCO_CLASSES\n    ch = 3\n\n    def __init__(self, model, device, args):\n        self.args = args\n        LOGGER.info(""""Loading RFDETR model"""")\n        self.model = RFDETRBase(device=""""cpu"""")\n\n    @torch.no_grad()\n    def __call__(self, im, augment, visualize, embed):\n        image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)  # Convert frame BGR2RGB for RFDETR\n        with torch.no_grad():\n            detections = self.model.predict(image, threshold=self.args.c","@@ -18,6 +18,7 @@ class RFDETRStrategy(YoloInterface):\n     fp16 = False\n     triton = False\n     names = COCO_CLASSES\n+    ch = 3\n \n     def __init__(self, model, device, args):\n         self.args = args\n",add,Add note about data volume to enable_metrics_collection
033c2470a2a841ddfa7f5c365456d50f283f7670,"gpu by default, cpu forced on macos (#2012)",boxmot/engine/detectors/yolox.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport cv2\nimport gdown\nimport numpy as np\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.engine.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    """"yolox_n.pt"""": """"https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX"""",\n    """"yolox_s.pt"""": """"https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj"""",\n    """"yolox_m.pt"""": """"https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun"""",\n    """"yolox_l.pt"""": """"https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz"""",\n    """"yolox_x.pt"""": """"https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5"""",\n    """"yolox_x_ablation.pt"""":","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport cv2\nimport gdown\nimport numpy as np\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.engine.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    """"yolox_n.pt"""": """"https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX"""",\n    """"yolox_s.pt"""": """"https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj"""",\n    """"yolox_m.pt"""": """"https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun"""",\n    """"yolox_l.pt"""": """"https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz"""",\n    """"yolox_x.pt"""": """"https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5"""",\n    """"yolox_x_ablation.pt"""":","@@ -114,6 +114,7 @@ class YoloXStrategy(YoloInterface):\n \n     def __init__(self, model, device, args):\n \n+        self.ch = 3\n         self.args = args\n         self.imgsz = args.imgsz\n         self.pt = False\n",add,Add note about data volume to enable
033c2470a2a841ddfa7f5c365456d50f283f7670,"gpu by default, cpu forced on macos (#2012)",pyproject.toml,"[[tool.uv.index]]\nname = """"testpypi""""\nurl = """"https://test.pypi.org/simple/""""\npublish-url = """"https://test.pypi.org/legacy/""""\nexplicit = true\n\n[[tool.pdm.source]]\nname = """"torchcpu""""\nurl = """"https://download.pytorch.org/whl/cpu""""\nverify_ssl = true\n\n[[tool.pdm.source]]\nname = """"torch_cuda121""""\nurl = """"https://download.pytorch.org/whl/cu121""""\nverify_ssl = true\n\n[build-system]\nrequires = [""""pdm-backend""""]\nbuild-backend = """"pdm.backend""""\n\n[[tool.uv.dependency-metadata]]\nname = """"yolox""""\nversion = """"0.3.0""""\nrequires-dist = [""""onnx>=1.17.0"""", """"onnxsim<1.0.0,>=0.4.36""""]\n\n[tool.uv]\nno-build-isolation-package = [""""yolox""""]\n\n[tool.flake8]\nmax-line-length = 120\nexclude = ["""".tox"""", """"*.egg"""", """"build"""", """"temp""""]\nselect = [""""E"""", """"W"""", """"F""""]\ndoctests = true\nverbose = 2\nformat = """"pylint""""\nignore = [""""E731"""", """"F405"""", """"E402"""", """"W504"""", """"W605"""", """"E741""""]\n\n[project]\nauthors = [\n    {name = """"Mikel BrostrÃ¶m""""},\n]\nlicense = {text = """"AGPL-3.0""""}\nrequir","# =====================================================\n# pyproject.toml for BoxMOT â UV with GPU by default\n# =====================================================\n\n#######################\n# Alternative Indexes #\n#######################\n\n[[tool.uv.index]]\nname     = """"torch-gpu""""\nurl      = """"https://download.pytorch.org/whl/cu121""""\nexplicit = true\n\n[[tool.uv.index]]\nname     = """"torch-cpu""""\nurl      = """"https://download.pytorch.org/whl/cpu""""\nexplicit = true\n\n#################################\n# Dependency-to-Index Mapping   #\n#################################\n\ntorch = [\n  # CUDA build on Linux/Windows only\n  { index = """"torch-gpu"""", marker = """"sys_platform != 'darwin'"""" },\n  # CPU build on macOS\n  { index = """"torch-cpu"""", marker = """"sys_platform == 'darwin'"""" }\n]\ntorchvision = [\n  { index = """"torch-gpu"""", marker = """"sys_platform != 'darwin'"""" },\n  { index = """"torch-cpu"""", marker = """"sys_platform == 'darwin'"""" }\n]\n\n#########################\n# UV-specif","@@ -1,144 +1,178 @@\n+# =====================================================\n+# pyproject.toml for BoxMOT â UV with GPU by default\n+# =====================================================\n+\n+#######################\n+# Alternative Indexes #\n+#######################\n+\n [[tool.uv.index]]\n-name = """"testpypi""""\n-url = """"https://test.pypi.org/simple/""""\n-publish-url = """"https://test.pypi.org/legacy/""""\n+name     = """"torch-gpu""""\n+url      = """"https://download.pytorch.org/whl/cu121""""\n explicit = true\n \n-[[tool.pdm.source]]\n-name = """"torchcpu""""\n-url = """"https://download.pytorch.org/whl/cpu""""\n-verify_ssl = true\n+[[tool.uv.index]]\n+name     = """"torch-cpu""""\n+url      = """"https://download.pytorch.org/whl/cpu""""\n+explicit = true\n \n-[[tool.pdm.source]]\n-name = """"torch_cuda121""""\n-url = """"https://download.pytorch.org/whl/cu121""""\n-verify_ssl = true\n+#################################\n+# Dependency-to-Index Mapping   #\n+#################################\n \n-[build-system]\n-requires = [""""pdm-backend""""]\n-build-backend = """"pdm.backend""""\n+torch = [\n+  # CUDA build on Linux/Windows only\n+  { index = """"torch-gpu"""", marker = """"sys_platform != 'darwin'"""" },\n+  # CPU build on macOS\n+  { index = """"torch-cpu"""", marker = """"sys_platform == 'darwin'"""" }\n+]\n+torchvision = [\n+  { index = """"torch-gpu"""", marker = """"sys_platform != 'darwin'"""" },\n+  { index = """"torch-cpu"""", marker = """"sys_platform == 'darwin'"""" }\n+]\n+\n+#########################\n+# UV-specific Overrides #\n+#########################\n \n [[tool.uv.dependency-metadata]]\n-name = """"yolox""""\n-version = """"0.3.0""""\n+name          = """"yolox""""\n+version       = """"0.3.0""""\n requires-dist = [""""onnx>=1.17.0"""", """"onnxsim<1.0.0,>=0.4.36""""]\n \n [tool.uv]\n no-build-isolation-package = [""""yolox""""]\n \n+############\n+# Flake8   #\n+############\n+\n [tool.flake8]\n max-line-length = 120\n-exclude = ["""".tox"""", """"*.egg"""", """"build"""", """"temp""""]\n-select = [""""E"""", """"W"""", """"F""""]\n-doctests = true\n-verbose = 2\n-fo",add,Add note about data volume to enable_metrics_collection
033c2470a2a841ddfa7f5c365456d50f283f7670,"gpu by default, cpu forced on macos (#2012)",uv.lock,"version = 1\nrevision = 2\nrequires-python = """">=3.9, <=3.12""""\nresolution-markers = [\n    """"python_full_version < '3.10' and platform_machine == 'arm64' and sys_platform == 'darwin'"""",\n    """"python_full_version < '3.10' and platform_machine == 'x86_64' and sys_platform == 'darwin'"""",\n    """"python_full_version < '3.10' and platform_machine != 'arm64' and platform_machine != 'x86_64' and sys_platform == 'darwin'"""",\n    """"python_full_version == '3.10.*' and platform_machine == 'x86_64' and sys_platform == 'darwin'"""",\n    """"python_full_version == '3.10.*' and platform_machine != 'x86_64' and sys_platform == 'darwin'"""",\n    """"python_version < '0'"""",\n    """"python_full_version == '3.11.*' and platform_machine == 'x86_64' and sys_platform == 'darwin'"""",\n    """"python_full_version == '3.11.*' and platform_machine != 'x86_64' and sys_platform == 'darwin'"""",\n    """"python_full_version >= '3.12' and platform_machine == 'x86_64' and sys_platform == 'darwin'"""",\n    """"python_full_version >= ",,"@@ -1,3449 +0,0 @@\n-version = 1\n-revision = 2\n-requires-python = """">=3.9, <=3.12""""\n-resolution-markers = [\n-    """"python_full_version < '3.10' and platform_machine == 'arm64' and sys_platform == 'darwin'"""",\n-    """"python_full_version < '3.10' and platform_machine == 'x86_64' and sys_platform == 'darwin'"""",\n-    """"python_full_version < '3.10' and platform_machine != 'arm64' and platform_machine != 'x86_64' and sys_platform == 'darwin'"""",\n-    """"python_full_version == '3.10.*' and platform_machine == 'x86_64' and sys_platform == 'darwin'"""",\n-    """"python_full_version == '3.10.*' and platform_machine != 'x86_64' and sys_platform == 'darwin'"""",\n-    """"python_version < '0'"""",\n-    """"python_full_version == '3.11.*' and platform_machine == 'x86_64' and sys_platform == 'darwin'"""",\n-    """"python_full_version == '3.11.*' and platform_machine != 'x86_64' and sys_platform == 'darwin'"""",\n-    """"python_full_version >= '3.12' and platform_machine == 'x86_64' and sys_platform == 'darwin'"""",\n-    """"python_full_version >= '3.12' and platform_machine != 'x86_64' and sys_platform == 'darwin'"""",\n-    """"python_full_version < '3.10' and sys_platform == 'win32'"""",\n-    """"python_full_version == '3.10.*' and sys_platform == 'win32'"""",\n-    """"python_full_version == '3.11.*' and sys_platform == 'win32'"""",\n-    """"python_full_version >= '3.12' and sys_platform == 'win32'"""",\n-    """"python_full_version < '3.10' and platform_machine == 'aarch64' and sys_platform == 'linux'"""",\n-    """"python_full_version < '3.10' and platform_machine != 'aarch64' and sys_platform == 'linux'"""",\n-    """"python_full_version == '3.10.*' and platform_machine == 'aarch64' and sys_platform == 'linux'"""",\n-    """"python_full_version == '3.10.*' and platform_machine != 'aarch64' and sys_platform == 'linux'"""",\n-    """"python_full_version == '3.11.*' and platform_machine == 'aarch64' and sys_platform == 'linux'"""",\n-    """"python_full_version == '3.11.*' and platform_machine != 'aarch64' and sys_platform == 'l",add,Add note about data volume to enable_metrics_collection
605ff37640c2759fdf034bfb8b029cf24351ef4d,Fix cuda mp (#2023),boxmot/engine/cli.py,"#!/usr/bin/env python3\n\nimport argparse\nfrom pathlib import Path\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES\n\n\ndef main():\n    # Parent parser for evaluation commands: allow multiple weights\n    eval_parent = argparse.ArgumentParser(add_help=False)\n    eval_parent.add_argument(\n        '--yolo-model', nargs='+', type=Path,\n        default=[WEIGHTS / 'yolov8n.pt'],\n        help='one or more YOLO weights for detection (only for generate/eval/tune)'\n    )\n    eval_parent.add_argument(\n        '--reid-model', nargs='+', type=Path,\n        default=[WEIGHTS / 'osnet_x0_25_msmt17.pt'],\n        help='one or more ReID model weights (only for generate/eval/tune)'\n    )\n    eval_parent.add_argument('--classes', nargs='+', type=int,\n        default=[0], help='filter by class indices')\n\n    # Common arguments for all commands (flags only, no positionals)\n    common_parser = argparse.ArgumentParser(add_help=False, conflict_handler='reso","#!/usr/bin/env python3\n\nimport argparse\nfrom pathlib import Path\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES\n\n\ndef main():\n    # Parent parser for evaluation commands: allow multiple weights\n    eval_parent = argparse.ArgumentParser(add_help=False)\n    eval_parent.add_argument(\n        '--yolo-model', nargs='+', type=Path,\n        default=[WEIGHTS / 'yolov8n.pt'],\n        help='one or more YOLO weights for detection (only for generate/eval/tune)'\n    )\n    eval_parent.add_argument(\n        '--reid-model', nargs='+', type=Path,\n        default=[WEIGHTS / 'osnet_x0_25_msmt17.pt'],\n        help='one or more ReID model weights (only for generate/eval/tune)'\n    )\n    eval_parent.add_argument('--classes', nargs='+', type=int,\n        default=[0], help='filter by class indices')\n\n    # Common arguments for all commands (flags only, no positionals)\n    common_parser = argparse.ArgumentParser(add_help=False, conflict_handler='reso","@@ -37,7 +37,7 @@ def main():\n     )\n     common_parser.add_argument('--imgsz', '--img-size', nargs='+', type=int,\n                                default=None, help='inference size h,w')\n-    common_parser.add_argument('--fps', type=int, default=None,\n+    common_parser.add_argument('--fps', type=int, default=30,\n                                help='video frame-rate')\n     common_parser.add_argument('--conf', type=float, default=0.01,\n                                help='min confidence threshold')\n",add,Add note about data volume to enable_metrics_collection
605ff37640c2759fdf034bfb8b029cf24351ef4d,Fix cuda mp (#2023),boxmot/engine/val.py,"\n# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport configparser\nimport shutil\nimport json\nimport cv2\nimport re\nimport os\nimport torch\nimport threading\nimport sys\nimport copy\nimport concurrent.futures\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.plots import MetricsPlotter\nfrom boxmot.utils.misc import increment_path, prompt_overwrite\nfrom boxmot.utils.clean import cleanup_mot17\nfrom typing import Optional, List, Dict, Generator, Union\n\nfrom boxmot.utils.dataloaders.MOT17 import MOT17DetEmbDataset\nfrom boxmot.postprocessing.gsi import gsi\n\nfrom ultralytics import YOLO\nfrom ultralytics.data.build import load_inference_source\n\nfrom boxmot.engi","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport multiprocessing as mp\nmp.set_start_method(""""spawn"""", force=True)\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport configparser\nimport shutil\nimport json\nimport cv2\nimport re\nimport os\nimport torch\nimport threading\nimport sys\nimport copy\nimport concurrent.futures\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.plots import MetricsPlotter\nfrom boxmot.utils.misc import increment_path, prompt_overwrite\nfrom boxmot.utils.clean import cleanup_mot17\nfrom typing import Optional, List, Dict, Generator, Union\n\nfrom boxmot.utils.dataloaders.MOT17 import MOT17DetEmbDataset\nfrom boxmot.postprocessing.gsi import gsi\n\nfrom ultralytics import YOLO\nfr","@@ -1,6 +1,8 @@\n-\n # Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n \n+import multiprocessing as mp\n+mp.set_start_method(""""spawn"""", force=True)\n+\n import argparse\n import subprocess\n from pathlib import Path\n@@ -151,8 +153,6 @@ def generate_dets_embs(args: argparse.Namespace, y: Path, source: Path) -> None:\n                 np.savetxt(f, embs, fmt='%f')\n \n \n-import re\n-\n def parse_mot_results(results: str) -> dict:\n     """"""""""""\n     Extracts COMBINED HOTA, MOTA, IDF1, AssA, AssRe, IDSW, and IDs from MOTChallenge evaluation output.\n",add,Add TraceV2 deletion to script
c8193f83b94e8c95f57a43005937109c1b7126d4,Fix deprecated pkg resources (#2024),boxmot/utils/checks.py,"import subprocess\nfrom pathlib import Path\nfrom typing import Optional\n\nimport pkg_resources\n\nfrom boxmot.utils import logger as LOGGER\n\nREQUIREMENTS_FILE = Path(""""requirements.txt"""")\n\n\nclass RequirementsChecker:\n    def __init__(self, group: str = None, requirements_file: Path = REQUIREMENTS_FILE):\n        """"""""""""\n        If `group` is provided, we'll sync that PDM/uv dependency-group.\n        Otherwise we'll read requirements_file and pip-install missing packages.\n        """"""""""""\n        self.group = group\n        self.requirements_file = requirements_file\n\n    def check_requirements(self):\n        if self.group:\n            self._sync_group(self.group)\n        else:\n            self._check_from_requirements()\n\n    def _check_from_requirements(self):\n        # parse requirements.txt\n        with self.requirements_file.open() as f:\n            reqs = pkg_resources.parse_requirements(f)\n        self._check_packages(reqs)\n\n    def check_packages(self, requi","import subprocess\nfrom pathlib import Path\nfrom typing import Iterable, Optional\n\nfrom boxmot.utils import logger as LOGGER\nfrom packaging.requirements import Requirement\nfrom importlib.metadata import version, PackageNotFoundError\n\n\nREQUIREMENTS_FILE = Path(""""requirements.txt"""")\n\n\nclass RequirementsChecker:\n    def __init__(self, group: str = None, requirements_file: Path = REQUIREMENTS_FILE):\n        """"""""""""\n        If `group` is provided, we'll sync that PDM/uv dependency-group.\n        Otherwise we'll read requirements_file and pip-install missing packages.\n        """"""""""""\n        self.group = group\n        self.requirements_file = requirements_file\n\n    def check_requirements(self):\n        if self.group:\n            self._sync_group(self.group)\n        else:\n            self._check_from_requirements()\n\n    def _check_from_requirements(self):\n        # parse requirements.txt into Requirement objects\n        reqs = []\n        with self.requirements_file.","@@ -1,10 +1,11 @@\n import subprocess\n from pathlib import Path\n-from typing import Optional\n-\n-import pkg_resources\n+from typing import Iterable, Optional\n \n from boxmot.utils import logger as LOGGER\n+from packaging.requirements import Requirement\n+from importlib.metadata import version, PackageNotFoundError\n+\n \n REQUIREMENTS_FILE = Path(""""requirements.txt"""")\n \n@@ -25,29 +26,52 @@ class RequirementsChecker:\n             self._check_from_requirements()\n \n     def _check_from_requirements(self):\n-        # parse requirements.txt\n+        # parse requirements.txt into Requirement objects\n+        reqs = []\n         with self.requirements_file.open() as f:\n-            reqs = pkg_resources.parse_requirements(f)\n+            for raw in f:\n+                line = raw.strip()\n+                if not line or line.startswith(""""#""""):\n+                    continue\n+                reqs.append(Requirement(line))\n         self._check_packages(reqs)\n \n-    def check_packages(self, requirements, cmds=[]):\n-        missing = []\n-        for r in requirements:\n+    def check_packages(self, requirements: Iterable[str], cmds: Optional[list[str]] = None):\n+        """"""""""""\n+        Check and install packages specified by requirement strings, e.g.\n+        [""""foo"""", """"bar>=1.2""""].\n+\n+        :param requirements: iterable of requirement specifiers as strings\n+        :param cmds: extra pip args (e.g. [""""--upgrade""""]).\n+        """"""""""""\n+        # turn each string into a Requirement\n+        specs = [Requirement(r) for r in requirements]\n+\n+        missing: list[str] = []\n+        for req in specs:\n+            name = req.name\n             try:\n-                pkg_resources.require(str(r))\n-            except Exception as e:\n-                LOGGER.error(f""""{e}"""")\n-                missing.append(str(r))\n+                inst_ver = version(name)\n+            except PackageNotFoundError:\n+                LOGGER.error(f""""Package {name!r} is",add,Add debug flag
301bc51c65c58679c3a83f917c6e8f6326eafdeb,fix multiprocessing logging mess (#2025),boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nimport threading\nfrom pathlib import Path\n\nimport numpy as np\n\n# global logger\nfrom loguru import logger\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\n\nDATA = ROOT / """"data""""\nBOXMOT = ROOT / """"boxmot""""\nTOML = ROOT / """"pyproject.toml""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\n\nEXAMPLES = BOXMOT / """"engine""""\nWEIGHTS = BOXMOT / """"engine"""" / """"weights""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))\n\n\ndef only_main_thread(record):\n    # Check if the current thread is the main thread\n    return threading.current_thread().name == """"MainThread""""\n","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nimport threading\nfrom pathlib import Path\n\nimport numpy as np\nimport multiprocessing as mp\n\n# global logger\nfrom loguru import logger\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # root directory\n\nDATA = ROOT / """"data""""\nBOXMOT = ROOT / """"boxmot""""\nTOML = ROOT / """"pyproject.toml""""\nTRACKER_CONFIGS = ROOT / """"boxmot"""" / """"configs""""\n\nEXAMPLES = BOXMOT / """"engine""""\nWEIGHTS = BOXMOT / """"engine"""" / """"weights""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))\n\ndef _is_main_process(record):\n    return mp.current_process().name == """"MainProcess""""\n\ndef configure_logging():\n    # this will remove *all* existing handlers and then add yours\n    logger.configure(handlers=[\n        {\n            """"sink"""": sys.stderr,\n            """"level"""":    """"INFO"""",\n            """"filter"""":   _is_main_process,\n        }\n    ])\n    \nconfigure_logging()","@@ -6,6 +6,7 @@ import threading\n from pathlib import Path\n \n import numpy as np\n+import multiprocessing as mp\n \n # global logger\n from loguru import logger\n@@ -23,7 +24,17 @@ WEIGHTS = BOXMOT / """"engine"""" / """"weights""""\n \n NUM_THREADS = min(8, max(1, os.cpu_count() - 1))\n \n-\n-def only_main_thread(record):\n-    # Check if the current thread is the main thread\n-    return threading.current_thread().name == """"MainThread""""\n+def _is_main_process(record):\n+    return mp.current_process().name == """"MainProcess""""\n+\n+def configure_logging():\n+    # this will remove *all* existing handlers and then add yours\n+    logger.configure(handlers=[\n+        {\n+            """"sink"""": sys.stderr,\n+            """"level"""":    """"INFO"""",\n+            """"filter"""":   _is_main_process,\n+        }\n+    ])\n+    \n+configure_logging()\n\ No newline at end of file\n",add,Added net . kano . joustsim . oscar . os
9b53be39237b8be437d90e827442fbcb3e6c6dcf,"fix: make --verbose enable logging instead of disabling (#2056)

Co-authored-by: zli <zli@mail.com>",boxmot/engine/cli.py,"#!/usr/bin/env python3\n\nimport argparse\nfrom pathlib import Path\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES\n\n\ndef main():\n    # Parent parser for evaluation commands: allow multiple weights\n    eval_parent = argparse.ArgumentParser(add_help=False)\n    eval_parent.add_argument(\n        '--yolo-model', nargs='+', type=Path,\n        default=[WEIGHTS / 'yolov8n.pt'],\n        help='one or more YOLO weights for detection (only for generate/eval/tune)'\n    )\n    eval_parent.add_argument(\n        '--reid-model', nargs='+', type=Path,\n        default=[WEIGHTS / 'osnet_x0_25_msmt17.pt'],\n        help='one or more ReID model weights (only for generate/eval/tune)'\n    )\n    eval_parent.add_argument('--classes', nargs='+', type=int,\n        default=[0], help='filter by class indices')\n\n    # Common arguments for all commands (flags only, no positionals)\n    common_parser = argparse.ArgumentParser(add_help=False, conflict_handler='reso","#!/usr/bin/env python3\n\nimport argparse\nfrom pathlib import Path\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES\n\n\ndef main():\n    # Parent parser for evaluation commands: allow multiple weights\n    eval_parent = argparse.ArgumentParser(add_help=False)\n    eval_parent.add_argument(\n        '--yolo-model', nargs='+', type=Path,\n        default=[WEIGHTS / 'yolov8n.pt'],\n        help='one or more YOLO weights for detection (only for generate/eval/tune)'\n    )\n    eval_parent.add_argument(\n        '--reid-model', nargs='+', type=Path,\n        default=[WEIGHTS / 'osnet_x0_25_msmt17.pt'],\n        help='one or more ReID model weights (only for generate/eval/tune)'\n    )\n    eval_parent.add_argument('--classes', nargs='+', type=int,\n        default=[0], help='filter by class indices')\n\n    # Common arguments for all commands (flags only, no positionals)\n    common_parser = argparse.ArgumentParser(add_help=False, conflict_handler='reso","@@ -66,7 +66,7 @@ def main():\n                                help='path to precomputed embeddings file')\n     common_parser.add_argument('--exp-folder-path', type=Path,\n                                help='path to experiment folder')\n-    common_parser.add_argument('--verbose', action='store_false',\n+    common_parser.add_argument('--verbose', action='store_true',\n                                help='print detailed logs')\n     common_parser.add_argument('--agnostic-nms', action='store_true',\n                                help='class-agnostic NMS')\n",add,Added KHR_gl_texture_2D_image extension string
bbdef72c8342fa473a0e9ad1ff86a5e1f546a84e,fix trackeval download path bug in clone vs pkg missmatch (#2066),boxmot/engine/cli.py,"#!/usr/bin/env python3\n\nimport argparse\nfrom pathlib import Path\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES\n\n\ndef main():\n    # Parent parser for evaluation commands: allow multiple weights\n    eval_parent = argparse.ArgumentParser(add_help=False)\n    eval_parent.add_argument(\n        '--yolo-model', nargs='+', type=Path,\n        default=[WEIGHTS / 'yolov8n.pt'],\n        help='one or more YOLO weights for detection (only for generate/eval/tune)'\n    )\n    eval_parent.add_argument(\n        '--reid-model', nargs='+', type=Path,\n        default=[WEIGHTS / 'osnet_x0_25_msmt17.pt'],\n        help='one or more ReID model weights (only for generate/eval/tune)'\n    )\n    eval_parent.add_argument('--classes', nargs='+', type=int,\n        default=[0], help='filter by class indices')\n\n    # Common arguments for all commands (flags only, no positionals)\n    common_parser = argparse.ArgumentParser(add_help=False, conflict_handler='reso","#!/usr/bin/env python3\n\nimport argparse\nfrom pathlib import Path\nfrom boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, TRACKEVAL\n\n\ndef main():\n    # Parent parser for evaluation commands: allow multiple weights\n    eval_parent = argparse.ArgumentParser(add_help=False)\n    eval_parent.add_argument(\n        '--yolo-model', nargs='+', type=Path,\n        default=[WEIGHTS / 'yolov8n.pt'],\n        help='one or more YOLO weights for detection (only for generate/eval/tune)'\n    )\n    eval_parent.add_argument(\n        '--reid-model', nargs='+', type=Path,\n        default=[WEIGHTS / 'osnet_x0_25_msmt17.pt'],\n        help='one or more ReID model weights (only for generate/eval/tune)'\n    )\n    eval_parent.add_argument('--classes', nargs='+', type=int,\n        default=[0], help='filter by class indices')\n\n    # Common arguments for all commands (flags only, no positionals)\n    common_parser = argparse.ArgumentParser(add_help=False, conflict_handler='res","@@ -2,7 +2,7 @@\n \n import argparse\n from pathlib import Path\n-from boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, EXAMPLES\n+from boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS, logger as LOGGER, TRACKEVAL\n \n \n def main():\n@@ -78,7 +78,7 @@ def main():\n                                default=[""""HOTA"""", """"MOTA"""", """"IDF1""""],\n                                help='objectives for tuning: HOTA, MOTA, IDF1')\n     common_parser.add_argument('--val-tools-path', type=Path,\n-                               default=EXAMPLES / 'trackeval',\n+                               default=TRACKEVAL,\n                                help='where to clone trackeval')\n     common_parser.add_argument('--split-dataset', action='store_true',\n                                help='use second half of dataset')\n",add,Add KHR_gl_texture_2D_image extension string .
bbdef72c8342fa473a0e9ad1ff86a5e1f546a84e,fix trackeval download path bug in clone vs pkg missmatch (#2066),boxmot/engine/evolve.py,"#!/usr/bin/env python3\n""""""""""""\nThis script runs a hyperparameter tuning process for a multi-object tracking (MOT) tracker using Ray Tune,\nwith support for resuming (restoring) previous tuning runs.\n""""""""""""\n\nimport os\nos.environ[""""RAY_CHDIR_TO_TRIAL_DIR""""] = """"0""""   # keep CWD constant for all trials\nfrom pathlib import Path\n\nimport yaml\nfrom ray import tune\nfrom ray.tune import RunConfig\nfrom ray.tune.search.optuna import OptunaSearch\n\nfrom boxmot.utils import EXAMPLES, NUM_THREADS, TRACKER_CONFIGS\nfrom boxmot.engine.val import (\n    eval_init,\n    load_dataset_cfg,\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n)\n\n\ndef load_yaml_config(tracking_method: str) -> dict:\n    """"""""""""\n    Loads the YAML configuration file for the given tracking method.\n    """"""""""""\n    config_path = TRACKER_CONFIGS / f""""{tracking_method}.yaml""""\n    with open(config_path, """"r"""") as file:\n        return yaml.safe_load(file)\n\n\ndef yaml_to_search_space(c","#!/usr/bin/env python3\n""""""""""""\nThis script runs a hyperparameter tuning process for a multi-object tracking (MOT) tracker using Ray Tune,\nwith support for resuming (restoring) previous tuning runs.\n""""""""""""\n\nimport os\nos.environ[""""RAY_CHDIR_TO_TRIAL_DIR""""] = """"0""""   # keep CWD constant for all trials\nfrom pathlib import Path\n\nimport yaml\nfrom ray import tune\nfrom ray.tune import RunConfig\nfrom ray.tune.search.optuna import OptunaSearch\n\nfrom boxmot.utils import NUM_THREADS, TRACKER_CONFIGS\nfrom boxmot.engine.val import (\n    eval_init,\n    load_dataset_cfg,\n    run_generate_dets_embs,\n    run_generate_mot_results,\n    run_trackeval,\n)\n\n\ndef load_yaml_config(tracking_method: str) -> dict:\n    """"""""""""\n    Loads the YAML configuration file for the given tracking method.\n    """"""""""""\n    config_path = TRACKER_CONFIGS / f""""{tracking_method}.yaml""""\n    with open(config_path, """"r"""") as file:\n        return yaml.safe_load(file)\n\n\ndef yaml_to_search_space(config: dic","@@ -13,7 +13,7 @@ from ray import tune\n from ray.tune import RunConfig\n from ray.tune.search.optuna import OptunaSearch\n \n-from boxmot.utils import EXAMPLES, NUM_THREADS, TRACKER_CONFIGS\n+from boxmot.utils import NUM_THREADS, TRACKER_CONFIGS\n from boxmot.engine.val import (\n     eval_init,\n     load_dataset_cfg,\n",add,Add note about data volume to enable_metrics_collection
bbdef72c8342fa473a0e9ad1ff86a5e1f546a84e,fix trackeval download path bug in clone vs pkg missmatch (#2066),boxmot/engine/val.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport multiprocessing as mp\nmp.set_start_method(""""spawn"""", force=True)\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport configparser\nimport shutil\nimport json\nimport yaml\nimport cv2\nimport re\nimport os\nimport torch\nimport threading\nimport sys\nimport copy\nimport concurrent.futures\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import NUM_THREADS, ROOT, WEIGHTS, TRACKER_CONFIGS, DATASET_CONFIGS, logger as LOGGER, EXAMPLES\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.plots import MetricsPlotter\nfrom boxmot.utils.misc import increment_path, prompt_overwrite\nfrom boxmot.utils.clean import cleanup_mot17\nfrom typing import Optional, List, Dict, Generator, Union\n\nfrom boxmot.utils.dataloaders.MOT17 import MOT17DetEmbDataset\nfrom boxmot.postprocessing.gsi imp","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport multiprocessing as mp\nmp.set_start_method(""""spawn"""", force=True)\n\nimport argparse\nimport subprocess\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport configparser\nimport shutil\nimport json\nimport yaml\nimport cv2\nimport re\nimport os\nimport torch\nimport threading\nimport sys\nimport copy\nimport concurrent.futures\n\nfrom boxmot.tracker_zoo import create_tracker\nfrom boxmot.utils import NUM_THREADS, ROOT, WEIGHTS, TRACKER_CONFIGS, DATASET_CONFIGS, logger as LOGGER, TRACKEVAL\nfrom boxmot.utils.checks import RequirementsChecker\nfrom boxmot.utils.torch_utils import select_device\nfrom boxmot.utils.plots import MetricsPlotter\nfrom boxmot.utils.misc import increment_path, prompt_overwrite\nfrom boxmot.utils.clean import cleanup_mot17\nfrom typing import Optional, List, Dict, Generator, Union\n\nfrom boxmot.utils.dataloaders.MOT17 import MOT17DetEmbDataset\nfrom boxmot.postprocessing.gsi im","@@ -22,7 +22,7 @@ import copy\n import concurrent.futures\n \n from boxmot.tracker_zoo import create_tracker\n-from boxmot.utils import NUM_THREADS, ROOT, WEIGHTS, TRACKER_CONFIGS, DATASET_CONFIGS, logger as LOGGER, EXAMPLES\n+from boxmot.utils import NUM_THREADS, ROOT, WEIGHTS, TRACKER_CONFIGS, DATASET_CONFIGS, logger as LOGGER, TRACKEVAL\n from boxmot.utils.checks import RequirementsChecker\n from boxmot.utils.torch_utils import select_device\n from boxmot.utils.plots import MetricsPlotter\n@@ -48,7 +48,7 @@ checker.check_packages(('ultralytics', ))  # install\n \n \n def eval_init(args,\n-              trackeval_dest: Path = Path(""""./boxmot/engine/trackeval""""),\n+              trackeval_dest: Path = TRACKEVAL,\n               branch: str = """"master"""",\n               overwrite: bool = False\n     ) -> None:\n@@ -72,7 +72,7 @@ def eval_init(args,\n         )\n         args.benchmark = cfg[""""benchmark""""][""""name""""]\n         args.split = cfg[""""benchmark""""][""""split""""]\n-        args.source = Path(f""""./boxmot/engine/trackeval/data/{args.benchmark}/{args.split}"""")\n+        args.source = TRACKEVAL / f""""data/{args.benchmark}/{args.split}""""\n         \n \n     # 3) finally, make source an absolute Path everywhere\n@@ -239,7 +239,7 @@ def trackeval(args: argparse.Namespace, seq_paths: list, save_dir: Path, MOT_res\n     d = [seq_path.parent.name for seq_path in seq_paths]\n \n     args = [\n-        sys.executable, EXAMPLES / 'trackeval' / 'scripts' / 'run_mot_challenge.py',\n+        sys.executable, TRACKEVAL / 'scripts' / 'run_mot_challenge.py',\n         """"--GT_FOLDER"""", str(gt_folder),\n         """"--BENCHMARK"""", """""""",\n         """"--TRACKERS_FOLDER"""", args.exp_folder_path,\n",add,Fix inspector expansion error .
bbdef72c8342fa473a0e9ad1ff86a5e1f546a84e,fix trackeval download path bug in clone vs pkg missmatch (#2066),boxmot/utils/__init__.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nimport threading\nfrom pathlib import Path\n\nimport numpy as np\nimport multiprocessing as mp\n\n# global logger\nfrom loguru import logger\n\nROOT = Path(__file__).resolve().parents[2]\nDATA = ROOT / """"data""""\nTOML = ROOT / """"pyproject.toml""""\n\nBOXMOT     = ROOT / """"boxmot""""\nCONFIGS    = BOXMOT / """"configs""""\nTRACKER_CONFIGS   = CONFIGS / """"trackers""""\nDATASET_CONFIGS   = CONFIGS / """"datasets""""\n\nENGINE   = BOXMOT / """"engine""""\nEXAMPLES = ENGINE\nWEIGHTS  = ENGINE / """"weights""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of multiprocessing threads\n\ndef _is_main_process(record):\n    return mp.current_process().name == """"MainProcess""""\n\ndef configure_logging():\n    # this will remove *all* existing handlers and then add yours\n    logger.configure(handlers=[\n        {\n            """"sink"""": sys.stderr,\n            """"level"""":    """"INFO"""",\n            """"filter"""":   _is_main_process,","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport os\nimport sys\nimport threading\nfrom pathlib import Path\n\nimport numpy as np\nimport multiprocessing as mp\n\n# global logger\nfrom loguru import logger\n\nROOT = Path(__file__).resolve().parents[2]\nDATA = ROOT / """"data""""\nTOML = ROOT / """"pyproject.toml""""\n\nBOXMOT     = ROOT / """"boxmot""""\nCONFIGS    = BOXMOT / """"configs""""\nTRACKER_CONFIGS   = CONFIGS / """"trackers""""\nDATASET_CONFIGS   = CONFIGS / """"datasets""""\n\nENGINE   = BOXMOT / """"engine""""\nWEIGHTS  = ENGINE / """"weights""""\nTRACKEVAL  = ENGINE / """"trackeval""""\n\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of multiprocessing threads\n\ndef _is_main_process(record):\n    return mp.current_process().name == """"MainProcess""""\n\ndef configure_logging():\n    # this will remove *all* existing handlers and then add yours\n    logger.configure(handlers=[\n        {\n            """"sink"""": sys.stderr,\n            """"level"""":    """"INFO"""",\n            """"filter"""":  ","@@ -21,8 +21,8 @@ TRACKER_CONFIGS   = CONFIGS / """"trackers""""\n DATASET_CONFIGS   = CONFIGS / """"datasets""""\n \n ENGINE   = BOXMOT / """"engine""""\n-EXAMPLES = ENGINE\n WEIGHTS  = ENGINE / """"weights""""\n+TRACKEVAL  = ENGINE / """"trackeval""""\n \n NUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of multiprocessing threads\n \n",add,Add bluetooth include path .
bbdef72c8342fa473a0e9ad1ff86a5e1f546a84e,fix trackeval download path bug in clone vs pkg missmatch (#2066),boxmot/utils/analysis/mot_ds_kf_tuning.py,"#!/usr/bin/env python3\nimport cv2\nimport numpy as np\nimport argparse\nfrom pathlib import Path\nfrom scipy.linalg import pinv\n\nfrom boxmot.motion.kalman_filters.aabb.xywh_kf import KalmanFilterXYWH\n\n\ndef build_tracks_from_sequence(\n    seq_dir: Path,\n    use_temp_gt: bool = False,\n    min_detections: int = 5,\n):\n    """"""""""""\n    Load ground-truth from a single MOT sequence folder,\n    build 8-D state/4-D measurement tracks for each object,\n    and return (tracks, widths, heights).\n    """"""""""""\n    # load GT\n    gt_file = seq_dir / """"gt"""" / (""""gt_temp.txt"""" if use_temp_gt else """"gt.txt"""")\n    orig_gt = np.loadtxt(gt_file, delimiter=',')\n    # filter distractors\n    MOT_DISTRACTOR_IDS = []\n    mask = ~np.isin(orig_gt[:,1].astype(int), MOT_DISTRACTOR_IDS)\n    orig_gt = orig_gt[mask]\n\n    dt = 1.0\n    tracks = []\n    all_ws = []\n    all_hs = []\n\n    for obj_id in np.unique(orig_gt[:,1].astype(int)):\n        sel = orig_gt[orig_gt[:,1] == obj_id]\n        sel = se","#!/usr/bin/env python3\nimport cv2\nimport numpy as np\nimport argparse\nfrom pathlib import Path\nfrom scipy.linalg import pinv\nfrom boxmot.utils import TRACKEVAL\n\nfrom boxmot.motion.kalman_filters.aabb.xywh_kf import KalmanFilterXYWH\n\n\ndef build_tracks_from_sequence(\n    seq_dir: Path,\n    use_temp_gt: bool = False,\n    min_detections: int = 5,\n):\n    """"""""""""\n    Load ground-truth from a single MOT sequence folder,\n    build 8-D state/4-D measurement tracks for each object,\n    and return (tracks, widths, heights).\n    """"""""""""\n    # load GT\n    gt_file = seq_dir / """"gt"""" / (""""gt_temp.txt"""" if use_temp_gt else """"gt.txt"""")\n    orig_gt = np.loadtxt(gt_file, delimiter=',')\n    # filter distractors\n    MOT_DISTRACTOR_IDS = []\n    mask = ~np.isin(orig_gt[:,1].astype(int), MOT_DISTRACTOR_IDS)\n    orig_gt = orig_gt[mask]\n\n    dt = 1.0\n    tracks = []\n    all_ws = []\n    all_hs = []\n\n    for obj_id in np.unique(orig_gt[:,1].astype(int)):\n        sel = orig_gt[orig_","@@ -4,6 +4,7 @@ import numpy as np\n import argparse\n from pathlib import Path\n from scipy.linalg import pinv\n+from boxmot.utils import TRACKEVAL\n \n from boxmot.motion.kalman_filters.aabb.xywh_kf import KalmanFilterXYWH\n \n@@ -175,7 +176,7 @@ if __name__ == """"__main__"""":\n     parser.add_argument(\n         """"--train_root"""", \n         type=Path,\n-        default=Path(""""./boxmot/engine/TrackEval/data/MOT17-ablation/train""""),\n+        default=TRACKEVAL / """"data/MOT17-ablation/train"""",\n         help=""""Root folder containing all MOT17-ablation train sequences""""\n     )\n     parser.add_argument(\n",add,Fix inspector wrapping layout
bbdef72c8342fa473a0e9ad1ff86a5e1f546a84e,fix trackeval download path bug in clone vs pkg missmatch (#2066),boxmot/utils/analysis/mot_seq_bb_plot.py,"#!/usr/bin/env python3\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom pathlib import Path\nfrom typing import Union\nimport argparse\n\ndef plot_gt_boxes_with_trajectories(\n    seq_dir: Union[str, Path],\n    use_temp_gt: bool = True,\n    pad: int = 0\n):\n    """"""""""""\n    Plot all ground-truth boxes for a MOT17 sequence, coloring each object ID\n    with a unique color and drawing its trajectory through the centers of its boxes.\n\n    Args:\n        seq_dir (str | Path): Path to the sequence folder (containing img1/ and gt/).\n        use_temp_gt (bool): If True, plot gt/gt_temp.txt (filtered by FPS).\n                            Otherwise plot gt/gt.txt.\n        pad (int): Extra padding (in pixels) to add around the outermost boxes.\n    """"""""""""\n    seq_dir = Path(seq_dir)\n    # --- 1) grab image size from first frame ---\n    img_files = sorted((seq_dir / """"img1"""").glob(""""*.jpg""""))\n    if not img_files:\n        rais","#!/usr/bin/env python3\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom pathlib import Path\nfrom typing import Union\nimport argparse\nfrom boxmot.utils import TRACKEVAL\n\ndef plot_gt_boxes_with_trajectories(\n    seq_dir: Union[str, Path],\n    use_temp_gt: bool = True,\n    pad: int = 0\n):\n    """"""""""""\n    Plot all ground-truth boxes for a MOT17 sequence, coloring each object ID\n    with a unique color and drawing its trajectory through the centers of its boxes.\n\n    Args:\n        seq_dir (str | Path): Path to the sequence folder (containing img1/ and gt/).\n        use_temp_gt (bool): If True, plot gt/gt_temp.txt (filtered by FPS).\n                            Otherwise plot gt/gt.txt.\n        pad (int): Extra padding (in pixels) to add around the outermost boxes.\n    """"""""""""\n    seq_dir = Path(seq_dir)\n    # --- 1) grab image size from first frame ---\n    img_files = sorted((seq_dir / """"img1"""").glob(""""*.jpg""""))\","@@ -6,6 +6,7 @@ import matplotlib.patches as patches\n from pathlib import Path\n from typing import Union\n import argparse\n+from boxmot.utils import TRACKEVAL\n \n def plot_gt_boxes_with_trajectories(\n     seq_dir: Union[str, Path],\n@@ -160,7 +161,7 @@ if __name__ == """"__main__"""":\n     )\n     parser.add_argument(\n         """"--seq_dir"""",\n-        default=""""./boxmot/engine/TrackEval/data/MOT17-ablation/train/MOT17-09"""",\n+        default=TRACKEVAL / """"data/MOT17-ablation/train/MOT17-09"""",\n         help=""""Path to the MOT17 sequence folder (must contain img1/ and gt/ subfolders)""""\n     )\n     parser.add_argument(\n",add,Don ' t define the dependency on real_commad twice
52dc19ee230bf57594dc8a645b765e94621dca7a,Fix warnings in detectors and trackers (#2093),boxmot/engine/detectors/rfdetr.py,"# Mikel BrostrÃ¶m ð¥ RFDETR Tracking ð§¾ AGPL-3.0 license\n\nimport cv2\nimport numpy as np\nimport torch\nfrom rfdetr import RFDETRBase\nfrom rfdetr.util.coco_classes import COCO_CLASSES\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect import DetectionPredictor\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.engine.detectors.yolo_interface import YoloInterface\n\n\nclass RFDETRStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = COCO_CLASSES\n    ch = 3\n\n    def __init__(self, model, device, args):\n        self.args = args\n        LOGGER.info(""""Loading RFDETR model"""")\n        self.model = RFDETRBase(device=""""cpu"""")\n\n    @torch.no_grad()\n    def __call__(self, im, augment, visualize, embed):\n        image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)  # Convert frame BGR2RGB for RFDETR\n        with torch.no_grad():\n            detections = self.model.predict(image, threshold=self.args.c","# Mikel BrostrÃ¶m ð¥ RFDETR Tracking ð§¾ AGPL-3.0 license\n\nimport cv2\nimport numpy as np\nimport torch\nfrom rfdetr import RFDETRBase\nfrom rfdetr.util.coco_classes import COCO_CLASSES\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect import DetectionPredictor\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.engine.detectors.yolo_interface import YoloInterface\n\n\nclass RFDETRStrategy(YoloInterface):\n    pt = False\n    stride = 32\n    fp16 = False\n    triton = False\n    names = COCO_CLASSES\n    ch = 3\n\n    def __init__(self, model, device, args):\n        self.args = args\n        LOGGER.info(""""Loading RFDETR model"""")\n        self.model = RFDETRBase(device=""""cpu"""")\n\n    @torch.no_grad()\n    def __call__(self, im, augment, visualize, embed):\n        image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)  # Convert frame BGR2RGB for RFDETR\n        with torch.no_grad():\n            detections = self.model.predict(image, threshold=self.args.c","@@ -45,10 +45,9 @@ class RFDETRStrategy(YoloInterface):\n         This function saves image paths for the current batch,\n         being passed as callback on_predict_batch_start\n         """"""""""""\n-        assert (\n-            isinstance(predictor, DetectionPredictor),\n-            """"Only ultralytics predictors are supported"""",\n-        )\n+        assert isinstance(\n+            predictor, DetectionPredictor\n+        ), """"Only ultralytics predictors are supported""""\n         self.im_paths = predictor.batch[0]\n \n     def preprocess(self, im) -> torch.Tensor:\n",add,Fix Snackbar theme on sw600dp
52dc19ee230bf57594dc8a645b765e94621dca7a,Fix warnings in detectors and trackers (#2093),boxmot/engine/detectors/yolox.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport cv2\nimport fnmatch\nimport gdown\nimport numpy as np\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.engine.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    """"yolox_n.pt"""": """"https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX"""",\n    """"yolox_s.pt"""": """"https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj"""",\n    """"yolox_m.pt"""": """"https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun"""",\n    """"yolox_l.pt"""": """"https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz"""",\n    """"yolox_x.pt"""": """"https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5"""",\n    """"yolox_","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nimport cv2\nimport fnmatch\nimport gdown\nimport numpy as np\nimport torch\nfrom ultralytics.engine.results import Results\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nfrom yolox.exp import get_exp\nfrom yolox.utils import postprocess\nfrom yolox.utils.model_utils import fuse_model\n\nfrom boxmot.utils import logger as LOGGER\nfrom boxmot.engine.detectors.yolo_interface import YoloInterface\n\n# default model weigths for these model names\nYOLOX_ZOO = {\n    """"yolox_n.pt"""": """"https://drive.google.com/uc?id=1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX"""",\n    """"yolox_s.pt"""": """"https://drive.google.com/uc?id=1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj"""",\n    """"yolox_m.pt"""": """"https://drive.google.com/uc?id=11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun"""",\n    """"yolox_l.pt"""": """"https://drive.google.com/uc?id=1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz"""",\n    """"yolox_x.pt"""": """"https://drive.google.com/uc?id=1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5"""",\n    """"yolox_","@@ -186,10 +186,9 @@ class YoloXStrategy(YoloInterface):\n         This function saves image paths for the current batch,\n         being passed as callback on_predict_batch_start\n         """"""""""""\n-        assert (\n-            isinstance(predictor, DetectionPredictor),\n-            """"Only ultralytics predictors are supported"""",\n-        )\n+        assert isinstance(\n+            predictor, DetectionPredictor\n+        ), """"Only ultralytics predictors are supported""""\n         self.im_paths = predictor.batch[0]\n \n     # This preprocess differs from the current version of YOLOX preprocess, but ByteTrack uses it\n",add,Fix Snackbar theme on sw600dp
52dc19ee230bf57594dc8a645b765e94621dca7a,Fix warnings in detectors and trackers (#2093),boxmot/motion/kalman_filters/obb/xywha_kf.py,"from __future__ import absolute_import, division\n\nimport sys\nfrom collections import deque\nfrom copy import deepcopy\nfrom math import exp, log, pi\n\nimport numpy as np\nimport numpy.linalg as linalg\nfrom filterpy.common import reshape_z\nfrom filterpy.stats import logpdf\nfrom numpy import dot, eye, isscalar, zeros\n\n\ndef speed_direction_obb(bbox1, bbox2):\n    cx1, cy1 = bbox1[0], bbox1[1]\n    cx2, cy2 = bbox2[0], bbox2[1]\n    speed = np.array([cy2 - cy1, cx2 - cx1])\n    norm = np.sqrt((cy2 - cy1) ** 2 + (cx2 - cx1) ** 2) + 1e-6\n    return speed / norm\n\n\nclass KalmanBoxTrackerOBB(object):\n    """"""""""""\n    This class represents the internal state of individual tracked objects observed as oriented bbox.\n    """"""""""""\n\n    count = 0\n\n    def __init__(\n        self,\n        bbox,\n        cls,\n        det_ind,\n        delta_t=3,\n        max_obs=50,\n        Q_xy_scaling=0.01,\n        Q_a_scaling=0.01,\n    ):\n        """"""""""""\n        Initialises a tracker using ini","from __future__ import absolute_import, division\n\nimport sys\nfrom collections import deque\nfrom copy import deepcopy\nfrom math import exp, log, pi\n\nimport numpy as np\nimport numpy.linalg as linalg\nfrom filterpy.common import reshape_z\nfrom filterpy.stats import logpdf\nfrom numpy import dot, eye, isscalar, zeros\n\n\ndef speed_direction_obb(bbox1, bbox2):\n    cx1, cy1 = bbox1[0], bbox1[1]\n    cx2, cy2 = bbox2[0], bbox2[1]\n    speed = np.array([cy2 - cy1, cx2 - cx1])\n    norm = np.sqrt((cy2 - cy1) ** 2 + (cx2 - cx1) ** 2) + 1e-6\n    return speed / norm\n\n\nclass KalmanBoxTrackerOBB(object):\n    """"""""""""\n    This class represents the internal state of individual tracked objects observed as oriented bbox.\n    """"""""""""\n\n    count = 0\n\n    def __init__(\n        self,\n        bbox,\n        cls,\n        det_ind,\n        delta_t=3,\n        max_obs=50,\n        Q_xy_scaling=0.01,\n        Q_a_scaling=0.01,\n    ):\n        """"""""""""\n        Initialises a tracker using ini","@@ -120,9 +120,7 @@ class KalmanBoxTrackerOBB(object):\n                         break\n                 if previous_box is None:\n                     previous_box = self.last_observation\n-                """"""""""""\n-                  Estimate the track speed direction with observations \Delta t steps away\n-                """"""""""""\n+                # Estimate the track speed direction with observations Ît steps away\n                 self.velocity = speed_direction_obb(previous_box, bbox)\n \n             """"""""""""\n",add,Add note about data volume to enable_metrics_collection
52dc19ee230bf57594dc8a645b765e94621dca7a,Fix warnings in detectors and trackers (#2093),boxmot/trackers/deepocsort/deepocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.aabb.xysr_kf import KalmanFilterXYSR\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.ops import xyxy2xysr\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\n      [x1,y1,x2,y2] where x1,y1 is the top ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\nfrom collections import deque\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\n\nfrom boxmot.appearance.reid.auto_backend import ReidAutoBackend\nfrom boxmot.motion.cmc import get_cmc_method\nfrom boxmot.motion.kalman_filters.aabb.xysr_kf import KalmanFilterXYSR\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.ops import xyxy2xysr\n\n\ndef k_previous_obs(observations, cur_age, k):\n    if len(observations) == 0:\n        return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\n      [x1,y1,x2,y2] where x1,y1 is the top ","@@ -160,9 +160,7 @@ class KalmanBoxTracker:\n                         break\n                 if previous_box is None:\n                     previous_box = self.last_observation\n-                """"""""""""\n-                  Estimate the track speed direction with observations \Delta t steps away\n-                """"""""""""\n+                # Estimate the track speed direction with observations Ît steps away\n                 self.velocity = speed_direction(previous_box, bbox)\n             """"""""""""\n               Insert new observations. This is a ugly way to maintain both self.observations\n",add,Add note about data volume to enable_metrics_collection
52dc19ee230bf57594dc8a645b765e94621dca7a,Fix warnings in detectors and trackers (#2093),boxmot/trackers/ocsort/ocsort.py,"# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nThis script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.aabb.xysr_kf import KalmanFilterXYSR\nfrom boxmot.motion.kalman_filters.obb.xywha_kf import KalmanBoxTrackerOBB\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.ops import xyxy2xysr\n\n\ndef k_previous_obs(observations, cur_age, k, is_obb=False):\n    if len(observations) == 0:\n        if is_obb:\n            return [-1, -1, -1, -1, -1, -1]\n        else:\n            return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Takes a ","# Mikel BrostrÃ¶m ð¥ Yolo Tracking ð§¾ AGPL-3.0 license\n\n""""""""""""\nThis script is adopted from the SORT script by Alex Bewley alex@bewley.ai\n""""""""""""\nfrom collections import deque\n\nimport numpy as np\n\nfrom boxmot.motion.kalman_filters.aabb.xysr_kf import KalmanFilterXYSR\nfrom boxmot.motion.kalman_filters.obb.xywha_kf import KalmanBoxTrackerOBB\nfrom boxmot.trackers.basetracker import BaseTracker\nfrom boxmot.utils.association import associate, linear_assignment\nfrom boxmot.utils.ops import xyxy2xysr\n\n\ndef k_previous_obs(observations, cur_age, k, is_obb=False):\n    if len(observations) == 0:\n        if is_obb:\n            return [-1, -1, -1, -1, -1, -1]\n        else:\n            return [-1, -1, -1, -1, -1]\n    for i in range(k):\n        dt = k - i\n        if cur_age - dt in observations:\n            return observations[cur_age - dt]\n    max_age = max(observations.keys())\n    return observations[max_age]\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""""""""\n    Takes a ","@@ -150,9 +150,7 @@ class KalmanBoxTracker(object):\n                         break\n                 if previous_box is None:\n                     previous_box = self.last_observation\n-                """"""""""""\n-                  Estimate the track speed direction with observations \Delta t steps away\n-                """"""""""""\n+                # Estimate the track speed direction with observations Ît steps away\n                 self.velocity = speed_direction(previous_box, bbox)\n \n             """"""""""""\n",add,Add note about data volume to enable_metrics_collection
